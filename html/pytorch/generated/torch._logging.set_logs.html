<h1 id="torch-logging-set-logs">torch._logging.set_logs</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch._logging.set_logs">
<code>torch._logging.set_logs(*, all=None, dynamo=None, aot=None, dynamic=None, inductor=None, distributed=None, onnx=None, bytecode=False, aot_graphs=False, aot_joint_graph=False, ddp_graphs=False, graph=False, graph_code=False, graph_breaks=False, graph_sizes=False, guards=False, recompiles=False, trace_source=False, trace_call=False, output_code=False, schedule=False, perf_hints=False, onnx_diagnostics=False, modules=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/_logging/_internal.html#set_logs"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sets the log level for individual components and toggles individual log artifact types.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This feature is a prototype and may have compatibility breaking changes in the future.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The <code>TORCH_LOGS</code> environment variable has complete precedence over this function, so if it was set, this function does nothing.</p> </div> <p>A component is a set of related features in PyTorch. All of the log messages emitted from a given component have their own log levels. If the log level of a particular message has priority greater than or equal to its component’s log level setting, it is emitted. Otherwise, it is supressed. This allows you to, for instance, silence large groups of log messages that are not relevant to you and increase verbosity of logs for components that are relevant. The expected log level values, ordered from highest to lowest priority, are:</p>  <ul class="simple"> <li><code>logging.CRITICAL</code></li> <li><code>logging.ERROR</code></li> <li><code>logging.WARNING</code></li> <li><code>logging.INFO</code></li> <li><code>logging.DEBUG</code></li> <li><code>logging.NOTSET</code></li> </ul>  <p>See documentation for the Python <code>logging</code> module for more information on log levels: <a class="reference external" href="https://docs.python.org/3/library/logging.html#logging-levels">https://docs.python.org/3/library/logging.html#logging-levels</a></p> <p>An artifact is a particular type of log message. Each artifact is assigned to a parent component. A component can emit many different kinds of artifacts. In general, an artifact is emitted if either its corresponding setting in the argument list below is turned on or if its parent component is set to a log level less than or equal to the log level of the artifact.</p> <dl class="field-list simple"> <dt class="field-odd">Keyword Arguments</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>all</strong> (<code>Optional[int]</code>) – The default log level for all components. Default: <code>logging.WARN</code>
</li> <li>
<strong>dynamo</strong> (<code>Optional[int]</code>) – The log level for the TorchDynamo component. Default: <code>logging.WARN</code>
</li> <li>
<strong>aot</strong> (<code>Optional[int]</code>) – The log level for the AOTAutograd component. Default: <code>logging.WARN</code>
</li> <li>
<strong>inductor</strong> (<code>Optional[int]</code>) – The log level for the TorchInductor component. Default: <code>logging.WARN</code>
</li> <li>
<strong>dynamic</strong> (<code>Optional[int]</code>) – The log level for dynamic shapes. Default: <code>logging.WARN</code>
</li> <li>
<strong>distributed</strong> (<code>Optional[int]</code>) – Whether to log communication operations and other debug info from pytorch distributed components. Default: <code>logging.WARN</code>
</li> <li>
<strong>onnx</strong> (<code>Optional[int]</code>) – The log level for the ONNX exporter component. Default: <code>logging.WARN</code>
</li> <li>
<strong>bytecode</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>) – Whether to emit the original and generated bytecode from TorchDynamo. Default: <code>False</code>
</li> <li>
<strong>aot_graphs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>) – Whether to emit the graphs generated by AOTAutograd. Default: <code>False</code>
</li> <li>
<strong>aot_joint_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>) – Whether to emit the joint forward-backward graph generated by AOTAutograd. Default: <code>False</code>
</li> <li>
<strong>ddp_graphs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>) – Whether to emit graphs generated by DDPOptimizer. Default: <code>False</code>
</li> <li>
<strong>graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>) – Whether to emit the graph captured by TorchDynamo in tabular format. Default: <code>False</code>
</li> <li>
<strong>graph_code</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>) – Whether to emit the python source of the graph captured by TorchDynamo. Default: <code>False</code>
</li> <li>
<strong>graph_breaks</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>) – Whether to emit the graph breaks encountered by TorchDynamo. Default: <code>False</code>
</li> <li>
<strong>graph_sizes</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>) – Whether to emit tensor sizes of the graph captured by TorchDynamo. Default: <code>False</code>
</li> <li>
<strong>guards</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>) – Whether to emit the guards generated by TorchDynamo for each compiled function. Default: <code>False</code>
</li> <li>
<strong>recompiles</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>) – Whether to emit a guard failure reason and message every time TorchDynamo recompiles a function. Default: <code>False</code>
</li> <li>
<strong>trace_source</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>) – Whether to emit when TorchDynamo begins tracing a new line. Default: <code>False</code>
</li> <li>
<strong>trace_call</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>) – Whether to emit detailed line location when TorchDynamo creates an FX node corresponding to function call. Python 3.11+ only. Default: <code>False</code>
</li> <li>
<strong>output_code</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>) – Whether to emit the TorchInductor output code. Default: <code>False</code>
</li> <li>
<strong>schedule</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>) – Whether to emit the TorchInductor schedule. Default: <code>False</code>
</li> <li>
<strong>perf_hints</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>) – Whether to emit the TorchInductor perf hints. Default: <code>False</code>
</li> <li>
<strong>onnx_diagnostics</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>) – Whether to emit the ONNX exporter diagnostics in logging. Default: <code>False</code>
</li> <li>
<strong>modules</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">dict</a>) – This argument provides an alternate way to specify the above log component and artifact settings, in the format of a keyword args dictionary given as a single argument. There are two cases where this is useful (1) if a new log component or artifact has been registered but a keyword argument for it has not been added to this function and (2) if the log level for an unregistered module needs to be set. This can be done by providing the fully-qualified module name as the key, with the log level as the value. Default: <code>None</code>
</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; import logging

# The following changes the "dynamo" component to emit DEBUG-level
# logs, and to emit "graph_code" artifacts.

&gt;&gt;&gt; torch._logging.set_logs(dynamo=logging.DEBUG, graph_code=True)

# The following enables the logs for a different module

&gt;&gt;&gt; torch._logging.set_logs(modules={"unregistered.module.name": logging.DEBUG})
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch._logging.set_logs.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch._logging.set_logs.html</a>
  </p>
</div>
