<h1 id="torch-fake-quantize-per-tensor-affine">torch.fake_quantize_per_tensor_affine</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.fake_quantize_per_tensor_affine">
<code>torch.fake_quantize_per_tensor_affine(input, scale, zero_point, quant_min, quant_max) → Tensor</code> </dt> <dd>
<p>Returns a new tensor with the data in <code>input</code> fake quantized using <code>scale</code>, <code>zero_point</code>, <code>quant_min</code> and <code>quant_max</code>.</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>output</mtext><mo>=</mo><mo stretchy="false">(</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mtext>quant_max</mtext><mo separator="true">,</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mtext>quant_min</mtext><mo separator="true">,</mo><mtext>std::nearby_int</mtext><mo stretchy="false">(</mo><mtext>input</mtext><mi mathvariant="normal">/</mi><mtext>scale</mtext><mo stretchy="false">)</mo><mo>+</mo><mtext>zero_point</mtext><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>−</mo><mtext>zero_point</mtext><mo stretchy="false">)</mo><mo>×</mo><mtext>scale</mtext></mrow><annotation encoding="application/x-tex">\text{output} = ( min( \text{quant\_max}, max( \text{quant\_min}, \text{std::nearby\_int}(\text{input} / \text{scale}) + \text{zero\_point} ) ) - \text{zero\_point} ) \times \text{scale} </annotation></semantics></math></span></span></span>
</div>
<dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input value(s), <code>torch.float32</code> tensor</li> <li>
<strong>scale</strong> (double scalar or <code>float32</code> Tensor) – quantization scale</li> <li>
<strong>zero_point</strong> (int64 scalar or <code>int32</code> Tensor) – quantization zero_point</li> <li>
<strong>quant_min</strong> (<em>int64</em>) – lower bound of the quantized domain</li> <li>
<strong>quant_max</strong> (<em>int64</em>) – upper bound of the quantized domain</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>A newly fake_quantized <code>torch.float32</code> tensor</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; x = torch.randn(4)
&gt;&gt;&gt; x
tensor([ 0.0552,  0.9730,  0.3973, -1.0780])
&gt;&gt;&gt; torch.fake_quantize_per_tensor_affine(x, 0.1, 0, 0, 255)
tensor([0.1000, 1.0000, 0.4000, 0.0000])
&gt;&gt;&gt; torch.fake_quantize_per_tensor_affine(x, torch.tensor(0.1), torch.tensor(0), 0, 255)
tensor([0.1000, 1.0000, 0.4000, 0.0000])
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.fake_quantize_per_tensor_affine.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.fake_quantize_per_tensor_affine.html</a>
  </p>
</div>
