<h1 id="torch-nn-modules-module-register-module-full-backward-hook">torch.nn.modules.module.register_module_full_backward_hook</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.modules.module.register_module_full_backward_hook">
<code>torch.nn.modules.module.register_module_full_backward_hook(hook)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/modules/module.html#register_module_full_backward_hook"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Registers a backward hook common to all the modules.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This adds global state to the <code>nn.module</code> module and it is only intended for debugging/profiling purposes.</p> </div> <p>The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature:</p> <pre data-language="python">hook(module, grad_input, grad_output) -&gt; Tensor or None
</pre> <p>The <code>grad_input</code> and <code>grad_output</code> are tuples. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments will not appear in the hook. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p> <p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Moduleâ€™s forward function.</p> <p>Global hooks are called before hooks registered with <code>register_backward_hook</code></p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><code>torch.utils.hooks.RemovableHandle</code></p> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.modules.module.register_module_full_backward_hook.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.modules.module.register_module_full_backward_hook.html</a>
  </p>
</div>
