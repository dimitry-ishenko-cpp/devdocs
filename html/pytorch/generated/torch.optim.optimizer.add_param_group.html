<h1 id="torch-optim-optimizer-add-param-group">torch.optim.Optimizer.add_param_group</h1> <dl class="py method"> <dt class="sig sig-object py" id="torch.optim.Optimizer.add_param_group">
<code>Optimizer.add_param_group(param_group)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/optim/optimizer.html#Optimizer.add_param_group"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Add a param group to the <a class="reference internal" href="../optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><code>Optimizer</code></a> s <code>param_groups</code>.</p> <p>This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the <a class="reference internal" href="../optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><code>Optimizer</code></a> as training progresses.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>param_group</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">dict</a>) â€“ Specifies what Tensors should be optimized along with group specific optimization options.</p> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.optim.Optimizer.add_param_group.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.optim.Optimizer.add_param_group.html</a>
  </p>
</div>
