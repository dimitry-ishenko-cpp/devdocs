<h1 id="randomstructured">RandomStructured</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.nn.utils.prune.RandomStructured">
<code>class torch.nn.utils.prune.RandomStructured(amount, dim=-1)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/utils/prune.html#RandomStructured"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Prune entire (currently unpruned) channels in a tensor at random.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>amount</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – quantity of parameters to prune. If <code>float</code>, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If <code>int</code>, it represents the absolute number of parameters to prune.</li> <li>
<strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em>) – index of the dim along which we define channels to prune. Default: -1.</li> </ul> </dd> </dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.nn.utils.prune.RandomStructured.apply">
<code>classmethod apply(module, name, amount, dim=-1)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/utils/prune.html#RandomStructured.apply"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>module</strong> (<a class="reference internal" href="torch.nn.module.html#torch.nn.Module" title="torch.nn.Module">nn.Module</a>) – module containing the tensor to prune</li> <li>
<strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – parameter name within <code>module</code> on which pruning will act.</li> <li>
<strong>amount</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – quantity of parameters to prune. If <code>float</code>, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If <code>int</code>, it represents the absolute number of parameters to prune.</li> <li>
<strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em>) – index of the dim along which we define channels to prune. Default: -1.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.nn.utils.prune.RandomStructured.apply_mask">
<code>apply_mask(module)</code> </dt> <dd>
<p>Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>module</strong> (<a class="reference internal" href="torch.nn.module.html#torch.nn.Module" title="torch.nn.Module">nn.Module</a>) – module containing the tensor to prune</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>pruned version of the input tensor</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p>pruned_tensor (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>)</p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.nn.utils.prune.RandomStructured.compute_mask">
<code>compute_mask(t, default_mask)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/utils/prune.html#RandomStructured.compute_mask"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Computes and returns a mask for the input tensor <code>t</code>. Starting from a base <code>default_mask</code> (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the <code>default_mask</code> by randomly zeroing out channels along the specified dim of the tensor.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>t</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>) – tensor representing the parameter to prune</li> <li>
<strong>default_mask</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>) – Base mask from previous pruning iterations, that need to be respected after the new mask is applied. Same dims as <code>t</code>.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>mask to apply to <code>t</code>, of same dims as <code>t</code></p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p>mask (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>)</p> </dd> <dt class="field-even">Raises</dt> <dd class="field-even">
<p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#IndexError" title="(in Python v3.12)"><strong>IndexError</strong></a> – if <code>self.dim &gt;= len(t.shape)</code></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.nn.utils.prune.RandomStructured.prune">
<code>prune(t, default_mask=None, importance_scores=None)</code> </dt> <dd>
<p>Computes and returns a pruned version of input tensor <code>t</code> according to the pruning rule specified in <a class="reference internal" href="#torch.nn.utils.prune.RandomStructured.compute_mask" title="torch.nn.utils.prune.RandomStructured.compute_mask"><code>compute_mask()</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>t</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>) – tensor to prune (of same dimensions as <code>default_mask</code>).</li> <li>
<strong>importance_scores</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>) – tensor of importance scores (of same shape as <code>t</code>) used to compute mask for pruning <code>t</code>. The values in this tensor indicate the importance of the corresponding elements in the <code>t</code> that is being pruned. If unspecified or None, the tensor <code>t</code> will be used in its place.</li> <li>
<strong>default_mask</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a><em>, </em><em>optional</em>) – mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>pruned version of tensor <code>t</code>.</p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.nn.utils.prune.RandomStructured.remove">
<code>remove(module)</code> </dt> <dd>
<p>Removes the pruning reparameterization from a module. The pruned parameter named <code>name</code> remains permanently pruned, and the parameter named <code>name+'_orig'</code> is removed from the parameter list. Similarly, the buffer named <code>name+'_mask'</code> is removed from the buffers.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Pruning itself is NOT undone or reversed!</p> </div> </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.utils.prune.RandomStructured.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.utils.prune.RandomStructured.html</a>
  </p>
</div>
