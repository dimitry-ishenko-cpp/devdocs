<h1 id="dual-level">dual_level</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.autograd.forward_ad.dual_level">
<code>class torch.autograd.forward_ad.dual_level</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/autograd/forward_ad.html#dual_level"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Context-manager that enables forward AD. All forward AD computation must be performed in a <code>dual_level</code> context.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The <code>dual_level</code> context appropriately enters and exit the dual level to controls the current forward AD level, which is used by default by the other functions in this API.</p> <p>We currently donâ€™t plan to support nested <code>dual_level</code> contexts, however, so only a single forward AD level is supported. To compute higher-order forward grads, one can use <a class="reference internal" href="torch.func.jvp.html#torch.func.jvp" title="torch.func.jvp"><code>torch.func.jvp()</code></a>.</p> </div> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; x = torch.tensor([1])
&gt;&gt;&gt; x_t = torch.tensor([1])
&gt;&gt;&gt; with dual_level():
...     inp = make_dual(x, x_t)
...     # Do computations with inp
...     out = your_fn(inp)
...     _, grad = unpack_dual(out)
&gt;&gt;&gt; grad is None
False
&gt;&gt;&gt; # After exiting the level, the grad is deleted
&gt;&gt;&gt; _, grad_after = unpack_dual(out)
&gt;&gt;&gt; grad is None
True
</pre> <p>Please see the <a class="reference external" href="https://pytorch.org/tutorials/intermediate/forward_ad_usage.html">forward-mode AD tutorial</a> for detailed steps on how to use this API.</p> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.autograd.forward_ad.dual_level.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.autograd.forward_ad.dual_level.html</a>
  </p>
</div>
