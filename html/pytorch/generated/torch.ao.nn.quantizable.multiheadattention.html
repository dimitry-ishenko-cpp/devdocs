<h1 id="multiheadattention">MultiheadAttention</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.ao.nn.quantizable.MultiheadAttention">
<code>class torch.ao.nn.quantizable.MultiheadAttention(embed_dim, num_heads, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None, batch_first=False, device=None, dtype=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/nn/quantizable/modules/activation.html#MultiheadAttention"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.nn.quantizable.MultiheadAttention.dequantize">
<code>dequantize()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/nn/quantizable/modules/activation.html#MultiheadAttention.dequantize"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Utility to convert the quantized MHA back to float.</p> <p>The motivation for this is that it is not trivial to conver the weights from the format that is used in the quantized version back to the float.</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.nn.quantizable.MultiheadAttention.forward">
<code>forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None, average_attn_weights=True, is_causal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/nn/quantizable/modules/activation.html#MultiheadAttention.forward"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<dl class="simple"> <dt>Note::</dt>
<dd>
<p>Please, refer to <a class="reference internal" href="torch.nn.multiheadattention.html#torch.nn.MultiheadAttention.forward" title="torch.nn.MultiheadAttention.forward"><code>forward()</code></a> for more information</p> </dd> </dl> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>query</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – map a query and a set of key-value pairs to an output. See “Attention Is All You Need” for more details.</li> <li>
<strong>key</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – map a query and a set of key-value pairs to an output. See “Attention Is All You Need” for more details.</li> <li>
<strong>value</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – map a query and a set of key-value pairs to an output. See “Attention Is All You Need” for more details.</li> <li>
<strong>key_padding_mask</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a><em>[</em><a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em>) – if provided, specified padding elements in the key will be ignored by the attention. When given a binary mask and a value is True, the corresponding value on the attention layer will be ignored.</li> <li>
<strong>need_weights</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – output attn_output_weights.</li> <li>
<strong>attn_mask</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a><em>[</em><a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em>) – 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all the batches while a 3D mask allows to specify a different mask for the entries of each batch.</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.12)">Tuple</a>[<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a>[<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>]]</p> </dd> </dl> <dl class="simple"> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Inputs:</li> <li>query: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>L</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(L, N, E)</annotation></semantics></math></span></span></span> where L is the target sequence length, N is the batch size, E is the embedding dimension. <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>L</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, L, E)</annotation></semantics></math></span></span></span> if <code>batch_first</code> is <code>True</code>.</li> <li>key: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>S</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(S, N, E)</annotation></semantics></math></span></span></span>, where S is the source sequence length, N is the batch size, E is the embedding dimension. <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>S</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, S, E)</annotation></semantics></math></span></span></span> if <code>batch_first</code> is <code>True</code>.</li> <li>value: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>S</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(S, N, E)</annotation></semantics></math></span></span></span> where S is the source sequence length, N is the batch size, E is the embedding dimension. <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>S</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, S, E)</annotation></semantics></math></span></span></span> if <code>batch_first</code> is <code>True</code>.</li> <li>key_padding_mask: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, S)</annotation></semantics></math></span></span></span> where N is the batch size, S is the source sequence length. If a BoolTensor is provided, the positions with the value of <code>True</code> will be ignored while the position with the value of <code>False</code> will be unchanged.</li> <li>attn_mask: 2D mask <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>L</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(L, S)</annotation></semantics></math></span></span></span> where L is the target sequence length, S is the source sequence length. 3D mask <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo>∗</mo><mi>n</mi><mi>u</mi><msub><mi>m</mi><mi>h</mi></msub><mi>e</mi><mi>a</mi><mi>d</mi><mi>s</mi><mo separator="true">,</mo><mi>L</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N*num_heads, L, S)</annotation></semantics></math></span></span></span> where N is the batch size, L is the target sequence length, S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked positions. If a BoolTensor is provided, positions with <code>True</code> is not allowed to attend while <code>False</code> values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.</li> <li>is_causal: If specified, applies a causal mask as attention mask. Mutually exclusive with providing attn_mask. Default: <code>False</code>.</li> <li>average_attn_weights: If true, indicates that the returned <code>attn_weights</code> should be averaged across heads. Otherwise, <code>attn_weights</code> are provided separately per head. Note that this flag only has an effect when <code>need_weights=True.</code>. Default: True (i.e. average weights across heads)</li> <li>Outputs:</li> <li>attn_output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>L</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(L, N, E)</annotation></semantics></math></span></span></span> where L is the target sequence length, N is the batch size, E is the embedding dimension. <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>L</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, L, E)</annotation></semantics></math></span></span></span> if <code>batch_first</code> is <code>True</code>.</li> <li>attn_output_weights: If <code>average_attn_weights=True</code>, returns attention weights averaged across heads of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>L</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, L, S)</annotation></semantics></math></span></span></span>, where N is the batch size, L is the target sequence length, S is the source sequence length. If <code>average_attn_weights=False</code>, returns attention weights per head of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>n</mi><mi>u</mi><msub><mi>m</mi><mi>h</mi></msub><mi>e</mi><mi>a</mi><mi>d</mi><mi>s</mi><mo separator="true">,</mo><mi>L</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, num_heads, L, S)</annotation></semantics></math></span></span></span>.</li> </ul> </dd> </dl> </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.ao.nn.quantizable.MultiheadAttention.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.ao.nn.quantizable.MultiheadAttention.html</a>
  </p>
</div>
