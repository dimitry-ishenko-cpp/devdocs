<h1 id="torch-asarray">torch.asarray</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.asarray">
<code>torch.asarray(obj, *, dtype=None, device=None, copy=None, requires_grad=False) → Tensor</code> </dt> <dd>
<p>Converts <code>obj</code> to a tensor.</p> <p><code>obj</code> can be one of:</p> <ol class="arabic simple"> <li>a tensor</li> <li>a NumPy array or a NumPy scalar</li> <li>a DLPack capsule</li> <li>an object that implements Python’s buffer protocol</li> <li>a scalar</li> <li>a sequence of scalars</li> </ol> <p>When <code>obj</code> is a tensor, NumPy array, or DLPack capsule the returned tensor will, by default, not require a gradient, have the same datatype as <code>obj</code>, be on the same device, and share memory with it. These properties can be controlled with the <a class="reference internal" href="../tensor_attributes.html#torch.dtype" title="torch.dtype"><code>dtype</code></a>, <a class="reference internal" href="../tensor_attributes.html#torch.device" title="torch.device"><code>device</code></a>, <code>copy</code>, and <code>requires_grad</code> keyword arguments. If the returned tensor is of a different datatype, on a different device, or a copy is requested then it will not share its memory with <code>obj</code>. If <code>requires_grad</code> is <code>True</code> then the returned tensor will require a gradient, and if <code>obj</code> is also a tensor with an autograd history then the returned tensor will have the same history.</p> <p>When <code>obj</code> is not a tensor, NumPy array, or DLPack capsule but implements Python’s buffer protocol then the buffer is interpreted as an array of bytes grouped according to the size of the datatype passed to the <a class="reference internal" href="../tensor_attributes.html#torch.dtype" title="torch.dtype"><code>dtype</code></a> keyword argument. (If no datatype is passed then the default floating point datatype is used, instead.) The returned tensor will have the specified datatype (or default floating point datatype if none is specified) and, by default, be on the CPU device and share memory with the buffer.</p> <p>When <code>obj</code> is a NumPy scalar, the returned tensor will be a 0-dimensional tensor on the CPU and that doesn’t share its memory (i.e. <code>copy=True</code>). By default datatype will be the PyTorch datatype corresponding to the NumPy’s scalar’s datatype.</p> <p>When <code>obj</code> is none of the above but a scalar, or a sequence of scalars then the returned tensor will, by default, infer its datatype from the scalar values, be on the current default device, and not share its memory.</p> <div class="admonition seealso"> <p class="admonition-title">See also</p> <p><a class="reference internal" href="torch.tensor.html#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> creates a tensor that always copies the data from the input object. <a class="reference internal" href="torch.from_numpy.html#torch.from_numpy" title="torch.from_numpy"><code>torch.from_numpy()</code></a> creates a tensor that always shares memory from NumPy arrays. <a class="reference internal" href="torch.frombuffer.html#torch.frombuffer" title="torch.frombuffer"><code>torch.frombuffer()</code></a> creates a tensor that always shares memory from objects that implement the buffer protocol. <a class="reference internal" href="torch.from_dlpack.html#torch.from_dlpack" title="torch.from_dlpack"><code>torch.from_dlpack()</code></a> creates a tensor that always shares memory from DLPack capsules.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>obj</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.12)">object</a>) – a tensor, NumPy array, DLPack Capsule, object that implements Python’s buffer protocol, scalar, or sequence of scalars.</p> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<ul class="simple"> <li>
<strong>dtype</strong> (<a class="reference internal" href="../tensor_attributes.html#torch.dtype" title="torch.dtype"><code>torch.dtype</code></a>, optional) – the datatype of the returned tensor. Default: <code>None</code>, which causes the datatype of the returned tensor to be inferred from <code>obj</code>.</li> <li>
<strong>copy</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – controls whether the returned tensor shares memory with <code>obj</code>. Default: <code>None</code>, which causes the returned tensor to share memory with <code>obj</code> whenever possible. If <code>True</code> then the returned tensor does not share its memory. If <code>False</code> then the returned tensor shares its memory with <code>obj</code> and an error is thrown if it cannot.</li> <li>
<strong>device</strong> (<a class="reference internal" href="../tensor_attributes.html#torch.device" title="torch.device"><code>torch.device</code></a>, optional) – the device of the returned tensor. Default: <code>None</code>, which causes the device of <code>obj</code> to be used. Or, if <code>obj</code> is a Python sequence, the current default device will be used.</li> <li>
<strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – whether the returned tensor requires grad. Default: <code>False</code>, which causes the returned tensor not to require a gradient. If <code>True</code>, then the returned tensor will require a gradient, and if <code>obj</code> is also a tensor with an autograd history then the returned tensor will have the same history.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; a = torch.tensor([1, 2, 3])
&gt;&gt;&gt; # Shares memory with tensor 'a'
&gt;&gt;&gt; b = torch.asarray(a)
&gt;&gt;&gt; a.data_ptr() == b.data_ptr()
True
&gt;&gt;&gt; # Forces memory copy
&gt;&gt;&gt; c = torch.asarray(a, copy=True)
&gt;&gt;&gt; a.data_ptr() == c.data_ptr()
False

&gt;&gt;&gt; a = torch.tensor([1., 2., 3.], requires_grad=True)
&gt;&gt;&gt; b = a + 2
&gt;&gt;&gt; b
tensor([3., 4., 5.], grad_fn=&lt;AddBackward0&gt;)
&gt;&gt;&gt; # Shares memory with tensor 'b', with no grad
&gt;&gt;&gt; c = torch.asarray(b)
&gt;&gt;&gt; c
tensor([3., 4., 5.])
&gt;&gt;&gt; # Shares memory with tensor 'b', retaining autograd history
&gt;&gt;&gt; d = torch.asarray(b, requires_grad=True)
&gt;&gt;&gt; d
tensor([3., 4., 5.], grad_fn=&lt;AddBackward0&gt;)

&gt;&gt;&gt; array = numpy.array([1, 2, 3])
&gt;&gt;&gt; # Shares memory with array 'array'
&gt;&gt;&gt; t1 = torch.asarray(array)
&gt;&gt;&gt; array.__array_interface__['data'][0] == t1.data_ptr()
True
&gt;&gt;&gt; # Copies memory due to dtype mismatch
&gt;&gt;&gt; t2 = torch.asarray(array, dtype=torch.float32)
&gt;&gt;&gt; array.__array_interface__['data'][0] == t2.data_ptr()
False

&gt;&gt;&gt; scalar = numpy.float64(0.5)
&gt;&gt;&gt; torch.asarray(scalar)
tensor(0.5000, dtype=torch.float64)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.asarray.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.asarray.html</a>
  </p>
</div>
