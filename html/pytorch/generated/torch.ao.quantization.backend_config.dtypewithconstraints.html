<h1 id="dtypewithconstraints">DTypeWithConstraints</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.ao.quantization.backend_config.DTypeWithConstraints">
<code>class torch.ao.quantization.backend_config.DTypeWithConstraints(dtype=None, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/backend_config/backend_config.html#DTypeWithConstraints"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Config for specifying additional constraints for a given dtype, such as quantization value ranges, scale value ranges, and fixed quantization params, to be used in <a class="reference internal" href="torch.ao.quantization.backend_config.dtypeconfig.html#torch.ao.quantization.backend_config.DTypeConfig" title="torch.ao.quantization.backend_config.DTypeConfig"><code>DTypeConfig</code></a>.</p> <p>The constraints currently supported are:</p> <ul class="simple"> <li>
<code>quant_min_lower_bound</code> and <code>quant_max_upper_bound</code>: Lower and upper bounds for the minimum and maximum quantized values respectively. If the QConfig’s <code>quant_min</code> and <code>quant_max</code> fall outside this range, then the QConfig will be ignored.</li> <li>
<code>scale_min_lower_bound</code> and <code>scale_max_upper_bound</code>: Lower and upper bounds for the minimum and maximum scale values respectively. If the QConfig’s minimum scale value (currently exposed as <code>eps</code>) falls below the lower bound, then the QConfig will be ignored. Note that the upper bound is currently not enforced.</li> <li>
<code>scale_exact_match</code> and <code>zero_point_exact_match</code>: Exact match requirements for scale and zero point, to be used for operators with fixed quantization parameters such as sigmoid and tanh. If the observer specified in the QConfig is neither <code>FixedQParamsObserver</code> nor <code>FixedQParamsFakeQuantize</code>, or if the quantization parameters don’t match, then the QConfig will be ignored.</li> </ul>  </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.ao.quantization.backend_config.DTypeWithConstraints.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.ao.quantization.backend_config.DTypeWithConstraints.html</a>
  </p>
</div>
