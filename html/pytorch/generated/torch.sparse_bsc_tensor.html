<h1 id="torch-sparse-bsc-tensor">torch.sparse_bsc_tensor</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.sparse_bsc_tensor">
<code>torch.sparse_bsc_tensor(ccol_indices, row_indices, values, size=None, *, dtype=None, device=None, requires_grad=False, check_invariants=None) → Tensor</code> </dt> <dd>
<p>Constructs a <a class="reference internal" href="../sparse.html#sparse-bsc-docs"><span class="std std-ref">sparse tensor in BSC (Block Compressed Sparse Column))</span></a> with specified 2-dimensional blocks at the given <code>ccol_indices</code> and <code>row_indices</code>. Sparse matrix multiplication operations in BSC format are typically faster than that for sparse tensors in COO format. Make you have a look at <a class="reference internal" href="../sparse.html#sparse-bsc-docs"><span class="std std-ref">the note on the data type of the indices</span></a>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If the <code>device</code> argument is not specified the device of the given <code>values</code> and indices tensor(s) must match. If, however, the argument is specified the input Tensors will be converted to the given device and in turn determine the device of the constructed sparse tensor.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>ccol_indices</strong> (<em>array_like</em>) – (B+1)-dimensional array of size <code>(*batchsize, ncolblocks + 1)</code>. The last element of each batch is the number of non-zeros. This tensor encodes the index in values and row_indices depending on where the given column starts. Each successive number in the tensor subtracted by the number before it denotes the number of elements in a given column.</li> <li>
<strong>row_indices</strong> (<em>array_like</em>) – Row block co-ordinates of each block in values. (B+1)-dimensional tensor with the same length as values.</li> <li>
<strong>values</strong> (<em>array_list</em>) – Initial blocks for the tensor. Can be a list, tuple, NumPy <code>ndarray</code>, and other types that represents a (1 + 2 + K)-dimensional tensor where <code>K</code> is the number of dense dimensions.</li> <li>
<strong>size</strong> (list, tuple, <code>torch.Size</code>, optional) – Size of the sparse tensor: <code>(*batchsize, nrows * blocksize[0], ncols *
blocksize[1], *densesize)</code> If not provided, the size will be inferred as the minimum size big enough to hold all non-zero blocks.</li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<ul class="simple"> <li>
<strong>dtype</strong> (<a class="reference internal" href="../tensor_attributes.html#torch.dtype" title="torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if None, infers data type from <code>values</code>.</li> <li>
<strong>device</strong> (<a class="reference internal" href="../tensor_attributes.html#torch.device" title="torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see <a class="reference internal" href="torch.set_default_tensor_type.html#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <a class="reference internal" href="../tensor_attributes.html#torch.device" title="torch.device"><code>device</code></a> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</li> <li>
<strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</li> <li>
<strong>check_invariants</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If sparse tensor invariants are checked. Default: as returned by <a class="reference internal" href="torch.sparse.check_sparse_tensor_invariants.html#torch.sparse.check_sparse_tensor_invariants.is_enabled" title="torch.sparse.check_sparse_tensor_invariants.is_enabled"><code>torch.sparse.check_sparse_tensor_invariants.is_enabled()</code></a>, initially False.</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; ccol_indices = [0, 1, 2]
&gt;&gt;&gt; row_indices = [0, 1]
&gt;&gt;&gt; values = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]
&gt;&gt;&gt; torch.sparse_bsc_tensor(torch.tensor(ccol_indices, dtype=torch.int64),
...                         torch.tensor(row_indices, dtype=torch.int64),
...                         torch.tensor(values), dtype=torch.double)
tensor(ccol_indices=tensor([0, 1, 2]),
       row_indices=tensor([0, 1]),
       values=tensor([[[1., 2.],
                       [3., 4.]],
                      [[5., 6.],
                       [7., 8.]]]), size=(2, 2), nnz=2, dtype=torch.float64,
       layout=torch.sparse_bsc)
</pre> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.sparse_bsc_tensor.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.sparse_bsc_tensor.html</a>
  </p>
</div>
