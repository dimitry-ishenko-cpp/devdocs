<h1 id="torch-tensor-index-reduce">torch.Tensor.index_reduce_</h1> <dl class="py method"> <dt class="sig sig-object py" id="torch.Tensor.index_reduce_">
<code>Tensor.index_reduce_(dim, index, source, reduce, *, include_self=True) → Tensor</code> </dt> <dd>
<p>Accumulate the elements of <code>source</code> into the <code>self</code> tensor by accumulating to the indices in the order given in <code>index</code> using the reduction given by the <code>reduce</code> argument. For example, if <code>dim == 0</code>, <code>index[i] == j</code>, <code>reduce == prod</code> and <code>include_self == True</code> then the <code>i</code>th row of <code>source</code> is multiplied by the <code>j</code>th row of <code>self</code>. If <code>include_self="True"</code>, the values in the <code>self</code> tensor are included in the reduction, otherwise, rows in the <code>self</code> tensor that are accumulated to are treated as if they were filled with the reduction identites.</p> <p>The <a class="reference internal" href="torch.tensor.dim.html#torch.Tensor.dim" title="torch.Tensor.dim"><code>dim</code></a>th dimension of <code>source</code> must have the same size as the length of <code>index</code> (which must be a vector), and all other dimensions must match <code>self</code>, or an error will be raised.</p> <p>For a 3-D tensor with <code>reduce="prod"</code> and <code>include_self=True</code> the output is given as:</p> <pre data-language="python">self[index[i], :, :] *= src[i, :, :]  # if dim == 0
self[:, index[i], :] *= src[:, i, :]  # if dim == 1
self[:, :, index[i]] *= src[:, :, i]  # if dim == 2
</pre> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This operation may behave nondeterministically when given tensors on a CUDA device. See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/randomness.html"><span class="doc">Reproducibility</span></a> for more information.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This function only supports floating point tensors.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This function is in beta and may change in the near future.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – dimension along which to index</li> <li>
<strong>index</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – indices of <code>source</code> to select from, should have dtype either <code>torch.int64</code> or <code>torch.int32</code>
</li> <li>
<strong>source</strong> (<em>FloatTensor</em>) – the tensor containing values to accumulate</li> <li>
<strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – the reduction operation to apply (<code>"prod"</code>, <code>"mean"</code>, <code>"amax"</code>, <code>"amin"</code>)</li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>include_self</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – whether the elements from the <code>self</code> tensor are included in the reduction</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; x = torch.empty(5, 3).fill_(2)
&gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], dtype=torch.float)
&gt;&gt;&gt; index = torch.tensor([0, 4, 2, 0])
&gt;&gt;&gt; x.index_reduce_(0, index, t, 'prod')
tensor([[20., 44., 72.],
        [ 2.,  2.,  2.],
        [14., 16., 18.],
        [ 2.,  2.,  2.],
        [ 8., 10., 12.]])
&gt;&gt;&gt; x = torch.empty(5, 3).fill_(2)
&gt;&gt;&gt; x.index_reduce_(0, index, t, 'prod', include_self=False)
tensor([[10., 22., 36.],
        [ 2.,  2.,  2.],
        [ 7.,  8.,  9.],
        [ 2.,  2.,  2.],
        [ 4.,  5.,  6.]])
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.Tensor.index_reduce_.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.Tensor.index_reduce_.html</a>
  </p>
</div>
