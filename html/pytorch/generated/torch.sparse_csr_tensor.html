<h1 id="torch-sparse-csr-tensor">torch.sparse_csr_tensor</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.sparse_csr_tensor">
<code>torch.sparse_csr_tensor(crow_indices, col_indices, values, size=None, *, dtype=None, device=None, requires_grad=False, check_invariants=None) → Tensor</code> </dt> <dd>
<p>Constructs a <a class="reference internal" href="../sparse.html#sparse-csr-docs"><span class="std std-ref">sparse tensor in CSR (Compressed Sparse Row)</span></a> with specified values at the given <code>crow_indices</code> and <code>col_indices</code>. Sparse matrix multiplication operations in CSR format are typically faster than that for sparse tensors in COO format. Make you have a look at <a class="reference internal" href="../sparse.html#sparse-csr-docs"><span class="std std-ref">the note on the data type of the indices</span></a>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If the <code>device</code> argument is not specified the device of the given <code>values</code> and indices tensor(s) must match. If, however, the argument is specified the input Tensors will be converted to the given device and in turn determine the device of the constructed sparse tensor.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>crow_indices</strong> (<em>array_like</em>) – (B+1)-dimensional array of size <code>(*batchsize, nrows + 1)</code>. The last element of each batch is the number of non-zeros. This tensor encodes the index in values and col_indices depending on where the given row starts. Each successive number in the tensor subtracted by the number before it denotes the number of elements in a given row.</li> <li>
<strong>col_indices</strong> (<em>array_like</em>) – Column co-ordinates of each element in values. (B+1)-dimensional tensor with the same length as values.</li> <li>
<strong>values</strong> (<em>array_list</em>) – Initial values for the tensor. Can be a list, tuple, NumPy <code>ndarray</code>, scalar, and other types that represents a (1+K)-dimensional tensor where <code>K</code> is the number of dense dimensions.</li> <li>
<strong>size</strong> (list, tuple, <code>torch.Size</code>, optional) – Size of the sparse tensor: <code>(*batchsize, nrows, ncols, *densesize)</code>. If not provided, the size will be inferred as the minimum size big enough to hold all non-zero elements.</li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<ul class="simple"> <li>
<strong>dtype</strong> (<a class="reference internal" href="../tensor_attributes.html#torch.dtype" title="torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if None, infers data type from <code>values</code>.</li> <li>
<strong>device</strong> (<a class="reference internal" href="../tensor_attributes.html#torch.device" title="torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see <a class="reference internal" href="torch.set_default_tensor_type.html#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <a class="reference internal" href="../tensor_attributes.html#torch.device" title="torch.device"><code>device</code></a> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</li> <li>
<strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</li> <li>
<strong>check_invariants</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If sparse tensor invariants are checked. Default: as returned by <a class="reference internal" href="torch.sparse.check_sparse_tensor_invariants.html#torch.sparse.check_sparse_tensor_invariants.is_enabled" title="torch.sparse.check_sparse_tensor_invariants.is_enabled"><code>torch.sparse.check_sparse_tensor_invariants.is_enabled()</code></a>, initially False.</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; crow_indices = [0, 2, 4]
&gt;&gt;&gt; col_indices = [0, 1, 0, 1]
&gt;&gt;&gt; values = [1, 2, 3, 4]
&gt;&gt;&gt; torch.sparse_csr_tensor(torch.tensor(crow_indices, dtype=torch.int64),
...                         torch.tensor(col_indices, dtype=torch.int64),
...                         torch.tensor(values), dtype=torch.double)
tensor(crow_indices=tensor([0, 2, 4]),
       col_indices=tensor([0, 1, 0, 1]),
       values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,
       dtype=torch.float64, layout=torch.sparse_csr)
</pre> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.sparse_csr_tensor.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.sparse_csr_tensor.html</a>
  </p>
</div>
