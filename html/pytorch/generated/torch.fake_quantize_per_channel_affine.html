<h1 id="torch-fake-quantize-per-channel-affine">torch.fake_quantize_per_channel_affine</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.fake_quantize_per_channel_affine">
<code>torch.fake_quantize_per_channel_affine(input, scale, zero_point, axis, quant_min, quant_max) → Tensor</code> </dt> <dd>
<p>Returns a new tensor with the data in <code>input</code> fake quantized per channel using <code>scale</code>, <code>zero_point</code>, <code>quant_min</code> and <code>quant_max</code>, across the channel specified by <code>axis</code>.</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>output</mtext><mo>=</mo><mo stretchy="false">(</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mtext>quant_max</mtext><mo separator="true">,</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mtext>quant_min</mtext><mo separator="true">,</mo><mtext>std::nearby_int</mtext><mo stretchy="false">(</mo><mtext>input</mtext><mi mathvariant="normal">/</mi><mtext>scale</mtext><mo stretchy="false">)</mo><mo>+</mo><mtext>zero_point</mtext><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>−</mo><mtext>zero_point</mtext><mo stretchy="false">)</mo><mo>×</mo><mtext>scale</mtext></mrow><annotation encoding="application/x-tex">\text{output} = ( min( \text{quant\_max}, max( \text{quant\_min}, \text{std::nearby\_int}(\text{input} / \text{scale}) + \text{zero\_point} ) ) - \text{zero\_point} ) \times \text{scale} </annotation></semantics></math></span></span></span>
</div>
<dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input value(s), in <code>torch.float32</code>
</li> <li>
<strong>scale</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – quantization scale, per channel in <code>torch.float32</code>
</li> <li>
<strong>zero_point</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – quantization zero_point, per channel in <code>torch.int32</code> or <code>torch.half</code> or <code>torch.float32</code>
</li> <li>
<strong>axis</strong> (<em>int32</em>) – channel axis</li> <li>
<strong>quant_min</strong> (<em>int64</em>) – lower bound of the quantized domain</li> <li>
<strong>quant_max</strong> (<em>int64</em>) – upper bound of the quantized domain</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>A newly fake_quantized per channel <code>torch.float32</code> tensor</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; x = torch.randn(2, 2, 2)
&gt;&gt;&gt; x
tensor([[[-0.2525, -0.0466],
         [ 0.3491, -0.2168]],

        [[-0.5906,  1.6258],
         [ 0.6444, -0.0542]]])
&gt;&gt;&gt; scales = (torch.randn(2) + 1) * 0.05
&gt;&gt;&gt; scales
tensor([0.0475, 0.0486])
&gt;&gt;&gt; zero_points = torch.zeros(2).to(torch.int32)
&gt;&gt;&gt; zero_points
tensor([0, 0])
&gt;&gt;&gt; torch.fake_quantize_per_channel_affine(x, scales, zero_points, 1, 0, 255)
tensor([[[0.0000, 0.0000],
         [0.3405, 0.0000]],

        [[0.0000, 1.6134],
        [0.6323, 0.0000]]])
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.fake_quantize_per_channel_affine.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.fake_quantize_per_channel_affine.html</a>
  </p>
</div>
