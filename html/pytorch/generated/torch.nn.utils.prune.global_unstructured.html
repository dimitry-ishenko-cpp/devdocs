<h1 id="torch-nn-utils-prune-global-unstructured">torch.nn.utils.prune.global_unstructured</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.utils.prune.global_unstructured">
<code>torch.nn.utils.prune.global_unstructured(parameters, pruning_method, importance_scores=None, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/utils/prune.html#global_unstructured"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Globally prunes tensors corresponding to all parameters in <code>parameters</code> by applying the specified <code>pruning_method</code>. Modifies modules in place by:</p> <ol class="arabic simple"> <li>adding a named buffer called <code>name+'_mask'</code> corresponding to the binary mask applied to the parameter <code>name</code> by the pruning method.</li> <li>replacing the parameter <code>name</code> by its pruned version, while the original (unpruned) parameter is stored in a new parameter named <code>name+'_orig'</code>.</li> </ol> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>parameters</strong> (<em>Iterable</em><em> of </em><em>(</em><em>module</em><em>, </em><em>name</em><em>) </em><em>tuples</em>) – parameters of the model to prune in a global fashion, i.e. by aggregating all weights prior to deciding which ones to prune. module must be of type <code>nn.Module</code>, and name must be a string.</li> <li>
<strong>pruning_method</strong> (<em>function</em>) – a valid pruning function from this module, or a custom one implemented by the user that satisfies the implementation guidelines and has <code>PRUNING_TYPE='unstructured'</code>.</li> <li>
<strong>importance_scores</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">dict</a>) – a dictionary mapping (module, name) tuples to the corresponding parameter’s importance scores tensor. The tensor should be the same shape as the parameter, and is used for computing mask for pruning. If unspecified or None, the parameter will be used in place of its importance scores.</li> <li>
<strong>kwargs</strong> – other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters. If <code>float</code>, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If <code>int</code>, it represents the absolute number of parameters to prune.</li> </ul> </dd> <dt class="field-even">Raises</dt> <dd class="field-even">
<p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><strong>TypeError</strong></a> – if <code>PRUNING_TYPE != 'unstructured'</code></p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Since global structured pruning doesn’t make much sense unless the norm is normalized by the size of the parameter, we now limit the scope of global pruning to unstructured methods.</p> </div> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from torch.nn.utils import prune
&gt;&gt;&gt; from collections import OrderedDict
&gt;&gt;&gt; net = nn.Sequential(OrderedDict([
...     ('first', nn.Linear(10, 4)),
...     ('second', nn.Linear(4, 1)),
... ]))
&gt;&gt;&gt; parameters_to_prune = (
...     (net.first, 'weight'),
...     (net.second, 'weight'),
... )
&gt;&gt;&gt; prune.global_unstructured(
...     parameters_to_prune,
...     pruning_method=prune.L1Unstructured,
...     amount=10,
... )
&gt;&gt;&gt; print(sum(torch.nn.utils.parameters_to_vector(net.buffers()) == 0))
tensor(10)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.utils.prune.global_unstructured.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.utils.prune.global_unstructured.html</a>
  </p>
</div>
