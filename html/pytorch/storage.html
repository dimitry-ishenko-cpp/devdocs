<h1 id="torch-storage">torch.Storage</h1> <p><code>torch.Storage</code> is an alias for the storage class that corresponds with the default data type (<a class="reference internal" href="generated/torch.get_default_dtype.html#torch.get_default_dtype" title="torch.get_default_dtype"><code>torch.get_default_dtype()</code></a>). For instance, if the default data type is <code>torch.float</code>, <code>torch.Storage</code> resolves to <a class="reference internal" href="#torch.FloatStorage" title="torch.FloatStorage"><code>torch.FloatStorage</code></a>.</p> <p>The <code>torch.&lt;type&gt;Storage</code> and <code>torch.cuda.&lt;type&gt;Storage</code> classes, like <a class="reference internal" href="#torch.FloatStorage" title="torch.FloatStorage"><code>torch.FloatStorage</code></a>, <a class="reference internal" href="#torch.IntStorage" title="torch.IntStorage"><code>torch.IntStorage</code></a>, etc., are not actually ever instantiated. Calling their constructors creates a <a class="reference internal" href="#torch.TypedStorage" title="torch.TypedStorage"><code>torch.TypedStorage</code></a> with the appropriate <a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code>torch.dtype</code></a> and <a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device"><code>torch.device</code></a>. <code>torch.&lt;type&gt;Storage</code> classes have all of the same class methods that <a class="reference internal" href="#torch.TypedStorage" title="torch.TypedStorage"><code>torch.TypedStorage</code></a> has.</p> <p>A <a class="reference internal" href="#torch.TypedStorage" title="torch.TypedStorage"><code>torch.TypedStorage</code></a> is a contiguous, one-dimensional array of elements of a particular <a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code>torch.dtype</code></a>. It can be given any <a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code>torch.dtype</code></a>, and the internal data will be interpreted appropriately. <a class="reference internal" href="#torch.TypedStorage" title="torch.TypedStorage"><code>torch.TypedStorage</code></a> contains a <a class="reference internal" href="#torch.UntypedStorage" title="torch.UntypedStorage"><code>torch.UntypedStorage</code></a> which holds the data as an untyped array of bytes.</p> <p>Every strided <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> contains a <a class="reference internal" href="#torch.TypedStorage" title="torch.TypedStorage"><code>torch.TypedStorage</code></a>, which stores all of the data that the <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> views.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>All storage classes except for <a class="reference internal" href="#torch.UntypedStorage" title="torch.UntypedStorage"><code>torch.UntypedStorage</code></a> will be removed in the future, and <a class="reference internal" href="#torch.UntypedStorage" title="torch.UntypedStorage"><code>torch.UntypedStorage</code></a> will be used in all cases.</p> </div> <dl class="py class"> <dt class="sig sig-object py" id="torch.TypedStorage">
<code>class torch.TypedStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.bfloat16">
<code>bfloat16()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.bfloat16"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Casts this storage to bfloat16 type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.bool">
<code>bool()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.bool"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Casts this storage to bool type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.byte">
<code>byte()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.byte"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Casts this storage to byte type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.char">
<code>char()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.char"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Casts this storage to char type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.clone">
<code>clone()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.clone"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a copy of this storage</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.complex_double">
<code>complex_double()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.complex_double"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Casts this storage to complex double type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.complex_float">
<code>complex_float()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.complex_float"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Casts this storage to complex float type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.copy_">
<code>copy_(source, non_blocking=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.copy_"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.cpu">
<code>cpu()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.cpu"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a CPU copy of this storage if it’s not already on the CPU</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.cuda">
<code>cuda(device=None, non_blocking=False, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.cuda"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a copy of this object in CUDA memory.</p> <p>If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – The destination GPU id. Defaults to the current device.</li> <li>
<strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If <code>True</code> and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect.</li> <li>
<strong>**kwargs</strong> – For compatibility, may contain the key <code>async</code> in place of the <code>non_blocking</code> argument.</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><em>T</em></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.data_ptr">
<code>data_ptr()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.data_ptr"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="py property"> <dt class="sig sig-object py" id="torch.TypedStorage.device">
<code>property device</code> </dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.double">
<code>double()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.double"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Casts this storage to double type</p> </dd>
</dl> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.TypedStorage.dtype">
<code>dtype: dtype</code> </dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.element_size">
<code>element_size()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.element_size"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.fill_">
<code>fill_(value)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.fill_"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.float">
<code>float()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.float"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Casts this storage to float type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.from_buffer">
<code>classmethod from_buffer(*args, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.from_buffer"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.from_file">
<code>classmethod from_file(filename, shared=False, size=0) → Storage</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.from_file"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>If <code>shared</code> is <code>True</code>, then memory is shared between all processes. All changes are written to the file. If <code>shared</code> is <code>False</code>, then the changes on the storage do not affect the file.</p> <p><code>size</code> is the number of elements in the storage. If <code>shared</code> is <code>False</code>, then the file must contain at least <code>size * sizeof(Type)</code> bytes (<code>Type</code> is the type of storage). If <code>shared</code> is <code>True</code> the file will be created if needed.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>filename</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – file name to map</li> <li>
<strong>shared</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – whether to share memory</li> <li>
<strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – number of elements in the storage</li> </ul> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.get_device">
<code>get_device()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.get_device"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.half">
<code>half()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.half"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Casts this storage to half type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.hpu">
<code>hpu(device=None, non_blocking=False, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.hpu"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a copy of this object in HPU memory.</p> <p>If this object is already in HPU memory and on the correct device, then no copy is performed and the original object is returned.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – The destination HPU id. Defaults to the current device.</li> <li>
<strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If <code>True</code> and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect.</li> <li>
<strong>**kwargs</strong> – For compatibility, may contain the key <code>async</code> in place of the <code>non_blocking</code> argument.</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><em>T</em></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.int">
<code>int()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.int"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Casts this storage to int type</p> </dd>
</dl> <dl class="py property"> <dt class="sig sig-object py" id="torch.TypedStorage.is_cuda">
<code>property is_cuda</code> </dt> 
</dl> <dl class="py property"> <dt class="sig sig-object py" id="torch.TypedStorage.is_hpu">
<code>property is_hpu</code> </dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.is_pinned">
<code>is_pinned(device='cuda')</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.is_pinned"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Determine whether the CPU TypedStorage is already pinned on device.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em> or </em><a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device">torch.device</a>) – The device to pin memory on. Default: <code>'cuda'</code></p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>A boolean variable.</p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.is_shared">
<code>is_shared()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.is_shared"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.TypedStorage.is_sparse">
<code>is_sparse = False</code> </dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.long">
<code>long()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.long"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Casts this storage to long type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.nbytes">
<code>nbytes()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.nbytes"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.pickle_storage_type">
<code>pickle_storage_type()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.pickle_storage_type"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.pin_memory">
<code>pin_memory(device='cuda')</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.pin_memory"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Copies the CPU TypedStorage to pinned memory, if it’s not already pinned.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em> or </em><a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device">torch.device</a>) – The device to pin memory on. Default: <code>'cuda'</code>.</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>A pinned CPU storage.</p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.resize_">
<code>resize_(size)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.resize_"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.share_memory_">
<code>share_memory_()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.share_memory_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Moves the storage to shared memory.</p> <p>This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized.</p> <p>Returns: self</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.short">
<code>short()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.short"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Casts this storage to short type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.size">
<code>size()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.size"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.tolist">
<code>tolist()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.tolist"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a list containing the elements of this storage</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.type">
<code>type(dtype=None, non_blocking=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.type"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the type if <code>dtype</code> is not provided, else casts this object to the specified type.</p> <p>If this is already of the correct type, no copy is performed and the original object is returned.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>dtype</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.12)">type</a><em> or </em><em>string</em>) – The desired type</li> <li>
<strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If <code>True</code>, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect.</li> <li>
<strong>**kwargs</strong> – For compatibility, may contain the key <code>async</code> in place of the <code>non_blocking</code> argument. The <code>async</code> arg is deprecated.</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.12)">Union</a>[<em>T</em>, <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>]</p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.TypedStorage.untyped">
<code>untyped()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#TypedStorage.untyped"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the internal <a class="reference internal" href="#torch.UntypedStorage" title="torch.UntypedStorage"><code>torch.UntypedStorage</code></a></p> </dd>
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.UntypedStorage">
<code>class torch.UntypedStorage(*args, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#UntypedStorage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.bfloat16">
<code>bfloat16()</code> </dt> <dd>
<p>Casts this storage to bfloat16 type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.bool">
<code>bool()</code> </dt> <dd>
<p>Casts this storage to bool type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.byte">
<code>byte()</code> </dt> <dd>
<p>Casts this storage to byte type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.byteswap">
<code>byteswap(dtype)</code> </dt> <dd>
<p>Swaps bytes in underlying data</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.char">
<code>char()</code> </dt> <dd>
<p>Casts this storage to char type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.clone">
<code>clone()</code> </dt> <dd>
<p>Returns a copy of this storage</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.complex_double">
<code>complex_double()</code> </dt> <dd>
<p>Casts this storage to complex double type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.complex_float">
<code>complex_float()</code> </dt> <dd>
<p>Casts this storage to complex float type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.copy_">
<code>copy_()</code> </dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.cpu">
<code>cpu()</code> </dt> <dd>
<p>Returns a CPU copy of this storage if it’s not already on the CPU</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.cuda">
<code>cuda(device=None, non_blocking=False, **kwargs)</code> </dt> <dd>
<p>Returns a copy of this object in CUDA memory.</p> <p>If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – The destination GPU id. Defaults to the current device.</li> <li>
<strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If <code>True</code> and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect.</li> <li>
<strong>**kwargs</strong> – For compatibility, may contain the key <code>async</code> in place of the <code>non_blocking</code> argument.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.data_ptr">
<code>data_ptr()</code> </dt> 
</dl> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.UntypedStorage.device">
<code>device: device</code> </dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.double">
<code>double()</code> </dt> <dd>
<p>Casts this storage to double type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.element_size">
<code>element_size()</code> </dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.fill_">
<code>fill_()</code> </dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.float">
<code>float()</code> </dt> <dd>
<p>Casts this storage to float type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.from_buffer">
<code>static from_buffer()</code> </dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.from_file">
<code>static from_file(filename, shared=False, size=0) → Storage</code> </dt> <dd>
<p>If <code>shared</code> is <code>True</code>, then memory is shared between all processes. All changes are written to the file. If <code>shared</code> is <code>False</code>, then the changes on the storage do not affect the file.</p> <p><code>size</code> is the number of elements in the storage. If <code>shared</code> is <code>False</code>, then the file must contain at least <code>size * sizeof(Type)</code> bytes (<code>Type</code> is the type of storage). If <code>shared</code> is <code>True</code> the file will be created if needed.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>filename</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – file name to map</li> <li>
<strong>shared</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – whether to share memory</li> <li>
<strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – number of elements in the storage</li> </ul> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.get_device">
<code>get_device()</code> </dt> <dd>
<dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.half">
<code>half()</code> </dt> <dd>
<p>Casts this storage to half type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.hpu">
<code>hpu(device=None, non_blocking=False, **kwargs)</code> </dt> <dd>
<p>Returns a copy of this object in HPU memory.</p> <p>If this object is already in HPU memory and on the correct device, then no copy is performed and the original object is returned.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – The destination HPU id. Defaults to the current device.</li> <li>
<strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If <code>True</code> and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect.</li> <li>
<strong>**kwargs</strong> – For compatibility, may contain the key <code>async</code> in place of the <code>non_blocking</code> argument.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.int">
<code>int()</code> </dt> <dd>
<p>Casts this storage to int type</p> </dd>
</dl> <dl class="py property"> <dt class="sig sig-object py" id="torch.UntypedStorage.is_cuda">
<code>property is_cuda</code> </dt> 
</dl> <dl class="py property"> <dt class="sig sig-object py" id="torch.UntypedStorage.is_hpu">
<code>property is_hpu</code> </dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.is_pinned">
<code>is_pinned(device='cuda')</code> </dt> <dd>
<p>Determine whether the CPU storage is already pinned on device.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em> or </em><a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device">torch.device</a>) – The device to pin memory on. Default: <code>'cuda'</code>.</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>A boolean variable.</p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.is_shared">
<code>is_shared()</code> </dt> 
</dl> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.UntypedStorage.is_sparse">
<code>is_sparse: bool = False</code> </dt> 
</dl> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.UntypedStorage.is_sparse_csr">
<code>is_sparse_csr: bool = False</code> </dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.long">
<code>long()</code> </dt> <dd>
<p>Casts this storage to long type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.mps">
<code>mps()</code> </dt> <dd>
<p>Returns a MPS copy of this storage if it’s not already on the MPS</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.nbytes">
<code>nbytes()</code> </dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.new">
<code>new()</code> </dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.pin_memory">
<code>pin_memory(device='cuda')</code> </dt> <dd>
<p>Copies the CPU storage to pinned memory, if it’s not already pinned.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em> or </em><a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device">torch.device</a>) – The device to pin memory on. Default: <code>'cuda'</code>.</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>A pinned CPU storage.</p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.resize_">
<code>resize_()</code> </dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.share_memory_">
<code>share_memory_(*args, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/storage.html#UntypedStorage.share_memory_"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.short">
<code>short()</code> </dt> <dd>
<p>Casts this storage to short type</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.size">
<code>size()</code> </dt> <dd>
<dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.tolist">
<code>tolist()</code> </dt> <dd>
<p>Returns a list containing the elements of this storage</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.type">
<code>type(dtype=None, non_blocking=False, **kwargs)</code> </dt> <dd>
<p>Returns the type if <code>dtype</code> is not provided, else casts this object to the specified type.</p> <p>If this is already of the correct type, no copy is performed and the original object is returned.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>dtype</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.12)">type</a><em> or </em><em>string</em>) – The desired type</li> <li>
<strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If <code>True</code>, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect.</li> <li>
<strong>**kwargs</strong> – For compatibility, may contain the key <code>async</code> in place of the <code>non_blocking</code> argument. The <code>async</code> arg is deprecated.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.UntypedStorage.untyped">
<code>untyped()</code> </dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.DoubleStorage">
<code>class torch.DoubleStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#DoubleStorage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py attribute"> <dt class="sig sig-object py" id="torch.DoubleStorage.dtype">
<code>dtype: dtype = torch.float64</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#DoubleStorage.dtype"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.FloatStorage">
<code>class torch.FloatStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#FloatStorage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py attribute"> <dt class="sig sig-object py" id="torch.FloatStorage.dtype">
<code>dtype: dtype = torch.float32</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#FloatStorage.dtype"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.HalfStorage">
<code>class torch.HalfStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#HalfStorage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py attribute"> <dt class="sig sig-object py" id="torch.HalfStorage.dtype">
<code>dtype: dtype = torch.float16</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#HalfStorage.dtype"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.LongStorage">
<code>class torch.LongStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#LongStorage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py attribute"> <dt class="sig sig-object py" id="torch.LongStorage.dtype">
<code>dtype: dtype = torch.int64</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#LongStorage.dtype"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.IntStorage">
<code>class torch.IntStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#IntStorage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py attribute"> <dt class="sig sig-object py" id="torch.IntStorage.dtype">
<code>dtype: dtype = torch.int32</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#IntStorage.dtype"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.ShortStorage">
<code>class torch.ShortStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#ShortStorage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py attribute"> <dt class="sig sig-object py" id="torch.ShortStorage.dtype">
<code>dtype: dtype = torch.int16</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#ShortStorage.dtype"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.CharStorage">
<code>class torch.CharStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#CharStorage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py attribute"> <dt class="sig sig-object py" id="torch.CharStorage.dtype">
<code>dtype: dtype = torch.int8</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#CharStorage.dtype"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.ByteStorage">
<code>class torch.ByteStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#ByteStorage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py attribute"> <dt class="sig sig-object py" id="torch.ByteStorage.dtype">
<code>dtype: dtype = torch.uint8</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#ByteStorage.dtype"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.BoolStorage">
<code>class torch.BoolStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#BoolStorage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py attribute"> <dt class="sig sig-object py" id="torch.BoolStorage.dtype">
<code>dtype: dtype = torch.bool</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#BoolStorage.dtype"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.BFloat16Storage">
<code>class torch.BFloat16Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#BFloat16Storage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py attribute"> <dt class="sig sig-object py" id="torch.BFloat16Storage.dtype">
<code>dtype: dtype = torch.bfloat16</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#BFloat16Storage.dtype"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.ComplexDoubleStorage">
<code>class torch.ComplexDoubleStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#ComplexDoubleStorage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py attribute"> <dt class="sig sig-object py" id="torch.ComplexDoubleStorage.dtype">
<code>dtype: dtype = torch.complex128</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#ComplexDoubleStorage.dtype"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.ComplexFloatStorage">
<code>class torch.ComplexFloatStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#ComplexFloatStorage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py attribute"> <dt class="sig sig-object py" id="torch.ComplexFloatStorage.dtype">
<code>dtype: dtype = torch.complex64</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#ComplexFloatStorage.dtype"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.QUInt8Storage">
<code>class torch.QUInt8Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#QUInt8Storage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py attribute"> <dt class="sig sig-object py" id="torch.QUInt8Storage.dtype">
<code>dtype: dtype = torch.quint8</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#QUInt8Storage.dtype"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.QInt8Storage">
<code>class torch.QInt8Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#QInt8Storage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py attribute"> <dt class="sig sig-object py" id="torch.QInt8Storage.dtype">
<code>dtype: dtype = torch.qint8</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#QInt8Storage.dtype"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.QInt32Storage">
<code>class torch.QInt32Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#QInt32Storage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py attribute"> <dt class="sig sig-object py" id="torch.QInt32Storage.dtype">
<code>dtype: dtype = torch.qint32</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#QInt32Storage.dtype"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.QUInt4x2Storage">
<code>class torch.QUInt4x2Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#QUInt4x2Storage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py attribute"> <dt class="sig sig-object py" id="torch.QUInt4x2Storage.dtype">
<code>dtype: dtype = torch.quint4x2</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#QUInt4x2Storage.dtype"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.QUInt2x4Storage">
<code>class torch.QUInt2x4Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#QUInt2x4Storage"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
 <dl class="py attribute"> <dt class="sig sig-object py" id="torch.QUInt2x4Storage.dtype">
<code>dtype: dtype = torch.quint2x4</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#QUInt2x4Storage.dtype"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/storage.html" class="_attribution-link">https://pytorch.org/docs/2.1/storage.html</a>
  </p>
</div>
