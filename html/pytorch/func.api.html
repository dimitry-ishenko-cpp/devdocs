<h1 id="torch-func-api-reference">torch.func API Reference</h1>  <h2 id="module-torch.func">Function Transforms</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.func.vmap.html#torch.func.vmap" title="torch.func.vmap"><code>vmap</code></a>
</td> <td><p>vmap is the vectorizing map; <code>vmap(func)</code> returns a new function that maps <code>func</code> over some dimension of the inputs.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.func.grad.html#torch.func.grad" title="torch.func.grad"><code>grad</code></a>
</td> <td><p><code>grad</code> operator helps computing gradients of <code>func</code> with respect to the input(s) specified by <code>argnums</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.func.grad_and_value.html#torch.func.grad_and_value" title="torch.func.grad_and_value"><code>grad_and_value</code></a>
</td> <td><p>Returns a function to compute a tuple of the gradient and primal, or forward, computation.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.func.vjp.html#torch.func.vjp" title="torch.func.vjp"><code>vjp</code></a>
</td> <td><p>Standing for the vector-Jacobian product, returns a tuple containing the results of <code>func</code> applied to <code>primals</code> and a function that, when given <code>cotangents</code>, computes the reverse-mode Jacobian of <code>func</code> with respect to <code>primals</code> times <code>cotangents</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.func.jvp.html#torch.func.jvp" title="torch.func.jvp"><code>jvp</code></a>
</td> <td><p>Standing for the Jacobian-vector product, returns a tuple containing the output of <code>func(*primals)</code> and the "Jacobian of <code>func</code> evaluated at <code>primals</code>" times <code>tangents</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.func.linearize.html#torch.func.linearize" title="torch.func.linearize"><code>linearize</code></a>
</td> <td><p>Returns the value of <code>func</code> at <code>primals</code> and linear approximation at <code>primals</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.func.jacrev.html#torch.func.jacrev" title="torch.func.jacrev"><code>jacrev</code></a>
</td> <td><p>Computes the Jacobian of <code>func</code> with respect to the arg(s) at index <code>argnum</code> using reverse mode autodiff</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.func.jacfwd.html#torch.func.jacfwd" title="torch.func.jacfwd"><code>jacfwd</code></a>
</td> <td><p>Computes the Jacobian of <code>func</code> with respect to the arg(s) at index <code>argnum</code> using forward-mode autodiff</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.func.hessian.html#torch.func.hessian" title="torch.func.hessian"><code>hessian</code></a>
</td> <td><p>Computes the Hessian of <code>func</code> with respect to the arg(s) at index <code>argnum</code> via a forward-over-reverse strategy.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.func.functionalize.html#torch.func.functionalize" title="torch.func.functionalize"><code>functionalize</code></a>
</td> <td><p>functionalize is a transform that can be used to remove (intermediate) mutations and aliasing from a function, while preserving the function's semantics.</p></td> </tr>  </table>   <h2 id="utilities-for-working-with-torch-nn-modules">Utilities for working with torch.nn.Modules</h2> <p>In general, you can transform over a function that calls a <code>torch.nn.Module</code>. For example, the following is an example of computing a jacobian of a function that takes three values and returns three values:</p> <pre data-language="python">model = torch.nn.Linear(3, 3)

def f(x):
    return model(x)

x = torch.randn(3)
jacobian = jacrev(f)(x)
assert jacobian.shape == (3, 3)
</pre> <p>However, if you want to do something like compute a jacobian over the parameters of the model, then there needs to be a way to construct a function where the parameters are the inputs to the function. That’s what <a class="reference internal" href="generated/torch.func.functional_call.html#torch.func.functional_call" title="torch.func.functional_call"><code>functional_call()</code></a> is for: it accepts an nn.Module, the transformed <code>parameters</code>, and the inputs to the Module’s forward pass. It returns the value of running the Module’s forward pass with the replaced parameters.</p> <p>Here’s how we would compute the Jacobian over the parameters</p> <pre data-language="python">model = torch.nn.Linear(3, 3)

def f(params, x):
    return torch.func.functional_call(model, params, x)

x = torch.randn(3)
jacobian = jacrev(f)(dict(model.named_parameters()), x)
</pre> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.func.functional_call.html#torch.func.functional_call" title="torch.func.functional_call"><code>functional_call</code></a>
</td> <td><p>Performs a functional call on the module by replacing the module parameters and buffers with the provided ones.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.func.stack_module_state.html#torch.func.stack_module_state" title="torch.func.stack_module_state"><code>stack_module_state</code></a>
</td> <td><p>Prepares a list of torch.nn.Modules for ensembling with <a class="reference internal" href="generated/torch.func.vmap.html#torch.func.vmap" title="torch.func.vmap"><code>vmap()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.func.replace_all_batch_norm_modules_.html#torch.func.replace_all_batch_norm_modules_" title="torch.func.replace_all_batch_norm_modules_"><code>replace_all_batch_norm_modules_</code></a>
</td> <td><p>In place updates <code>root</code> by setting the <code>running_mean</code> and <code>running_var</code> to be None and setting track_running_stats to be False for any nn.BatchNorm module in <code>root</code></p></td> </tr>  </table> <p>If you’re looking for information on fixing Batch Norm modules, please follow the guidance here</p>  <ul> <li class="toctree-l1"><a class="reference internal" href="func.batch_norm.html">Patching Batch Norm</a></li> </ul><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/func.api.html" class="_attribution-link">https://pytorch.org/docs/2.1/func.api.html</a>
  </p>
</div>
