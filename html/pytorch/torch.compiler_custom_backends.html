<h1 id="custom-backends">Custom Backends</h1>  <h2 id="overview">Overview</h2> <p><code>torch.compile</code> provides a straightforward method to enable users to define custom backends.</p> <p>A backend function has the contract <code>(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]) -&gt; Callable</code>.</p> <p>Backend functions can be called by TorchDynamo, the graph tracing component of <code>torch.compile</code>, after tracing an FX graph and are expected to return a compiled function that is equivalent to the traced FX graph. The returned callable should have the same contract as the <code>forward</code> function of the original <code>torch.fx.GraphModule</code> passed into the backend: <code>(*args: torch.Tensor) -&gt; List[torch.Tensor]</code>.</p> <p>In order for TorchDynamo to call your backend, pass your backend function as the <code>backend</code> kwarg in <code>torch.compile</code>. For example,</p> <pre data-language="python">import torch

def my_custom_backend(gm, example_inputs):
    return gm.forward

def f(...):
    ...

f_opt = torch.compile(f, backend=my_custom_backend)

@torch.compile(backend=my_custom_backend)
def g(...):
    ...
</pre> <p>See below for more examples.</p>   <h2 id="registering-custom-backends">Registering Custom Backends</h2> <p>You can register your backend using the <code>register_backend</code> decorator, for example,</p> <pre data-language="python">from torch._dynamo.optimizations import register_backend

@register_backend
def my_compiler(gm, example_inputs):
    ...
</pre> <p>Besides the <code>register_backend</code> decorator, if your backend is in another python package, you could also register your backend through entry points of python package, which provides a way for a package to register a plugin for another one.</p> <div class="admonition hint"> <p class="admonition-title">Hint</p> <p>You can learn more about <code>entry_points</code> in the <a class="reference external" href="https://setuptools.pypa.io/en/latest/userguide/entry_point.html">python packaging documentation</a>.</p> </div> <p>To register your backend through <code>entry_points</code>, you could add your backend function to the <code>torch_dynamo_backends</code> entry point group in the <code>setup.py</code> file of your package like:</p> <pre data-language="python">...
setup(
    ...
    'torch_dynamo_backends': [
        'my_compiler = your_module.submodule:my_compiler',
    ]
    ...
)
</pre> <p>Please replace the <code>my_compiler</code> before <code>=</code> to the name of your backend’s name and replace the part after <code>=</code> to the module and function name of your backend function. The entry point will be added to your python environment after the installation of the package. When you call <code>torch.compile(model, backend="my_compiler")</code>, PyTorch would first search the backend named <code>my_compiler</code> that has been registered with <code>register_backend</code>. If not found, it will continue to search in all backends registered via <code>entry_points</code>.</p> <p>Registration serves two purposes:</p> <ul class="simple"> <li>You can pass a string containing your backend function’s name to <code>torch.compile</code> instead of the function itself, for example, <code>torch.compile(model, backend="my_compiler")</code>.</li> <li>It is required for use with the <a class="reference external" href="https://pytorch.org/docs/main/compile/troubleshooting.html">minifier</a>. Any generated code from the minifier must call your code that registers your backend function, typically through an <code>import</code> statement.</li> </ul>   <h2 id="custom-backends-after-aotautograd">Custom Backends after AOTAutograd</h2> <p>It is possible to define custom backends that are called by AOTAutograd rather than TorchDynamo. This is useful for 2 main reasons:</p> <ul class="simple"> <li>Users can define backends that support model training, as AOTAutograd can generate the backward graph for compilation.</li> <li>AOTAutograd produces FX graphs consisting of <a class="reference external" href="https://pytorch.org/docs/main/ir.html#canonical-aten-ir">canonical Aten ops</a>. As a result, custom backends only need to support the canonical Aten opset, which is a significantly smaller opset than the entire torch/Aten opset.</li> </ul> <p>Wrap your backend with <code>torch._dynamo.optimizations.training.aot_autograd</code> and use <code>torch.compile</code> with the <code>backend</code> kwarg as before. Backend functions wrapped by <code>aot_autograd</code> should have the same contract as before.</p> <p>Backend functions are passed to <code>aot_autograd</code> through the <code>fw_compiler</code> (forward compiler) or <code>bw_compiler</code> (backward compiler) kwargs. If <code>bw_compiler</code> is not specified, the backward compile function defaults to the forward compile function.</p> <p>One caveat is that AOTAutograd requires compiled functions returned by backends to be “boxed”. This can be done by wrapping the compiled function with <code>functorch.compile.make_boxed_func</code>.</p> <p>For example,</p> <pre data-language="python">from torch._dynamo.optimizations.training import aot_autograd
from functorch.compile import make_boxed_func

def my_compiler(gm, example_inputs):
    return make_boxed_func(gm.forward)

my_backend = aot_autograd(fw_compiler=my_compiler)  # bw_compiler=my_compiler

model_opt = torch.compile(model, backend=my_backend)
</pre>   <h2 id="examples">Examples</h2>  <h3 id="debugging-backend">Debugging Backend</h3> <p>If you want to better understand what is going on during a compilation, you can create a custom compiler, which is referred to as backend in this section, that will print pretty print the fx <code>GraphModule</code> extracted from Dynamo’s bytecode analysis and return a <code>forward()</code> callable.</p> <p>For example:</p> <pre data-language="python">from typing import List
import torch
def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):
    print("my_compiler() called with FX graph:")
    gm.graph.print_tabular()
    return gm.forward  # return a python callable
@torch.compile(backend=my_compiler)
def fn(x, y):
    a = torch.cos(x)
    b = torch.sin(y)
    return a + b
fn(torch.randn(10), torch.randn(10))
</pre> <p>Running the above example produces the following output:</p> <pre data-language="python">my_compiler() called with FX graph:
opcode         name    target                                                  args        kwargs
-------------  ------  ------------------------------------------------------  ----------  --------
placeholder    x       x                                                       ()          {}
placeholder    y       y                                                       ()          {}
call_function  cos     &lt;built-in method cos of type object at 0x7f1a894649a8&gt;  (x,)        {}
call_function  sin     &lt;built-in method sin of type object at 0x7f1a894649a8&gt;  (y,)        {}
call_function  add     &lt;built-in function add&gt;                                 (cos, sin)  {}
output         output  output                                                  ((add,),)   {}
</pre> <p>This works for <code>torch.nn.Module</code> as well as shown below:</p> <pre data-language="python">from typing import List
import torch
def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):
    print("my_compiler() called with FX graph:")
    gm.graph.print_tabular()
    return gm.forward  # return a python callable
class MockModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.relu = torch.nn.ReLU()
    def forward(self, x):
        return self.relu(torch.cos(x))
mod = MockModule()
optimized_mod = torch.compile(mod, backend=my_compiler)
optimized_mod(torch.randn(10))
</pre> <p>Let’s take a look at one more example with control flow:</p> <pre data-language="python">from typing import List
import torch
def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):
    print("my_compiler() called with FX graph:")
    gm.graph.print_tabular()
    return gm.forward  # return a python callable
@torch.compile(backend=my_compiler)
def toy_example(a, b):
    x = a / (torch.abs(a) + 1)
    if b.sum() &lt; 0:
        b = b * -1
    return x * b
for _ in range(100):
    toy_example(torch.randn(10), torch.randn(10))
</pre> <p>Running this example produces the following output:</p> <pre data-language="python">my_compiler() called with FX graph:
opcode         name     target                                                  args              kwargs
-------------  -------  ------------------------------------------------------  ----------------  --------
placeholder    a        a                                                       ()                {}
placeholder    b        b                                                       ()                {}
call_function  abs_1    &lt;built-in method abs of type object at 0x7f8d259298a0&gt;  (a,)              {}
call_function  add      &lt;built-in function add&gt;                                 (abs_1, 1)        {}
call_function  truediv  &lt;built-in function truediv&gt;                             (a, add)          {}
call_method    sum_1    sum                                                     (b,)              {}
call_function  lt       &lt;built-in function lt&gt;                                  (sum_1, 0)        {}
output         output   output                                                  ((truediv, lt),)  {}

my_compiler() called with FX graph:
opcode         name    target                   args         kwargs
-------------  ------  -----------------------  -----------  --------
placeholder    b       b                        ()           {}
placeholder    x       x                        ()           {}
call_function  mul     &lt;built-in function mul&gt;  (b, -1)      {}
call_function  mul_1   &lt;built-in function mul&gt;  (x, mul)     {}
output         output  output                   ((mul_1,),)  {}

my_compiler() called with FX graph:
opcode         name    target                   args       kwargs
-------------  ------  -----------------------  ---------  --------
placeholder    b       b                        ()         {}
placeholder    x       x                        ()         {}
call_function  mul     &lt;built-in function mul&gt;  (x, b)     {}
output         output  output                   ((mul,),)  {}
</pre> <p>The order of the last two graphs is nondeterministic depending on which one is encountered first by the just-in-time compiler.</p>   <h3 id="speedy-backend">Speedy Backend</h3> <p>Integrating a custom backend that offers superior performance is also easy and we’ll integrate a real one with <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html">optimize_for_inference</a>:</p> <pre data-language="python">def optimize_for_inference_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):
    scripted = torch.jit.script(gm)
    return torch.jit.optimize_for_inference(scripted)
</pre> <p>And then you should be able to optimize any existing code with:</p> <pre data-language="python">@torch.compile(backend=optimize_for_inference_compiler)
def code_to_accelerate():
    ...
</pre>   <h3 id="composable-backends">Composable Backends</h3> <p>TorchDynamo includes many backends, which can be found in <a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/optimizations/backends.py">backends.py</a> or <code>torch._dynamo.list_backends()</code>. You can combine these backends together with the following code:</p> <pre data-language="python">from torch._dynamo.optimizations import BACKENDS
 def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):
     try:
         trt_compiled = BACKENDS["tensorrt"](gm, example_inputs)
         if trt_compiled is not None:
             return trt_compiled
     except Exception:
         pass
     # first backend failed, try something else...
     try:
         inductor_compiled = BACKENDS["inductor"](gm, example_inputs)
         if inductor_compiled is not None:
             return inductor_compiled
     except Exception:
         pass
     return gm.forward
</pre><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/torch.compiler_custom_backends.html" class="_attribution-link">https://pytorch.org/docs/2.1/torch.compiler_custom_backends.html</a>
  </p>
</div>
