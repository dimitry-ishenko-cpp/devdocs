<h1 id="torch-mps">torch.mps</h1> <p id="module-torch.mps">This package enables an interface for accessing MPS (Metal Performance Shaders) backend in Python. Metal is Appleâ€™s API for programming metal GPU (graphics processor unit). Using MPS means that increased performance can be achieved, by running work on the metal GPU(s). See <a class="reference external" href="https://developer.apple.com/documentation/metalperformanceshaders">https://developer.apple.com/documentation/metalperformanceshaders</a> for more details.</p> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.mps.synchronize.html#torch.mps.synchronize" title="torch.mps.synchronize"><code>synchronize</code></a>
</td> <td><p>Waits for all kernels in all streams on a MPS device to complete.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.mps.get_rng_state.html#torch.mps.get_rng_state" title="torch.mps.get_rng_state"><code>get_rng_state</code></a>
</td> <td><p>Returns the random number generator state as a ByteTensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.mps.set_rng_state.html#torch.mps.set_rng_state" title="torch.mps.set_rng_state"><code>set_rng_state</code></a>
</td> <td><p>Sets the random number generator state.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.mps.manual_seed.html#torch.mps.manual_seed" title="torch.mps.manual_seed"><code>manual_seed</code></a>
</td> <td><p>Sets the seed for generating random numbers.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.mps.seed.html#torch.mps.seed" title="torch.mps.seed"><code>seed</code></a>
</td> <td><p>Sets the seed for generating random numbers to a random number.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.mps.empty_cache.html#torch.mps.empty_cache" title="torch.mps.empty_cache"><code>empty_cache</code></a>
</td> <td><p>Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU applications.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.mps.set_per_process_memory_fraction.html#torch.mps.set_per_process_memory_fraction" title="torch.mps.set_per_process_memory_fraction"><code>set_per_process_memory_fraction</code></a>
</td> <td><p>Set memory fraction for limiting process's memory allocation on MPS device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.mps.current_allocated_memory.html#torch.mps.current_allocated_memory" title="torch.mps.current_allocated_memory"><code>current_allocated_memory</code></a>
</td> <td><p>Returns the current GPU memory occupied by tensors in bytes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.mps.driver_allocated_memory.html#torch.mps.driver_allocated_memory" title="torch.mps.driver_allocated_memory"><code>driver_allocated_memory</code></a>
</td> <td><p>Returns total GPU memory allocated by Metal driver for the process in bytes.</p></td> </tr>  </table>  <h2 id="mps-profiler">MPS Profiler</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td><p><a class="reference internal" href="generated/torch.mps.profiler.start.html#torch.mps.profiler.start" title="torch.mps.profiler.start"><code>profiler.start</code></a></p></td> <td><p>Start OS Signpost tracing from MPS backend.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.mps.profiler.stop.html#torch.mps.profiler.stop" title="torch.mps.profiler.stop"><code>profiler.stop</code></a></p></td> <td><p>Stops generating OS Signpost tracing from MPS backend.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.mps.profiler.profile.html#torch.mps.profiler.profile" title="torch.mps.profiler.profile"><code>profiler.profile</code></a></p></td> <td><p>Context Manager to enabling generating OS Signpost tracing from MPS backend.</p></td> </tr>  </table>   <h2 id="mps-event">MPS Event</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td><p><a class="reference internal" href="generated/torch.mps.event.event.html#torch.mps.event.Event" title="torch.mps.event.Event"><code>event.Event</code></a></p></td> <td><p>Wrapper around an MPS event.</p></td> </tr>  </table><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/mps.html" class="_attribution-link">https://pytorch.org/docs/2.1/mps.html</a>
  </p>
</div>
