<h1 id="patching-batch-norm">Patching Batch Norm</h1>  <h2 id="what-s-happening">What’s happening?</h2> <p>Batch Norm requires in-place updates to running_mean and running_var of the same size as the input. Functorch does not support inplace update to a regular tensor that takes in a batched tensor (i.e. <code>regular.add_(batched)</code> is not allowed). So when vmapping over a batch of inputs to a single module, we end up with this error</p>   <h2 id="how-to-fix">How to fix</h2> <p>One of the best supported ways is to switch BatchNorm for GroupNorm. Options 1 and 2 support this</p> <p>All of these options assume that you don’t need running stats. If you’re using a module this means that it’s assumed you won’t use batch norm in evaluation mode. If you have a use case that involves running batch norm with vmap in evaluation mode, please file an issue</p>  <h3 id="option-1-change-the-batchnorm">Option 1: Change the BatchNorm</h3> <p>If you want to change for GroupNorm, anywhere that you have BatchNorm, replace it with:</p> <pre data-language="python">BatchNorm2d(C, G, track_running_stats=False)
</pre> <p>Here <code>C</code> is the same <code>C</code> as in the original BatchNorm. <code>G</code> is the number of groups to break <code>C</code> into. As such, <code>C % G == 0</code> and as a fallback, you can set <code>C == G</code>, meaning each channel will be treated separately.</p> <p>If you must use BatchNorm and you’ve built the module yourself, you can change the module to not use running stats. In other words, anywhere that there’s a BatchNorm module, set the <code>track_running_stats</code> flag to be False</p> <pre data-language="python">BatchNorm2d(64, track_running_stats=False)
</pre>   <h3 id="option-2-torchvision-parameter">Option 2: torchvision parameter</h3> <p>Some torchvision models, like resnet and regnet, can take in a <code>norm_layer</code> parameter. These are often defaulted to be BatchNorm2d if they’ve been defaulted.</p> <p>Instead you can set it to be GroupNorm.</p> <pre data-language="python">import torchvision
from functools import partial
torchvision.models.resnet18(norm_layer=lambda c: GroupNorm(num_groups=g, c))
</pre> <p>Here, once again, <code>c % g == 0</code> so as a fallback, set <code>g = c</code>.</p> <p>If you are attached to BatchNorm, be sure to use a version that doesn’t use running stats</p> <pre data-language="python">import torchvision
from functools import partial
torchvision.models.resnet18(norm_layer=partial(BatchNorm2d, track_running_stats=False))
</pre>   <h3 id="option-3-functorch-s-patching">Option 3: functorch’s patching</h3> <p>functorch has added some functionality to allow for quick, in-place patching of the module to not use running stats. Changing the norm layer is more fragile, so we have not offered that. If you have a net where you want the BatchNorm to not use running stats, you can run <code>replace_all_batch_norm_modules_</code> to update the module in-place to not use running stats</p> <pre data-language="python">from torch.func import replace_all_batch_norm_modules_
replace_all_batch_norm_modules_(net)
</pre>   <h3 id="option-4-eval-mode">Option 4: eval mode</h3> <p>When run under eval mode, the running_mean and running_var will not be updated. Therefore, vmap can support this mode</p> <pre data-language="python">model.eval()
vmap(model)(x)
model.train()
</pre><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/func.batch_norm.html" class="_attribution-link">https://pytorch.org/docs/2.1/func.batch_norm.html</a>
  </p>
</div>
