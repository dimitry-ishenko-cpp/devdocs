<h1 id="torch-func">torch.func</h1> <p>torch.func, previously known as “functorch”, is <a class="reference external" href="https://github.com/google/jax">JAX-like</a> composable function transforms for PyTorch.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This library is currently in <a class="reference external" href="https://pytorch.org/blog/pytorch-feature-classification-changes/#beta">beta</a>. What this means is that the features generally work (unless otherwise documented) and we (the PyTorch team) are committed to bringing this library forward. However, the APIs may change under user feedback and we don’t have full coverage over PyTorch operations.</p> <p>If you have suggestions on the API or use-cases you’d like to be covered, please open an GitHub issue or reach out. We’d love to hear about how you’re using the library.</p> </div>  <h2 id="what-are-composable-function-transforms">What are composable function transforms?</h2> <ul class="simple"> <li>A “function transform” is a higher-order function that accepts a numerical function and returns a new function that computes a different quantity.</li> <li>
<a class="reference internal" href="func.api.html#module-torch.func" title="torch.func"><code>torch.func</code></a> has auto-differentiation transforms (<code>grad(f)</code> returns a function that computes the gradient of <code>f</code>), a vectorization/batching transform (<code>vmap(f)</code> returns a function that computes <code>f</code> over batches of inputs), and others.</li> <li>These function transforms can compose with each other arbitrarily. For example, composing <code>vmap(grad(f))</code> computes a quantity called per-sample-gradients that stock PyTorch cannot efficiently compute today.</li> </ul>   <h2 id="why-composable-function-transforms">Why composable function transforms?</h2> <p>There are a number of use cases that are tricky to do in PyTorch today:</p> <ul class="simple"> <li>computing per-sample-gradients (or other per-sample quantities)</li> <li>running ensembles of models on a single machine</li> <li>efficiently batching together tasks in the inner-loop of MAML</li> <li>efficiently computing Jacobians and Hessians</li> <li>efficiently computing batched Jacobians and Hessians</li> </ul> <p>Composing <a class="reference internal" href="generated/torch.func.vmap.html#torch.func.vmap" title="torch.func.vmap"><code>vmap()</code></a>, <a class="reference internal" href="generated/torch.func.grad.html#torch.func.grad" title="torch.func.grad"><code>grad()</code></a>, and <a class="reference internal" href="generated/torch.func.vjp.html#torch.func.vjp" title="torch.func.vjp"><code>vjp()</code></a> transforms allows us to express the above without designing a separate subsystem for each. This idea of composable function transforms comes from the <a class="reference external" href="https://github.com/google/jax">JAX framework</a>.</p>   <h2 id="read-more">Read More</h2>  <ul> <li class="toctree-l1">
<a class="reference internal" href="func.whirlwind_tour.html">torch.func Whirlwind Tour</a><ul> <li class="toctree-l2"><a class="reference internal" href="func.whirlwind_tour.html#what-is-torch-func">What is torch.func?</a></li> <li class="toctree-l2"><a class="reference internal" href="func.whirlwind_tour.html#why-composable-function-transforms">Why composable function transforms?</a></li> <li class="toctree-l2"><a class="reference internal" href="func.whirlwind_tour.html#what-are-the-transforms">What are the transforms?</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="func.api.html">torch.func API Reference</a><ul> <li class="toctree-l2"><a class="reference internal" href="func.api.html#function-transforms">Function Transforms</a></li> <li class="toctree-l2"><a class="reference internal" href="func.api.html#utilities-for-working-with-torch-nn-modules">Utilities for working with torch.nn.Modules</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="func.ux_limitations.html">UX Limitations</a><ul> <li class="toctree-l2"><a class="reference internal" href="func.ux_limitations.html#general-limitations">General limitations</a></li> <li class="toctree-l2"><a class="reference internal" href="func.ux_limitations.html#torch-autograd-apis">torch.autograd APIs</a></li> <li class="toctree-l2"><a class="reference internal" href="func.ux_limitations.html#vmap-limitations">vmap limitations</a></li> <li class="toctree-l2"><a class="reference internal" href="func.ux_limitations.html#randomness">Randomness</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="func.migrating.html">Migrating from functorch to torch.func</a><ul> <li class="toctree-l2"><a class="reference internal" href="func.migrating.html#function-transforms">function transforms</a></li> <li class="toctree-l2"><a class="reference internal" href="func.migrating.html#nn-module-utilities">NN module utilities</a></li> <li class="toctree-l2"><a class="reference internal" href="func.migrating.html#functorch-compile">functorch.compile</a></li> </ul> </li> </ul><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/func.html" class="_attribution-link">https://pytorch.org/docs/2.1/func.html</a>
  </p>
</div>
