<h1 id="torch-backends">torch.backends</h1> <p id="module-torch.backends"><code>torch.backends</code> controls the behavior of various backends that PyTorch supports.</p> <p>These backends include:</p> <ul class="simple"> <li><code>torch.backends.cpu</code></li> <li><code>torch.backends.cuda</code></li> <li><code>torch.backends.cudnn</code></li> <li><code>torch.backends.mps</code></li> <li><code>torch.backends.mkl</code></li> <li><code>torch.backends.mkldnn</code></li> <li><code>torch.backends.openmp</code></li> <li><code>torch.backends.opt_einsum</code></li> <li><code>torch.backends.xeon</code></li> </ul>  <h2 id="torch-backends-cpu">torch.backends.cpu</h2> <dl class="py function" id="module-torch.backends.cpu"> <dt class="sig sig-object py" id="torch.backends.cpu.get_cpu_capability">
<code>torch.backends.cpu.get_cpu_capability()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/cpu.html#get_cpu_capability"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns cpu capability as a string value.</p> <p>Possible values: - “DEFAULT” - “VSX” - “Z VECTOR” - “NO AVX” - “AVX2” - “AVX512”</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a></p> </dd> </dl> </dd>
</dl>   <h2 id="torch-backends-cuda">torch.backends.cuda</h2> <dl class="py function" id="module-torch.backends.cuda"> <dt class="sig sig-object py" id="torch.backends.cuda.is_built">
<code>torch.backends.cuda.is_built()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#is_built"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns whether PyTorch is built with CUDA support. Note that this doesn’t necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it.</p> </dd>
</dl> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.backends.cuda.matmul.allow_tf32">
<code>torch.backends.cuda.matmul.allow_tf32</code> </dt> <dd>
<p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a> that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/cuda.html#tf32-on-ampere"><span class="std std-ref">TensorFloat-32(TF32) on Ampere devices</span></a>.</p> </dd>
</dl> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction">
<code>torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction</code> </dt> <dd>
<p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a> that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs.</p> </dd>
</dl> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction">
<code>torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction</code> </dt> <dd>
<p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a> that controls whether reduced precision reductions are allowed with bf16 GEMMs.</p> </dd>
</dl> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.backends.cuda.cufft_plan_cache">
<code>torch.backends.cuda.cufft_plan_cache</code> </dt> <dd>
<p><code>cufft_plan_cache</code> contains the cuFFT plan caches for each CUDA device. Query a specific device <code>i</code>’s cache via <code>torch.backends.cuda.cufft_plan_cache[i]</code>.</p> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.backends.cuda.cufft_plan_cache.size">
<code>torch.backends.cuda.cufft_plan_cache.size</code> </dt> <dd>
<p>A readonly <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><code>int</code></a> that shows the number of plans currently in a cuFFT plan cache.</p> </dd>
</dl> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.backends.cuda.cufft_plan_cache.max_size">
<code>torch.backends.cuda.cufft_plan_cache.max_size</code> </dt> <dd>
<p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><code>int</code></a> that controls the capacity of a cuFFT plan cache.</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.backends.cuda.cufft_plan_cache.clear">
<code>torch.backends.cuda.cufft_plan_cache.clear()</code> </dt> <dd>
<p>Clears a cuFFT plan cache.</p> </dd>
</dl> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.backends.cuda.preferred_linalg_library">
<code>torch.backends.cuda.preferred_linalg_library(backend=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#preferred_linalg_library"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This flag is experimental and subject to change.</p> </div> <p>When PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries, and if both are available it decides which to use with a heuristic. This flag (a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><code>str</code></a>) allows overriding those heuristics.</p> <ul class="simple"> <li>If <code>“cusolver”</code> is set then cuSOLVER will be used wherever possible.</li> <li>If <code>“magma”</code> is set then MAGMA will be used wherever possible.</li> <li>If <code>“default”</code> (the default) is set then heuristics will be used to pick between cuSOLVER and MAGMA if both are available.</li> <li>When no input is given, this function returns the currently preferred library.</li> <li>User may use the environment variable TORCH_LINALG_PREFER_CUSOLVER=1 to set the preferred library to cuSOLVER globally. This flag only sets the initial value of the preferred library and the preferred library may still be overridden by this function call later in your script.</li> </ul> <p>Note: When a library is preferred other libraries may still be used if the preferred library doesn’t implement the operation(s) called. This flag may achieve better performance if PyTorch’s heuristic library selection is incorrect for your application’s inputs.</p> <p>Currently supported linalg operators:</p> <ul class="simple"> <li><a class="reference internal" href="generated/torch.linalg.inv.html#torch.linalg.inv" title="torch.linalg.inv"><code>torch.linalg.inv()</code></a></li> <li><a class="reference internal" href="generated/torch.linalg.inv_ex.html#torch.linalg.inv_ex" title="torch.linalg.inv_ex"><code>torch.linalg.inv_ex()</code></a></li> <li><a class="reference internal" href="generated/torch.linalg.cholesky.html#torch.linalg.cholesky" title="torch.linalg.cholesky"><code>torch.linalg.cholesky()</code></a></li> <li><a class="reference internal" href="generated/torch.linalg.cholesky_ex.html#torch.linalg.cholesky_ex" title="torch.linalg.cholesky_ex"><code>torch.linalg.cholesky_ex()</code></a></li> <li><a class="reference internal" href="generated/torch.cholesky_solve.html#torch.cholesky_solve" title="torch.cholesky_solve"><code>torch.cholesky_solve()</code></a></li> <li><a class="reference internal" href="generated/torch.cholesky_inverse.html#torch.cholesky_inverse" title="torch.cholesky_inverse"><code>torch.cholesky_inverse()</code></a></li> <li><a class="reference internal" href="generated/torch.linalg.lu_factor.html#torch.linalg.lu_factor" title="torch.linalg.lu_factor"><code>torch.linalg.lu_factor()</code></a></li> <li><a class="reference internal" href="generated/torch.linalg.lu.html#torch.linalg.lu" title="torch.linalg.lu"><code>torch.linalg.lu()</code></a></li> <li><a class="reference internal" href="generated/torch.linalg.lu_solve.html#torch.linalg.lu_solve" title="torch.linalg.lu_solve"><code>torch.linalg.lu_solve()</code></a></li> <li><a class="reference internal" href="generated/torch.linalg.qr.html#torch.linalg.qr" title="torch.linalg.qr"><code>torch.linalg.qr()</code></a></li> <li><a class="reference internal" href="generated/torch.linalg.eigh.html#torch.linalg.eigh" title="torch.linalg.eigh"><code>torch.linalg.eigh()</code></a></li> <li><code>torch.linalg.eighvals()</code></li> <li><a class="reference internal" href="generated/torch.linalg.svd.html#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a></li> <li><a class="reference internal" href="generated/torch.linalg.svdvals.html#torch.linalg.svdvals" title="torch.linalg.svdvals"><code>torch.linalg.svdvals()</code></a></li> </ul> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><em>_LinalgBackend</em></p> </dd> </dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.backends.cuda.SDPBackend">
<code>class torch.backends.cuda.SDPBackend(value)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#SDPBackend"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Enum class for the scaled dot product attention backends.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This class is in beta and subject to change.</p> </div> <p>This class needs to stay aligned with the enum defined in: pytorch/aten/src/ATen/native/transformers/sdp_utils_cpp.h</p> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.backends.cuda.flash_sdp_enabled">
<code>torch.backends.cuda.flash_sdp_enabled()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#flash_sdp_enabled"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This flag is beta and subject to change.</p> </div> <p>Returns whether flash scaled dot product attention is enabled or not.</p> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.backends.cuda.enable_mem_efficient_sdp">
<code>torch.backends.cuda.enable_mem_efficient_sdp(enabled)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#enable_mem_efficient_sdp"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This flag is beta and subject to change.</p> </div> <p>Enables or disables memory efficient scaled dot product attention.</p>  </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.backends.cuda.mem_efficient_sdp_enabled">
<code>torch.backends.cuda.mem_efficient_sdp_enabled()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#mem_efficient_sdp_enabled"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This flag is beta and subject to change.</p> </div> <p>Returns whether memory efficient scaled dot product attention is enabled or not.</p> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.backends.cuda.enable_flash_sdp">
<code>torch.backends.cuda.enable_flash_sdp(enabled)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#enable_flash_sdp"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This flag is beta and subject to change.</p> </div> <p>Enables or disables flash scaled dot product attention.</p>  </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.backends.cuda.math_sdp_enabled">
<code>torch.backends.cuda.math_sdp_enabled()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#math_sdp_enabled"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This flag is beta and subject to change.</p> </div> <p>Returns whether math scaled dot product attention is enabled or not.</p> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.backends.cuda.enable_math_sdp">
<code>torch.backends.cuda.enable_math_sdp(enabled)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#enable_math_sdp"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This flag is beta and subject to change.</p> </div> <p>Enables or disables math scaled dot product attention.</p>  </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.backends.cuda.sdp_kernel">
<code>torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=True, enable_mem_efficient=True)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/cuda.html#sdp_kernel"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This flag is beta and subject to change.</p> </div> <p>This context manager can be used to temporarily enable or disable any of the three backends for scaled dot product attention. Upon exiting the context manager, the previous state of the flags will be restored.</p>  </dd>
</dl>   <h2 id="torch-backends-cudnn">torch.backends.cudnn</h2> <dl class="py function" id="module-torch.backends.cudnn"> <dt class="sig sig-object py" id="torch.backends.cudnn.version">
<code>torch.backends.cudnn.version()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/cudnn.html#version"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the version of cuDNN</p> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.backends.cudnn.is_available">
<code>torch.backends.cudnn.is_available()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/cudnn.html#is_available"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a bool indicating if CUDNN is currently available.</p> </dd>
</dl> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.backends.cudnn.enabled">
<code>torch.backends.cudnn.enabled</code> </dt> <dd>
<p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a> that controls whether cuDNN is enabled.</p> </dd>
</dl> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.backends.cudnn.allow_tf32">
<code>torch.backends.cudnn.allow_tf32</code> </dt> <dd>
<p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a> that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/cuda.html#tf32-on-ampere"><span class="std std-ref">TensorFloat-32(TF32) on Ampere devices</span></a>.</p> </dd>
</dl> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.backends.cudnn.deterministic">
<code>torch.backends.cudnn.deterministic</code> </dt> <dd>
<p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a> that, if True, causes cuDNN to only use deterministic convolution algorithms. See also <a class="reference internal" href="generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled" title="torch.are_deterministic_algorithms_enabled"><code>torch.are_deterministic_algorithms_enabled()</code></a> and <a class="reference internal" href="generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms" title="torch.use_deterministic_algorithms"><code>torch.use_deterministic_algorithms()</code></a>.</p> </dd>
</dl> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.backends.cudnn.benchmark">
<code>torch.backends.cudnn.benchmark</code> </dt> <dd>
<p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a> that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest.</p> </dd>
</dl> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.backends.cudnn.benchmark_limit">
<code>torch.backends.cudnn.benchmark_limit</code> </dt> <dd>
<p>A <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><code>int</code></a> that specifies the maximum number of cuDNN convolution algorithms to try when <code>torch.backends.cudnn.benchmark</code> is True. Set <code>benchmark_limit</code> to zero to try every available algorithm. Note that this setting only affects convolutions dispatched via the cuDNN v8 API.</p> </dd>
</dl>   <h2 id="torch-backends-mps">torch.backends.mps</h2> <dl class="py function" id="module-torch.backends.mps"> <dt class="sig sig-object py" id="torch.backends.mps.is_available">
<code>torch.backends.mps.is_available()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/mps.html#is_available"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a bool indicating if MPS is currently available.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a></p> </dd> </dl> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.backends.mps.is_built">
<code>torch.backends.mps.is_built()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/mps.html#is_built"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns whether PyTorch is built with MPS support. Note that this doesn’t necessarily mean MPS is available; just that if this PyTorch binary were run a machine with working MPS drivers and devices, we would be able to use it.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a></p> </dd> </dl> </dd>
</dl>   <h2 id="torch-backends-mkl">torch.backends.mkl</h2> <dl class="py function" id="module-torch.backends.mkl"> <dt class="sig sig-object py" id="torch.backends.mkl.is_available">
<code>torch.backends.mkl.is_available()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/mkl.html#is_available"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns whether PyTorch is built with MKL support.</p> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.backends.mkl.verbose">
<code>class torch.backends.mkl.verbose(enable)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/mkl.html#verbose"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>On-demand oneMKL verbosing functionality To make it easier to debug performance issues, oneMKL can dump verbose messages containing execution information like duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named <code>MKL_VERBOSE</code>. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.</p> <pre data-language="python">import torch
model(data)
with torch.backends.mkl.verbose(torch.backends.mkl.VERBOSE_ON):
    model(data)
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>level</strong> – Verbose level - <code>VERBOSE_OFF</code>: Disable verbosing - <code>VERBOSE_ON</code>: Enable verbosing</p> </dd> </dl> </dd>
</dl>   <h2 id="torch-backends-mkldnn">torch.backends.mkldnn</h2> <dl class="py function" id="module-torch.backends.mkldnn"> <dt class="sig sig-object py" id="torch.backends.mkldnn.is_available">
<code>torch.backends.mkldnn.is_available()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/mkldnn.html#is_available"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns whether PyTorch is built with MKL-DNN support.</p> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.backends.mkldnn.verbose">
<code>class torch.backends.mkldnn.verbose(level)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/mkldnn.html#verbose"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>On-demand oneDNN (former MKL-DNN) verbosing functionality To make it easier to debug performance issues, oneDNN can dump verbose messages containing information like kernel size, input data size and execution duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named <code>DNNL_VERBOSE</code>. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.</p> <pre data-language="python">import torch
model(data)
with torch.backends.mkldnn.verbose(torch.backends.mkldnn.VERBOSE_ON):
    model(data)
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>level</strong> – Verbose level - <code>VERBOSE_OFF</code>: Disable verbosing - <code>VERBOSE_ON</code>: Enable verbosing - <code>VERBOSE_ON_CREATION</code>: Enable verbosing, including oneDNN kernel creation</p> </dd> </dl> </dd>
</dl>   <h2 id="torch-backends-openmp">torch.backends.openmp</h2> <dl class="py function" id="module-torch.backends.openmp"> <dt class="sig sig-object py" id="torch.backends.openmp.is_available">
<code>torch.backends.openmp.is_available()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/openmp.html#is_available"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns whether PyTorch is built with OpenMP support.</p> </dd>
</dl>   <h2 id="torch-backends-opt-einsum">torch.backends.opt_einsum</h2> <dl class="py function" id="module-torch.backends.opt_einsum"> <dt class="sig sig-object py" id="torch.backends.opt_einsum.is_available">
<code>torch.backends.opt_einsum.is_available()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/opt_einsum.html#is_available"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a bool indicating if opt_einsum is currently available.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a></p> </dd> </dl> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.backends.opt_einsum.get_opt_einsum">
<code>torch.backends.opt_einsum.get_opt_einsum()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/backends/opt_einsum.html#get_opt_einsum"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the opt_einsum package if opt_einsum is currently available, else None.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">Any</a></p> </dd> </dl> </dd>
</dl> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.backends.opt_einsum.enabled">
<code>torch.backends.opt_einsum.enabled</code> </dt> <dd>
<p>A :class:<code>bool</code> that controls whether opt_einsum is enabled (<code>True</code> by default). If so, torch.einsum will use opt_einsum (<a class="reference external" href="https://optimized-einsum.readthedocs.io/en/stable/path_finding.html">https://optimized-einsum.readthedocs.io/en/stable/path_finding.html</a>) if available to calculate an optimal path of contraction for faster performance.</p> <p>If opt_einsum is not available, torch.einsum will fall back to the default contraction path of left to right.</p> </dd>
</dl> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.backends.opt_einsum.strategy">
<code>torch.backends.opt_einsum.strategy</code> </dt> <dd>
<p>A :class:<code>str</code> that specifies which strategies to try when <code>torch.backends.opt_einsum.enabled</code> is <code>True</code>. By default, torch.einsum will try the “auto” strategy, but the “greedy” and “optimal” strategies are also supported. Note that the “optimal” strategy is factorial on the number of inputs as it tries all possible paths. See more details in opt_einsum’s docs (<a class="reference external" href="https://optimized-einsum.readthedocs.io/en/stable/path_finding.html">https://optimized-einsum.readthedocs.io/en/stable/path_finding.html</a>).</p> </dd>
</dl>   <h2 id="torch-backends-xeon">torch.backends.xeon</h2><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/backends.html" class="_attribution-link">https://pytorch.org/docs/2.1/backends.html</a>
  </p>
</div>
