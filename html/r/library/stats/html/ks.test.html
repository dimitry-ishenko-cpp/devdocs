<div class="container">
<main>  <h2>
<code>ks.test</code> Kolmogorov-Smirnov Tests</h2> <h3>Description</h3> <p>Perform a one- or two-sample Kolmogorov-Smirnov test. </p> <h3>Usage</h3> <pre data-language="r"><code class="language-R">ks.test(x, ...)
## Default S3 method:
ks.test(x, y, ...,
        alternative = c("two.sided", "less", "greater"),
        exact = NULL, simulate.p.value = FALSE, B = 2000)
## S3 method for class 'formula'
ks.test(formula, data, subset, na.action, ...)
</code></pre> <h3>Arguments</h3> <table> <tr>
<td><code id="x">x</code></td> <td> <p>a numeric vector of data values.</p> </td>
</tr> <tr>
<td><code id="y">y</code></td> <td> <p>either a numeric vector of data values, or a character string naming a cumulative distribution function or an actual cumulative distribution function such as <code>pnorm</code>. Only continuous CDFs are valid.</p> </td>
</tr> <tr>
<td><code id="...">...</code></td> <td> <p>for the default method, parameters of the distribution specified (as a character string) by <code>y</code>. Otherwise, further arguments to be passed to or from methods.</p> </td>
</tr> <tr>
<td><code id="alternative">alternative</code></td> <td> <p>indicates the alternative hypothesis and must be one of <code>"two.sided"</code> (default), <code>"less"</code>, or <code>"greater"</code>. You can specify just the initial letter of the value, but the argument name must be given in full. See ‘Details’ for the meanings of the possible values.</p> </td>
</tr> <tr>
<td><code id="exact">exact</code></td> <td> <p><code>NULL</code> or a logical indicating whether an exact p-value should be computed. See ‘Details’ for the meaning of <code>NULL</code>.</p> </td>
</tr> <tr>
<td><code id="simulate.p.value">simulate.p.value</code></td> <td> <p>a logical indicating whether to compute p-values by Monte Carlo simulation. (Ignored for the one-sample test.)</p> </td>
</tr> <tr>
<td><code id="B">B</code></td> <td> <p>an integer specifying the number of replicates used in the Monte Carlo test.</p> </td>
</tr> <tr>
<td><code id="formula">formula</code></td> <td> <p>a formula of the form <code>lhs ~ rhs</code> where <code>lhs</code> is a numeric variable giving the data values and <code>rhs</code> either <code>1</code> for a one-sample test or a factor with two levels giving the corresponding groups for a two-sample test.</p> </td>
</tr> <tr>
<td><code id="data">data</code></td> <td> <p>an optional matrix or data frame (or similar: see <code><a href="model.frame.html">model.frame</a></code>) containing the variables in the formula <code>formula</code>. By default the variables are taken from <code>environment(formula)</code>.</p> </td>
</tr> <tr>
<td><code id="subset">subset</code></td> <td> <p>an optional vector specifying a subset of observations to be used.</p> </td>
</tr> <tr>
<td><code id="na.action">na.action</code></td> <td> <p>a function which indicates what should happen when the data contain <code>NA</code>s. Defaults to <code>getOption("na.action")</code>.</p> </td>
</tr> </table> <h3>Details</h3> <p>If <code>y</code> is numeric, a two-sample (Smirnov) test of the null hypothesis that <code>x</code> and <code>y</code> were drawn from the same distribution is performed. </p> <p>Alternatively, <code>y</code> can be a character string naming a continuous (cumulative) distribution function, or such a function. In this case, a one-sample (Kolmogorov) test is carried out of the null that the distribution function which generated <code>x</code> is distribution <code>y</code> with parameters specified by <code>...</code>. The presence of ties always generates a warning in the one-sample case, as continuous distributions do not generate them. If the ties arose from rounding the tests may be approximately valid, but even modest amounts of rounding can have a significant effect on the calculated statistic. </p> <p>Missing values are silently omitted from <code>x</code> and (in the two-sample case) <code>y</code>. </p> <p>The possible values <code>"two.sided"</code>, <code>"less"</code> and <code>"greater"</code> of <code>alternative</code> specify the null hypothesis that the true cumulative distribution function (CDF) of <code>x</code> is equal to, not less than or not greater than the hypothesized CDF (one-sample case) or the CDF of <code>y</code> (two-sample case), respectively. The test compares the CDFs taking their maximal difference as test statistic, with the statistic in the <code>"greater"</code> alternative being <code class="reqn">D^+ = \max_u [ F_x(u) - F_y(u) ]</code>. Thus in the two-sample case <code>alternative = "greater"</code> includes distributions for which <code>x</code> is stochastically <em>smaller</em> than <code>y</code> (the CDF of <code>x</code> lies above and hence to the left of that for <code>y</code>), in contrast to <code><a href="t.test.html">t.test</a></code> or <code><a href="wilcox.test.html">wilcox.test</a></code>. </p> <p>Exact p-values are not available for the one-sample case in the presence of ties. If <code>exact = NULL</code> (the default), an exact p-value is computed if the sample size is less than 100 in the one-sample case <em>and there are no ties</em>, and if the product of the sample sizes is less than 10000 in the two-sample case, with or without ties (using the algorithm described in Schröer and Trenkler (1995)). Otherwise, the p-value is computed via Monte Carlo simulation in the two-sample case if <code>simulate.p.value</code> is <code>TRUE</code>, or else asymptotic distributions are used whose approximations may be inaccurate in small samples. In the one-sample two-sided case, exact p-values are obtained as described in Marsaglia, Tsang &amp; Wang (2003) (but not using the optional approximation in the right tail, so this can be slow for small p-values). The formula of Birnbaum &amp; Tingey (1951) is used for the one-sample one-sided case. </p> <p>If a one-sample test is used, the parameters specified in <code>...</code> must be pre-specified and not estimated from the data. There is some more refined distribution theory for the KS test with estimated parameters (see Durbin, 1973), but that is not implemented in <code>ks.test</code>. </p> <h3>Value</h3> <p>A list inheriting from classes <code>"ks.test"</code> and <code>"htest"</code> containing the following components: </p> <table> <tr>
<td><code>statistic</code></td> <td> <p>the value of the test statistic.</p> </td>
</tr> <tr>
<td><code>p.value</code></td> <td> <p>the p-value of the test.</p> </td>
</tr> <tr>
<td><code>alternative</code></td> <td> <p>a character string describing the alternative hypothesis.</p> </td>
</tr> <tr>
<td><code>method</code></td> <td> <p>a character string indicating what type of test was performed.</p> </td>
</tr> <tr>
<td><code>data.name</code></td> <td> <p>a character string giving the name(s) of the data.</p> </td>
</tr> </table> <h3>Source</h3> <p>The two-sided one-sample distribution comes <em>via</em> Marsaglia, Tsang and Wang (2003). </p> <p>Exact distributions for the two-sample (Smirnov) test are computed by the algorithm proposed by Schröer (1991) and Schröer &amp; Trenkler (1995) using numerical improvements along the lines of Viehmann (2021). </p> <h3>References</h3> <p>Z. W. Birnbaum and Fred H. Tingey (1951). One-sided confidence contours for probability distribution functions. <em>The Annals of Mathematical Statistics</em>, <b>22</b>/4, 592–596. <a href="https://doi.org/10.1214/aoms/1177729550">doi:10.1214/aoms/1177729550</a>. </p> <p>William J. Conover (1971). <em>Practical Nonparametric Statistics</em>. New York: John Wiley &amp; Sons. Pages 295–301 (one-sample Kolmogorov test), 309–314 (two-sample Smirnov test). </p> <p>Durbin, J. (1973). <em>Distribution theory for tests based on the sample distribution function</em>. SIAM. </p> <p>W. Feller (1948). On the Kolmogorov-Smirnov limit theorems for empirical distributions. <em>The Annals of Mathematical Statistics</em>, <b>19</b>(2), 177–189. <a href="https://doi.org/10.1214/aoms/1177730243">doi:10.1214/aoms/1177730243</a>. </p> <p>George Marsaglia, Wai Wan Tsang and Jingbo Wang (2003). Evaluating Kolmogorov's distribution. <em>Journal of Statistical Software</em>, <b>8</b>/18. <a href="https://doi.org/10.18637/jss.v008.i18">doi:10.18637/jss.v008.i18</a>. </p> <p>Gunar Schröer (1991). Computergestützte statistische Inferenz am Beispiel der Kolmogorov-Smirnov Tests. Diplomarbeit Universität Osnabrück. </p> <p>Gunar Schröer and Dietrich Trenkler (1995). Exact and Randomization Distributions of Kolmogorov-Smirnov Tests for Two or Three Samples. <em>Computational Statistics &amp; Data Analysis</em>, <b>20</b>(2), 185–202. <a href="https://doi.org/10.1016/0167-9473%2894%2900040-P">doi:10.1016/0167-9473(94)00040-P</a>. </p> <p>Thomas Viehmann (2021). Numerically more stable computation of the p-values for the two-sample Kolmogorov-Smirnov test. <a href="https://arxiv.org/abs/2102.08037">https://arxiv.org/abs/2102.08037</a>. </p> <h3>See Also</h3> <p><code><a href="smirnov.html">psmirnov</a></code>. </p> <p><code><a href="shapiro.test.html">shapiro.test</a></code> which performs the Shapiro-Wilk test for normality. </p> <h3>Examples</h3> <pre data-language="r"><code class="language-R">require("graphics")

x &lt;- rnorm(50)
y &lt;- runif(30)
# Do x and y come from the same distribution?
ks.test(x, y)
# Does x come from a shifted gamma distribution with shape 3 and rate 2?
ks.test(x+2, "pgamma", 3, 2) # two-sided, exact
ks.test(x+2, "pgamma", 3, 2, exact = FALSE)
ks.test(x+2, "pgamma", 3, 2, alternative = "gr")

# test if x is stochastically larger than x2
x2 &lt;- rnorm(50, -1)
plot(ecdf(x), xlim = range(c(x, x2)))
plot(ecdf(x2), add = TRUE, lty = "dashed")
t.test(x, x2, alternative = "g")
wilcox.test(x, x2, alternative = "g")
ks.test(x, x2, alternative = "l")

# with ties, example from Schröer and Trenkler (1995)
# D = 3/7, p = 8/33 = 0.242424..
ks.test(c(1, 2, 2, 3, 3),
        c(1, 2, 3, 3, 4, 5, 6))# -&gt; exact

# formula interface, see ?wilcox.test
ks.test(Ozone ~ Month, data = airquality,
        subset = Month %in% c(5, 8))
</code></pre> </main> </div><div class="_attribution">
  <p class="_attribution-p">
    Copyright (&copy;) 1999–2012 R Foundation for Statistical Computing.<br>Licensed under the <a href="https://www.gnu.org/copyleft/gpl.html">GNU General Public License</a>.<br>
    
  </p>
</div>
