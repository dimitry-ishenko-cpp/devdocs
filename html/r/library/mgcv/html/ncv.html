<div class="container">
<main>  <h2>
<code>NCV</code> Neighbourhood Cross Validation</h2> <h3>Description</h3> <p>NCV estimates smoothing parameters by optimizing the average ability of a model to predict subsets of data when subsets of data are omitted from fitting. Usually the predicted subset is a subset of the omitted subset. If both subsets are the same single datapoint, and the average is over all datapoints, then NCV is leave-one-out cross validation. QNCV is a quadratic approximation to NCV, guaranteed finite for any family link combination. </p> <p>In detail, suppose that a model is estimated by minimizing a penalized loss </p> <p style="text-align: center;"><code class="reqn">\sum_i D(y_i,\theta_i) + \sum_j \lambda_j \beta^{\sf T} {S}_j \beta </code> </p> <p>where <code class="reqn">D</code> is a loss (such as a negative log likelihood), dependent on response <code class="reqn">y_i</code> and parameter vector <code class="reqn">\theta_i</code>, which in turn depends on covariates via one or more smooth linear predictors with coefficients <code class="reqn">\beta</code>. The quadratic penalty terms penalize model complexity: <code class="reqn">S_j</code> is a known matrix and <code class="reqn">\lambda_j</code> an unknown smoothing parameter. Given smoothing parameters the penalized loss is readily minimized to estimate <code class="reqn">\beta</code>. </p> <p>The smoothing parameters also have to be estimated. To this end, choose <code class="reqn">k = 1,\ldots,m</code> subsets <code class="reqn">\alpha(k)\subset \{1,\ldots,n\}</code> and <code class="reqn">\delta(k)\subset \{1,\ldots,n\}</code>. Usually <code class="reqn">\delta(k)</code> is a subset of (or equal to) <code class="reqn">\alpha(k)</code>. Let <code class="reqn">\theta_i^{\alpha(k)}</code> denote the estimate of <code class="reqn">\theta_i</code> when the points indexed by <code class="reqn">\alpha(k)</code> are omitted from fitting. Then the NCV criterion </p> <p style="text-align: center;"><code class="reqn">V = \sum_{k=1}^m \sum_{i \in \alpha(k)} D(y_i,\theta_i^{\alpha(k)})</code> </p> <p>is minimized w.r.t. the smoothing parameters, <code class="reqn">\lambda_j</code>. If <code class="reqn">m=n</code> and <code class="reqn">\alpha(k)=\delta(k)=k</code> then ordinary leave-one-out cross validation is recovered. This formulation covers many of the variants of cross validation reviewed in Arlot and Celisse (2010), for example. </p> <p>Except for a quadratic loss, <code class="reqn">V</code> can not be computed exactly, but it can be computed to <code class="reqn">O(n^{-2})</code> accuracy (fixed basis size), by taking single Newton optimization steps from the full data <code class="reqn">\beta</code> estimates to the equivalent when each <code class="reqn">\alpha(k)</code> is dropped. This is what <code>mgcv</code> does. The Newton steps require update of the full model Hessian to the equivalent when each datum is dropped. This can be achieved at <code class="reqn">O(p^2)</code> cost, where <code class="reqn">p</code> is the dimension of <code class="reqn">\beta</code>. Hence, for example, the ordinary cross validation criterion is computable at the <code class="reqn">O(np^2)</code> cost of estimating the model given smoothing parameters. </p> <p>The NCV score computed in this way is optimized using a BFGS quasi-Newton method, adapted to the case in which smoothing parameters tending to infinity may cause indefiniteness. </p> <h3>Spatial and temporal short range autocorrelation</h3> <p>A routine applied problem is that smoothing parameters tend to be underestimated in the presence of un-modelled short range autocorrelation, as the smooths try to fit the local excursions in the data caused by the local autocorrelation. Cross validation will tend to 'fit the noise' when there is autocorellation, since a model that fits the noise in the data correlated with an omitted datum, will also tend to closely fit the noise in the omitted datum, because of the correlation. That is autocorrelation works against the avoidance of overfit that cross validation seeks to achieve. </p> <p>For short range autocorrelation the problems can be avoided, or at least mitigated, by predicting each datum when all the data in its ‘local’ neighbourhood are omitted. The neighbourhoods being constructed in order that un-modelled correlation is minimized between the point of interest and points outside its neighbourhood. That is we set <code class="reqn">m=n</code>, <code class="reqn">\delta(k)=k</code> and <code class="reqn">\alpha(k) = {\tt nei}(k)</code>, where <code>nei(k)</code> are the indices of the neighbours of point <code class="reqn">k</code>. This approach has been known for a long time (e.g. Chu and Marron, 1991; Robert et al. 2017), but was previously rather too expensive for regular use for smoothing parameter estimation.</p> <h3>Specifying the neighbourhoods</h3> <p>The neighbourhood subsets <code class="reqn">\alpha(k)</code> and <code class="reqn">\delta(k)</code> have to be supplied to <code><a href="gam.html">gam</a></code>, and the <code>nei</code> argument does this. It is a list with the following arguments. </p> <ul> <li> <p><code>k</code> is the vector of indices to be dropped for each neighbourhood. </p> </li> <li> <p><code>m</code> gives the end of each neighbourhood. So <code>nei$k[(nei$m[j-1]+1):nei$m[j]]</code> gives the points dropped for the neighbourhood <code>j</code>: that is <code class="reqn">\alpha(j)</code>. </p> </li> <li> <p><code>i</code> is the vector of indices of points to predict. </p> </li> <li> <p><code>mi</code> gives the corresponding endpoints <code>mi</code>. So <code>nei$i[(nei$mi[j-1]+1):nei$mi[j]]</code> indexes the points to predict for neighbourhood j: that is <code class="reqn">\delta(j)</code>. </p> </li> <li> <p><code>jackknife</code> is an optional element. If supplied and <code>TRUE</code> then variance estimates are based on the raw Jackkife estimate, if <code>FALSE</code> then on the standard Bayesian results. If not supplied (usual) then an estimator accounting for the neighbourhood structure is used, that largely accounts for any correlation present within neighbourhoods. <code>jackknife</code> is ignored if NCV is being calculated for a model where another method is used for smoothing parameter selection. </p> </li>
</ul> <p>If <code>nei==NULL</code> (or <code>k</code> or <code>m</code> are missing) then leave-one-out cross validation is used. If <code>nei</code> is supplied but NCV is not selected as the smoothing parameter estimation method, then it is simply computed (but not optimized). </p> <h3>Numerical issues</h3> <p>If a model is specified in which some coefficient values, <code class="reqn">\beta</code>, have non-finite likelihood then the NCV criterion computed with single Newton steps could also be non-finite. A simple fix replaces the NCV criterion with a quadratic approximation to the criterion around the full data fit. The quadratic approximation is always finite. This 'QNCV' is essential for some families, such as <code><a href="gevlss.html">gevlss</a></code>. </p> <p>Although the leading order cost of NCV is the same as REML or GCV, the actual cost is higher because the dominant operations costs are in matrix-vector, rather than matrix-matrix, operations, so BLAS speed ups are small. However multi-core computing is worthwhile for NCV. See the option <code>ncv.threads</code> in <code><a href="gam.control.html">gam.control</a></code>. </p> <h3>Author(s)</h3> <p> Simon N. Wood <a href="mailto:simon.wood@r-project.org.html">simon.wood@r-project.org</a></p> <h3>References</h3> <p>Chu and Marron (1991) Comparison of two bandwidth selectors with dependent errors. The Annals of Statistics. 19, 1906-1918 </p> <p>Arlot, S. and A. Celisse (2010). A survey of cross-validation procedures for model selection. Statistics Surveys 4, 40-79 </p> <p>Roberts et al. (2017) Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure. Ecography 40(8), 913-929. </p> <p>Wood S.N. (2023) On Neighbourhood Cross Validation. in prep. </p> <h3>Examples</h3> <pre data-language="r"><code class="language-R">require(mgcv)
nei.cor &lt;- function(h,n) { ## construct nei structure
  nei &lt;- list(mi=1:n,i=1:n)
  nei$m &lt;- cumsum(c((h+1):(2*h+1),rep(2*h+1,n-2*h-2),(2*h+1):(h+1)))
  k0 &lt;- rep(0,0); if (h&gt;0) for (i in 1:h) k0 &lt;- c(k0,1:(h+i))
  k1 &lt;- n-k0[length(k0):1]+1
  nei$k &lt;- c(k0,1:(2*h+1)+rep(0:(n-2*h-1),each=2*h+1),k1)
  nei
}
set.seed(1)
n &lt;- 500;sig &lt;- .6
x &lt;- 0:(n-1)/(n-1)
f &lt;- sin(4*pi*x)*exp(-x*2)*5/2
e &lt;- rnorm(n,0,sig)
for (i in 2:n) e[i] &lt;- 0.6*e[i-1] + e[i]
y &lt;- f + e ## autocorrelated data
nei &lt;- nei.cor(4,n) ## construct neighbourhoods to mitigate 
b0 &lt;- gam(y~s(x,k=40)) ## GCV based fit
gc &lt;- gam.control(ncv.threads=2)
b1 &lt;- gam(y~s(x,k=40),method="NCV",nei=nei,control=gc)
## use "QNCV", which is identical here...
b2 &lt;- gam(y~s(x,k=40),method="QNCV",nei=nei,control=gc)
## plot GCV and NCV based fits...
f &lt;- f - mean(f)
par(mfrow=c(1,2))
plot(b0,rug=FALSE,scheme=1);lines(x,f,col=2)
plot(b1,rug=FALSE,scheme=1);lines(x,f,col=2)
</code></pre> </main> </div><div class="_attribution">
  <p class="_attribution-p">
    Copyright (&copy;) 1999–2012 R Foundation for Statistical Computing.<br>Licensed under the <a href="https://www.gnu.org/copyleft/gpl.html">GNU General Public License</a>.<br>
    
  </p>
</div>
