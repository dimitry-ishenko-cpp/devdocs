<div class="container">
<main>  <h2>
<code>one.se.rule</code> The one standard error rule for smoother models</h2> <h3>Description</h3> <p> The ‘one standard error rule’ (see e.g. Hastie, Tibshirani and Friedman, 2009) is a way of producing smoother models than those directly estimated by automatic smoothing parameter selection methods. In the single smoothing parameter case, we select the largest smoothing parameter within one standard error of the optimum of the smoothing parameter selection criterion. This approach can be generalized to multiple smoothing parameters estimated by REML or ML.</p> <h3>Details</h3> <p>Under REML or ML smoothing parameter selection an asyptotic distributional approximation is available for the log smoothing parameters. Let <code class="reqn">\rho</code> denote the log smoothing parameters that we want to increase to obtain a smoother model. The large sample distribution of the estimator of <code class="reqn">\rho</code> is <code class="reqn">N(\rho,V)</code> where <code class="reqn">V</code> is the matrix returned by <code><a href="sp.vcov.html">sp.vcov</a></code>. Drop any elements of <code class="reqn">\rho</code> that are already at ‘effective infinity’, along with the corresponding rows and columns of <code class="reqn">V</code>. The standard errors of the log smoothing parameters can be obtained from the leading diagonal of <code class="reqn">V</code>. Let the vector of these be <code class="reqn">d</code>. Now suppose that we want to increase the estimated log smoothing parameters by an amount <code class="reqn">\alpha d</code>. We choose <code class="reqn">\alpha</code> so that <code class="reqn">\alpha d^T V^{-1}d = \sqrt{2p}</code>, where p is the dimension of d and 2p the variance of a chi-squared r.v. with p degrees of freedom. </p> <p>The idea is that we increase the log smoothing parameters in proportion to their standard deviation, until the RE/ML is increased by 1 standard deviation according to its asypmtotic distribution. </p> <h3>Author(s)</h3> <p>Simon N. Wood <a href="mailto:simon.wood@r-project.org.html">simon.wood@r-project.org</a> </p> <h3>References</h3> <p>Hastie, T, R. Tibshirani and J. Friedman (2009) The Elements of Statistical Learning 2nd ed. Springer.</p> <h3>See Also</h3> <p><code><a href="gam.html">gam</a></code></p> <h3>Examples</h3> <pre data-language="r"><code class="language-R"> 
require(mgcv)
set.seed(2) ## simulate some data...
dat &lt;- gamSim(1,n=400,dist="normal",scale=2)
b &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="REML")
b
## only the first 3 smoothing parameters are candidates for
## increasing here...
V &lt;- sp.vcov(b)[1:3,1:3] ## the approx cov matrix of sps
d &lt;- diag(V)^.5          ## sp se.
## compute the log smoothing parameter step...
d &lt;- sqrt(2*length(d))/d
sp &lt;- b$sp ## extract original sp estimates
sp[1:3] &lt;- sp[1:3]*exp(d) ## apply the step
## refit with the increased smoothing parameters...
b1 &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="REML",sp=sp)
b;b1 ## compare fits
</code></pre> </main> </div><div class="_attribution">
  <p class="_attribution-p">
    Copyright (&copy;) 1999–2012 R Foundation for Statistical Computing.<br>Licensed under the <a href="https://www.gnu.org/copyleft/gpl.html">GNU General Public License</a>.<br>
    
  </p>
</div>
