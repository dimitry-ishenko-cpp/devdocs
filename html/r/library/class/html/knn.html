<div class="container">
<main>  <h2>
<code>knn</code>  k-Nearest Neighbour Classification </h2> <h3>Description</h3> <p>k-nearest neighbour classification for test set from training set. For each row of the test set, the <code>k</code> nearest (in Euclidean distance) training set vectors are found, and the classification is decided by majority vote, with ties broken at random. If there are ties for the <code>k</code>th nearest vector, all candidates are included in the vote. </p> <h3>Usage</h3> <pre data-language="r"><code class="language-R">knn(train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE)
</code></pre> <h3>Arguments</h3> <table> <tr>
<td><code id="train">train</code></td> <td> <p>matrix or data frame of training set cases. </p> </td>
</tr> <tr>
<td><code id="test">test</code></td> <td> <p>matrix or data frame of test set cases. A vector will be interpreted as a row vector for a single case. </p> </td>
</tr> <tr>
<td><code id="cl">cl</code></td> <td> <p>factor of true classifications of training set </p> </td>
</tr> <tr>
<td><code id="k">k</code></td> <td> <p>number of neighbours considered. </p> </td>
</tr> <tr>
<td><code id="l">l</code></td> <td> <p>minimum vote for definite decision, otherwise <code>doubt</code>. (More precisely, less than <code>k-l</code> dissenting votes are allowed, even if <code>k</code> is increased by ties.) </p> </td>
</tr> <tr>
<td><code id="prob">prob</code></td> <td> <p>If this is true, the proportion of the votes for the winning class are returned as attribute <code>prob</code>. </p> </td>
</tr> <tr>
<td><code id="use.all">use.all</code></td> <td> <p>controls handling of ties. If true, all distances equal to the <code>k</code>th largest are included. If false, a random selection of distances equal to the <code>k</code>th is chosen to use exactly <code>k</code> neighbours. </p> </td>
</tr>
</table> <h3>Value</h3> <p>Factor of classifications of test set. <code>doubt</code> will be returned as <code>NA</code>. </p> <h3>References</h3> <p>Ripley, B. D. (1996) <em>Pattern Recognition and Neural Networks.</em> Cambridge. </p> <p>Venables, W. N. and Ripley, B. D. (2002) <em>Modern Applied Statistics with S.</em> Fourth edition. Springer. </p> <h3>See Also</h3> <p><code><a href="knn1.html">knn1</a></code>, <code><a href="knn.cv.html">knn.cv</a></code> </p> <h3>Examples</h3> <pre data-language="r"><code class="language-R">train &lt;- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
test &lt;- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
cl &lt;- factor(c(rep("s",25), rep("c",25), rep("v",25)))
knn(train, test, cl, k = 3, prob=TRUE)
attributes(.Last.value)
</code></pre> </main> </div><div class="_attribution">
  <p class="_attribution-p">
    Copyright (&copy;) 1999â€“2012 R Foundation for Statistical Computing.<br>Licensed under the <a href="https://www.gnu.org/copyleft/gpl.html">GNU General Public License</a>.<br>
    
  </p>
</div>
