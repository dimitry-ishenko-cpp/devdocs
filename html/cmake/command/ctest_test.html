<h1 id="command:ctest_test">ctest_test</h1> <p>Perform the <a class="reference internal" href="../manual/ctest.1.html#ctest-test-step"><span class="std std-ref">CTest Test Step</span></a> as a <a class="reference internal" href="../manual/ctest.1.html#dashboard-client"><span class="std std-ref">Dashboard Client</span></a>.</p> <pre data-language="cmake">ctest_test([BUILD &lt;build-dir&gt;] [APPEND]
           [START &lt;start-number&gt;]
           [END &lt;end-number&gt;]
           [STRIDE &lt;stride-number&gt;]
           [EXCLUDE &lt;exclude-regex&gt;]
           [INCLUDE &lt;include-regex&gt;]
           [EXCLUDE_LABEL &lt;label-exclude-regex&gt;]
           [INCLUDE_LABEL &lt;label-include-regex&gt;]
           [EXCLUDE_FIXTURE &lt;regex&gt;]
           [EXCLUDE_FIXTURE_SETUP &lt;regex&gt;]
           [EXCLUDE_FIXTURE_CLEANUP &lt;regex&gt;]
           [PARALLEL_LEVEL &lt;level&gt;]
           [RESOURCE_SPEC_FILE &lt;file&gt;]
           [TEST_LOAD &lt;threshold&gt;]
           [SCHEDULE_RANDOM &lt;ON|OFF&gt;]
           [STOP_ON_FAILURE]
           [STOP_TIME &lt;time-of-day&gt;]
           [RETURN_VALUE &lt;result-var&gt;]
           [CAPTURE_CMAKE_ERROR &lt;result-var&gt;]
           [REPEAT &lt;mode&gt;:&lt;n&gt;]
           [OUTPUT_JUNIT &lt;file&gt;]
           [QUIET]
           )
</pre> <p>Run tests in the project build tree and store results in <code>Test.xml</code> for submission with the <a class="reference internal" href="ctest_submit.html#command:ctest_submit" title="ctest_submit" id="index-0-command:ctest_submit"><code>ctest_submit()</code></a> command.</p> <p>The options are:</p> <dl> <dt>
<code>BUILD &lt;build-dir&gt;</code> </dt>
<dd>
<p>Specify the top-level build directory. If not given, the <a class="reference internal" href="../variable/ctest_binary_directory.html#variable:CTEST_BINARY_DIRECTORY" title="CTEST_BINARY_DIRECTORY" id="index-0-variable:CTEST_BINARY_DIRECTORY"><code>CTEST_BINARY_DIRECTORY</code></a> variable is used.</p> </dd> <dt>
<code>APPEND</code> </dt>
<dd>
<p>Mark <code>Test.xml</code> for append to results previously submitted to a dashboard server since the last <a class="reference internal" href="ctest_start.html#command:ctest_start" title="ctest_start" id="index-0-command:ctest_start"><code>ctest_start()</code></a> call. Append semantics are defined by the dashboard server in use. This does <em>not</em> cause results to be appended to a <code>.xml</code> file produced by a previous call to this command.</p> </dd> <dt>
<code>START &lt;start-number&gt;</code> </dt>
<dd>
<p>Specify the beginning of a range of test numbers.</p> </dd> <dt>
<code>END &lt;end-number&gt;</code> </dt>
<dd>
<p>Specify the end of a range of test numbers.</p> </dd> <dt>
<code>STRIDE &lt;stride-number&gt;</code> </dt>
<dd>
<p>Specify the stride by which to step across a range of test numbers.</p> </dd> <dt>
<code>EXCLUDE &lt;exclude-regex&gt;</code> </dt>
<dd>
<p>Specify a regular expression matching test names to exclude.</p> </dd> <dt>
<code>INCLUDE &lt;include-regex&gt;</code> </dt>
<dd>
<p>Specify a regular expression matching test names to include. Tests not matching this expression are excluded.</p> </dd> <dt>
<code>EXCLUDE_LABEL &lt;label-exclude-regex&gt;</code> </dt>
<dd>
<p>Specify a regular expression matching test labels to exclude.</p> </dd> <dt>
<code>INCLUDE_LABEL &lt;label-include-regex&gt;</code> </dt>
<dd>
<p>Specify a regular expression matching test labels to include. Tests not matching this expression are excluded.</p> </dd> <dt>
<code>EXCLUDE_FIXTURE &lt;regex&gt;</code> </dt>
<dd>
<div class="versionadded"> <p><span class="versionmodified added">New in version 3.7.</span></p> </div> <p>If a test in the set of tests to be executed requires a particular fixture, that fixture's setup and cleanup tests would normally be added to the test set automatically. This option prevents adding setup or cleanup tests for fixtures matching the <code>&lt;regex&gt;</code>. Note that all other fixture behavior is retained, including test dependencies and skipping tests that have fixture setup tests that fail.</p> </dd> <dt>
<code>EXCLUDE_FIXTURE_SETUP &lt;regex&gt;</code> </dt>
<dd>
<div class="versionadded"> <p><span class="versionmodified added">New in version 3.7.</span></p> </div> <p>Same as <code>EXCLUDE_FIXTURE</code> except only matching setup tests are excluded.</p> </dd> <dt>
<code>EXCLUDE_FIXTURE_CLEANUP &lt;regex&gt;</code> </dt>
<dd>
<div class="versionadded"> <p><span class="versionmodified added">New in version 3.7.</span></p> </div> <p>Same as <code>EXCLUDE_FIXTURE</code> except only matching cleanup tests are excluded.</p> </dd> <dt>
<code>PARALLEL_LEVEL &lt;level&gt;</code> </dt>
<dd>
<p>Specify a positive number representing the number of tests to be run in parallel.</p> </dd> <dt>
<code>RESOURCE_SPEC_FILE &lt;file&gt;</code> </dt>
<dd>
<div class="versionadded"> <p><span class="versionmodified added">New in version 3.16.</span></p> </div> <p>Specify a <a class="reference internal" href="../manual/ctest.1.html#ctest-resource-specification-file"><span class="std std-ref">resource specification file</span></a>. See <a class="reference internal" href="../manual/ctest.1.html#ctest-resource-allocation"><span class="std std-ref">Resource Allocation</span></a> for more information.</p> </dd> <dt>
<code>TEST_LOAD &lt;threshold&gt;</code> </dt>
<dd>
<div class="versionadded"> <p><span class="versionmodified added">New in version 3.4.</span></p> </div> <p>While running tests in parallel, try not to start tests when they may cause the CPU load to pass above a given threshold. If not specified the <a class="reference internal" href="../variable/ctest_test_load.html#variable:CTEST_TEST_LOAD" title="CTEST_TEST_LOAD" id="index-0-variable:CTEST_TEST_LOAD"><code>CTEST_TEST_LOAD</code></a> variable will be checked, and then the <a class="reference internal" href="../manual/ctest.1.html#cmdoption-ctest-test-load"><code>--test-load</code></a> command-line argument to <a class="reference internal" href="../manual/ctest.1.html#manual:ctest(1)" title="ctest(1)" id="index-0-manual:ctest(1)"><code>ctest(1)</code></a>. See also the <code>TestLoad</code> setting in the <a class="reference internal" href="../manual/ctest.1.html#ctest-test-step"><span class="std std-ref">CTest Test Step</span></a>.</p> </dd> <dt>
<code>REPEAT &lt;mode&gt;:&lt;n&gt;</code> </dt>
<dd>
<div class="versionadded"> <p><span class="versionmodified added">New in version 3.17.</span></p> </div> <p>Run tests repeatedly based on the given <code>&lt;mode&gt;</code> up to <code>&lt;n&gt;</code> times. The modes are:</p> <dl class="simple"> <dt>
<code>UNTIL_FAIL</code> </dt>
<dd>
<p>Require each test to run <code>&lt;n&gt;</code> times without failing in order to pass. This is useful in finding sporadic failures in test cases.</p> </dd> <dt>
<code>UNTIL_PASS</code> </dt>
<dd>
<p>Allow each test to run up to <code>&lt;n&gt;</code> times in order to pass. Repeats tests if they fail for any reason. This is useful in tolerating sporadic failures in test cases.</p> </dd> <dt>
<code>AFTER_TIMEOUT</code> </dt>
<dd>
<p>Allow each test to run up to <code>&lt;n&gt;</code> times in order to pass. Repeats tests only if they timeout. This is useful in tolerating sporadic timeouts in test cases on busy machines.</p> </dd> </dl> </dd> <dt>
<code>SCHEDULE_RANDOM &lt;ON|OFF&gt;</code> </dt>
<dd>
<p>Launch tests in a random order. This may be useful for detecting implicit test dependencies.</p> </dd> <dt>
<code>STOP_ON_FAILURE</code> </dt>
<dd>
<div class="versionadded"> <p><span class="versionmodified added">New in version 3.18.</span></p> </div> <p>Stop the execution of the tests once one has failed.</p> </dd> <dt>
<code>STOP_TIME &lt;time-of-day&gt;</code> </dt>
<dd>
<p>Specify a time of day at which the tests should all stop running.</p> </dd> <dt>
<code>RETURN_VALUE &lt;result-var&gt;</code> </dt>
<dd>
<p>Store in the <code>&lt;result-var&gt;</code> variable <code>0</code> if all tests passed. Store non-zero if anything went wrong.</p> </dd> <dt>
<code>CAPTURE_CMAKE_ERROR &lt;result-var&gt;</code> </dt>
<dd>
<div class="versionadded"> <p><span class="versionmodified added">New in version 3.7.</span></p> </div> <p>Store in the <code>&lt;result-var&gt;</code> variable -1 if there are any errors running the command and prevent ctest from returning non-zero if an error occurs.</p> </dd> <dt>
<code>OUTPUT_JUNIT &lt;file&gt;</code> </dt>
<dd>
<div class="versionadded"> <p><span class="versionmodified added">New in version 3.21.</span></p> </div> <p>Write test results to <code>&lt;file&gt;</code> in JUnit XML format. If <code>&lt;file&gt;</code> is a relative path, it will be placed in the build directory. If <code>&lt;file&gt;</code> already exists, it will be overwritten. Note that the resulting JUnit XML file is <strong>not</strong> uploaded to CDash because it would be redundant with CTest's <code>Test.xml</code> file.</p> </dd> <dt>
<code>QUIET</code> </dt>
<dd>
<div class="versionadded"> <p><span class="versionmodified added">New in version 3.3.</span></p> </div> <p>Suppress any CTest-specific non-error messages that would have otherwise been printed to the console. Output from the underlying test command is not affected. Summary info detailing the percentage of passing tests is also unaffected by the <code>QUIET</code> option.</p> </dd> </dl> <p>See also the <a class="reference internal" href="../variable/ctest_custom_maximum_passed_test_output_size.html#variable:CTEST_CUSTOM_MAXIMUM_PASSED_TEST_OUTPUT_SIZE" title="CTEST_CUSTOM_MAXIMUM_PASSED_TEST_OUTPUT_SIZE" id="index-0-variable:CTEST_CUSTOM_MAXIMUM_PASSED_TEST_OUTPUT_SIZE"><code>CTEST_CUSTOM_MAXIMUM_PASSED_TEST_OUTPUT_SIZE</code></a>, <a class="reference internal" href="../variable/ctest_custom_maximum_failed_test_output_size.html#variable:CTEST_CUSTOM_MAXIMUM_FAILED_TEST_OUTPUT_SIZE" title="CTEST_CUSTOM_MAXIMUM_FAILED_TEST_OUTPUT_SIZE" id="index-0-variable:CTEST_CUSTOM_MAXIMUM_FAILED_TEST_OUTPUT_SIZE"><code>CTEST_CUSTOM_MAXIMUM_FAILED_TEST_OUTPUT_SIZE</code></a> and <a class="reference internal" href="../variable/ctest_custom_test_output_truncation.html#variable:CTEST_CUSTOM_TEST_OUTPUT_TRUNCATION" title="CTEST_CUSTOM_TEST_OUTPUT_TRUNCATION" id="index-0-variable:CTEST_CUSTOM_TEST_OUTPUT_TRUNCATION"><code>CTEST_CUSTOM_TEST_OUTPUT_TRUNCATION</code></a> variables, along with their corresponding <a class="reference internal" href="../manual/ctest.1.html#manual:ctest(1)" title="ctest(1)" id="index-1-manual:ctest(1)"><code>ctest(1)</code></a> command line options <a class="reference internal" href="../manual/ctest.1.html#cmdoption-ctest-test-output-size-passed"><code>--test-output-size-passed</code></a>, <a class="reference internal" href="../manual/ctest.1.html#cmdoption-ctest-test-output-size-failed"><code>--test-output-size-failed</code></a>, and <a class="reference internal" href="../manual/ctest.1.html#cmdoption-ctest-test-output-truncation"><code>--test-output-truncation</code></a>.</p>  <h2 id="id1">Additional Test Measurements</h2> <p>CTest can parse the output of your tests for extra measurements to report to CDash.</p> <p>When run as a <a class="reference internal" href="../manual/ctest.1.html#dashboard-client"><span class="std std-ref">Dashboard Client</span></a>, CTest will include these custom measurements in the <code>Test.xml</code> file that gets uploaded to CDash.</p> <p>Check the <a class="reference external" href="https://github.com/Kitware/CDash/blob/master/docs/test_measurements.md">CDash test measurement documentation</a> for more information on the types of test measurements that CDash recognizes.</p> <p>The following example demonstrates how to output a variety of custom test measurements.</p> <pre data-language="c++">std::cout &lt;&lt;
  "&lt;CTestMeasurement type=\"numeric/double\" name=\"score\"&gt;28.3&lt;/CTestMeasurement&gt;"
  &lt;&lt; std::endl;

std::cout &lt;&lt;
  "&lt;CTestMeasurement type=\"text/string\" name=\"color\"&gt;red&lt;/CTestMeasurement&gt;"
  &lt;&lt; std::endl;

std::cout &lt;&lt;
  "&lt;CTestMeasurement type=\"text/link\" name=\"CMake URL\"&gt;https://cmake.org&lt;/CTestMeasurement&gt;"
  &lt;&lt; std::endl;

std::cout &lt;&lt;
  "&lt;CTestMeasurement type=\"text/preformatted\" name=\"Console Output\"&gt;" &lt;&lt;
  "line 1.\n" &lt;&lt;
  "  \033[31;1m line 2. Bold red, and indented!\033[0;0ml\n" &lt;&lt;
  "line 3. Not bold or indented...\n" &lt;&lt;
  "&lt;/CTestMeasurement&gt;" &lt;&lt; std::endl;
</pre>  <h3>Image Measurements</h3> <p>The following example demonstrates how to upload test images to CDash.</p> <pre data-language="c++">std::cout &lt;&lt;
  "&lt;CTestMeasurementFile type=\"image/jpg\" name=\"TestImage\"&gt;" &lt;&lt;
  "/dir/to/test_img.jpg&lt;/CTestMeasurementFile&gt;" &lt;&lt; std::endl;

std::cout &lt;&lt;
  "&lt;CTestMeasurementFile type=\"image/gif\" name=\"ValidImage\"&gt;" &lt;&lt;
  "/dir/to/valid_img.gif&lt;/CTestMeasurementFile&gt;" &lt;&lt; std::endl;

std::cout &lt;&lt;
  "&lt;CTestMeasurementFile type=\"image/png\" name=\"AlgoResult\"&gt;" &lt;&lt;
  "/dir/to/img.png&lt;/CTestMeasurementFile&gt;"
  &lt;&lt; std::endl;
</pre> <p>Images will be displayed together in an interactive comparison mode on CDash if they are provided with two or more of the following names.</p> <ul class="simple"> <li><code>TestImage</code></li> <li><code>ValidImage</code></li> <li><code>BaselineImage</code></li> <li><code>DifferenceImage2</code></li> </ul> <p>By convention, <code>TestImage</code> is the image generated by your test, and <code>ValidImage</code> (or <code>BaselineImage</code>) is basis of comparison used to determine if the test passed or failed.</p> <p>If another image name is used it will be displayed by CDash as a static image separate from the interactive comparison UI.</p>   <h3>Attached Files</h3> <div class="versionadded"> <p><span class="versionmodified added">New in version 3.21.</span></p> </div> <p>The following example demonstrates how to upload non-image files to CDash.</p> <pre data-language="c++">std::cout &lt;&lt;
  "&lt;CTestMeasurementFile type=\"file\" name=\"TestInputData1\"&gt;" &lt;&lt;
  "/dir/to/data1.csv&lt;/CTestMeasurementFile&gt;\n"                   &lt;&lt;
  "&lt;CTestMeasurementFile type=\"file\" name=\"TestInputData2\"&gt;" &lt;&lt;
  "/dir/to/data2.csv&lt;/CTestMeasurementFile&gt;"                     &lt;&lt; std::endl;
</pre> <p>If the name of the file to upload is known at configure time, you can use the <a class="reference internal" href="../prop_test/attached_files.html#prop_test:ATTACHED_FILES" title="ATTACHED_FILES" id="index-0-prop_test:ATTACHED_FILES"><code>ATTACHED_FILES</code></a> or <a class="reference internal" href="../prop_test/attached_files_on_fail.html#prop_test:ATTACHED_FILES_ON_FAIL" title="ATTACHED_FILES_ON_FAIL" id="index-0-prop_test:ATTACHED_FILES_ON_FAIL"><code>ATTACHED_FILES_ON_FAIL</code></a> test properties instead.</p>   <h3>Custom Details</h3> <div class="versionadded"> <p><span class="versionmodified added">New in version 3.21.</span></p> </div> <p>The following example demonstrates how to specify a custom value for the <code>Test Details</code> field displayed on CDash.</p> <pre data-language="c++">std::cout &lt;&lt;
  "&lt;CTestDetails&gt;My Custom Details Value&lt;/CTestDetails&gt;" &lt;&lt; std::endl;
</pre>   <h3 id="id2">Additional Labels</h3> <div class="versionadded"> <p><span class="versionmodified added">New in version 3.22.</span></p> </div> <p>The following example demonstrates how to add additional labels to a test at runtime.</p> <pre data-language="c++">std::cout &lt;&lt;
  "&lt;CTestLabel&gt;Custom Label 1&lt;/CTestLabel&gt;\n" &lt;&lt;
  "&lt;CTestLabel&gt;Custom Label 2&lt;/CTestLabel&gt;"   &lt;&lt; std::endl;
</pre> <p>Use the <a class="reference internal" href="../prop_test/labels.html#prop_test:LABELS" title="LABELS" id="index-0-prop_test:LABELS"><code>LABELS</code></a> test property instead for labels that can be determined at configure time.</p>    <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2000&ndash;2023 Kitware, Inc. and Contributors<br>Licensed under the BSD 3-clause License.<br>
    <a href="https://cmake.org/cmake/help/v3.26/command/ctest_test.html" class="_attribution-link">https://cmake.org/cmake/help/v3.26/command/ctest_test.html</a>
  </p>
</div>
