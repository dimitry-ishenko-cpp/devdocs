<h1 id="sooner-faster-quicker">11. Hints</h1>
<div class="_sphinx"> <div itemprop="articleBody"> <section id="hints">  <p>Please advise us of other “helpful hints” that should go here!</p> <section id="sooner-producing-a-program-more-quickly"> <h2 id="sooner">
<span class="section-number">11.1. </span>Sooner: producing a program more quickly</h2> <dl id="index-0"> <dt>
<code>Don’t use -O or (especially) -O2:</code> </dt>
<dd>
<p>By using them, you are telling GHC that you are willing to suffer longer compilation times for better-quality code.</p> <p>GHC is surprisingly zippy for normal compilations without <a class="reference internal" href="using-optimisation.html#ghc-flag-O"><code>-O</code></a>!</p> </dd> <dt>Use more memory:</dt>
<dd>
<p>Within reason, more memory for heap space means less garbage collection for GHC, which means less compilation time. If you use the <code>-Rghc-timing</code> option, you’ll get a garbage-collector report. (Again, you can use the cheap-and-nasty <code>+RTS -S -RTS</code> option to send the GC stats straight to standard error.)</p> <p id="index-1">If it says you’re using more than 20% of total time in garbage collecting, then more memory might help: use the <code>-H⟨size⟩</code> (see <a class="reference internal" href="runtime_control.html#rts-flag-H-size"><code>-H [⟨size⟩]</code></a>) option. Increasing the default allocation area size used by the compiler’s RTS might also help: use the <code>+RTS -A⟨size⟩
-RTS</code> option (see <a class="reference internal" href="runtime_control.html#rts-flag-A-size"><code>-A ⟨size⟩</code></a>).</p> <p id="index-2">If GHC persists in being a bad memory citizen, please report it as a bug.</p> </dd> <dt>Don’t use too much memory!</dt>
<dd>
<p>As soon as GHC plus its “fellow citizens” (other processes on your machine) start using more than the <em>real memory</em> on your machine, and the machine starts “thrashing,” <em>the party is over</em>. Compile times will be worse than terrible! Use something like the csh builtin <strong class="command">time</strong> command to get a report on how many page faults you’re getting.</p> <p>If you don’t know what virtual memory, thrashing, and page faults are, or you don’t know the memory configuration of your machine, <em>don’t</em> try to be clever about memory use: you’ll just make your life a misery (and for other people, too, probably).</p> </dd> <dt>Try to use local disks when linking:</dt>
<dd>
<p>Because Haskell objects and libraries tend to be large, it can take many real seconds to slurp the bits to/from a remote filesystem.</p> <p>It would be quite sensible to <em>compile</em> on a fast machine using remotely-mounted disks; then <em>link</em> on a slow machine that had your disks directly mounted.</p> </dd> <dt>
<code>Don’t derive/use Read unnecessarily:</code> </dt>
<dd>
<p>It’s ugly and slow.</p> </dd> <dt>GHC compiles some program constructs slowly:</dt>
<dd>
<p>We’d rather you reported such behaviour as a bug, so that we can try to correct it.</p> <p id="index-3">To figure out which part of the compiler is badly behaved, the <code>-v2</code> option is your friend.</p> </dd> </dl> </section> <section id="faster-producing-a-program-that-runs-quicker"> <h2 id="faster">
<span class="section-number">11.2. </span>Faster: producing a program that runs quicker</h2> <p id="index-4">The key tool to use in making your Haskell program run faster are GHC’s profiling facilities, described separately in <a class="reference internal" href="profiling.html#profiling"><span class="std std-ref">Profiling</span></a>. There is <em>no substitute</em> for finding where your program’s time/space is <em>really</em> going, as opposed to where you imagine it is going.</p> <p>Another point to bear in mind: By far the best way to improve a program’s performance <em>dramatically</em> is to use better algorithms. Once profiling has thrown the spotlight on the guilty time-consumer(s), it may be better to re-think your program than to try all the tweaks listed below.</p> <p>Another extremely efficient way to make your program snappy is to use library code that has been Seriously Tuned By Someone Else. You <em>might</em> be able to write a better quicksort than the one in <code>Data.List</code>, but it will take you much longer than typing <code>import Data.List</code>.</p> <p>Please report any overly-slow GHC-compiled programs. Since GHC doesn’t have any credible competition in the performance department these days it’s hard to say what overly-slow means, so just use your judgement! Of course, if a GHC compiled program runs slower than the same program compiled with NHC or Hugs, then it’s definitely a bug.</p> <dl> <dt>
<code>Optimise, using -O or -O2:</code> </dt>
<dd>
<p>This is the most basic way to make your program go faster. Compilation time will be slower, especially with <code>-O2</code>.</p> <p>At present, <code>-O2</code> is nearly indistinguishable from <code>-O</code>.</p> </dd> <dt>Compile via LLVM:</dt>
<dd>
<p>The <a class="reference internal" href="codegens.html#llvm-code-gen"><span class="std std-ref">LLVM code generator</span></a> can sometimes do a far better job at producing fast code than the <a class="reference internal" href="codegens.html#native-code-gen"><span class="std std-ref">native code generator</span></a>. This is not universal and depends on the code. Numeric heavy code seems to show the best improvement when compiled via LLVM. You can also experiment with passing specific flags to LLVM with the <a class="reference internal" href="phases.html#ghc-flag-optlo-option"><code>-optlo ⟨option⟩</code></a> and <a class="reference internal" href="phases.html#ghc-flag-optlc-option"><code>-optlc ⟨option⟩</code></a> flags. Be careful though as setting these flags stops GHC from setting its usual flags for the LLVM optimiser and compiler.</p> </dd> <dt>Overloaded functions are not your friend:</dt>
<dd>
<p>Haskell’s overloading (using type classes) is elegant, neat, etc., etc., but it is death to performance if left to linger in an inner loop. How can you squash it?</p> </dd> <dt>Give explicit type signatures:</dt>
<dd>
<p>Signatures are the basic trick; putting them on exported, top-level functions is good software-engineering practice, anyway. (Tip: using the <a class="reference internal" href="using-warnings.html#ghc-flag-Wmissing-signatures"><code>-Wmissing-signatures</code></a> option can help enforce good signature-practice).</p> <p>The automatic specialisation of overloaded functions (with <code>-O</code>) should take care of overloaded local and/or unexported functions.</p> </dd> <dt>
<code>Use SPECIALIZE pragmas:</code> </dt>
<dd>
<p id="index-6">Specialize the overloading on key functions in your program. See <a class="reference internal" href="exts/pragmas.html#specialize-pragma"><span class="std std-ref">SPECIALIZE pragma</span></a> and <a class="reference internal" href="exts/pragmas.html#specialize-instance-pragma"><span class="std std-ref">SPECIALIZE instance pragma</span></a>.</p> </dd> <dt>“But how do I know where overloading is creeping in?”</dt>
<dd>
<p>A low-tech way: grep (search) your interface files for overloaded type signatures. You can view interface files using the <a class="reference internal" href="using.html#ghc-flag-show-iface-file"><code>--show-iface ⟨file⟩</code></a> option (see <a class="reference internal" href="separate_compilation.html#hi-options"><span class="std std-ref">Other options related to interface files</span></a>).</p> <pre data-language="sh">$ ghc --show-iface Foo.hi | grep -E '^[a-z].*::.*=&gt;'
</pre> </dd> <dt>Strict functions are your dear friends:</dt>
<dd>
<p>And, among other things, lazy pattern-matching is your enemy.</p> <p>(If you don’t know what a “strict function” is, please consult a functional-programming textbook. A sentence or two of explanation here probably would not do much good.)</p> <p>Consider these two code fragments:</p> <pre data-language="haskell">f (Wibble x y) =  ... # strict

f arg = let { (Wibble x y) = arg } in ... # lazy
</pre> <p>The former will result in far better code.</p> <p>A less contrived example shows the use of <code>BangPatterns</code> on <code>lets</code> to get stricter code (a good thing):</p> <pre data-language="haskell">f (Wibble x y)
      = let
            !(a1, b1, c1) = unpackFoo x
            !(a2, b2, c2) = unpackFoo y
        in ...
</pre> </dd> <dt>GHC loves single-constructor data-types:</dt>
<dd>
<p>It’s all the better if a function is strict in a single-constructor type (a type with only one data-constructor; for example, tuples are single-constructor types).</p> </dd> <dt>Newtypes are better than datatypes:</dt>
<dd>
<p>If your datatype has a single constructor with a single field, use a <code>newtype</code> declaration instead of a <code>data</code> declaration. The <code>newtype</code> will be optimised away in most cases.</p> </dd> <dt>“How do I find out a function’s strictness?”</dt>
<dd>
<p>Don’t guess—look it up.</p> <p>Look for your function in the interface file, then for the third field in the pragma; it should say <code>Strictness: ⟨string⟩</code>. The ⟨string⟩ gives the strictness of the function’s arguments: see <a class="reference external" href="https://gitlab.haskell.org/ghc/ghc/wikis/commentary/compiler/demand">the GHC Commentary</a> for a description of the strictness notation.</p> <p>For an “unpackable” <code>U(...)</code> argument, the info inside tells the strictness of its components. So, if the argument is a pair, and it says <code>U(AU(LSS))</code>, that means “the first component of the pair isn’t used; the second component is itself unpackable, with three components (lazy in the first, strict in the second \&amp; third).”</p> <p>If the function isn’t exported, just compile with the extra flag <a class="reference internal" href="debugging.html#ghc-flag-ddump-simpl"><code>-ddump-simpl</code></a>; next to the signature for any binder, it will print the self-same pragmatic information as would be put in an interface file. (Besides, Core syntax is fun to look at!)</p> </dd> <dt>
<code>Force key functions to be INLINEd (esp. monads):</code> </dt>
<dd>
<p>Placing <code>INLINE</code> pragmas on certain functions that are used a lot can have a dramatic effect. See <a class="reference internal" href="exts/pragmas.html#inline-pragma"><span class="std std-ref">INLINE pragma</span></a>.</p> </dd> <dt>
<code>Explicit export list:</code> </dt>
<dd>
<p>If you do not have an explicit export list in a module, GHC must assume that everything in that module will be exported. This has various pessimising effects. For example, if a bit of code is actually <em>unused</em> (perhaps because of unfolding effects), GHC will not be able to throw it away, because it is exported and some other module may be relying on its existence.</p> <p>GHC can be quite a bit more aggressive with pieces of code if it knows they are not exported.</p> </dd> <dt>Look at the Core syntax!</dt>
<dd>
<p>(The form in which GHC manipulates your code.) Just run your compilation with <a class="reference internal" href="debugging.html#ghc-flag-ddump-simpl"><code>-ddump-simpl</code></a> (don’t forget the <a class="reference internal" href="using-optimisation.html#ghc-flag-O"><code>-O</code></a>).</p> <p>If profiling has pointed the finger at particular functions, look at their Core code. <code>lets</code> are bad, <code>cases</code> are good, dictionaries (<code>d.⟨Class⟩.⟨Unique⟩</code>) [or anything overloading-ish] are bad, nested lambdas are bad, explicit data constructors are good, primitive operations (e.g., <code>==#</code>) are good, …</p> </dd> <dt>Use strictness annotations:</dt>
<dd>
<p>Putting a strictness annotation (<code>!</code>) on a constructor field helps in two ways: it adds strictness to the program, which gives the strictness analyser more to work with, and it might help to reduce space leaks.</p> <p>It can also help in a third way: when used with <a class="reference internal" href="using-optimisation.html#ghc-flag-funbox-strict-fields"><code>-funbox-strict-fields</code></a> (see <a class="reference internal" href="using-optimisation.html#options-f"><span class="std std-ref">-f*: platform-independent flags</span></a>), a strict field can be unpacked or unboxed in the constructor, and one or more levels of indirection may be removed. Unpacking only happens for single-constructor datatypes (<code>Int</code> is a good candidate, for example).</p> <p>Using <a class="reference internal" href="using-optimisation.html#ghc-flag-funbox-strict-fields"><code>-funbox-strict-fields</code></a> is only really a good idea in conjunction with <a class="reference internal" href="using-optimisation.html#ghc-flag-O"><code>-O</code></a>, because otherwise the extra packing and unpacking won’t be optimised away. In fact, it is possible that <a class="reference internal" href="using-optimisation.html#ghc-flag-funbox-strict-fields"><code>-funbox-strict-fields</code></a> may worsen performance even <em>with</em> <a class="reference internal" href="using-optimisation.html#ghc-flag-O"><code>-O</code></a>, but this is unlikely (let us know if it happens to you).</p> </dd> <dt>Use unboxed types (a GHC extension):</dt>
<dd>
<p>When you are <em>really</em> desperate for speed, and you want to get right down to the “raw bits.” Please see <a class="reference internal" href="exts/primitives.html#glasgow-unboxed"><span class="std std-ref">Unboxed types</span></a> for some information about using unboxed types.</p> <p>Before resorting to explicit unboxed types, try using strict constructor fields and <a class="reference internal" href="using-optimisation.html#ghc-flag-funbox-strict-fields"><code>-funbox-strict-fields</code></a> first (see above). That way, your code stays portable.</p> </dd> <dt>
<code>Use foreign import (a GHC extension) to plug into fast libraries:</code> </dt>
<dd>
<p>This may take real work, but… There exist piles of massively-tuned library code, and the best thing is not to compete with it, but link with it.</p> <p><a class="reference internal" href="exts/ffi.html#ffi"><span class="std std-ref">Foreign function interface (FFI)</span></a> describes the foreign function interface.</p> </dd> <dt>
<code>Use unboxed arrays (UArray)</code> </dt>
<dd>
<p>GHC supports arrays of unboxed elements, for several basic arithmetic element types including <code>Int</code> and <code>Char</code>: see the <a class="reference external" href="../libraries/array-0.5.8.0-8d84/data-array-unboxed.html">Data.Array.Unboxed</a> library for details. These arrays are likely to be much faster than using standard Haskell 98 arrays from the <a class="reference external" href="../libraries/array-0.5.8.0-8d84/data-array.html">Data.Array</a> library.</p> </dd> <dt>Use a bigger heap!</dt>
<dd>
<p>If your program’s GC stats (<a class="reference internal" href="runtime_control.html#rts-flag-S-file"><code>-S [⟨file⟩]</code></a> RTS option) indicate that it’s doing lots of garbage-collection (say, more than 20% of execution time), more memory might help — with the <a class="reference internal" href="runtime_control.html#rts-flag-H-size"><code>-H [⟨size⟩]</code></a> or <a class="reference internal" href="runtime_control.html#rts-flag-A-size"><code>-A ⟨size⟩</code></a> RTS options (see <a class="reference internal" href="runtime_control.html#rts-options-gc"><span class="std std-ref">RTS options to control the garbage collector</span></a>). As a rule of thumb, try setting <a class="reference internal" href="runtime_control.html#rts-flag-H-size"><code>-H [⟨size⟩]</code></a> to the amount of memory you’re willing to let your process consume, or perhaps try passing <a class="reference internal" href="runtime_control.html#rts-flag-H-size"><code>-H [⟨size⟩]</code></a> without any argument to let GHC calculate a value based on the amount of live data.</p> </dd> <dt>Compact your data:</dt>
<dd>
<p>The <a class="reference external" href="https://downloads.haskell.org/~ghc/9.12.1/docs/libraries/ghc-compact-0.1.0.0-d7ba/GHC-Compact.html">GHC.Compact</a> module provides a way to make garbage collection more efficient for long-lived data structures. Compacting a data structure collects the objects together in memory, where they are treated as a single object by the garbage collector and not traversed individually.</p> </dd> </dl> </section> <section id="smaller-producing-a-program-that-is-smaller"> <h2 id="smaller">
<span class="section-number">11.3. </span>Smaller: producing a program that is smaller</h2> <p id="index-7">Decrease the “go-for-it” threshold for unfolding smallish expressions. Give a <a class="reference internal" href="using-optimisation.html#ghc-flag-funfolding-use-threshold-n"><code>-funfolding-use-threshold=0</code></a> option for the extreme case. (“Only unfoldings with zero cost should proceed.”) Warning: except in certain specialised cases (like Happy parsers) this is likely to actually <em>increase</em> the size of your program, because unfolding generally enables extra simplifying optimisations to be performed.</p> <p>Avoid <a class="reference external" href="../libraries/base-4.21.0.0-8e62/prelude.html#t:Read">Prelude.Read</a>.</p> <p>Use <strong class="command">strip</strong> on your executables.</p> </section> <section id="thriftier-producing-a-program-that-gobbles-less-heap-space"> <h2 id="thriftier">
<span class="section-number">11.4. </span>Thriftier: producing a program that gobbles less heap space</h2> <p id="index-8">“I think I have a space leak…”</p> <p>Re-run your program with <a class="reference internal" href="runtime_control.html#rts-flag-S-file"><code>+RTS -S</code></a>, and remove all doubt! (You’ll see the heap usage get bigger and bigger…) (Hmmm… this might be even easier with the <a class="reference internal" href="runtime_control.html#rts-flag-G-generations"><code>-G1</code></a> RTS option; so… <code>./a.out +RTS -S -G1</code>)</p> <p id="index-9">Once again, the profiling facilities (<a class="reference internal" href="profiling.html#profiling"><span class="std std-ref">Profiling</span></a>) are the basic tool for demystifying the space behaviour of your program.</p> <p>Strict functions are good for space usage, as they are for time, as discussed in the previous section. Strict functions get right down to business, rather than filling up the heap with closures (the system’s notes to itself about how to evaluate something, should it eventually be required).</p> </section> <section id="controlling-inlining-via-optimisation-flags"> <h2 id="control-inlining">
<span class="section-number">11.5. </span>Controlling inlining via optimisation flags.</h2> <p id="index-10">Inlining is one of the major optimizations GHC performs. Partially because inlining often allows other optimizations to be triggered. Sadly this is also a double edged sword. While inlining can often cut through runtime overheads this usually comes at the cost of not just program size, but also compiler performance. In extreme cases making it impossible to compile certain code.</p> <p>For this reason GHC offers various ways to tune inlining behaviour.</p> <section id="unfolding-creation"> <h3 id="inlining-unfolding-creation">
<span class="section-number">11.5.1. </span>Unfolding creation</h3> <p>In order for a function from a different module to be inlined GHC requires the functions unfolding. The following flags can be used to control unfolding creation. Making their creation more or less likely:</p> <ul class="simple"> <li><a class="reference internal" href="using-optimisation.html#ghc-flag-funfolding-creation-threshold-n"><code>-funfolding-creation-threshold=⟨n⟩</code></a></li> <li><a class="reference internal" href="using-optimisation.html#ghc-flag-fexpose-overloaded-unfoldings"><code>-fexpose-overloaded-unfoldings</code></a></li> <li><a class="reference internal" href="using-optimisation.html#ghc-flag-fexpose-all-unfoldings"><code>-fexpose-all-unfoldings</code></a></li> </ul> </section> <section id="inlining-decisions"> <h3>
<span class="section-number">11.5.2. </span>Inlining decisions</h3> <p>If a unfolding is available the following flags can impact GHC’s decision about inlining a specific binding.</p> <ul class="simple"> <li><a class="reference internal" href="using-optimisation.html#ghc-flag-funfolding-use-threshold-n"><code>-funfolding-use-threshold=⟨n⟩</code></a></li> <li><a class="reference internal" href="using-optimisation.html#ghc-flag-funfolding-case-threshold-n"><code>-funfolding-case-threshold=⟨n⟩</code></a></li> <li><a class="reference internal" href="using-optimisation.html#ghc-flag-funfolding-case-scaling-n"><code>-funfolding-case-scaling=⟨n⟩</code></a></li> <li><a class="reference internal" href="using-optimisation.html#ghc-flag-funfolding-dict-discount-n"><code>-funfolding-dict-discount=⟨n⟩</code></a></li> <li><a class="reference internal" href="using-optimisation.html#ghc-flag-funfolding-fun-discount-n"><code>-funfolding-fun-discount=⟨n⟩</code></a></li> </ul> <p>Should the simplifier run out of ticks because of a inlining loop users are encouraged to try decreasing <a class="reference internal" href="using-optimisation.html#ghc-flag-funfolding-case-threshold-n"><code>-funfolding-case-threshold=⟨n⟩</code></a> or <a class="reference internal" href="using-optimisation.html#ghc-flag-funfolding-case-scaling-n"><code>-funfolding-case-scaling=⟨n⟩</code></a> to limit inlining into deeply nested expressions while allowing a higher tick factor.</p> <p>The defaults of these are tuned such that we don’t expect regressions for most user programs. Using a <a class="reference internal" href="using-optimisation.html#ghc-flag-funfolding-case-threshold-n"><code>-funfolding-case-threshold=⟨n⟩</code></a> of 1-2 with a <a class="reference internal" href="using-optimisation.html#ghc-flag-funfolding-case-scaling-n"><code>-funfolding-case-scaling=⟨n⟩</code></a> of 15-25 can cause usually small runtime regressions but will prevent most inlining loops from getting out of control.</p> <p>In extreme cases lowering scaling and threshold further can be useful, but at that point it’s very likely that beneficial inlining is prevented as well resulting in significant runtime regressions.</p> <p>In such cases it’s recommended to move the problematic piece of code into it’s own module and changing inline parameters for the offending module only.</p> </section> <section id="inlining-generics"> <h3>
<span class="section-number">11.5.3. </span>Inlining generics</h3> <p>There are also flags specific to the inlining of generics:</p> <ul class="simple"> <li><a class="reference internal" href="using-optimisation.html#ghc-flag-finline-generics"><code>-finline-generics</code></a></li> <li><a class="reference internal" href="using-optimisation.html#ghc-flag-finline-generics-aggressively"><code>-finline-generics-aggressively</code></a></li> </ul> </section> </section> <section id="controlling-specialization"> <h2 id="control-specialization">
<span class="section-number">11.6. </span>Controlling specialization</h2> <p id="index-11">GHC has the ability to optimize polymorphic code for specific type class instances at the use site. We call this specialisation and it’s enabled through <a class="reference internal" href="using-optimisation.html#ghc-flag-fspecialise"><code>-fspecialise</code></a> which is enabled by default at <code>-O1</code> or higher.</p> <p>GHC does this by creating a copy of the overloaded function, optimizing this copy for a given type class instance. Calls to the overloaded function using a statically known typeclass we created a specialization for will then be replaced by a call to the specialized version of the function.</p> <p>This can often be crucial to avoid overhead at runtime. However since this involves potentially making many copies of overloaded functions GHC doesn’t always apply this optimization by default even in cases where it could do so.</p> <p>For GHC to be able to specialise, at a miminum the instance it specializes for must be known and the overloaded functions unfolding must be available.</p> <section id="commonly-used-flag-pragma-combinations"> <h3>
<span class="section-number">11.6.1. </span>Commonly used flag/pragma combinations</h3> <p>For applications which aren’t very compute heavy the defaults are often good enough as they try to strike a reasonable balance between compile time and runtime.</p> <p>For libraries, if exported functions would benefit significantly from specialization, it’s recommended to enable <a class="reference internal" href="using-optimisation.html#ghc-flag-fexpose-overloaded-unfoldings"><code>-fexpose-overloaded-unfoldings</code></a> or manually attach INLINEABLE pragmas to performance relevant functions. This will ensure downstream users can specialize any overloaded functions exposed by the library if it’s beneficial.</p> <p>If there are key parts of an application which rely on specialization for performance using <code>SPECIALIZE</code> pragmas in combination with either <a class="reference internal" href="using-optimisation.html#ghc-flag-fexpose-overloaded-unfoldings"><code>-fexpose-overloaded-unfoldings</code></a> or <code>INLINEABLE</code> on key overloaded functions should allow for these functions to specialize without affecting overall compile times too much.</p> <p>For compute heavy code reliant on elimination of as much overhead as possible it’s recommended to use a combination of <a class="reference internal" href="using-optimisation.html#ghc-flag-fspecialise-aggressively"><code>-fspecialise-aggressively</code></a> and <a class="reference internal" href="using-optimisation.html#ghc-flag-fexpose-overloaded-unfoldings"><code>-fexpose-overloaded-unfoldings</code></a> or <a class="reference internal" href="using-optimisation.html#ghc-flag-fexpose-all-unfoldings"><code>-fexpose-all-unfoldings</code></a>. However this comes at a big cost to compile time.</p> </section> <section id="unfolding-availabiliy"> <h3>
<span class="section-number">11.6.2. </span>Unfolding availabiliy</h3> <p>Unfolding availabiliy is primarily determined by <a class="reference internal" href="#inlining-unfolding-creation"><span class="std std-ref">these flags</span></a>.</p> <p>Of particular interest for specialization are:</p> <ul class="simple"> <li><a class="reference internal" href="using-optimisation.html#ghc-flag-fexpose-all-unfoldings"><code>-fexpose-all-unfoldings</code></a></li> <li><a class="reference internal" href="using-optimisation.html#ghc-flag-fexpose-overloaded-unfoldings"><code>-fexpose-overloaded-unfoldings</code></a></li> </ul> <p>The former making <em>all</em> unfoldings available, potentially at high compile time cost. The later only makes available the functions that are overloaded. It’s generally better to use <a class="reference internal" href="using-optimisation.html#ghc-flag-fexpose-overloaded-unfoldings"><code>-fexpose-overloaded-unfoldings</code></a> over <a class="reference internal" href="using-optimisation.html#ghc-flag-fexpose-all-unfoldings"><code>-fexpose-all-unfoldings</code></a> when the goal is to ensure specializations.</p> </section> <section id="when-does-ghc-generate-specializations"> <h3>
<span class="section-number">11.6.3. </span>When does GHC generate specializations</h3> <p>Functions get considered for specialization either implicitly when GHC sees a use of an overloaded function used with concrete typeclass instances or explicitly when a user requests it through pragmas, see <a class="reference internal" href="exts/pragmas.html#specialize-pragma"><span class="std std-ref">SPECIALIZE pragma</span></a> and <a class="reference internal" href="exts/pragmas.html#specialize-instance-pragma"><span class="std std-ref">SPECIALIZE instance pragma</span></a>.</p> <p>The specializer then checks a number of conditions <em>in order</em> to decide weither or not specialization should happen. Below is a best effort of the list of conditions GHC checks currently.</p> <ul class="simple"> <li>If any of the type class instances have type arguments and <a class="reference internal" href="using-optimisation.html#ghc-flag-fpolymorphic-specialisation"><code>-fpolymorphic-specialisation</code></a> is not enabled (off by default) the function <strong>won’t</strong> be specialised, otherwise</li> <li>if the specialization was requested through a pragma GHC <strong>will</strong> try to create a specialization, otherwise</li> <li>if the function is imported and: + if the unfolding is not available the function <strong>can’t</strong> be specialized, otherwise + if <a class="reference internal" href="using-optimisation.html#ghc-flag-fcross-module-specialise"><code>-fcross-module-specialise</code></a> is not enabled (enabled by <code>-O</code>) the function <strong>won’t</strong> be specialised, otherwise + if the flag is enabled, and the function has no INLINABLE/INLINE pragma it <strong>won’t</strong> be specialised, otherwise</li> <li>if <a class="reference internal" href="using-optimisation.html#ghc-flag-fspecialise-aggressively"><code>-fspecialise-aggressively</code></a> is enabled GHC <strong>will</strong> try to create a specialization, otherwise</li> <li>if the overloaded function is defined in the current module, and all type class instances are statically known it <strong>will</strong> be specialized, otherwise</li> <li>the function <strong>won’t</strong> be specialized.</li> </ul> <p>Note that there are some cases in which GHC will try to specialize a function and fail. For example if a functions has an OPAQUE pragma or the unfolding is not available.</p> <p>Once a function is specialized GHC will create a rule, similar to these created by <code>RULE</code> pragmas which will fire at call sites involving known instances, replacing calls to the overloaded function with calls to the specialized function when possible.</p> </section> </section> <section id="understanding-how-os-memory-usage-corresponds-to-live-data"> <h2 id="hints-os-memory">
<span class="section-number">11.7. </span>Understanding how OS memory usage corresponds to live data</h2> <p>A confusing aspect about the RTS is the sometimes big difference between OS reported memory usage and the amount of live data reported by heap profiling or <code>GHC.Stats</code>.</p> <p>There are two main factors which determine OS memory usage.</p> <p>Firstly the collection strategy used by the oldest generation. By default a copying strategy is used which requires at least 2 times the amount of currently live data in order to perform a major collection. For example, if your program’s live data is 1G then you would expect the OS to report at minimum 2G.</p> <p>If instead you are using the compacting (<a class="reference internal" href="runtime_control.html#rts-flag-c"><code>-c</code></a>) or nonmoving (<a class="reference internal" href="runtime_control.html#rts-flag-xn"><code>-xn</code></a>) strategies for the oldest generation then less overhead is required as the strategy immediately reuses already allocated memory by overwriting. For a program with heap size 1G then you might expect the OS to report at minimum a small percentage above 1G.</p> <p>Secondly, after doing some allocation GHC is quite reluctant to return the memory to the OS. This is because after performing a major collection the program might still be allocating a lot and it costs to have to request more memory. Therefore the RTS keeps an extra amount to reuse which depends on the <a class="reference internal" href="runtime_control.html#rts-flag-F-factor"><code>-F ⟨factor⟩</code></a> option. By default the RTS will keep up to <code>(2 + F) * live_bytes</code> after performing a major collection due to exhausting the available heap. The default value is <code>F = 2</code> so you can see OS memory usage reported to be as high as 4 times the amount used by your program.</p> <p>Without further intervention, once your program has topped out at this high threshold, no more memory would be returned to the OS so memory usage would always remain at 4 times the live data. If you had a server with 1.5G live data, then if there was a memory spike up to 6G for a short period, then OS reported memory would never dip below 6G. This is what happened before GHC 9.2. In GHC 9.2 memory is gradually returned to the OS so OS memory usage returns closer to the theoretical minimums.</p> <p>The <a class="reference internal" href="runtime_control.html#rts-flag-Fd-factor"><code>-Fd ⟨factor⟩</code></a> option controls the rate at which memory is returned to the OS. On consecutive major collections which are not triggered by heap overflows, a counter (<code>t</code>) is increased and the <code>F</code> factor is inversly scaled according to the value of <code>t</code> and <code>Fd</code>. The factor is scaled by the equation:</p> <div class="math notranslate nohighlight"> \[\texttt{F}' = \texttt{F} \times {2 ^ \frac{- \texttt{t}}{\texttt{Fd}}}\]</div> <p>By default <code>Fd = 4</code>, increasing <code>Fd</code> decreases the rate memory is returned.</p> <p>Major collections which are not triggered by heap overflows arise mainly in two ways.</p>  <ol class="arabic simple"> <li>Idle collections (controlled by <a class="reference internal" href="runtime_control.html#rts-flag-I-seconds"><code>-I  ⟨seconds⟩</code></a>)</li> <li>Explicit trigger using <code>performMajorGC</code>.</li> </ol>  <p>For example, idle collections happen by default after 0.3 seconds of inactivity. If you are running your application and have also set <code>-Iw30</code>, so that the minimum period between idle GCs is 30 seconds, then say you do a small amount of work every 5 seconds, there will be about 10 idle collections about 5 minutes. This number of consecutive idle collections will scale the <code>F</code> factor as follows:</p> <div class="math notranslate nohighlight"> \[\texttt{F}' = 2 \times {2^{\frac{-10}{4}}} \approx 0.35\]</div> <p>and hence we will only retain <code>(0.35 + 2) * live_bytes</code> rather than the original 4 times. If you want less frequent idle collections then you should also decrease <code>Fd</code> so that more memory is returned each time a collection takes place.</p> <p>If you set <code>-Fd0</code> then GHC will not attempt to return memory, which corresponds with the behaviour from releases prior to 9.2. You probably don’t want to do this as unless you have idle periods in your program the behaviour will be similar anyway. If you want to retain a specific amount of memory then it’s better to set <code>-H1G</code> in order to communicate that you are happy with a heap size of <code>1G</code>. If you do this then OS reported memory will never decrease below this amount if it ever reaches this threshold.</p> <p>The collecting strategy also affects the fragmentation of the heap and hence how easy it is to return memory to a theoretical baseline. Memory is allocated firstly in the unit of megablocks which is then further divided into blocks. Block-level fragmentation is how much unused space within the allocated megablocks there is. In a fragmented heap there will be many megablocks which are only partially full.</p> <p>In theory the compacting strategy has a lower memory baseline but practically it can be hard to reach the baseline due to how compacting never defragments. On the other hand, the copying collecting has a higher theoretical baseline but we can often get very close to it because the act of copying leads to lower fragmentation.</p> <p>There are some other flags which affect the amount of retained memory as well. Setting the maximum heap size using <a class="reference internal" href="runtime_control.html#rts-flag-M-size"><code>-M ⟨size⟩</code></a> will make sure we don’t try and retain more memory than the maximum size and explicitly setting <a class="reference internal" href="runtime_control.html#rts-flag-H-size"><code>-H [⟨size⟩]</code></a> will mean that we will always try and retain at least <code>H</code> bytes irrespective of the amount of live data.</p> </section> </section> </div> </div><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2002&ndash;2007 The University Court of the University of Glasgow. All rights reserved.<br>Licensed under the Glasgow Haskell Compiler License.<br>
    <a href="https://downloads.haskell.org/~ghc/9.12.1/docs/users_guide/hints.html" class="_attribution-link">https://downloads.haskell.org/~ghc/9.12.1/docs/users_guide/hints.html</a>
  </p>
</div>
