<h1 class="devsite-page-title">tf.scan</h1> <devsite-bookmark></devsite-bookmark>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/ops/functional_ops.py#L693-L814">  View source on GitHub </a> </td> </table> <p>scan on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0. (deprecated argument values)</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.scan(
    fn,
    elems,
    initializer=None,
    parallel_iterations=10,
    back_prop=True,
    swap_memory=False,
    infer_shape=True,
    reverse=False,
    name=None
)
</pre>  <aside class="deprecated"><strong>Deprecated:</strong><span> SOME ARGUMENT VALUES ARE DEPRECATED: <code translate="no" dir="ltr">(back_prop=False)</code>. They will be removed in a future version. Instructions for updating: back_prop=False is deprecated. Consider using tf.stop_gradient instead. Instead of: results = tf.scan(fn, elems, back_prop=False) Use: results = tf.nest.map_structure(tf.stop_gradient, tf.scan(fn, elems))</span></aside> <p>The simplest version of <code translate="no" dir="ltr">scan</code> repeatedly applies the callable <code translate="no" dir="ltr">fn</code> to a sequence of elements from first to last. The elements are made of the tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0. The callable fn takes two tensors as arguments. The first argument is the accumulated value computed from the preceding invocation of fn, and the second is the value at the current position of <code translate="no" dir="ltr">elems</code>. If <code translate="no" dir="ltr">initializer</code> is None, <code translate="no" dir="ltr">elems</code> must contain at least one element, and its first element is used as the initializer.</p> <p>Suppose that <code translate="no" dir="ltr">elems</code> is unpacked into <code translate="no" dir="ltr">values</code>, a list of tensors. The shape of the result tensor is <code translate="no" dir="ltr">[len(values)] + fn(initializer, values[0]).shape</code>. If reverse=True, it's fn(initializer, values[-1]).shape.</p> <p>This method also allows multi-arity <code translate="no" dir="ltr">elems</code> and accumulator. If <code translate="no" dir="ltr">elems</code> is a (possibly nested) list or tuple of tensors, then each of these tensors must have a matching first (unpack) dimension. The second argument of <code translate="no" dir="ltr">fn</code> must match the structure of <code translate="no" dir="ltr">elems</code>.</p> <p>If no <code translate="no" dir="ltr">initializer</code> is provided, the output structure and dtypes of <code translate="no" dir="ltr">fn</code> are assumed to be the same as its input; and in this case, the first argument of <code translate="no" dir="ltr">fn</code> must match the structure of <code translate="no" dir="ltr">elems</code>.</p> <p>If an <code translate="no" dir="ltr">initializer</code> is provided, then the output of <code translate="no" dir="ltr">fn</code> must have the same structure as <code translate="no" dir="ltr">initializer</code>; and the first argument of <code translate="no" dir="ltr">fn</code> must match this structure.</p> <p>For example, if <code translate="no" dir="ltr">elems</code> is <code translate="no" dir="ltr">(t1, [t2, t3])</code> and <code translate="no" dir="ltr">initializer</code> is <code translate="no" dir="ltr">[i1, i2]</code> then an appropriate signature for <code translate="no" dir="ltr">fn</code> in <code translate="no" dir="ltr">python2</code> is: <code translate="no" dir="ltr">fn = lambda (acc_p1, acc_p2), (t1, [t2, t3]):</code> and <code translate="no" dir="ltr">fn</code> must return a list, <code translate="no" dir="ltr">[acc_n1, acc_n2]</code>. An alternative correct signature for <code translate="no" dir="ltr">fn</code>, and the one that works in <code translate="no" dir="ltr">python3</code>, is: <code translate="no" dir="ltr">fn = lambda a, t:</code>, where <code translate="no" dir="ltr">a</code> and <code translate="no" dir="ltr">t</code> correspond to the input tuples.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">fn</code> </td> <td> The callable to be performed. It accepts two arguments. The first will have the same structure as <code translate="no" dir="ltr">initializer</code> if one is provided, otherwise it will have the same structure as <code translate="no" dir="ltr">elems</code>. The second will have the same (possibly nested) structure as <code translate="no" dir="ltr">elems</code>. Its output must have the same structure as <code translate="no" dir="ltr">initializer</code> if one is provided, otherwise it must have the same structure as <code translate="no" dir="ltr">elems</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">elems</code> </td> <td> A tensor or (possibly nested) sequence of tensors, each of which will be unpacked along their first dimension. The nested sequence of the resulting slices will be the first argument to <code translate="no" dir="ltr">fn</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">initializer</code> </td> <td> (optional) A tensor or (possibly nested) sequence of tensors, initial value for the accumulator, and the expected output type of <code translate="no" dir="ltr">fn</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">parallel_iterations</code> </td> <td> (optional) The number of iterations allowed to run in parallel. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">back_prop</code> </td> <td> (optional) Deprecated. False disables support for back propagation. Prefer using <a href="stop_gradient.html"><code translate="no" dir="ltr">tf.stop_gradient</code></a> instead. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">swap_memory</code> </td> <td> (optional) True enables GPU-CPU memory swapping. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">infer_shape</code> </td> <td> (optional) False disables tests for consistent output shapes. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">reverse</code> </td> <td> (optional) True scans the tensor last to first (instead of first to last). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> (optional) Name prefix for the returned tensors. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A tensor or (possibly nested) sequence of tensors. Each tensor packs the results of applying <code translate="no" dir="ltr">fn</code> to tensors unpacked from <code translate="no" dir="ltr">elems</code> along the first dimension, and the previous accumulator value(s), from first to last (or last to first, if <code translate="no" dir="ltr">reverse=True</code>). </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if <code translate="no" dir="ltr">fn</code> is not callable or the structure of the output of <code translate="no" dir="ltr">fn</code> and <code translate="no" dir="ltr">initializer</code> do not match. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if the lengths of the output of <code translate="no" dir="ltr">fn</code> and <code translate="no" dir="ltr">initializer</code> do not match. </td> </tr> </table> <h4 id="examples" data-text="Examples:">Examples:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">elems = np.array([1, 2, 3, 4, 5, 6])
sum = scan(lambda a, x: a + x, elems)
# sum == [1, 3, 6, 10, 15, 21]
sum = scan(lambda a, x: a + x, elems, reverse=True)
# sum == [21, 20, 18, 15, 11, 6]
</pre>
<pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">elems = np.array([1, 2, 3, 4, 5, 6])
initializer = np.array(0)
sum_one = scan(
    lambda a, x: x[0] - x[1] + a, (elems + 1, elems), initializer)
# sum_one == [1, 2, 3, 4, 5, 6]
</pre>
<pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">elems = np.array([1, 0, 0, 0, 0, 0])
initializer = (np.array(0), np.array(1))
fibonaccis = scan(lambda a, _: (a[1], a[0] + a[1]), elems, initializer)
# fibonaccis == ([1, 1, 2, 3, 5, 8], [1, 2, 3, 5, 8, 13])
</pre>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/scan" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/scan</a>
  </p>
</div>
