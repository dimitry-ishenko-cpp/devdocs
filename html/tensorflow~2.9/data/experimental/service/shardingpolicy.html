<h1 class="devsite-page-title">tf.data.experimental.service.ShardingPolicy</h1> <devsite-bookmark></devsite-bookmark>       <p>Specifies how to shard data among tf.data service workers.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/service/ShardingPolicy"><code translate="no" dir="ltr">tf.compat.v1.data.experimental.service.ShardingPolicy</code></a></p> </section>  <p>OFF: No sharding will be performed. Each worker produces the entire dataset without any sharding. With this mode, the best practice is to shuffle the dataset nondeterministically so that workers process the dataset in different orders. If workers are restarted or join the cluster mid-job, they will begin processing the dataset from the beginning.</p> <p>DYNAMIC: The input dataset is dynamically split among workers at runtime. Each worker gets the next split when it reads data from the dispatcher. Data is produced non-deterministically in this mode. Dynamic sharding works well with varying-sized tf.data service clusters, e.g., when you need to auto-scale your workers. Dynamic sharding provides at-most once visitation guarantees. No examples will be repeated, but some may be missed if a tf.data service worker gets restarted while processing a file.</p> <p>The following are static sharding policies. The semantics are similar to <a href="../autoshardpolicy.html"><code translate="no" dir="ltr">tf.data.experimental.AutoShardPolicy</code></a>. These policies require:</p> <ul> <li>The tf.data service cluster is configured with a fixed list of workers in DispatcherConfig.</li> <li>Each client only reads from the local tf.data service worker.</li> </ul> <p>If a worker is restarted while performing static sharding, the worker will begin processing its shard again from the beginning.</p> <p>FILE: Shards by input files (i.e. each worker will get a fixed set of files to process). When this option is selected, make sure that there is at least as many files as workers. If there are fewer input files than workers, a runtime error will be raised.</p> <p>DATA: Shards by elements produced by the dataset. Each worker will process the whole dataset and discard the portion that is not for itself. Note that for this mode to correctly partition the dataset elements, the dataset needs to produce elements in a deterministic order.</p> <p>FILE_OR_DATA: Attempts FILE-based sharding, falling back to DATA-based sharding on failure.</p> <p>HINT: Looks for the presence of <code translate="no" dir="ltr">shard(SHARD_HINT, ...)</code> which is treated as a placeholder to replace with <code translate="no" dir="ltr">shard(num_workers, worker_index)</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Class Variables</th></tr> 
<tr> <td> DATA </td> <td> <code translate="no" dir="ltr">&lt;ShardingPolicy.DATA: 3&gt;</code> </td> </tr>
<tr> <td> DYNAMIC </td> <td> <code translate="no" dir="ltr">&lt;ShardingPolicy.DYNAMIC: 1&gt;</code> </td> </tr>
<tr> <td> FILE </td> <td> <code translate="no" dir="ltr">&lt;ShardingPolicy.FILE: 2&gt;</code> </td> </tr>
<tr> <td> FILE_OR_DATA </td> <td> <code translate="no" dir="ltr">&lt;ShardingPolicy.FILE_OR_DATA: 4&gt;</code> </td> </tr>
<tr> <td> HINT </td> <td> <code translate="no" dir="ltr">&lt;ShardingPolicy.HINT: 5&gt;</code> </td> </tr>
<tr> <td> OFF </td> <td> <code translate="no" dir="ltr">&lt;ShardingPolicy.OFF: 0&gt;</code> </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/data/experimental/service/ShardingPolicy" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/data/experimental/service/ShardingPolicy</a>
  </p>
</div>
