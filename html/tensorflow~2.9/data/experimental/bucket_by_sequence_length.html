<h1 class="devsite-page-title">tf.data.experimental.bucket_by_sequence_length</h1> <devsite-bookmark></devsite-bookmark>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/data/experimental/ops/grouping.py#L110-L257">  View source on GitHub </a> </td> </table> <p>A transformation that buckets elements in a <code translate="no" dir="ltr">Dataset</code> by length. (deprecated)</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/bucket_by_sequence_length"><code translate="no" dir="ltr">tf.compat.v1.data.experimental.bucket_by_sequence_length</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.data.experimental.bucket_by_sequence_length(
    element_length_func,
    bucket_boundaries,
    bucket_batch_sizes,
    padded_shapes=None,
    padding_values=None,
    pad_to_bucket_boundary=False,
    no_padding=False,
    drop_remainder=False
)
</pre>  <aside class="deprecated"><strong>Deprecated:</strong><span> THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use <a href="../dataset.html#bucket_by_sequence_length"><code translate="no" dir="ltr">tf.data.Dataset.bucket_by_sequence_length(...)</code></a>.</span></aside> <p>Elements of the <code translate="no" dir="ltr">Dataset</code> are grouped together by length and then are padded and batched.</p> <p>This is useful for sequence tasks in which the elements have variable length. Grouping together elements that have similar lengths reduces the total fraction of padding in a batch which increases training step efficiency.</p> <p>Below is an example to bucketize the input data to the 3 buckets "[0, 3), [3, 5), [5, inf)" based on sequence length, with batch size 2.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
elements = [
  [0], [1, 2, 3, 4], [5, 6, 7],
  [7, 8, 9, 10, 11], [13, 14, 15, 16, 19, 20], [21, 22]]
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
dataset = tf.data.Dataset.from_generator(
    lambda: elements, tf.int64, output_shapes=[None])
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
dataset = dataset.apply(
    tf.data.experimental.bucket_by_sequence_length(
        element_length_func=lambda elem: tf.shape(elem)[0],
        bucket_boundaries=[3, 5],
        bucket_batch_sizes=[2, 2, 2]))
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
for elem in dataset.as_numpy_iterator():
  print(elem)
[[1 2 3 4]
 [5 6 7 0]]
[[ 7  8  9 10 11  0]
 [13 14 15 16 19 20]]
[[ 0  0]
 [21 22]]
</pre> <p>There is also a possibility to pad the dataset till the bucket boundary. You can also provide which value to be used while padding the data. Below example uses <code translate="no" dir="ltr">-1</code> as padding and it also shows the input data being bucketizied to two buckets "[0,3], [4,6]".</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
elements = [
  [0], [1, 2, 3, 4], [5, 6, 7],
  [7, 8, 9, 10, 11], [13, 14, 15, 16, 19, 20], [21, 22]]
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
dataset = tf.data.Dataset.from_generator(
  lambda: elements, tf.int32, output_shapes=[None])
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
dataset = dataset.apply(
    tf.data.experimental.bucket_by_sequence_length(
        element_length_func=lambda elem: tf.shape(elem)[0],
        bucket_boundaries=[4, 7],
        bucket_batch_sizes=[2, 2, 2],
        pad_to_bucket_boundary=True,
        padding_values=-1))
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
for elem in dataset.as_numpy_iterator():
  print(elem)
[[ 0 -1 -1]
 [ 5  6  7]]
[[ 1  2  3  4 -1 -1]
 [ 7  8  9 10 11 -1]]
[[21 22 -1]]
[[13 14 15 16 19 20]]
</pre> <p>When using <code translate="no" dir="ltr">pad_to_bucket_boundary</code> option, it can be seen that it is not always possible to maintain the bucket batch size. You can drop the batches that do not maintain the bucket batch size by using the option <code translate="no" dir="ltr">drop_remainder</code>. Using the same input data as in the above example you get the following result.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
elements = [
  [0], [1, 2, 3, 4], [5, 6, 7],
  [7, 8, 9, 10, 11], [13, 14, 15, 16, 19, 20], [21, 22]]
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
dataset = tf.data.Dataset.from_generator(
  lambda: elements, tf.int32, output_shapes=[None])
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
dataset = dataset.apply(
    tf.data.experimental.bucket_by_sequence_length(
        element_length_func=lambda elem: tf.shape(elem)[0],
        bucket_boundaries=[4, 7],
        bucket_batch_sizes=[2, 2, 2],
        pad_to_bucket_boundary=True,
        padding_values=-1,
        drop_remainder=True))
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
for elem in dataset.as_numpy_iterator():
  print(elem)
[[ 0 -1 -1]
 [ 5  6  7]]
[[ 1  2  3  4 -1 -1]
 [ 7  8  9 10 11 -1]]
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">element_length_func</code> </td> <td> function from element in <code translate="no" dir="ltr">Dataset</code> to <a href="../../../tf.html#int32"><code translate="no" dir="ltr">tf.int32</code></a>, determines the length of the element, which will determine the bucket it goes into. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">bucket_boundaries</code> </td> <td> <code translate="no" dir="ltr">list&lt;int&gt;</code>, upper length boundaries of the buckets. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">bucket_batch_sizes</code> </td> <td> <code translate="no" dir="ltr">list&lt;int&gt;</code>, batch size per bucket. Length should be <code translate="no" dir="ltr">len(bucket_boundaries) + 1</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">padded_shapes</code> </td> <td> Nested structure of <a href="../../tensorshape.html"><code translate="no" dir="ltr">tf.TensorShape</code></a> to pass to <a href="../dataset.html#padded_batch"><code translate="no" dir="ltr">tf.data.Dataset.padded_batch</code></a>. If not provided, will use <code translate="no" dir="ltr">dataset.output_shapes</code>, which will result in variable length dimensions being padded out to the maximum length in each batch. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">padding_values</code> </td> <td> Values to pad with, passed to <a href="../dataset.html#padded_batch"><code translate="no" dir="ltr">tf.data.Dataset.padded_batch</code></a>. Defaults to padding with 0. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">pad_to_bucket_boundary</code> </td> <td> bool, if <code translate="no" dir="ltr">False</code>, will pad dimensions with unknown size to maximum length in batch. If <code translate="no" dir="ltr">True</code>, will pad dimensions with unknown size to bucket boundary minus 1 (i.e., the maximum length in each bucket), and caller must ensure that the source <code translate="no" dir="ltr">Dataset</code> does not contain any elements with length longer than <code translate="no" dir="ltr">max(bucket_boundaries)</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">no_padding</code> </td> <td> <code translate="no" dir="ltr">bool</code>, indicates whether to pad the batch features (features need to be either of type <a href="../../sparse/sparsetensor.html"><code translate="no" dir="ltr">tf.sparse.SparseTensor</code></a> or of same shape). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">drop_remainder</code> </td> <td> (Optional.) A <a href="../../../tf.html#bool"><code translate="no" dir="ltr">tf.bool</code></a> scalar <a href="../../tensor.html"><code translate="no" dir="ltr">tf.Tensor</code></a>, representing whether the last batch should be dropped in the case it has fewer than <code translate="no" dir="ltr">batch_size</code> elements; the default behavior is not to drop the smaller batch. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <code translate="no" dir="ltr">Dataset</code> transformation function, which can be passed to <a href="../dataset.html#apply"><code translate="no" dir="ltr">tf.data.Dataset.apply</code></a>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if <code translate="no" dir="ltr">len(bucket_batch_sizes) != len(bucket_boundaries) + 1</code>. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/data/experimental/bucket_by_sequence_length" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/data/experimental/bucket_by_sequence_length</a>
  </p>
</div>
