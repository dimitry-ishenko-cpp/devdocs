<h1 class="devsite-page-title">tf.keras.callbacks.TensorBoard</h1> <devsite-bookmark></devsite-bookmark>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/callbacks.py#L2073-L2639">  View source on GitHub </a> </td> </table> <p>Enable visualizations for TensorBoard.</p> <p>Inherits From: <a href="callback.html"><code translate="no" dir="ltr">Callback</code></a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.keras.callbacks.TensorBoard(
    log_dir='logs',
    histogram_freq=0,
    write_graph=True,
    write_images=False,
    write_steps_per_second=False,
    update_freq='epoch',
    profile_batch=0,
    embeddings_freq=0,
    embeddings_metadata=None,
    **kwargs
)
</pre>  <p>TensorBoard is a visualization tool provided with TensorFlow.</p> <p>This callback logs events for TensorBoard, including:</p> <ul> <li>Metrics summary plots</li> <li>Training graph visualization</li> <li>Weight histograms</li> <li>Sampled profiling</li> </ul> <p>When used in <a href="../model.html#evaluate"><code translate="no" dir="ltr">Model.evaluate</code></a>, in addition to epoch summaries, there will be a summary that records evaluation metrics vs <code translate="no" dir="ltr">Model.optimizer.iterations</code> written. The metric names will be prepended with <code translate="no" dir="ltr">evaluation</code>, with <code translate="no" dir="ltr">Model.optimizer.iterations</code> being the step in the visualized TensorBoard.</p> <p>If you have installed TensorFlow with pip, you should be able to launch TensorBoard from the command line:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">tensorboard --logdir=path_to_your_logs
</pre> <p>You can find more information about TensorBoard <a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard">here</a>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">log_dir</code> </td> <td> the path of the directory where to save the log files to be parsed by TensorBoard. e.g. log_dir = os.path.join(working_dir, 'logs') This directory should not be reused by any other callbacks. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">histogram_freq</code> </td> <td> frequency (in epochs) at which to compute weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">write_graph</code> </td> <td> whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">write_images</code> </td> <td> whether to write model weights to visualize as image in TensorBoard. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">write_steps_per_second</code> </td> <td> whether to log the training steps per second into Tensorboard. This supports both epoch and batch frequency logging. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">update_freq</code> </td> <td> <code translate="no" dir="ltr">'batch'</code> or <code translate="no" dir="ltr">'epoch'</code> or integer. When using <code translate="no" dir="ltr">'batch'</code>, writes the losses and metrics to TensorBoard after each batch. The same applies for <code translate="no" dir="ltr">'epoch'</code>. If using an integer, let's say <code translate="no" dir="ltr">1000</code>, the callback will write the metrics and losses to TensorBoard every 1000 batches. Note that writing too frequently to TensorBoard can slow down your training. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">profile_batch</code> </td> <td> Profile the batch(es) to sample compute characteristics. profile_batch must be a non-negative integer or a tuple of integers. A pair of positive integers signify a range of batches to profile. By default, profiling is disabled. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">embeddings_freq</code> </td> <td> frequency (in epochs) at which embedding layers will be visualized. If set to 0, embeddings won't be visualized. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">embeddings_metadata</code> </td> <td> Dictionary which maps embedding layer names to the filename of a file in which to save metadata for the embedding layer. In case the same metadata file is to be used for all embedding layers, a single filename can be passed. </td> </tr> </table> <h4 id="examples" data-text="Examples:">Examples:</h4> <h4 id="basic_usage" data-text="Basic usage:">Basic usage:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir="./logs")
model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])
# Then run the tensorboard command to view the visualizations.
</pre> <p>Custom batch-level summaries in a subclassed Model:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">class MyModel(tf.keras.Model):

  def build(self, _):
    self.dense = tf.keras.layers.Dense(10)

  def call(self, x):
    outputs = self.dense(x)
    tf.summary.histogram('outputs', outputs)
    return outputs

model = MyModel()
model.compile('sgd', 'mse')

# Make sure to set `update_freq=N` to log a batch-level summary every N batches.
# In addition to any `tf.summary` contained in `Model.call`, metrics added in
# `Model.compile` will be logged every N batches.
tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)
model.fit(x_train, y_train, callbacks=[tb_callback])
</pre> <p>Custom batch-level summaries in a Functional API Model:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">def my_summary(x):
  tf.summary.histogram('x', x)
  return x

inputs = tf.keras.Input(10)
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Lambda(my_summary)(x)
model = tf.keras.Model(inputs, outputs)
model.compile('sgd', 'mse')

# Make sure to set `update_freq=N` to log a batch-level summary every N batches.
# In addition to any `tf.summary` contained in `Model.call`, metrics added in
# `Model.compile` will be logged every N batches.
tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)
model.fit(x_train, y_train, callbacks=[tb_callback])
</pre> <h4 id="profiling" data-text="Profiling:">Profiling:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># Profile a single batch, e.g. the 5th batch.
tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir='./logs', profile_batch=5)
model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])

# Profile a range of batches, e.g. from 10 to 20.
tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir='./logs', profile_batch=(10,20))
model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])
</pre> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="set_model" data-text="set_model"><code translate="no" dir="ltr">set_model</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/callbacks.py#L2266-L2284">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
set_model(
    model
)
</pre> <p>Sets Keras model and writes graph if specified.</p> <h3 id="set_params" data-text="set_params"><code translate="no" dir="ltr">set_params</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/callbacks.py#L644-L645">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
set_params(
    params
)
</pre>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/callbacks/TensorBoard" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/callbacks/TensorBoard</a>
  </p>
</div>
