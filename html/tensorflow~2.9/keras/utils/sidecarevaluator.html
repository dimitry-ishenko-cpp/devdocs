<h1 class="devsite-page-title">tf.keras.utils.SidecarEvaluator</h1> <devsite-bookmark></devsite-bookmark>   <p><devsite-mathjax config="TeX-AMS-MML_SVG"></devsite-mathjax> </p>    <p>A class designed for a dedicated evaluator task.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.keras.utils.SidecarEvaluator(
    model,
    data,
    checkpoint_dir,
    steps=None,
    max_evaluations=None,
    callbacks=None
)
</pre>  <p><code translate="no" dir="ltr">SidecarEvaluator</code> is expected to be run in a process on a separate machine from the training cluster. It is meant for the purpose of a dedicated evaluator, evaluating the metric results of a training cluster which has one or more workers performing the training, and saving checkpoints.</p> <p>The <code translate="no" dir="ltr">SidecarEvaluator</code> API is compatible with both Custom Training Loop (CTL), and Keras <a href="../model.html#fit"><code translate="no" dir="ltr">Model.fit</code></a> to be used in the training cluster. Using the model (with compiled metrics) provided at <code translate="no" dir="ltr">__init__</code>, <code translate="no" dir="ltr">SidecarEvaluator</code> repeatedly performs evaluation "epochs" when it finds a checkpoint that has not yet been used. Depending on the <code translate="no" dir="ltr">steps</code> argument, an eval epoch is evaluation over all eval data, or up to certain number of steps (batches). See examples below for how the training program should save the checkpoints in order to be recognized by <code translate="no" dir="ltr">SidecarEvaluator</code>.</p> <p>Since under the hood, <code translate="no" dir="ltr">SidecarEvaluator</code> uses <code translate="no" dir="ltr">model.evaluate</code> for evaluation, it also supports arbitrary Keras callbacks. That is, if one or more callbacks are provided, their <code translate="no" dir="ltr">on_test_batch_begin</code> and <code translate="no" dir="ltr">on_test_batch_end</code> methods are called at the start and end of a batch, and their <code translate="no" dir="ltr">on_test_begin</code> and <code translate="no" dir="ltr">on_test_end</code> are called at the start and end of an evaluation epoch. Note that <code translate="no" dir="ltr">SidecarEvaluator</code> may skip some checkpoints because it always picks up the latest checkpoint available, and during an evaluation epoch, multiple checkpoints can be produced from the training side.</p> <h4 id="example" data-text="Example:">Example:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">model = tf.keras.models.Sequential(...)
model.compile(metrics=tf.keras.metrics.SparseCategoricalAccuracy(
    name="eval_metrics"))
data = tf.data.Dataset.from_tensor_slices(...)

tf.keras.SidecarEvaluator(
    model=model,
    data=data,
    checkpoint_dir='/tmp/checkpoint_dir',  # dir for training-saved checkpoint
    steps=None,  # Eval until dataset is exhausted
    max_evaluations=None,  # The evaluation needs to be stopped manually
    callbacks=[tf.keras.callbacks.TensorBoard(log_dir='/tmp/log_dir')]
).start()
</pre> <p><a href="sidecarevaluator.html#start"><code translate="no" dir="ltr">SidecarEvaluator.start</code></a> writes a series of summary files which can be visualized by tensorboard (which provides a webpage link):</p> <pre class="prettyprint lang-bash" translate="no" dir="ltr" data-language="cpp">$ tensorboard --logdir=/tmp/log_dir/validation
...
TensorBoard 2.4.0a0 at http://host:port (Press CTRL+C to quit)
</pre> <p>If the training cluster uses a CTL, the <code translate="no" dir="ltr">checkpoint_dir</code> should contain checkpoints that track both <code translate="no" dir="ltr">model</code> and <code translate="no" dir="ltr">optimizer</code>, to fulfill <code translate="no" dir="ltr">SidecarEvaluator</code>'s expectation. This can be done by a <a href="../../train/checkpoint.html"><code translate="no" dir="ltr">tf.train.Checkpoint</code></a> and a <a href="../../train/checkpointmanager.html"><code translate="no" dir="ltr">tf.train.CheckpointManager</code></a>:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">checkpoint_dir = ...  # Same `checkpoint_dir` supplied to `SidecarEvaluator`.
checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)
checkpoint_manager = tf.train.CheckpointManager(
    checkpoint, checkpoint_dir=..., max_to_keep=...)
checkpoint_manager.save()
</pre> <p>If the training cluster uses Keras <a href="../model.html#fit"><code translate="no" dir="ltr">Model.fit</code></a> API, a <a href="../callbacks/modelcheckpoint.html"><code translate="no" dir="ltr">tf.keras.callbacks.ModelCheckpoint</code></a> should be used, with <code translate="no" dir="ltr">save_weights_only=True</code>, and the <code translate="no" dir="ltr">filepath</code> should have 'ckpt-{epoch}' appended:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">checkpoint_dir = ...  # Same `checkpoint_dir` supplied to `SidecarEvaluator`.
model_checkpoint = tf.keras.callbacks.ModelCheckpoint(
    filepath=os.path.join(checkpoint_dir, 'ckpt-{epoch}'),
    save_weights_only=True)
model.fit(dataset, epochs, callbacks=[model_checkpoint])
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">model</code> </td> <td> Model to use for evaluation. The model object used here should be a <a href="../model.html"><code translate="no" dir="ltr">tf.keras.Model</code></a>, and should be the same as the one that is used in training, where <a href="../model.html"><code translate="no" dir="ltr">tf.keras.Model</code></a>s are checkpointed. The model should have one or more metrics compiled before using <code translate="no" dir="ltr">SidecarEvaluator</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">data</code> </td> <td> The input data for evaluation. <code translate="no" dir="ltr">SidecarEvaluator</code> supports all data types that Keras <code translate="no" dir="ltr">model.evaluate</code> supports as the input data <code translate="no" dir="ltr">x</code>, such as a <a href="../../data/dataset.html"><code translate="no" dir="ltr">tf.data.Dataset</code></a>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">checkpoint_dir</code> </td> <td> Directory where checkpoint files are saved. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">steps</code> </td> <td> Number of steps to perform evaluation for, when evaluating a single checkpoint file. If <code translate="no" dir="ltr">None</code>, evaluation continues until the dataset is exhausted. For repeated evaluation dataset, user must specify <code translate="no" dir="ltr">steps</code> to avoid infinite evaluation loop. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">max_evaluations</code> </td> <td> Maximum number of the checkpoint file to be evaluated, for <code translate="no" dir="ltr">SidecarEvaluator</code> to know when to stop. The evaluator will stop after it evaluates a checkpoint filepath ending with '<ckpt_name>-<max_evaluations>'. If using <a href="../../train/checkpointmanager.html#save"><code translate="no" dir="ltr">tf.train.CheckpointManager.save</code></a> for saving checkpoints, the kth saved checkpoint has the filepath suffix '<ckpt_name>-<k>' (k=1 for the first saved), and if checkpoints are saved every epoch after training, the filepath saved at the kth epoch would end with '<ckpt_name>-<k>. Thus, if training runs for n epochs, and the evaluator should end after the training finishes, use n for this parameter. Note that this is not necessarily equal to the number of total evaluations, since some checkpoints may be skipped if evaluation is slower than checkpoint creation. If <code translate="no" dir="ltr">None</code>, <code translate="no" dir="ltr">SidecarEvaluator</code> will evaluate indefinitely, and the user must terminate evaluator program themselves. </k></ckpt_name></k></ckpt_name></max_evaluations></ckpt_name>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">callbacks</code> </td> <td> List of <a href="../callbacks/callback.html"><code translate="no" dir="ltr">keras.callbacks.Callback</code></a> instances to apply during evaluation. See <a href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks">callbacks</a>. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="start" data-text="start"><code translate="no" dir="ltr">start</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/distribute/sidecar_evaluator.py#L189-L261">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
start()
</pre> <p>Starts the evaluation loop.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/utils/SidecarEvaluator" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/utils/SidecarEvaluator</a>
  </p>
</div>
