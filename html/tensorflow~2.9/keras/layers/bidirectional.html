<h1 class="devsite-page-title">tf.keras.layers.Bidirectional</h1> <devsite-bookmark></devsite-bookmark>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/layers/rnn/bidirectional.py#L34-L467">  View source on GitHub </a> </td> </table> <p>Bidirectional wrapper for RNNs.</p> <p>Inherits From: <a href="wrapper.html"><code translate="no" dir="ltr">Wrapper</code></a>, <a href="layer.html"><code translate="no" dir="ltr">Layer</code></a>, <a href="../../module.html"><code translate="no" dir="ltr">Module</code></a></p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional"><code translate="no" dir="ltr">tf.compat.v1.keras.layers.Bidirectional</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.keras.layers.Bidirectional(
    layer,
    merge_mode='concat',
    weights=None,
    backward_layer=None,
    **kwargs
)
</pre>   
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">layer</code> </td> <td> <a href="rnn.html"><code translate="no" dir="ltr">keras.layers.RNN</code></a> instance, such as <a href="lstm.html"><code translate="no" dir="ltr">keras.layers.LSTM</code></a> or <a href="gru.html"><code translate="no" dir="ltr">keras.layers.GRU</code></a>. It could also be a <a href="layer.html"><code translate="no" dir="ltr">keras.layers.Layer</code></a> instance that meets the following criteria: <ol> <li>Be a sequence-processing layer (accepts 3D+ inputs).</li> <li>Have a <code translate="no" dir="ltr">go_backwards</code>, <code translate="no" dir="ltr">return_sequences</code> and <code translate="no" dir="ltr">return_state</code> attribute (with the same semantics as for the <code translate="no" dir="ltr">RNN</code> class).</li> <li>Have an <code translate="no" dir="ltr">input_spec</code> attribute.</li> <li>Implement serialization via <code translate="no" dir="ltr">get_config()</code> and <code translate="no" dir="ltr">from_config()</code>. Note that the recommended way to create new RNN layers is to write a custom RNN cell and use it with <a href="rnn.html"><code translate="no" dir="ltr">keras.layers.RNN</code></a>, instead of subclassing <a href="layer.html"><code translate="no" dir="ltr">keras.layers.Layer</code></a> directly.</li> <li>When the <code translate="no" dir="ltr">returns_sequences</code> is true, the output of the masked timestep will be zero regardless of the layer's original <code translate="no" dir="ltr">zero_output_for_mask</code> value. </li>
</ol>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">merge_mode</code> </td> <td> Mode by which outputs of the forward and backward RNNs will be combined. One of {'sum', 'mul', 'concat', 'ave', None}. If None, the outputs will not be combined, they will be returned as a list. Default value is 'concat'. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">backward_layer</code> </td> <td> Optional <a href="rnn.html"><code translate="no" dir="ltr">keras.layers.RNN</code></a>, or <a href="layer.html"><code translate="no" dir="ltr">keras.layers.Layer</code></a> instance to be used to handle backwards input processing. If <code translate="no" dir="ltr">backward_layer</code> is not provided, the layer instance passed as the <code translate="no" dir="ltr">layer</code> argument will be used to generate the backward layer automatically. Note that the provided <code translate="no" dir="ltr">backward_layer</code> layer should have properties matching those of the <code translate="no" dir="ltr">layer</code> argument, in particular it should have the same values for <code translate="no" dir="ltr">stateful</code>, <code translate="no" dir="ltr">return_states</code>, <code translate="no" dir="ltr">return_sequences</code>, etc. In addition, <code translate="no" dir="ltr">backward_layer</code> and <code translate="no" dir="ltr">layer</code> should have different <code translate="no" dir="ltr">go_backwards</code> argument values. A <code translate="no" dir="ltr">ValueError</code> will be raised if these requirements are not met. </td> </tr> </table> <h4 id="call_arguments" data-text="Call arguments:">Call arguments:</h4> <p>The call arguments for this layer are the same as those of the wrapped RNN layer. Beware that when passing the <code translate="no" dir="ltr">initial_state</code> argument during the call of this layer, the first half in the list of elements in the <code translate="no" dir="ltr">initial_state</code> list will be passed to the forward RNN call and the last half in the list of elements will be passed to the backward RNN call.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> <ol> <li>If <code translate="no" dir="ltr">layer</code> or <code translate="no" dir="ltr">backward_layer</code> is not a <code translate="no" dir="ltr">Layer</code> instance.</li> <li>In case of invalid <code translate="no" dir="ltr">merge_mode</code> argument.</li> <li>If <code translate="no" dir="ltr">backward_layer</code> has mismatched properties compared to <code translate="no" dir="ltr">layer</code>. </li>
</ol>
</td> </tr> </table> <h4 id="examples" data-text="Examples:">Examples:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">model = Sequential()
model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

 # With custom backward layer
 model = Sequential()
 forward_layer = LSTM(10, return_sequences=True)
 backward_layer = LSTM(10, activation='relu', return_sequences=True,
                       go_backwards=True)
 model.add(Bidirectional(forward_layer, backward_layer=backward_layer,
                         input_shape=(5, 10)))
 model.add(Dense(5))
 model.add(Activation('softmax'))
 model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">constraints</code> </td> <td> 
</td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="reset_states" data-text="reset_states"><code translate="no" dir="ltr">reset_states</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/layers/rnn/bidirectional.py#L401-L403">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
reset_states()
</pre>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/Bidirectional" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/Bidirectional</a>
  </p>
</div>
