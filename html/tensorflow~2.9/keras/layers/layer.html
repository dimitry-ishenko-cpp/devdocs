<h1 class="devsite-page-title">tf.keras.layers.Layer</h1> <devsite-bookmark></devsite-bookmark>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_layer.py#L126-L3203">  View source on GitHub </a> </td> </table> <p>This is the class from which all layers inherit.</p> <p>Inherits From: <a href="../../module.html"><code translate="no" dir="ltr">Module</code></a></p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer"><code translate="no" dir="ltr">tf.compat.v1.keras.layers.Layer</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.keras.layers.Layer(
    trainable=True, name=None, dtype=None, dynamic=False, **kwargs
)
</pre>  <p>A layer is a callable object that takes as input one or more tensors and that outputs one or more tensors. It involves <em>computation</em>, defined in the <code translate="no" dir="ltr">call()</code> method, and a <em>state</em> (weight variables). State can be created in various places, at the convenience of the subclass implementer:</p> <ul> <li>in <code translate="no" dir="ltr">__init__()</code>;</li> <li>in the optional <code translate="no" dir="ltr">build()</code> method, which is invoked by the first <code translate="no" dir="ltr">__call__()</code> to the layer, and supplies the shape(s) of the input(s), which may not have been known at initialization time;</li> <li>in the first invocation of <code translate="no" dir="ltr">call()</code>, with some caveats discussed below.</li> </ul> <p>Users will just instantiate a layer and then treat it as a callable.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">trainable</code> </td> <td> Boolean, whether the layer's variables should be trainable. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> String name of the layer. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dtype</code> </td> <td> The dtype of the layer's computations and weights. Can also be a <a href="../mixed_precision/policy.html"><code translate="no" dir="ltr">tf.keras.mixed_precision.Policy</code></a>, which allows the computation and weight dtype to differ. Default of <code translate="no" dir="ltr">None</code> means to use <a href="../mixed_precision/global_policy.html"><code translate="no" dir="ltr">tf.keras.mixed_precision.global_policy()</code></a>, which is a float32 policy unless set to different value. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dynamic</code> </td> <td> Set this to <code translate="no" dir="ltr">True</code> if your layer should only be run eagerly, and should not be used to generate a static computation graph. This would be the case for a Tree-RNN or a recursive network, for example, or generally for any layer that manipulates tensors using Python control flow. If <code translate="no" dir="ltr">False</code>, we assume that the layer can safely be used to generate a static computation graph. </td> </tr> </table> <p>We recommend that descendants of <code translate="no" dir="ltr">Layer</code> implement the following methods:</p> <ul> <li>
<code translate="no" dir="ltr">__init__()</code>: Defines custom layer attributes, and creates layer weights that do not depend on input shapes, using <code translate="no" dir="ltr">add_weight()</code>, or other state.</li> <li>
<code translate="no" dir="ltr">build(self, input_shape)</code>: This method can be used to create weights that depend on the shape(s) of the input(s), using <code translate="no" dir="ltr">add_weight()</code>, or other state. <code translate="no" dir="ltr">__call__()</code> will automatically build the layer (if it has not been built yet) by calling <code translate="no" dir="ltr">build()</code>.</li> <li>
<code translate="no" dir="ltr">call(self, inputs, *args, **kwargs)</code>: Called in <code translate="no" dir="ltr">__call__</code> after making sure <code translate="no" dir="ltr">build()</code> has been called. <code translate="no" dir="ltr">call()</code> performs the logic of applying the layer to the <code translate="no" dir="ltr">inputs</code>. The first invocation may additionally create state that could not be conveniently created in <code translate="no" dir="ltr">build()</code>; see its docstring for details. Two reserved keyword arguments you can optionally use in <code translate="no" dir="ltr">call()</code> are: <ul> <li>
<code translate="no" dir="ltr">training</code> (boolean, whether the call is in inference mode or training mode). See more details in <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_training_argument_in_the_call_method">the layer/model subclassing guide</a>
</li> <li>
<code translate="no" dir="ltr">mask</code> (boolean tensor encoding masked timesteps in the input, used in RNN layers). See more details in <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_mask_argument_in_the_call_method">the layer/model subclassing guide</a> A typical signature for this method is <code translate="no" dir="ltr">call(self, inputs)</code>, and user could optionally add <code translate="no" dir="ltr">training</code> and <code translate="no" dir="ltr">mask</code> if the layer need them. <code translate="no" dir="ltr">*args</code> and <code translate="no" dir="ltr">**kwargs</code> is only useful for future extension when more input parameters are planned to be added.</li> </ul>
</li> <li>
<code translate="no" dir="ltr">get_config(self)</code>: Returns a dictionary containing the configuration used to initialize this layer. If the keys differ from the arguments in <code translate="no" dir="ltr">__init__</code>, then override <code translate="no" dir="ltr">from_config(self)</code> as well. This method is used when saving the layer or a model that contains this layer.</li> </ul> <h4 id="examples" data-text="Examples:">Examples:</h4> <p>Here's a basic example: a layer with two variables, <code translate="no" dir="ltr">w</code> and <code translate="no" dir="ltr">b</code>, that returns <code translate="no" dir="ltr">y = w . x + b</code>. It shows how to implement <code translate="no" dir="ltr">build()</code> and <code translate="no" dir="ltr">call()</code>. Variables set as attributes of a layer are tracked as weights of the layers (in <code translate="no" dir="ltr">layer.weights</code>).</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):  # Create the state of the layer (weights)
    w_init = tf.random_normal_initializer()
    self.w = tf.Variable(
        initial_value=w_init(shape=(input_shape[-1], self.units),
                             dtype='float32'),
        trainable=True)
    b_init = tf.zeros_initializer()
    self.b = tf.Variable(
        initial_value=b_init(shape=(self.units,), dtype='float32'),
        trainable=True)

  def call(self, inputs):  # Defines the computation from inputs to outputs
      return tf.matmul(inputs, self.w) + self.b

# Instantiates the layer.
linear_layer = SimpleDense(4)

# This will also call `build(input_shape)` and create the weights.
y = linear_layer(tf.ones((2, 2)))
assert len(linear_layer.weights) == 2

# These weights are trainable, so they're listed in `trainable_weights`:
assert len(linear_layer.trainable_weights) == 2
</pre> <p>Note that the method <code translate="no" dir="ltr">add_weight()</code> offers a shortcut to create weights:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):
      self.w = self.add_weight(shape=(input_shape[-1], self.units),
                               initializer='random_normal',
                               trainable=True)
      self.b = self.add_weight(shape=(self.units,),
                               initializer='random_normal',
                               trainable=True)

  def call(self, inputs):
      return tf.matmul(inputs, self.w) + self.b
</pre> <p>Besides trainable weights, updated via backpropagation during training, layers can also have non-trainable weights. These weights are meant to be updated manually during <code translate="no" dir="ltr">call()</code>. Here's a example layer that computes the running sum of its inputs:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">class ComputeSum(Layer):

  def __init__(self, input_dim):
      super(ComputeSum, self).__init__()
      # Create a non-trainable weight.
      self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),
                               trainable=False)

  def call(self, inputs):
      self.total.assign_add(tf.reduce_sum(inputs, axis=0))
      return self.total

my_sum = ComputeSum(2)
x = tf.ones((2, 2))

y = my_sum(x)
print(y.numpy())  # [2. 2.]

y = my_sum(x)
print(y.numpy())  # [4. 4.]

assert my_sum.weights == [my_sum.total]
assert my_sum.non_trainable_weights == [my_sum.total]
assert my_sum.trainable_weights == []
</pre> <p>For more information about creating layers, see the guide <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models">Making new Layers and Models via subclassing</a></p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> The name of the layer (string). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dtype</code> </td> <td> The dtype of the layer's weights. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">variable_dtype</code> </td> <td> Alias of <code translate="no" dir="ltr">dtype</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">compute_dtype</code> </td> <td> The dtype of the layer's computations. Layers automatically cast inputs to this dtype which causes the computations and output to also be in this dtype. When mixed precision is used with a <a href="../mixed_precision/policy.html"><code translate="no" dir="ltr">tf.keras.mixed_precision.Policy</code></a>, this will be different than <code translate="no" dir="ltr">variable_dtype</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dtype_policy</code> </td> <td> The layer's dtype policy. See the <a href="../mixed_precision/policy.html"><code translate="no" dir="ltr">tf.keras.mixed_precision.Policy</code></a> documentation for details. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">trainable_weights</code> </td> <td> List of variables to be included in backprop. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">non_trainable_weights</code> </td> <td> List of variables that should not be included in backprop. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">weights</code> </td> <td> The concatenation of the lists trainable_weights and non_trainable_weights (in this order). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">trainable</code> </td> <td> Whether the layer should be trained (boolean), i.e. whether its potentially-trainable weights should be returned as part of <code translate="no" dir="ltr">layer.trainable_weights</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">input_spec</code> </td> <td> Optional (list of) <code translate="no" dir="ltr">InputSpec</code> object(s) specifying the constraints on inputs that can be accepted by the layer. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">activity_regularizer</code> </td> <td> Optional regularizer function for the output of this layer. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dynamic</code> </td> <td> Whether the layer is dynamic (eager-only); set in the constructor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">input</code> </td> <td> Retrieves the input tensor(s) of a layer. <p>Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">losses</code> </td> <td> List of losses added using the <code translate="no" dir="ltr">add_loss()</code> API. <p>Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing <code translate="no" dir="ltr">losses</code> under a <a href="../../gradienttape.html"><code translate="no" dir="ltr">tf.GradientTape</code></a> will propagate gradients back to the corresponding variables.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
class MyLayer(tf.keras.layers.Layer):
  def call(self, inputs):
    self.add_loss(tf.abs(tf.reduce_mean(inputs)))
    return inputs
l = MyLayer()
l(np.ones((10, 1)))
l.losses
[1.0]
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Activity regularization.
len(model.losses)
0
model.add_loss(tf.abs(tf.reduce_mean(x)))
len(model.losses)
1
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
inputs = tf.keras.Input(shape=(10,))
d = tf.keras.layers.Dense(10, kernel_initializer='ones')
x = d(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Weight regularization.
model.add_loss(lambda: tf.reduce_mean(d.kernel))
model.losses
[&lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0&gt;]
</pre> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">metrics</code> </td> <td> List of metrics added using the <code translate="no" dir="ltr">add_metric()</code> API. <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
input = tf.keras.layers.Input(shape=(3,))
d = tf.keras.layers.Dense(2)
output = d(input)
d.add_metric(tf.reduce_max(output), name='max')
d.add_metric(tf.reduce_min(output), name='min')
[m.name for m in d.metrics]
['max', 'min']
</pre> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">output</code> </td> <td> Retrieves the output tensor(s) of a layer. <p>Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">supports_masking</code> </td> <td> Whether this layer supports computing a mask using <code translate="no" dir="ltr">compute_mask</code>. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="add_loss" data-text="add_loss"><code translate="no" dir="ltr">add_loss</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_layer.py#L1273-L1389">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
add_loss(
    losses, **kwargs
)
</pre> <p>Add loss tensor(s), potentially dependent on layer inputs.</p> <p>Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs <code translate="no" dir="ltr">a</code> and <code translate="no" dir="ltr">b</code>, some entries in <code translate="no" dir="ltr">layer.losses</code> may be dependent on <code translate="no" dir="ltr">a</code> and some on <code translate="no" dir="ltr">b</code>. This method automatically keeps track of dependencies.</p> <p>This method can be used inside a subclassed layer or model's <code translate="no" dir="ltr">call</code> function, in which case <code translate="no" dir="ltr">losses</code> should be a Tensor or list of Tensors.</p> <h4 id="example" data-text="Example:">Example:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">class MyLayer(tf.keras.layers.Layer):
  def call(self, inputs):
    self.add_loss(tf.abs(tf.reduce_mean(inputs)))
    return inputs
</pre> <p>This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's <code translate="no" dir="ltr">Input</code>s. These losses become part of the model's topology and are tracked in <code translate="no" dir="ltr">get_config</code>.</p> <h4 id="example_2" data-text="Example:">Example:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Activity regularization.
model.add_loss(tf.abs(tf.reduce_mean(x)))
</pre> <p>If this is not the case for your loss (if, for example, your loss references a <code translate="no" dir="ltr">Variable</code> of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized.</p> <h4 id="example_3" data-text="Example:">Example:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">inputs = tf.keras.Input(shape=(10,))
d = tf.keras.layers.Dense(10)
x = d(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Weight regularization.
model.add_loss(lambda: tf.reduce_mean(d.kernel))
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">losses</code> </td> <td> Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> Used for backwards compatibility only. </td> </tr> </table> <h3 id="add_metric" data-text="add_metric"><code translate="no" dir="ltr">add_metric</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_layer.py#L1416-L1536">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
add_metric(
    value, name=None, **kwargs
)
</pre> <p>Adds metric tensor to the layer.</p> <p>This method can be used inside the <code translate="no" dir="ltr">call()</code> method of a subclassed layer or model.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">class MyMetricLayer(tf.keras.layers.Layer):
  def __init__(self):
    super(MyMetricLayer, self).__init__(name='my_metric_layer')
    self.mean = tf.keras.metrics.Mean(name='metric_1')

  def call(self, inputs):
    self.add_metric(self.mean(inputs))
    self.add_metric(tf.reduce_sum(inputs), name='metric_2')
    return inputs
</pre> <p>This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's <code translate="no" dir="ltr">Input</code>s. These metrics become part of the model's topology and are tracked when you save the model via <code translate="no" dir="ltr">save()</code>.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_metric(math_ops.reduce_sum(x), name='metric_1')
</pre>
<blockquote class="note">
<strong>Note:</strong><span> Calling <code translate="no" dir="ltr">add_metric()</code> with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs.</span>
</blockquote>
<pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> Metric tensor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> String metric name. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> Additional keyword arguments for backward compatibility. Accepted values: <code translate="no" dir="ltr">aggregation</code> - When the <code translate="no" dir="ltr">value</code> tensor provided is not the result of calling a <code translate="no" dir="ltr">keras.Metric</code> instance, it will be aggregated by default using a <code translate="no" dir="ltr">keras.Metric.Mean</code>. </td> </tr> </table> <h3 id="add_weight" data-text="add_weight"><code translate="no" dir="ltr">add_weight</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_layer.py#L530-L703">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
add_weight(
    name=None,
    shape=None,
    dtype=None,
    initializer=None,
    regularizer=None,
    trainable=None,
    constraint=None,
    use_resource=None,
    synchronization=tf.VariableSynchronization.AUTO,
    aggregation=tf.VariableAggregation.NONE,
    **kwargs
)
</pre> <p>Adds a new variable to the layer.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Variable name. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">shape</code> </td> <td> Variable shape. Defaults to scalar if unspecified. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dtype</code> </td> <td> The type of the variable. Defaults to <code translate="no" dir="ltr">self.dtype</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">initializer</code> </td> <td> Initializer instance (callable). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">regularizer</code> </td> <td> Regularizer instance (callable). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">trainable</code> </td> <td> Boolean, whether the variable should be part of the layer's "trainable_variables" (e.g. variables, biases) or "non_trainable_variables" (e.g. BatchNorm mean and variance). Note that <code translate="no" dir="ltr">trainable</code> cannot be <code translate="no" dir="ltr">True</code> if <code translate="no" dir="ltr">synchronization</code> is set to <code translate="no" dir="ltr">ON_READ</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">constraint</code> </td> <td> Constraint instance (callable). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">use_resource</code> </td> <td> Whether to use <code translate="no" dir="ltr">ResourceVariable</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">synchronization</code> </td> <td> Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class <a href="../../variablesynchronization.html"><code translate="no" dir="ltr">tf.VariableSynchronization</code></a>. By default the synchronization is set to <code translate="no" dir="ltr">AUTO</code> and the current <code translate="no" dir="ltr">DistributionStrategy</code> chooses when to synchronize. If <code translate="no" dir="ltr">synchronization</code> is set to <code translate="no" dir="ltr">ON_READ</code>, <code translate="no" dir="ltr">trainable</code> must not be set to <code translate="no" dir="ltr">True</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">aggregation</code> </td> <td> Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class <a href="../../variableaggregation.html"><code translate="no" dir="ltr">tf.VariableAggregation</code></a>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> Additional keyword arguments. Accepted values are <code translate="no" dir="ltr">getter</code>, <code translate="no" dir="ltr">collections</code>, <code translate="no" dir="ltr">experimental_autocast</code> and <code translate="no" dir="ltr">caching_device</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> The variable created. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as <code translate="no" dir="ltr">ON_READ</code>. </td> </tr> </table> <h3 id="build" data-text="build"><code translate="no" dir="ltr">build</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_layer.py#L462-L483">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
build(
    input_shape
)
</pre> <p>Creates the variables of the layer (optional, for subclass implementers).</p> <p>This is a method that implementers of subclasses of <code translate="no" dir="ltr">Layer</code> or <code translate="no" dir="ltr">Model</code> can override if they need a state-creation step in-between layer instantiation and layer call. It is invoked automatically before the first execution of <code translate="no" dir="ltr">call()</code>.</p> <p>This is typically used to create the weights of <code translate="no" dir="ltr">Layer</code> subclasses (at the discretion of the subclass implementer).</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input_shape</code> </td> <td> Instance of <code translate="no" dir="ltr">TensorShape</code>, or list of instances of <code translate="no" dir="ltr">TensorShape</code> if the layer expects a list of inputs (one instance per input). </td> </tr> </table> <h3 id="call" data-text="call"><code translate="no" dir="ltr">call</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_layer.py#L485-L528">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
call(
    inputs, *args, **kwargs
)
</pre> <p>This is where the layer's logic lives.</p> <p>The <code translate="no" dir="ltr">call()</code> method may not create state (except in its first invocation, wrapping the creation of variables or other resources in <a href="../../init_scope.html"><code translate="no" dir="ltr">tf.init_scope()</code></a>). It is recommended to create state in <code translate="no" dir="ltr">__init__()</code>, or the <code translate="no" dir="ltr">build()</code> method that is called automatically before <code translate="no" dir="ltr">call()</code> executes the first time.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">inputs</code> </td> <td> Input tensor, or dict/list/tuple of input tensors. The first positional <code translate="no" dir="ltr">inputs</code> argument is subject to special rules: <ul> <li>
<code translate="no" dir="ltr">inputs</code> must be explicitly passed. A layer cannot have zero arguments, and <code translate="no" dir="ltr">inputs</code> cannot be provided via the default value of a keyword argument.</li> <li>NumPy array or Python scalar values in <code translate="no" dir="ltr">inputs</code> get cast as tensors.</li> <li>Keras mask metadata is only collected from <code translate="no" dir="ltr">inputs</code>.</li> <li>Layers are built (<code translate="no" dir="ltr">build(input_shape)</code> method) using shape info from <code translate="no" dir="ltr">inputs</code> only.</li> <li>
<code translate="no" dir="ltr">input_spec</code> compatibility is only checked against <code translate="no" dir="ltr">inputs</code>.</li> <li>Mixed precision input casting is only applied to <code translate="no" dir="ltr">inputs</code>. If a layer has tensor arguments in <code translate="no" dir="ltr">*args</code> or <code translate="no" dir="ltr">**kwargs</code>, their casting behavior in mixed precision should be handled manually.</li> <li>The SavedModel input specification is generated using <code translate="no" dir="ltr">inputs</code> only.</li> <li>Integration with various ecosystem packages like TFMOT, TFLite, TF.js, etc is only supported for <code translate="no" dir="ltr">inputs</code> and not for tensors in positional and keyword arguments. </li>
</ul>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">*args</code> </td> <td> Additional positional arguments. May contain tensors, although this is not recommended, for the reasons above. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> Additional keyword arguments. May contain tensors, although this is not recommended, for the reasons above. The following optional keyword arguments are reserved: <li>
<code translate="no" dir="ltr">training</code>: Boolean scalar tensor of Python boolean indicating whether the <code translate="no" dir="ltr">call</code> is meant for training or inference.</li> <li>
<code translate="no" dir="ltr">mask</code>: Boolean input mask. If the layer's <code translate="no" dir="ltr">call()</code> method takes a <code translate="no" dir="ltr">mask</code> argument, its default value will be set to the mask generated for <code translate="no" dir="ltr">inputs</code> by the previous layer (if <code translate="no" dir="ltr">input</code> did come from a layer that generated a corresponding mask, i.e. if it came from a Keras layer with masking support). </li>
</td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A tensor or list/tuple of tensors. </td> </tr> 
</table> <h3 id="compute_mask" data-text="compute_mask"><code translate="no" dir="ltr">compute_mask</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_layer.py#L871-L891">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
compute_mask(
    inputs, mask=None
)
</pre> <p>Computes an output mask tensor.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">inputs</code> </td> <td> Tensor or list of tensors. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">mask</code> </td> <td> Tensor or list of tensors. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> None or a tensor (or list of tensors, one per output tensor of the layer). </td> </tr> 
</table> <h3 id="compute_output_shape" data-text="compute_output_shape"><code translate="no" dir="ltr">compute_output_shape</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_layer.py#L785-L829">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
compute_output_shape(
    input_shape
)
</pre> <p>Computes the output shape of the layer.</p> <p>This method will cause the layer's state to be built, if that has not happened before. This requires that the layer will later be used with inputs that match the input shape provided here.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input_shape</code> </td> <td> Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An input shape tuple. </td> </tr> 
</table> <h3 id="compute_output_signature" data-text="compute_output_signature"><code translate="no" dir="ltr">compute_output_signature</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_layer.py#L831-L869">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
compute_output_signature(
    input_signature
)
</pre> <p>Compute the output tensor signature of the layer based on the inputs.</p> <p>Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use <code translate="no" dir="ltr">compute_output_shape</code>, and will assume that the output dtype matches the input dtype.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input_signature</code> </td> <td> Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If input_signature contains a non-TensorSpec object. </td> </tr> </table> <h3 id="count_params" data-text="count_params"><code translate="no" dir="ltr">count_params</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_layer.py#L1948-L1968">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
count_params()
</pre> <p>Count the total number of scalars composing the weights.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An integer count. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if the layer isn't yet built (in which case its weights aren't yet defined). </td> </tr> </table> <h3 id="from_config" data-text="from_config"><code translate="no" dir="ltr">from_config</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_layer.py#L767-L783">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
@classmethod
from_config(
    config
)
</pre> <p>Creates a layer from its config.</p> <p>This method is the reverse of <code translate="no" dir="ltr">get_config</code>, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by <code translate="no" dir="ltr">set_weights</code>).</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">config</code> </td> <td> A Python dictionary, typically the output of get_config. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A layer instance. </td> </tr> 
</table> <h3 id="get_config" data-text="get_config"><code translate="no" dir="ltr">get_config</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_layer.py#L705-L765">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_config()
</pre> <p>Returns the config of the layer.</p> <p>A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration.</p> <p>The config of a layer does not include connectivity information, nor the layer class name. These are handled by <code translate="no" dir="ltr">Network</code> (one layer of abstraction above).</p> <p>Note that <code translate="no" dir="ltr">get_config()</code> does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Python dictionary. </td> </tr> 
</table> <h3 id="get_weights" data-text="get_weights"><code translate="no" dir="ltr">get_weights</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_layer.py#L1656-L1698">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_weights()
</pre> <p>Returns the current weights of the layer, as NumPy arrays.</p> <p>The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers.</p> <p>For example, a <code translate="no" dir="ltr">Dense</code> layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another <code translate="no" dir="ltr">Dense</code> layer:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
layer_a = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
layer_b = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
[array([[2.],
       [2.],
       [2.]], dtype=float32), array([0.], dtype=float32)]
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Weights values as a list of NumPy arrays. </td> </tr> 
</table> <h3 id="set_weights" data-text="set_weights"><code translate="no" dir="ltr">set_weights</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_layer.py#L1570-L1654">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
set_weights(
    weights
)
</pre> <p>Sets the weights of the layer, from NumPy arrays.</p> <p>The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer.</p> <p>For example, a <code translate="no" dir="ltr">Dense</code> layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another <code translate="no" dir="ltr">Dense</code> layer:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
layer_a = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
layer_b = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
[array([[2.],
       [2.],
       [2.]], dtype=float32), array([0.], dtype=float32)]
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">weights</code> </td> <td> a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of <code translate="no" dir="ltr">get_weights</code>). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If the provided weights list does not match the layer's specifications. </td> </tr> </table> <h3 id="__call__" data-text="__call__"><code translate="no" dir="ltr">__call__</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_layer.py#L893-L1023">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
__call__(
    *args, **kwargs
)
</pre> <p>Wraps <code translate="no" dir="ltr">call</code>, applying pre- and post-processing steps.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">*args</code> </td> <td> Positional arguments to be passed to <code translate="no" dir="ltr">self.call</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> Keyword arguments to be passed to <code translate="no" dir="ltr">self.call</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Output tensor(s). </td> </tr> 
</table> <h4 id="note" data-text="Note:">Note:</h4> <ul> <li>The following optional keyword arguments are reserved for specific uses: <ul> <li>
<code translate="no" dir="ltr">training</code>: Boolean scalar tensor of Python boolean indicating whether the <code translate="no" dir="ltr">call</code> is meant for training or inference.</li> <li>
<code translate="no" dir="ltr">mask</code>: Boolean input mask.</li> </ul>
</li> <li>If the layer's <code translate="no" dir="ltr">call</code> method takes a <code translate="no" dir="ltr">mask</code> argument (as some Keras layers do), its default value will be set to the mask generated for <code translate="no" dir="ltr">inputs</code> by the previous layer (if <code translate="no" dir="ltr">input</code> did come from a layer that generated a corresponding mask, i.e. if it came from a Keras layer with masking support.</li> <li>If the layer is not built, the method will call <code translate="no" dir="ltr">build</code>.</li> </ul>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if the layer's <code translate="no" dir="ltr">call</code> method returns None (an invalid value). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> if <code translate="no" dir="ltr">super().__init__()</code> was not called in the constructor. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/Layer" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/Layer</a>
  </p>
</div>
