<h1 class="devsite-page-title">Module: tf.keras.optimizers.legacy</h1> <devsite-bookmark></devsite-bookmark>       <p>Public API for tf.keras.optimizers.legacy namespace.</p> <h2 id="classes" data-text="Classes">Classes</h2> <p><a href="legacy/adadelta.html"><code translate="no" dir="ltr">class Adadelta</code></a>: Optimizer that implements the Adadelta algorithm.</p> <p><a href="legacy/adagrad.html"><code translate="no" dir="ltr">class Adagrad</code></a>: Optimizer that implements the Adagrad algorithm.</p> <p><a href="legacy/adam.html"><code translate="no" dir="ltr">class Adam</code></a>: Optimizer that implements the Adam algorithm.</p> <p><a href="legacy/adamax.html"><code translate="no" dir="ltr">class Adamax</code></a>: Optimizer that implements the Adamax algorithm.</p> <p><a href="legacy/ftrl.html"><code translate="no" dir="ltr">class Ftrl</code></a>: Optimizer that implements the FTRL algorithm.</p> <p><a href="legacy/nadam.html"><code translate="no" dir="ltr">class Nadam</code></a>: Optimizer that implements the NAdam algorithm.</p> <p><a href="legacy/optimizer.html"><code translate="no" dir="ltr">class Optimizer</code></a>: Base class for Keras optimizers.</p> <p><a href="legacy/rmsprop.html"><code translate="no" dir="ltr">class RMSprop</code></a>: Optimizer that implements the RMSprop algorithm.</p> <p><a href="legacy/sgd.html"><code translate="no" dir="ltr">class SGD</code></a>: Gradient descent (with momentum) optimizer.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/optimizers/legacy" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/optimizers/legacy</a>
  </p>
</div>
