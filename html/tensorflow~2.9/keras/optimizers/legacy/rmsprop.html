<h1 class="devsite-page-title">tf.keras.optimizers.legacy.RMSprop</h1> <devsite-bookmark></devsite-bookmark>       <p>Optimizer that implements the RMSprop algorithm.</p> <p>Inherits From: <a href="../rmsprop.html"><code translate="no" dir="ltr">RMSprop</code></a>, <a href="../optimizer.html"><code translate="no" dir="ltr">Optimizer</code></a></p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/legacy/RMSprop"><code translate="no" dir="ltr">tf.compat.v1.keras.optimizers.legacy.RMSprop</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.keras.optimizers.legacy.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    name='RMSprop',
    **kwargs
)
</pre>  <p>The gist of RMSprop is to:</p> <ul> <li>Maintain a moving (discounted) average of the square of gradients</li> <li>Divide the gradient by the root of this average</li> </ul> <p>This implementation of RMSprop uses plain momentum, not Nesterov momentum.</p> <p>The centered version additionally maintains a moving average of the gradients, and uses that average to estimate the variance.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">learning_rate</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code>, floating point value, or a schedule that is a <a href="../schedules/learningrateschedule.html"><code translate="no" dir="ltr">tf.keras.optimizers.schedules.LearningRateSchedule</code></a>, or a callable that takes no arguments and returns the actual value to use. The learning rate. Defaults to 0.001. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">rho</code> </td> <td> Discounting factor for the history/coming gradient. Defaults to 0.9. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">momentum</code> </td> <td> A scalar or a scalar <code translate="no" dir="ltr">Tensor</code>. Defaults to 0.0. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">epsilon</code> </td> <td> A small constant for numerical stability. This epsilon is "epsilon hat" in the Kingma and Ba paper (in the formula just before Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to 1e-7. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">centered</code> </td> <td> Boolean. If <code translate="no" dir="ltr">True</code>, gradients are normalized by the estimated variance of the gradient; if False, by the uncentered second moment. Setting this to <code translate="no" dir="ltr">True</code> may help with training, but is slightly more expensive in terms of computation and memory. Defaults to <code translate="no" dir="ltr">False</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name prefix for the operations created when applying gradients. Defaults to <code translate="no" dir="ltr">"RMSprop"</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> keyword arguments. Allowed arguments are <code translate="no" dir="ltr">clipvalue</code>, <code translate="no" dir="ltr">clipnorm</code>, <code translate="no" dir="ltr">global_clipnorm</code>. If <code translate="no" dir="ltr">clipvalue</code> (float) is set, the gradient of each weight is clipped to be no higher than this value. If <code translate="no" dir="ltr">clipnorm</code> (float) is set, the gradient of each weight is individually clipped so that its norm is no higher than this value. If <code translate="no" dir="ltr">global_clipnorm</code> (float) is set the gradient of all weights is clipped so that their global norm is no higher than this value. </td> </tr> </table> <p>Note that in the dense implementation of this algorithm, variables and their corresponding accumulators (momentum, gradient moving average, square gradient moving average) will be updated even if the gradient is zero (i.e. accumulators will decay, momentum will be applied). The sparse implementation (used when the gradient is an <code translate="no" dir="ltr">IndexedSlices</code> object, typically because of <a href="../../../gather.html"><code translate="no" dir="ltr">tf.gather</code></a> or an embedding lookup in the forward pass) will not update variable slices or their accumulators unless those slices were used in the forward pass (nor is there an "eventual" correction to account for these omitted updates). This leads to more efficient updates for large embedding lookup tables (where most of the slices are not accessed in a particular graph execution), but differs from the published algorithm.</p> <h4 id="usage" data-text="Usage:">Usage:</h4> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
opt = tf.keras.optimizers.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
loss = lambda: (var1 ** 2) / 2.0    # d(loss) / d(var1) = var1
step_count = opt.minimize(loss, [var1]).numpy()
var1.numpy()
9.683772
</pre> <h4 id="reference" data-text="Reference:">Reference:</h4> <ul> <li><a href="http://www.cs.toronto.edu/%7Etijmen/csc321/slides/lecture_slides_lec6.pdf">Hinton, 2012</a></li> </ul>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/optimizers/legacy/RMSprop" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/optimizers/legacy/RMSprop</a>
  </p>
</div>
