<h1 class="devsite-page-title">tf.keras.activations.gelu</h1> <devsite-bookmark></devsite-bookmark>       <p>Applies the Gaussian error linear unit (GELU) activation function.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.keras.activations.gelu(
    x, approximate=False
)
</pre>  <p>Gaussian error linear unit (GELU) computes <code translate="no" dir="ltr">x * P(X &lt;= x)</code>, where <code translate="no" dir="ltr">P(X) ~ N(0, 1)</code>. The (GELU) nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLU.</p> <h4 id="for_example" data-text="For example:">For example:</h4> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
x = tf.constant([-3.0, -1.0, 0.0, 1.0, 3.0], dtype=tf.float32)
y = tf.keras.activations.gelu(x)
y.numpy()
array([-0.00404951, -0.15865529,  0.        ,  0.8413447 ,  2.9959507 ],
    dtype=float32)
y = tf.keras.activations.gelu(x, approximate=True)
y.numpy()
array([-0.00363752, -0.15880796,  0.        ,  0.841192  ,  2.9963627 ],
    dtype=float32)
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">x</code> </td> <td> Input tensor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">approximate</code> </td> <td> A <code translate="no" dir="ltr">bool</code>, whether to enable approximation. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> The gaussian error linear activation: <code translate="no" dir="ltr">0.5 * x * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x^3)))</code> if <code translate="no" dir="ltr">approximate</code> is <code translate="no" dir="ltr">True</code> or <code translate="no" dir="ltr">x * P(X &lt;= x) = 0.5 * x * (1 + erf(x / sqrt(2)))</code>, where <code translate="no" dir="ltr">P(X) ~ N(0, 1)</code>, if <code translate="no" dir="ltr">approximate</code> is <code translate="no" dir="ltr">False</code>. </td> </tr> 
</table> <h4 id="reference" data-text="Reference:">Reference:</h4> <ul> <li><a href="https://arxiv.org/abs/1606.08415">Gaussian Error Linear Units (GELUs)</a></li> </ul>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/activations/gelu" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/activations/gelu</a>
  </p>
</div>
