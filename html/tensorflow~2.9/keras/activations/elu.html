<h1 class="devsite-page-title">tf.keras.activations.elu</h1> <devsite-bookmark></devsite-bookmark>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/activations.py#L95-L142">  View source on GitHub </a> </td> </table> <p>Exponential Linear Unit.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/elu"><code translate="no" dir="ltr">tf.compat.v1.keras.activations.elu</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.keras.activations.elu(
    x, alpha=1.0
)
</pre>  <p>The exponential linear unit (ELU) with <code translate="no" dir="ltr">alpha &gt; 0</code> is: <code translate="no" dir="ltr">x</code> if <code translate="no" dir="ltr">x &gt; 0</code> and <code translate="no" dir="ltr">alpha * (exp(x) - 1)</code> if <code translate="no" dir="ltr">x &lt; 0</code> The ELU hyperparameter <code translate="no" dir="ltr">alpha</code> controls the value to which an ELU saturates for negative net inputs. ELUs diminish the vanishing gradient effect.</p> <p>ELUs have negative values which pushes the mean of the activations closer to zero. Mean activations that are closer to zero enable faster learning as they bring the gradient closer to the natural gradient. ELUs saturate to a negative value when the argument gets smaller. Saturation means a small derivative which decreases the variation and the information that is propagated to the next layer.</p> <h4 id="example_usage" data-text="Example Usage:">Example Usage:</h4> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
import tensorflow as tf
model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='elu',
         input_shape=(28, 28, 1)))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))
</pre> <p><tensorflow.python.keras.engine.sequential.sequential object ...></tensorflow.python.keras.engine.sequential.sequential></p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">x</code> </td> <td> Input tensor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">alpha</code> </td> <td> A scalar, slope of negative section. <code translate="no" dir="ltr">alpha</code> controls the value to which an ELU saturates for negative net inputs. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> The exponential linear unit (ELU) activation function: <code translate="no" dir="ltr">x</code> if <code translate="no" dir="ltr">x &gt; 0</code> and <code translate="no" dir="ltr">alpha * (exp(x) - 1)</code> if <code translate="no" dir="ltr">x &lt; 0</code>. </td> </tr> 
</table> <h4 id="reference" data-text="Reference:">Reference:</h4> <p><a href="https://arxiv.org/abs/1511.07289">Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) (Clevert et al, 2016)</a></p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/activations/elu" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/activations/elu</a>
  </p>
</div>
