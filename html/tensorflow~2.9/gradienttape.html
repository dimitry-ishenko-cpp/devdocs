<h1 class="devsite-page-title">tf.GradientTape</h1> <devsite-bookmark></devsite-bookmark>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/eager/backprop.py#L739-L1384">  View source on GitHub </a> </td> </table> <p>Record operations for automatic differentiation.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Main aliases</b> </p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape"><code translate="no" dir="ltr">tf.autodiff.GradientTape</code></a></p> <b>Compat aliases for migration</b> <p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape"><code translate="no" dir="ltr">tf.compat.v1.GradientTape</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.GradientTape(
    persistent=False, watch_accessed_variables=True
)
</pre>  <p>Operations are recorded if they are executed within this context manager and at least one of their inputs is being "watched".</p> <p>Trainable variables (created by <a href="variable.html"><code translate="no" dir="ltr">tf.Variable</code></a> or <a href="compat/v1/get_variable.html"><code translate="no" dir="ltr">tf.compat.v1.get_variable</code></a>, where <code translate="no" dir="ltr">trainable=True</code> is default in both cases) are automatically watched. Tensors can be manually watched by invoking the <code translate="no" dir="ltr">watch</code> method on this context manager.</p> <p>For example, consider the function <code translate="no" dir="ltr">y = x * x</code>. The gradient at <code translate="no" dir="ltr">x = 3.0</code> can be computed as:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
x = tf.constant(3.0)
with tf.GradientTape() as g:
  g.watch(x)
  y = x * x
dy_dx = g.gradient(y, x)
print(dy_dx)
tf.Tensor(6.0, shape=(), dtype=float32)
</pre> <p>GradientTapes can be nested to compute higher-order derivatives. For example,</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
x = tf.constant(5.0)
with tf.GradientTape() as g:
  g.watch(x)
  with tf.GradientTape() as gg:
    gg.watch(x)
    y = x * x
  dy_dx = gg.gradient(y, x)  # dy_dx = 2 * x
d2y_dx2 = g.gradient(dy_dx, x)  # d2y_dx2 = 2
print(dy_dx)
tf.Tensor(10.0, shape=(), dtype=float32)
print(d2y_dx2)
tf.Tensor(2.0, shape=(), dtype=float32)
</pre> <p>By default, the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected. For example:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
x = tf.constant(3.0)
with tf.GradientTape(persistent=True) as g:
  g.watch(x)
  y = x * x
  z = y * y
dz_dx = g.gradient(z, x)  # (4*x^3 at x = 3)
print(dz_dx)
tf.Tensor(108.0, shape=(), dtype=float32)
dy_dx = g.gradient(y, x)
print(dy_dx)
tf.Tensor(6.0, shape=(), dtype=float32)
</pre> <p>By default GradientTape will automatically watch any trainable variables that are accessed inside the context. If you want fine grained control over which variables are watched you can disable automatic tracking by passing <code translate="no" dir="ltr">watch_accessed_variables=False</code> to the tape constructor:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
x = tf.Variable(2.0)
w = tf.Variable(5.0)
with tf.GradientTape(
    watch_accessed_variables=False, persistent=True) as tape:
  tape.watch(x)
  y = x ** 2  # Gradients will be available for `x`.
  z = w ** 3  # No gradients will be available as `w` isn't being watched.
dy_dx = tape.gradient(y, x)
print(dy_dx)
tf.Tensor(4.0, shape=(), dtype=float32)
# No gradients will be available as `w` isn't being watched.
dz_dw = tape.gradient(z, w)
print(dz_dw)
None
</pre> <p>Note that when using models you should ensure that your variables exist when using <code translate="no" dir="ltr">watch_accessed_variables=False</code>. Otherwise it's quite easy to make your first iteration not have any gradients:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">a = tf.keras.layers.Dense(32)
b = tf.keras.layers.Dense(32)

with tf.GradientTape(watch_accessed_variables=False) as tape:
  tape.watch(a.variables)  # Since `a.build` has not been called at this point
                           # `a.variables` will return an empty list and the
                           # tape will not be watching anything.
  result = b(a(inputs))
  tape.gradient(result, a.variables)  # The result of this computation will be
                                      # a list of `None`s since a's variables
                                      # are not being watched.
</pre> <p>Note that only tensors with real or complex dtypes are differentiable.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">persistent</code> </td> <td> Boolean controlling whether a persistent gradient tape is created. False by default, which means at most one call can be made to the gradient() method on this object. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">watch_accessed_variables</code> </td> <td> Boolean controlling whether the tape will automatically <code translate="no" dir="ltr">watch</code> any (trainable) variables accessed while the tape is active. Defaults to True meaning gradients can be requested from any result computed in the tape derived from reading a trainable <code translate="no" dir="ltr">Variable</code>. If False users must explicitly <code translate="no" dir="ltr">watch</code> any <code translate="no" dir="ltr">Variable</code>s they want to request gradients from. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="batch_jacobian" data-text="batch_jacobian"><code translate="no" dir="ltr">batch_jacobian</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/eager/backprop.py#L1234-L1384">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
batch_jacobian(
    target,
    source,
    unconnected_gradients=tf.UnconnectedGradients.NONE,
    parallel_iterations=None,
    experimental_use_pfor=True
)
</pre> <p>Computes and stacks per-example jacobians.</p> <p>See <a href="http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant">wikipedia article</a> for the definition of a Jacobian. This function is essentially an efficient implementation of the following:</p> <p><code translate="no" dir="ltr">tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])])</code>.</p> <p>Note that compared to <a href="gradienttape.html#jacobian"><code translate="no" dir="ltr">GradientTape.jacobian</code></a> which computes gradient of each output value w.r.t each input value, this function is useful when <code translate="no" dir="ltr">target[i,...]</code> is independent of <code translate="no" dir="ltr">source[j,...]</code> for <code translate="no" dir="ltr">j != i</code>. This assumption allows more efficient computation as compared to <a href="gradienttape.html#jacobian"><code translate="no" dir="ltr">GradientTape.jacobian</code></a>. The output, as well as intermediate activations, are lower dimensional and avoid a bunch of redundant zeros which would result in the jacobian computation given the independence assumption.</p> <blockquote class="note">
<strong>Note:</strong><span> Unless you set <code translate="no" dir="ltr">persistent=True</code> a GradientTape can only be used to compute one set of gradients (or jacobians).</span>
</blockquote>
<blockquote class="note">
<strong>Note:</strong><span> By default the batch_jacobian implementation uses parallel for (pfor), which creates a tf.function under the hood for each batch_jacobian call. For better performance, and to avoid recompilation and vectorization rewrites on each call, enclose GradientTape code in @tf.function.</span>
</blockquote> <h4 id="example_usage" data-text="Example usage:">Example usage:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">with tf.GradientTape() as g:
  x = tf.constant([[1., 2.], [3., 4.]], dtype=tf.float32)
  g.watch(x)
  y = x * x
batch_jacobian = g.batch_jacobian(y, x)
# batch_jacobian is [[[2,  0], [0,  4]], [[6,  0], [0,  8]]]
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">target</code> </td> <td> A tensor with rank 2 or higher and with shape [b, y1, ..., y_n]. <code translate="no" dir="ltr">target[i,...]</code> should only depend on <code translate="no" dir="ltr">source[i,...]</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">source</code> </td> <td> A tensor with rank 2 or higher and with shape [b, x1, ..., x_m]. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">unconnected_gradients</code> </td> <td> a value which can either hold 'none' or 'zero' and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">parallel_iterations</code> </td> <td> A knob to control how many iterations are dispatched in parallel. This knob can be used to control the total memory usage. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_use_pfor</code> </td> <td> If true, uses pfor for computing the Jacobian. Else uses a tf.while_loop. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A tensor <code translate="no" dir="ltr">t</code> with shape [b, y_1, ..., y_n, x1, ..., x_m] where <code translate="no" dir="ltr">t[i, ...]</code> is the jacobian of <code translate="no" dir="ltr">target[i, ...]</code> w.r.t. <code translate="no" dir="ltr">source[i, ...]</code>, i.e. stacked per-example jacobians. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> If called on a used, non-persistent tape. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> If called on a non-persistent tape with eager execution enabled and without enabling experimental_use_pfor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If vectorization of jacobian computation fails or if first dimension of <code translate="no" dir="ltr">target</code> and <code translate="no" dir="ltr">source</code> do not match. </td> </tr> </table> <h3 id="gradient" data-text="gradient"><code translate="no" dir="ltr">gradient</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/eager/backprop.py#L995-L1116">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
gradient(
    target,
    sources,
    output_gradients=None,
    unconnected_gradients=tf.UnconnectedGradients.NONE
)
</pre> <p>Computes the gradient using operations recorded in context of this tape.</p> <blockquote class="note">
<strong>Note:</strong><span> Unless you set <code translate="no" dir="ltr">persistent=True</code> a GradientTape can only be used to compute one set of gradients (or jacobians).</span>
</blockquote> <p>In addition to Tensors, gradient also supports RaggedTensors. For example,</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
x = tf.ragged.constant([[1.0, 2.0], [3.0]])
with tf.GradientTape() as g:
  g.watch(x)
  y = x * x
g.gradient(y, x)
&lt;tf.RaggedTensor [[2.0, 4.0], [6.0]]&gt;
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">target</code> </td> <td> a list or nested structure of Tensors or Variables or CompositeTensors to be differentiated. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">sources</code> </td> <td> a list or nested structure of Tensors or Variables or CompositeTensors. <code translate="no" dir="ltr">target</code> will be differentiated against elements in <code translate="no" dir="ltr">sources</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">output_gradients</code> </td> <td> a list of gradients, one for each differentiable element of target. Defaults to None. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">unconnected_gradients</code> </td> <td> a value which can either hold 'none' or 'zero' and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> a list or nested structure of Tensors (or IndexedSlices, or None, or CompositeTensor), one for each element in <code translate="no" dir="ltr">sources</code>. Returned structure is the same as the structure of <code translate="no" dir="ltr">sources</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> If called on a used, non-persistent tape. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> If called inside the context of the tape. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If the target is a None object. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If the target is a variable or if unconnected gradients is called with an unknown value. </td> </tr> </table> <h3 id="jacobian" data-text="jacobian"><code translate="no" dir="ltr">jacobian</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/eager/backprop.py#L1118-L1232">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
jacobian(
    target,
    sources,
    unconnected_gradients=tf.UnconnectedGradients.NONE,
    parallel_iterations=None,
    experimental_use_pfor=True
)
</pre> <p>Computes the jacobian using operations recorded in context of this tape.</p> <blockquote class="note">
<strong>Note:</strong><span> Unless you set <code translate="no" dir="ltr">persistent=True</code> a GradientTape can only be used to compute one set of gradients (or jacobians).</span>
</blockquote>
<blockquote class="note">
<strong>Note:</strong><span> By default the jacobian implementation uses parallel for (pfor), which creates a tf.function under the hood for each jacobian call. For better performance, and to avoid recompilation and vectorization rewrites on each call, enclose GradientTape code in @tf.function.</span>
</blockquote> <p>See<a href="http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant">wikipedia article</a> for the definition of a Jacobian.</p> <h4 id="example_usage_2" data-text="Example usage:">Example usage:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">with tf.GradientTape() as g:
  x  = tf.constant([1.0, 2.0])
  g.watch(x)
  y = x * x
jacobian = g.jacobian(y, x)
# jacobian value is [[2., 0.], [0., 4.]]
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">target</code> </td> <td> Tensor to be differentiated. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">sources</code> </td> <td> a list or nested structure of Tensors or Variables. <code translate="no" dir="ltr">target</code> will be differentiated against elements in <code translate="no" dir="ltr">sources</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">unconnected_gradients</code> </td> <td> a value which can either hold 'none' or 'zero' and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">parallel_iterations</code> </td> <td> A knob to control how many iterations are dispatched in parallel. This knob can be used to control the total memory usage. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_use_pfor</code> </td> <td> If true, vectorizes the jacobian computation. Else falls back to a sequential while_loop. Vectorization can sometimes fail or lead to excessive memory usage. This option can be used to disable vectorization in such cases. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list or nested structure of Tensors (or None), one for each element in <code translate="no" dir="ltr">sources</code>. Returned structure is the same as the structure of <code translate="no" dir="ltr">sources</code>. Note if any gradient is sparse (IndexedSlices), jacobian function currently makes it dense and returns a Tensor instead. This may change in the future. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> If called on a used, non-persistent tape. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> If called on a non-persistent tape with eager execution enabled and without enabling experimental_use_pfor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If vectorization of jacobian computation fails. </td> </tr> </table> <h3 id="reset" data-text="reset"><code translate="no" dir="ltr">reset</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/eager/backprop.py#L953-L987">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
reset()
</pre> <p>Clears all information stored in this tape.</p> <p>Equivalent to exiting and reentering the tape context manager with a new tape. For example, the two following code blocks are equivalent:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">with tf.GradientTape() as t:
  loss = loss_fn()
with tf.GradientTape() as t:
  loss += other_loss_fn()
t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn


# The following is equivalent to the above
with tf.GradientTape() as t:
  loss = loss_fn()
  t.reset()
  loss += other_loss_fn()
t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn
</pre> <p>This is useful if you don't want to exit the context manager for the tape, or can't because the desired reset point is inside a control flow construct:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">with tf.GradientTape() as t:
  loss = ...
  if loss &gt; k:
    t.reset()
</pre> <h3 id="stop_recording" data-text="stop_recording"><code translate="no" dir="ltr">stop_recording</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/eager/backprop.py#L921-L951">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
@tf_contextlib.contextmanager
stop_recording()
</pre> <p>Temporarily stops recording operations on this tape.</p> <p>Operations executed while this context manager is active will not be recorded on the tape. This is useful for reducing the memory used by tracing all computations.</p> <h4 id="for_example" data-text="For example:">For example:</h4> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
x = tf.constant(4.0)
with tf.GradientTape() as tape:
  with tape.stop_recording():
    y = x ** 2
dy_dx = tape.gradient(y, x)
print(dy_dx)
None
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Yields</th></tr> <tr class="alt"> <td colspan="2"> None </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> if the tape is not currently recording. </td> </tr> </table> <h3 id="watch" data-text="watch"><code translate="no" dir="ltr">watch</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/eager/backprop.py#L896-L919">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
watch(
    tensor
)
</pre> <p>Ensures that <code translate="no" dir="ltr">tensor</code> is being traced by this tape.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">tensor</code> </td> <td> a Tensor or list of Tensors. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if it encounters something that is not a tensor. </td> </tr> </table> <h3 id="watched_variables" data-text="watched_variables"><code translate="no" dir="ltr">watched_variables</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/eager/backprop.py#L989-L993">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
watched_variables()
</pre> <p>Returns variables watched by this tape in order of construction.</p> <h3 id="__enter__" data-text="__enter__"><code translate="no" dir="ltr">__enter__</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/eager/backprop.py#L855-L858">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
__enter__()
</pre> <p>Enters a context inside which operations are recorded on this tape.</p> <h3 id="__exit__" data-text="__exit__"><code translate="no" dir="ltr">__exit__</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/eager/backprop.py#L860-L863">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
__exit__(
    typ, value, traceback
)
</pre> <p>Exits the recording context, no further operations are traced.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/GradientTape" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/GradientTape</a>
  </p>
</div>
