<h1 class="devsite-page-title">tf.tpu.experimental.embedding.TPUEmbeddingForServing</h1> <devsite-bookmark></devsite-bookmark>       <p>The TPUEmbedding mid level API running on CPU for serving.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/tpu/experimental/embedding/TPUEmbeddingForServing"><code translate="no" dir="ltr">tf.compat.v1.tpu.experimental.embedding.TPUEmbeddingForServing</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.tpu.experimental.embedding.TPUEmbeddingForServing(
    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable],
    optimizer: Optional[tpu_embedding_v2_utils._Optimizer]
)
</pre>  <blockquote class="note">
<strong>Note:</strong><span> This class is intended to be used for embedding tables that are trained on TPU and to be served on CPU. Therefore the class should be only initialized under non-TPU strategy. Otherwise an error will be raised.</span>
</blockquote> <p>You can first train your model using the TPUEmbedding class and save the checkpoint. Then use this class to restore the checkpoint to do serving.</p> <p>First train a model and save the checkpoint.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">model = model_fn(...)
strategy = tf.distribute.TPUStrategy(...)
with strategy.scope():
  embedding = tf.tpu.experimental.embedding.TPUEmbedding(
      feature_config=feature_config,
      optimizer=tf.tpu.experimental.embedding.SGD(0.1))

# Your custom training code.

checkpoint = tf.train.Checkpoint(model=model, embedding=embedding)
checkpoint.save(...)

</pre> <p>Then restore the checkpoint and do serving.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">
# Restore the model on CPU.
model = model_fn(...)
embedding = tf.tpu.experimental.embedding.TPUEmbeddingForServing(
      feature_config=feature_config,
      optimizer=tf.tpu.experimental.embedding.SGD(0.1))

checkpoint = tf.train.Checkpoint(model=model, embedding=embedding)
checkpoint.restore(...)

result = embedding(...)
table = embedding.embedding_table
</pre>
<blockquote class="note">
<strong>Note:</strong><span> This class can also be used to do embedding training on CPU. But it requires the conversion between keras optimizer and embedding optimizers so that the slot variables can stay consistent between them.</span>
</blockquote>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">feature_config</code> </td> <td> A nested structure of <a href="featureconfig.html"><code translate="no" dir="ltr">tf.tpu.experimental.embedding.FeatureConfig</code></a> configs. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">optimizer</code> </td> <td> An instance of one of <a href="sgd.html"><code translate="no" dir="ltr">tf.tpu.experimental.embedding.SGD</code></a>, <a href="adagrad.html"><code translate="no" dir="ltr">tf.tpu.experimental.embedding.Adagrad</code></a> or <a href="adam.html"><code translate="no" dir="ltr">tf.tpu.experimental.embedding.Adam</code></a>. When not created under TPUStrategy may be set to None to avoid the creation of the optimizer slot variables, useful for optimizing memory consumption when exporting the model for serving where slot variables aren't needed. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> If created under TPUStrategy. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">embedding_tables</code> </td> <td> Returns a dict of embedding tables, keyed by <code translate="no" dir="ltr">TableConfig</code>. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="build" data-text="build"><code translate="no" dir="ltr">build</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/tpu/tpu_embedding_base.py#L140-L145">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
build()
</pre> <p>Create variables and slots variables for TPU embeddings.</p> <h3 id="embedding_lookup" data-text="embedding_lookup"><code translate="no" dir="ltr">embedding_lookup</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/tpu/tpu_embedding_for_serving.py#L152-L173">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
embedding_lookup(
    features: Any, weights: Optional[Any] = None
) -&gt; Any
</pre> <p>Apply standard lookup ops on CPU.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">features</code> </td> <td> A nested structure of <a href="../../../tensor.html"><code translate="no" dir="ltr">tf.Tensor</code></a>s, <a href="../../../sparse/sparsetensor.html"><code translate="no" dir="ltr">tf.SparseTensor</code></a>s or <a href="../../../raggedtensor.html"><code translate="no" dir="ltr">tf.RaggedTensor</code></a>s, with the same structure as <code translate="no" dir="ltr">feature_config</code>. Inputs will be downcast to <a href="../../../../tf.html#int32"><code translate="no" dir="ltr">tf.int32</code></a>. Only one type out of <a href="../../../sparse/sparsetensor.html"><code translate="no" dir="ltr">tf.SparseTensor</code></a> or <a href="../../../raggedtensor.html"><code translate="no" dir="ltr">tf.RaggedTensor</code></a> is supported per call. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">weights</code> </td> <td> If not <code translate="no" dir="ltr">None</code>, a nested structure of <a href="../../../tensor.html"><code translate="no" dir="ltr">tf.Tensor</code></a>s, <a href="../../../sparse/sparsetensor.html"><code translate="no" dir="ltr">tf.SparseTensor</code></a>s or <a href="../../../raggedtensor.html"><code translate="no" dir="ltr">tf.RaggedTensor</code></a>s, matching the above, except that the tensors should be of float type (and they will be downcast to <a href="../../../../tf.html#float32"><code translate="no" dir="ltr">tf.float32</code></a>). For <a href="../../../sparse/sparsetensor.html"><code translate="no" dir="ltr">tf.SparseTensor</code></a>s we assume the <code translate="no" dir="ltr">indices</code> are the same for the parallel entries from <code translate="no" dir="ltr">features</code> and similarly for <a href="../../../raggedtensor.html"><code translate="no" dir="ltr">tf.RaggedTensor</code></a>s we assume the row_splits are the same. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A nested structure of Tensors with the same structure as input features. </td> </tr> 
</table> <h3 id="__call__" data-text="__call__"><code translate="no" dir="ltr">__call__</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/tpu/tpu_embedding_base.py#L147-L151">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
__call__(
    features: Any, weights: Optional[Any] = None
) -&gt; Any
</pre> <p>Call the mid level api to do embedding lookup.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/tpu/experimental/embedding/TPUEmbeddingForServing" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/tpu/experimental/embedding/TPUEmbeddingForServing</a>
  </p>
</div>
