<h1 class="devsite-page-title">tf.nn.sigmoid_cross_entropy_with_logits</h1> <devsite-bookmark></devsite-bookmark>   <p><devsite-mathjax config="TeX-AMS-MML_SVG"></devsite-mathjax> </p>   <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/ops/nn_impl.py#L153-L245">  View source on GitHub </a> </td> </table> <p>Computes sigmoid cross entropy given <code translate="no" dir="ltr">logits</code>.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.nn.sigmoid_cross_entropy_with_logits(
    labels=None, logits=None, name=None
)
</pre>  <p>Measures the probability error in tasks with two outcomes in which each outcome is independent and need not have a fully certain label. For instance, one could perform a regression where the probability of an event happening is known and used as a label. This loss may also be used for binary classification, where labels are either zero or one.</p> <p>For brevity, let <code translate="no" dir="ltr">x = logits</code>, <code translate="no" dir="ltr">z = labels</code>. The logistic loss is</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">  z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))
= z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))
= z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))
= z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))
= (1 - z) * x + log(1 + exp(-x))
= x - x * z + log(1 + exp(-x))
</pre> <p>For x &lt; 0, to avoid overflow in exp(-x), we reformulate the above</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">  x - x * z + log(1 + exp(-x))
= log(exp(x)) - x * z + log(1 + exp(-x))
= - x * z + log(1 + exp(x))
</pre> <p>Hence, to ensure stability and avoid overflow, the implementation uses this equivalent formulation</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">max(x, 0) - x * z + log(1 + exp(-abs(x)))
</pre> <p><code translate="no" dir="ltr">logits</code> and <code translate="no" dir="ltr">labels</code> must have the same type and shape.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
logits = tf.constant([1., -1., 0., 1., -1., 0., 0.])
labels = tf.constant([0., 0., 0., 1., 1., 1., 0.5])
tf.nn.sigmoid_cross_entropy_with_logits(
    labels=labels, logits=logits).numpy()
array([1.3132617, 0.3132617, 0.6931472, 0.3132617, 1.3132617, 0.6931472,
       0.6931472], dtype=float32)
</pre> <p>Compared to the losses which handle multiple outcomes, <a href="softmax_cross_entropy_with_logits.html"><code translate="no" dir="ltr">tf.nn.softmax_cross_entropy_with_logits</code></a> for general multi-class classification and <a href="sparse_softmax_cross_entropy_with_logits.html"><code translate="no" dir="ltr">tf.nn.sparse_softmax_cross_entropy_with_logits</code></a> for more efficient multi-class classification with hard labels, <code translate="no" dir="ltr">sigmoid_cross_entropy_with_logits</code> is a slight simplification for binary classification:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">sigmoid(x) = softmax([x, 0])[0]
</pre> <p>\[\frac{1}{1 + e^{-x} } = \frac{e^x}{e^x + e^0}\]</p> <p>While <code translate="no" dir="ltr">sigmoid_cross_entropy_with_logits</code> works for soft binary labels (probabilities between 0 and 1), it can also be used for binary classification where the labels are hard. There is an equivalence between all three symbols in this case, with a probability 0 indicating the second class or 1 indicating the first class:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
sigmoid_logits = tf.constant([1., -1., 0.])
softmax_logits = tf.stack([sigmoid_logits, tf.zeros_like(sigmoid_logits)],
                          axis=-1)
soft_binary_labels = tf.constant([1., 1., 0.])
soft_multiclass_labels = tf.stack(
    [soft_binary_labels, 1. - soft_binary_labels], axis=-1)
hard_labels = tf.constant([0, 0, 1])
tf.nn.sparse_softmax_cross_entropy_with_logits(
    labels=hard_labels, logits=softmax_logits).numpy()
array([0.31326166, 1.3132616 , 0.6931472 ], dtype=float32)
tf.nn.softmax_cross_entropy_with_logits(
    labels=soft_multiclass_labels, logits=softmax_logits).numpy()
array([0.31326166, 1.3132616, 0.6931472], dtype=float32)
tf.nn.sigmoid_cross_entropy_with_logits(
    labels=soft_binary_labels, logits=sigmoid_logits).numpy()
array([0.31326166, 1.3132616, 0.6931472], dtype=float32)
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">labels</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> of the same type and shape as <code translate="no" dir="ltr">logits</code>. Between 0 and 1, inclusive. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">logits</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">float32</code> or <code translate="no" dir="ltr">float64</code>. Any real number. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <code translate="no" dir="ltr">Tensor</code> of the same shape as <code translate="no" dir="ltr">logits</code> with the componentwise logistic losses. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If <code translate="no" dir="ltr">logits</code> and <code translate="no" dir="ltr">labels</code> do not have the same shape. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits</a>
  </p>
</div>
