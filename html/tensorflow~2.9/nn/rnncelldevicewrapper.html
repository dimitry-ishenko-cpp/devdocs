<h1 class="devsite-page-title">tf.nn.RNNCellDeviceWrapper</h1> <devsite-bookmark></devsite-bookmark>       <p>Operator that ensures an RNNCell runs on a particular device.</p> <p>Inherits From: <a href="../module.html"><code translate="no" dir="ltr">Module</code></a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.nn.RNNCellDeviceWrapper(
    *args, **kwargs
)
</pre>   
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">cell</code> </td> <td> An instance of <code translate="no" dir="ltr">RNNCell</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">device</code> </td> <td> A device string or function, for passing to <a href="../device.html"><code translate="no" dir="ltr">tf.device</code></a>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> dict of keyword arguments for base layer. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">activity_regularizer</code> </td> <td> Optional regularizer function for the output of this layer. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">compute_dtype</code> </td> <td> The dtype of the layer's computations. <p>This is equivalent to <code translate="no" dir="ltr">Layer.dtype_policy.compute_dtype</code>. Unless mixed precision is used, this is the same as <a href="../keras/layers/layer.html#dtype"><code translate="no" dir="ltr">Layer.dtype</code></a>, the dtype of the weights.</p> <p>Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in <a href="../keras/layers/layer.html#__call__"><code translate="no" dir="ltr">Layer.<strong>call</strong></code></a>, so you do not have to insert these casts if implementing your own layer.</p> <p>Layers often perform certain internal computations in higher precision when <code translate="no" dir="ltr">compute_dtype</code> is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">dtype</code> </td> <td> The dtype of the layer weights. <p>This is equivalent to <code translate="no" dir="ltr">Layer.dtype_policy.variable_dtype</code>. Unless mixed precision is used, this is the same as <a href="../keras/layers/layer.html#compute_dtype"><code translate="no" dir="ltr">Layer.compute_dtype</code></a>, the dtype of the layer's computations. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">dtype_policy</code> </td> <td> The dtype policy associated with this layer. <p>This is an instance of a <a href="../keras/mixed_precision/policy.html"><code translate="no" dir="ltr">tf.keras.mixed_precision.Policy</code></a>. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">dynamic</code> </td> <td> Whether the layer is dynamic (eager-only); set in the constructor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">input</code> </td> <td> Retrieves the input tensor(s) of a layer. <p>Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">input_spec</code> </td> <td> <code translate="no" dir="ltr">InputSpec</code> instance(s) describing the input format for this layer. <p>When you create a layer subclass, you can set <code translate="no" dir="ltr">self.input_spec</code> to enable the layer to run input compatibility checks when it is called. Consider a <code translate="no" dir="ltr">Conv2D</code> layer: it can only be called on a single input tensor of rank 4. As such, you can set, in <code translate="no" dir="ltr">__init__()</code>:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">self.input_spec = tf.keras.layers.InputSpec(ndim=4)
</pre> <p>Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape <code translate="no" dir="ltr">(2,)</code>, it will raise a nicely-formatted error:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">ValueError: Input 0 of layer conv2d is incompatible with the layer:
expected ndim=4, found ndim=1. Full shape received: [2]
</pre> <p>Input checks that can be specified via <code translate="no" dir="ltr">input_spec</code> include:</p> <ul> <li>Structure (e.g. a single input, a list of 2 inputs, etc)</li> <li>Shape</li> <li>Rank (ndim)</li> <li>Dtype</li> </ul> <p>For more information, see <a href="../keras/layers/inputspec.html"><code translate="no" dir="ltr">tf.keras.layers.InputSpec</code></a>. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">losses</code> </td> <td> List of losses added using the <code translate="no" dir="ltr">add_loss()</code> API. <p>Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing <code translate="no" dir="ltr">losses</code> under a <a href="../gradienttape.html"><code translate="no" dir="ltr">tf.GradientTape</code></a> will propagate gradients back to the corresponding variables.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
class MyLayer(tf.keras.layers.Layer):
  def call(self, inputs):
    self.add_loss(tf.abs(tf.reduce_mean(inputs)))
    return inputs
l = MyLayer()
l(np.ones((10, 1)))
l.losses
[1.0]
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Activity regularization.
len(model.losses)
0
model.add_loss(tf.abs(tf.reduce_mean(x)))
len(model.losses)
1
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
inputs = tf.keras.Input(shape=(10,))
d = tf.keras.layers.Dense(10, kernel_initializer='ones')
x = d(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Weight regularization.
model.add_loss(lambda: tf.reduce_mean(d.kernel))
model.losses
[&lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0&gt;]
</pre> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">metrics</code> </td> <td> List of metrics added using the <code translate="no" dir="ltr">add_metric()</code> API. <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
input = tf.keras.layers.Input(shape=(3,))
d = tf.keras.layers.Dense(2)
output = d(input)
d.add_metric(tf.reduce_max(output), name='max')
d.add_metric(tf.reduce_min(output), name='min')
[m.name for m in d.metrics]
['max', 'min']
</pre> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">non_trainable_weights</code> </td> <td> List of all non-trainable weights tracked by this layer. <p>Non-trainable weights are <em>not</em> updated during training. They are expected to be updated manually in <code translate="no" dir="ltr">call()</code>. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">output</code> </td> <td> Retrieves the output tensor(s) of a layer. <p>Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">output_size</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">state_size</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">supports_masking</code> </td> <td> Whether this layer supports computing a mask using <code translate="no" dir="ltr">compute_mask</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">trainable</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">trainable_weights</code> </td> <td> List of all trainable weights tracked by this layer. <p>Trainable weights are updated via gradient descent during training. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">variable_dtype</code> </td> <td> Alias of <a href="../keras/layers/layer.html#dtype"><code translate="no" dir="ltr">Layer.dtype</code></a>, the dtype of the weights. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">weights</code> </td> <td> Returns the list of all layer variables/weights. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="add_loss" data-text="add_loss"><code translate="no" dir="ltr">add_loss</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/keras/engine/base_layer.py#L1412-L1530">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
add_loss(
    losses, **kwargs
)
</pre> <p>Add loss tensor(s), potentially dependent on layer inputs.</p> <p>Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs <code translate="no" dir="ltr">a</code> and <code translate="no" dir="ltr">b</code>, some entries in <code translate="no" dir="ltr">layer.losses</code> may be dependent on <code translate="no" dir="ltr">a</code> and some on <code translate="no" dir="ltr">b</code>. This method automatically keeps track of dependencies.</p> <p>This method can be used inside a subclassed layer or model's <code translate="no" dir="ltr">call</code> function, in which case <code translate="no" dir="ltr">losses</code> should be a Tensor or list of Tensors.</p> <h4 id="example" data-text="Example:">Example:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">class MyLayer(tf.keras.layers.Layer):
  def call(self, inputs):
    self.add_loss(tf.abs(tf.reduce_mean(inputs)))
    return inputs
</pre> <p>This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's <code translate="no" dir="ltr">Input</code>s. These losses become part of the model's topology and are tracked in <code translate="no" dir="ltr">get_config</code>.</p> <h4 id="example_2" data-text="Example:">Example:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Activity regularization.
model.add_loss(tf.abs(tf.reduce_mean(x)))
</pre> <p>If this is not the case for your loss (if, for example, your loss references a <code translate="no" dir="ltr">Variable</code> of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized.</p> <h4 id="example_3" data-text="Example:">Example:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">inputs = tf.keras.Input(shape=(10,))
d = tf.keras.layers.Dense(10)
x = d(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
# Weight regularization.
model.add_loss(lambda: tf.reduce_mean(d.kernel))
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">losses</code> </td> <td> Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. </td> </tr> </table> <h3 id="add_metric" data-text="add_metric"><code translate="no" dir="ltr">add_metric</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/keras/engine/base_layer.py#L1565-L1684">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
add_metric(
    value, name=None, **kwargs
)
</pre> <p>Adds metric tensor to the layer.</p> <p>This method can be used inside the <code translate="no" dir="ltr">call()</code> method of a subclassed layer or model.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">class MyMetricLayer(tf.keras.layers.Layer):
  def __init__(self):
    super(MyMetricLayer, self).__init__(name='my_metric_layer')
    self.mean = tf.keras.metrics.Mean(name='metric_1')

  def call(self, inputs):
    self.add_metric(self.mean(inputs))
    self.add_metric(tf.reduce_sum(inputs), name='metric_2')
    return inputs
</pre> <p>This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's <code translate="no" dir="ltr">Input</code>s. These metrics become part of the model's topology and are tracked when you save the model via <code translate="no" dir="ltr">save()</code>.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_metric(math_ops.reduce_sum(x), name='metric_1')
</pre>
<blockquote class="note">
<strong>Note:</strong><span> Calling <code translate="no" dir="ltr">add_metric()</code> with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs.</span>
</blockquote>
<pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">inputs = tf.keras.Input(shape=(10,))
x = tf.keras.layers.Dense(10)(inputs)
outputs = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs, outputs)
model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> Metric tensor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> String metric name. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> Additional keyword arguments for backward compatibility. Accepted values: <code translate="no" dir="ltr">aggregation</code> - When the <code translate="no" dir="ltr">value</code> tensor provided is not the result of calling a <code translate="no" dir="ltr">keras.Metric</code> instance, it will be aggregated by default using a <code translate="no" dir="ltr">keras.Metric.Mean</code>. </td> </tr> </table> <h3 id="build" data-text="build"><code translate="no" dir="ltr">build</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/keras/layers/rnn_cell_wrapper_v2.py#L70-L73">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
build(
    inputs_shape
)
</pre> <p>Builds the wrapped cell.</p> <h3 id="compute_mask" data-text="compute_mask"><code translate="no" dir="ltr">compute_mask</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/keras/engine/base_layer.py#L910-L930">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
compute_mask(
    inputs, mask=None
)
</pre> <p>Computes an output mask tensor.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">inputs</code> </td> <td> Tensor or list of tensors. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">mask</code> </td> <td> Tensor or list of tensors. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> None or a tensor (or list of tensors, one per output tensor of the layer). </td> </tr> 
</table> <h3 id="compute_output_shape" data-text="compute_output_shape"><code translate="no" dir="ltr">compute_output_shape</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/keras/engine/base_layer.py#L756-L799">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
compute_output_shape(
    input_shape
)
</pre> <p>Computes the output shape of the layer.</p> <p>If the layer has not been built, this method will call <code translate="no" dir="ltr">build</code> on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input_shape</code> </td> <td> Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An input shape tuple. </td> </tr> 
</table> <h3 id="count_params" data-text="count_params"><code translate="no" dir="ltr">count_params</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/keras/engine/base_layer.py#L2129-L2148">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
count_params()
</pre> <p>Count the total number of scalars composing the weights.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An integer count. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if the layer isn't yet built (in which case its weights aren't yet defined). </td> </tr> </table> <h3 id="from_config" data-text="from_config"><code translate="no" dir="ltr">from_config</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/keras/layers/rnn_cell_wrapper_v2.py#L85-L90">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
@classmethod
from_config(
    config, custom_objects=None
)
</pre> <p>Creates a layer from its config.</p> <p>This method is the reverse of <code translate="no" dir="ltr">get_config</code>, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by <code translate="no" dir="ltr">set_weights</code>).</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">config</code> </td> <td> A Python dictionary, typically the output of get_config. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A layer instance. </td> </tr> 
</table> <h3 id="get_config" data-text="get_config"><code translate="no" dir="ltr">get_config</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_wrapper_impl.py#L441-L444">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_config()
</pre> <h3 id="get_initial_state" data-text="get_initial_state"><code translate="no" dir="ltr">get_initial_state</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/keras/layers/recurrent.py#L1092-L1093">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_initial_state(
    inputs=None, batch_size=None, dtype=None
)
</pre> <h3 id="get_weights" data-text="get_weights"><code translate="no" dir="ltr">get_weights</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/keras/engine/base_layer.py#L1808-L1850">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_weights()
</pre> <p>Returns the current weights of the layer, as NumPy arrays.</p> <p>The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers.</p> <p>For example, a <code translate="no" dir="ltr">Dense</code> layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another <code translate="no" dir="ltr">Dense</code> layer:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
layer_a = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
layer_b = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
[array([[2.],
       [2.],
       [2.]], dtype=float32), array([0.], dtype=float32)]
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Weights values as a list of NumPy arrays. </td> </tr> 
</table> <h3 id="set_weights" data-text="set_weights"><code translate="no" dir="ltr">set_weights</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/keras/engine/base_layer.py#L1723-L1806">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
set_weights(
    weights
)
</pre> <p>Sets the weights of the layer, from NumPy arrays.</p> <p>The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer.</p> <p>For example, a <code translate="no" dir="ltr">Dense</code> layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another <code translate="no" dir="ltr">Dense</code> layer:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
layer_a = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(1.))
a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))
layer_a.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
layer_b = tf.keras.layers.Dense(1,
  kernel_initializer=tf.constant_initializer(2.))
b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))
layer_b.get_weights()
[array([[2.],
       [2.],
       [2.]], dtype=float32), array([0.], dtype=float32)]
layer_b.set_weights(layer_a.get_weights())
layer_b.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">weights</code> </td> <td> a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of <code translate="no" dir="ltr">get_weights</code>). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If the provided weights list does not match the layer's specifications. </td> </tr> </table> <h3 id="zero_state" data-text="zero_state"><code translate="no" dir="ltr">zero_state</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_wrapper_impl.py#L431-L434">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
zero_state(
    batch_size, dtype
)
</pre> <h3 id="__call__" data-text="__call__"><code translate="no" dir="ltr">__call__</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/keras/engine/base_layer.py#L932-L1053">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
__call__(
    *args, **kwargs
)
</pre> <p>Wraps <code translate="no" dir="ltr">call</code>, applying pre- and post-processing steps.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">*args</code> </td> <td> Positional arguments to be passed to <code translate="no" dir="ltr">self.call</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> Keyword arguments to be passed to <code translate="no" dir="ltr">self.call</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Output tensor(s). </td> </tr> 
</table> <h4 id="note" data-text="Note:">Note:</h4> <ul> <li>The following optional keyword arguments are reserved for specific uses: <ul> <li>
<code translate="no" dir="ltr">training</code>: Boolean scalar tensor of Python boolean indicating whether the <code translate="no" dir="ltr">call</code> is meant for training or inference.</li> <li>
<code translate="no" dir="ltr">mask</code>: Boolean input mask.</li> </ul>
</li> <li>If the layer's <code translate="no" dir="ltr">call</code> method takes a <code translate="no" dir="ltr">mask</code> argument (as some Keras layers do), its default value will be set to the mask generated for <code translate="no" dir="ltr">inputs</code> by the previous layer (if <code translate="no" dir="ltr">input</code> did come from a layer that generated a corresponding mask, i.e. if it came from a Keras layer with masking support.</li> <li>If the layer is not built, the method will call <code translate="no" dir="ltr">build</code>.</li> </ul>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if the layer's <code translate="no" dir="ltr">call</code> method returns None (an invalid value). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> if <code translate="no" dir="ltr">super().__init__()</code> was not called in the constructor. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/nn/RNNCellDeviceWrapper" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/nn/RNNCellDeviceWrapper</a>
  </p>
</div>
