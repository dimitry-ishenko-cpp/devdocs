<h1 class="devsite-page-title">tf.nn.safe_embedding_lookup_sparse</h1> <devsite-bookmark></devsite-bookmark>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/ops/embedding_ops.py#L682-L781">  View source on GitHub </a> </td> </table> <p>Lookup embedding results, accounting for invalid IDs and empty features.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.nn.safe_embedding_lookup_sparse(
    embedding_weights,
    sparse_ids,
    sparse_weights=None,
    combiner='mean',
    default_id=None,
    max_norm=None,
    name=None
)
</pre>  <p>The partitioned embedding in <code translate="no" dir="ltr">embedding_weights</code> must all be the same shape except for the first dimension. The first dimension is allowed to vary as the vocabulary size is not necessarily a multiple of num of shards.</p> <p>Invalid IDs (&lt; 0) are pruned from input IDs and weights, as well as any IDs with non-positive weight. For an entry with no features, the embedding vector for <code translate="no" dir="ltr">default_id</code> is returned, or the 0-vector if <code translate="no" dir="ltr">default_id</code> is not supplied.</p> <p>The ids and weights may be multi-dimensional. Embeddings are always aggregated along the last dimension.</p> <p>If <code translate="no" dir="ltr">len(embedding_weights) &gt; 1</code>, each element <code translate="no" dir="ltr">id</code> of <code translate="no" dir="ltr">ids</code> is partitioned between the elements of <code translate="no" dir="ltr">embedding_weights</code> according to the "div" partition strategy, which means we assign ids to partitions in a contiguous manner. For instance, 13 ids are split across 5 partitions as: <code translate="no" dir="ltr">[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]]</code>.</p> <p>If the id space does not evenly divide the number of partitions, each of the first <code translate="no" dir="ltr">(max_id + 1) % len(embedding_weights)</code> partitions will be assigned one more id.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">embedding_weights</code> </td> <td> A single tensor representing the complete embedding tensor, or a list of tensors all of same shape except for the first dimension, representing sharded embedding tensors following "div" partition strategy. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">sparse_ids</code> </td> <td> <code translate="no" dir="ltr">SparseTensor</code> of shape <code translate="no" dir="ltr">[d_0, d_1, ..., d_n]</code> containing the ids. <code translate="no" dir="ltr">d_0</code> is typically batch size. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">sparse_weights</code> </td> <td> <code translate="no" dir="ltr">SparseTensor</code> of same shape as <code translate="no" dir="ltr">sparse_ids</code>, containing float weights corresponding to <code translate="no" dir="ltr">sparse_ids</code>, or <code translate="no" dir="ltr">None</code> if all weights are be assumed to be 1.0. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">combiner</code> </td> <td> A string specifying how to combine embedding results for each entry. Currently "mean", "sqrtn" and "sum" are supported, with "mean" the default. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">default_id</code> </td> <td> The id to use for an entry with no features. Defaults to 0-vector. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">max_norm</code> </td> <td> If not <code translate="no" dir="ltr">None</code>, all embeddings are l2-normalized to max_norm before combining. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> A name for this operation (optional). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A dense tensor representing the combined embeddings for the sparse ids. For each row in the dense tensor represented by <code translate="no" dir="ltr">sparse_ids</code>, the op looks up the embeddings for all ids in that row, multiplies them by the corresponding weight, and combines these embeddings as specified. <p>In other words, if</p> <p><code translate="no" dir="ltr">shape(combined embedding_weights) = [p0, p1, ..., pm]</code></p> <p>and</p> <p><code translate="no" dir="ltr">shape(sparse_ids) = shape(sparse_weights) = [d0, d1, ..., dn]</code></p> <p>then</p> <p><code translate="no" dir="ltr">shape(output) = [d0, d1, ... dn-1, p1, ..., pm]</code>.</p> <p>For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">[0, 0]: id 1, weight 2.0
[0, 1]: id 3, weight 0.5
[1, 0]: id -1, weight 1.0
[2, 3]: id 1, weight 3.0
</pre> <p><code translate="no" dir="ltr">default_id</code> is 0.</p> <p>with <code translate="no" dir="ltr">combiner</code>="mean", then the output will be a 3x20 matrix where</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)
output[1, :] = (params[0, :] * 1.0) / 1.0
output[2, :] = (params[1, :] * 3.0) / 3.0
</pre> 
</td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if <code translate="no" dir="ltr">embedding_weights</code> is empty. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/nn/safe_embedding_lookup_sparse" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/nn/safe_embedding_lookup_sparse</a>
  </p>
</div>
