<h1 class="devsite-page-title">tf.compat.v1.train.ProximalAdagradOptimizer</h1> <devsite-bookmark></devsite-bookmark>       <p>Optimizer that implements the Proximal Adagrad algorithm.</p> <p>Inherits From: <a href="optimizer.html"><code translate="no" dir="ltr">Optimizer</code></a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.train.ProximalAdagradOptimizer(
    learning_rate,
    initial_accumulator_value=0.1,
    l1_regularization_strength=0.0,
    l2_regularization_strength=0.0,
    use_locking=False,
    name='ProximalAdagrad'
)
</pre>  <h4 id="references" data-text="References:">References:</h4> <p>Adaptive Subgradient Methods for Online Learning and Stochastic Optimization: <a href="http://jmlr.org/papers/v12/duchi11a.html">Duchi et al., 2011</a> (<a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">pdf</a>) Efficient Learning using Forward-Backward Splitting: <a href="http://papers.nips.cc/paper/3793-efficient-learning-using-forward-backward-splitting">Duchi et al., 2009</a> (<a href="http://papers.nips.cc/paper/3793-efficient-learning-using-forward-backward-splitting.pdf">pdf</a>)</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">learning_rate</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> or a floating point value. The learning rate. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">initial_accumulator_value</code> </td> <td> A floating point value. Starting value for the accumulators, must be positive. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">l1_regularization_strength</code> </td> <td> A float value, must be greater than or equal to zero. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">l2_regularization_strength</code> </td> <td> A float value, must be greater than or equal to zero. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">use_locking</code> </td> <td> If <code translate="no" dir="ltr">True</code> use locks for update operations. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name prefix for the operations created when applying gradients. Defaults to "Adagrad". </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If the <code translate="no" dir="ltr">initial_accumulator_value</code> is invalid. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="apply_gradients" data-text="apply_gradients"><code translate="no" dir="ltr">apply_gradients</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/optimizer.py#L626-L744">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
apply_gradients(
    grads_and_vars, global_step=None, name=None
)
</pre> <p>Apply gradients to variables.</p> <p>This is the second part of <code translate="no" dir="ltr">minimize()</code>. It returns an <code translate="no" dir="ltr">Operation</code> that applies gradients.</p> <p>@compatibility(TF2)</p> <h4 id="how_to_map_arguments" data-text="How to Map Arguments">How to Map Arguments</h4> <table> <thead> <tr> <th style="text-align: left">TF1 Arg Name</th> <th style="text-align: left">TF2 Arg Name</th> <th style="text-align: left">Note</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><code translate="no" dir="ltr">grads_and_vars</code></td> <td style="text-align: left"><code translate="no" dir="ltr">grads_and_vars</code></td> <td style="text-align: left">-</td> </tr> <tr> <td style="text-align: left"><code translate="no" dir="ltr">global_step</code></td> <td style="text-align: left">Not supported.</td> <td style="text-align: left">Use <code translate="no" dir="ltr">optimizer.iterations</code>
</td> </tr> <tr> <td style="text-align: left"><code translate="no" dir="ltr">name</code></td> <td style="text-align: left"><code translate="no" dir="ltr">name.</code></td> <td style="text-align: left">-</td> </tr> </tbody> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">grads_and_vars</code> </td> <td> List of (gradient, variable) pairs as returned by <code translate="no" dir="ltr">compute_gradients()</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">global_step</code> </td> <td> Optional <code translate="no" dir="ltr">Variable</code> to increment by one after the variables have been updated. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the returned operation. Default to the name passed to the <code translate="no" dir="ltr">Optimizer</code> constructor. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An <code translate="no" dir="ltr">Operation</code> that applies the specified gradients. If <code translate="no" dir="ltr">global_step</code> was not None, that operation also increments <code translate="no" dir="ltr">global_step</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If <code translate="no" dir="ltr">grads_and_vars</code> is malformed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If none of the variables have gradients. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> If you should use <code translate="no" dir="ltr">_distributed_apply()</code> instead. </td> </tr> </table> <h3 id="compute_gradients" data-text="compute_gradients"><code translate="no" dir="ltr">compute_gradients</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/optimizer.py#L493-L614">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
compute_gradients(
    loss,
    var_list=None,
    gate_gradients=GATE_OP,
    aggregation_method=None,
    colocate_gradients_with_ops=False,
    grad_loss=None
)
</pre> <p>Compute gradients of <code translate="no" dir="ltr">loss</code> for the variables in <code translate="no" dir="ltr">var_list</code>.</p> <p><section><devsite-expandable expanded> <h4 class="showalways" id="migrate-to-tf2" data-text="Migrate to TF2">Migrate to TF2</h4></devsite-expandable></section></p> <aside class="caution"><strong>Caution:</strong><span> This API was designed for TensorFlow v1. Continue reading for details on how to migrate from this API to a native TensorFlow v2 equivalent. See the <a href="https://www.tensorflow.org/guide/migrate">TensorFlow v1 to TensorFlow v2 migration guide</a> for instructions on how to migrate the rest of your code.</span></aside> <p><a href="../../../keras/optimizers/optimizer.html"><code translate="no" dir="ltr">tf.keras.optimizers.Optimizer</code></a> in TF2 does not provide a <code translate="no" dir="ltr">compute_gradients</code> method, and you should use a <a href="../../../gradienttape.html"><code translate="no" dir="ltr">tf.GradientTape</code></a> to obtain the gradients:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">@tf.function
def train step(inputs):
  batch_data, labels = inputs
  with tf.GradientTape() as tape:
    predictions = model(batch_data, training=True)
    loss = tf.keras.losses.CategoricalCrossentropy(
        reduction=tf.keras.losses.Reduction.NONE)(labels, predictions)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
</pre> <p>Args: loss: A Tensor containing the value to minimize or a callable taking no arguments which returns the value to minimize. When eager execution is enabled it must be a callable. var_list: Optional list or tuple of <a href="../../../variable.html"><code translate="no" dir="ltr">tf.Variable</code></a> to update to minimize <code translate="no" dir="ltr">loss</code>. Defaults to the list of variables collected in the graph under the key <code translate="no" dir="ltr">GraphKeys.TRAINABLE_VARIABLES</code>. gate_gradients: How to gate the computation of gradients. Can be <code translate="no" dir="ltr">GATE_NONE</code>, <code translate="no" dir="ltr">GATE_OP</code>, or <code translate="no" dir="ltr">GATE_GRAPH</code>. aggregation_method: Specifies the method used to combine gradient terms. Valid values are defined in the class <code translate="no" dir="ltr">AggregationMethod</code>. colocate_gradients_with_ops: If True, try colocating gradients with the corresponding op. grad_loss: Optional. A <code translate="no" dir="ltr">Tensor</code> holding the gradient computed for <code translate="no" dir="ltr">loss</code>.</p> <p>Returns: A list of (gradient, variable) pairs. Variable is always present, but gradient can be <code translate="no" dir="ltr">None</code>.</p> <p>Raises: TypeError: If <code translate="no" dir="ltr">var_list</code> contains anything else than <code translate="no" dir="ltr">Variable</code> objects. ValueError: If some arguments are invalid. RuntimeError: If called with eager execution enabled and <code translate="no" dir="ltr">loss</code> is not callable.</p> <p>@compatibility(eager) When eager execution is enabled, <code translate="no" dir="ltr">gate_gradients</code>, <code translate="no" dir="ltr">aggregation_method</code>, and <code translate="no" dir="ltr">colocate_gradients_with_ops</code> are ignored.</p>  <h4 id="description" data-text="Description">Description</h4> <p>This is the first part of <code translate="no" dir="ltr">minimize()</code>. It returns a list of (gradient, variable) pairs where "gradient" is the gradient for "variable". Note that "gradient" can be a <code translate="no" dir="ltr">Tensor</code>, an <code translate="no" dir="ltr">IndexedSlices</code>, or <code translate="no" dir="ltr">None</code> if there is no gradient for the given variable.</p> <h3 id="get_name" data-text="get_name"><code translate="no" dir="ltr">get_name</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/optimizer.py#L430-L431">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_name()
</pre> <h3 id="get_slot" data-text="get_slot"><code translate="no" dir="ltr">get_slot</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/optimizer.py#L841-L867">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_slot(
    var, name
)
</pre> <p>Return a slot named <code translate="no" dir="ltr">name</code> created for <code translate="no" dir="ltr">var</code> by the Optimizer.</p> <p>Some <code translate="no" dir="ltr">Optimizer</code> subclasses use additional variables. For example <code translate="no" dir="ltr">Momentum</code> and <code translate="no" dir="ltr">Adagrad</code> use variables to accumulate updates. This method gives access to these <code translate="no" dir="ltr">Variable</code> objects if for some reason you need them.</p> <p>Use <code translate="no" dir="ltr">get_slot_names()</code> to get the list of slot names created by the <code translate="no" dir="ltr">Optimizer</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">var</code> </td> <td> A variable passed to <code translate="no" dir="ltr">minimize()</code> or <code translate="no" dir="ltr">apply_gradients()</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> A string. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> The <code translate="no" dir="ltr">Variable</code> for the slot if it was created, <code translate="no" dir="ltr">None</code> otherwise. </td> </tr> 
</table> <h3 id="get_slot_names" data-text="get_slot_names"><code translate="no" dir="ltr">get_slot_names</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/optimizer.py#L869-L877">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_slot_names()
</pre> <p>Return a list of the names of slots created by the <code translate="no" dir="ltr">Optimizer</code>.</p> <p>See <code translate="no" dir="ltr">get_slot()</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of strings. </td> </tr> 
</table> <h3 id="minimize" data-text="minimize"><code translate="no" dir="ltr">minimize</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/optimizer.py#L433-L491">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
minimize(
    loss,
    global_step=None,
    var_list=None,
    gate_gradients=GATE_OP,
    aggregation_method=None,
    colocate_gradients_with_ops=False,
    name=None,
    grad_loss=None
)
</pre> <p>Add operations to minimize <code translate="no" dir="ltr">loss</code> by updating <code translate="no" dir="ltr">var_list</code>.</p> <p>This method simply combines calls <code translate="no" dir="ltr">compute_gradients()</code> and <code translate="no" dir="ltr">apply_gradients()</code>. If you want to process the gradient before applying them call <code translate="no" dir="ltr">compute_gradients()</code> and <code translate="no" dir="ltr">apply_gradients()</code> explicitly instead of using this function.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">loss</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> containing the value to minimize. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">global_step</code> </td> <td> Optional <code translate="no" dir="ltr">Variable</code> to increment by one after the variables have been updated. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">var_list</code> </td> <td> Optional list or tuple of <code translate="no" dir="ltr">Variable</code> objects to update to minimize <code translate="no" dir="ltr">loss</code>. Defaults to the list of variables collected in the graph under the key <code translate="no" dir="ltr">GraphKeys.TRAINABLE_VARIABLES</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">gate_gradients</code> </td> <td> How to gate the computation of gradients. Can be <code translate="no" dir="ltr">GATE_NONE</code>, <code translate="no" dir="ltr">GATE_OP</code>, or <code translate="no" dir="ltr">GATE_GRAPH</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">aggregation_method</code> </td> <td> Specifies the method used to combine gradient terms. Valid values are defined in the class <code translate="no" dir="ltr">AggregationMethod</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">colocate_gradients_with_ops</code> </td> <td> If True, try colocating gradients with the corresponding op. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the returned operation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">grad_loss</code> </td> <td> Optional. A <code translate="no" dir="ltr">Tensor</code> holding the gradient computed for <code translate="no" dir="ltr">loss</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An Operation that updates the variables in <code translate="no" dir="ltr">var_list</code>. If <code translate="no" dir="ltr">global_step</code> was not <code translate="no" dir="ltr">None</code>, that operation also increments <code translate="no" dir="ltr">global_step</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If some of the variables are not <code translate="no" dir="ltr">Variable</code> objects. </td> </tr> </table> <p><section><devsite-expandable expanded> <h4 class="showalways" id="eager-compatibility" data-text="eager compatibility">eager compatibility</h4></devsite-expandable></section></p> <p>When eager execution is enabled, <code translate="no" dir="ltr">loss</code> should be a Python function that takes no arguments and computes the value to be minimized. Minimization (and gradient computation) is done with respect to the elements of <code translate="no" dir="ltr">var_list</code> if not None, else with respect to any trainable variables created during the execution of the <code translate="no" dir="ltr">loss</code> function. <code translate="no" dir="ltr">gate_gradients</code>, <code translate="no" dir="ltr">aggregation_method</code>, <code translate="no" dir="ltr">colocate_gradients_with_ops</code> and <code translate="no" dir="ltr">grad_loss</code> are ignored when eager execution is enabled.</p>  <h3 id="variables" data-text="variables"><code translate="no" dir="ltr">variables</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/optimizer.py#L879-L905">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
variables()
</pre> <p>A list of variables which encode the current state of <code translate="no" dir="ltr">Optimizer</code>.</p> <p>Includes slot variables and additional global variables created by the optimizer in the current default graph.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of variables. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Class Variables</th></tr> 
<tr> <td> GATE_GRAPH </td> <td> <code translate="no" dir="ltr">2</code> </td> </tr>
<tr> <td> GATE_NONE </td> <td> <code translate="no" dir="ltr">0</code> </td> </tr>
<tr> <td> GATE_OP </td> <td> <code translate="no" dir="ltr">1</code> </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/train/ProximalAdagradOptimizer" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/train/ProximalAdagradOptimizer</a>
  </p>
</div>
