<h1 class="devsite-page-title">tf.compat.v1.fixed_size_partitioner</h1> <devsite-bookmark></devsite-bookmark>       <p>Partitioner to specify a fixed number of shards along given axis.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.fixed_size_partitioner(
    num_shards, axis=0
)
</pre> <p><section><devsite-expandable expanded> <h2 class="showalways" id="migrate-to-tf2" data-text="Migrate to TF2">Migrate to TF2</h2></devsite-expandable></section></p> <aside class="caution"><strong>Caution:</strong><span> This API was designed for TensorFlow v1. Continue reading for details on how to migrate from this API to a native TensorFlow v2 equivalent. See the <a href="https://www.tensorflow.org/guide/migrate">TensorFlow v1 to TensorFlow v2 migration guide</a> for instructions on how to migrate the rest of your code.</span></aside> <p>This API is deprecated in TF2. In TF2, partitioner is no longer part of the variable declaration via <a href="../../variable.html"><code translate="no" dir="ltr">tf.Variable</code></a>. <a href="https://www.tensorflow.org/tutorials/distribute/parameter_server_training">ParameterServer Training</a> handles partitioning of variables. The corresponding TF2 partitioner class of <code translate="no" dir="ltr">fixed_size_partitioner</code> is <a href="../../distribute/experimental/partitioners/fixedshardspartitioner.html"><code translate="no" dir="ltr">tf.distribute.experimental.partitioners.FixedShardsPartitioner</code></a>.</p> <p>Check the <a href="https://www.tensorflow.org/guide/migrate#2_use_python_objects_to_track_variables_and_losses">migration guide</a> on the differences in treatment of variables and losses between TF1 and TF2.</p> <p>Before:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">x = tf.compat.v1.get_variable(
  "x", shape=(2,), partitioner=tf.compat.v1.fixed_size_partitioner(2)
)
</pre> <p>After:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">partitioner = (
    tf.distribute.experimental.partitioners.FixedShardsPartitioner(
        num_shards=2)
)
strategy = tf.distribute.experimental.ParameterServerStrategy(
               cluster_resolver=cluster_resolver,
               variable_partitioner=partitioner)

with strategy.scope():
  x = tf.Variable([1.0, 2.0])
</pre>  <h2 id="description" data-text="Description">Description</h2>   
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">num_shards</code> </td> <td> <code translate="no" dir="ltr">int</code>, number of shards to partition variable. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">axis</code> </td> <td> <code translate="no" dir="ltr">int</code>, axis to partition on. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A partition function usable as the <code translate="no" dir="ltr">partitioner</code> argument to <code translate="no" dir="ltr">variable_scope</code> and <code translate="no" dir="ltr">get_variable</code>. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/fixed_size_partitioner" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/fixed_size_partitioner</a>
  </p>
</div>
