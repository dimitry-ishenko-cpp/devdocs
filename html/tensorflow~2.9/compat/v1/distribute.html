<h1 class="devsite-page-title">Module: tf.compat.v1.distribute</h1> <devsite-bookmark></devsite-bookmark>       <p>Library for running a computation across multiple devices.</p> <p>The intent of this library is that you can write an algorithm in a stylized way and it will be usable with a variety of different <a href="../../distribute/strategy.html"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> implementations. Each descendant will implement a different strategy for distributing the algorithm across multiple devices/machines. Furthermore, these changes can be hidden inside the specific layers and other library classes that need special treatment to run in a distributed setting, so that most users' model definition code can run unchanged. The <a href="../../distribute/strategy.html"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> API works the same way with eager and graph execution.</p> <p><em>Guides</em></p> <ul> <li><a href="https://www.tensorflow.org/guide/distributed_training">TensorFlow v2.x</a></li> <li><a href="https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/distribute_strategy.ipynb">TensorFlow v1.x</a></li> </ul> <p><em>Tutorials</em></p> <ul> <li>
<p><a href="https://www.tensorflow.org/tutorials/distribute/">Distributed Training Tutorials</a></p> <p>The tutorials cover how to use <a href="../../distribute/strategy.html"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> to do distributed training with native Keras APIs, custom training loops, and Estimator APIs. They also cover how to save/load model when using <a href="../../distribute/strategy.html"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a>.</p>
</li> </ul> <p><em>Glossary</em></p> <ul> <li>
<em>Data parallelism</em> is where we run multiple copies of the model on different slices of the input data. This is in contrast to <em>model parallelism</em> where we divide up a single copy of a model across multiple devices. Note: we only support data parallelism for now, but hope to add support for model parallelism in the future.</li> <li>A <em>device</em> is a CPU or accelerator (e.g. GPUs, TPUs) on some machine that TensorFlow can run operations on (see e.g. <a href="../../device.html"><code translate="no" dir="ltr">tf.device</code></a>). You may have multiple devices on a single machine, or be connected to devices on multiple machines. Devices used to run computations are called <em>worker devices</em>. Devices used to store variables are <em>parameter devices</em>. For some strategies, such as <a href="../../distribute/mirroredstrategy.html"><code translate="no" dir="ltr">tf.distribute.MirroredStrategy</code></a>, the worker and parameter devices will be the same (see mirrored variables below). For others they will be different. For example, <a href="../../distribute/experimental/centralstoragestrategy.html"><code translate="no" dir="ltr">tf.distribute.experimental.CentralStorageStrategy</code></a> puts the variables on a single device (which may be a worker device or may be the CPU), and <a href="../../distribute/experimental/parameterserverstrategy.html"><code translate="no" dir="ltr">tf.distribute.experimental.ParameterServerStrategy</code></a> puts the variables on separate machines called <em>parameter servers</em> (see below).</li> <li>A <em>replica</em> is one copy of the model, running on one slice of the input data. Right now each replica is executed on its own worker device, but once we add support for model parallelism a replica may span multiple worker devices.</li> <li>A <em>host</em> is the CPU device on a machine with worker devices, typically used for running input pipelines.</li> <li>A <em>worker</em> is defined to be the physical machine(s) containing the physical devices (e.g. GPUs, TPUs) on which the replicated computation is executed. A worker may contain one or more replicas, but contains at least one replica. Typically one worker will correspond to one machine, but in the case of very large models with model parallelism, one worker may span multiple machines. We typically run one input pipeline per worker, feeding all the replicas on that worker.</li> <li>
<em>Synchronous</em>, or more commonly <em>sync</em>, training is where the updates from each replica are aggregated together before updating the model variables. This is in contrast to <em>asynchronous</em>, or <em>async</em> training, where each replica updates the model variables independently. You may also have replicas partitioned into groups which are in sync within each group but async between groups.</li> <li><p><em>Parameter servers</em>: These are machines that hold a single copy of parameters/variables, used by some strategies (right now just <a href="../../distribute/experimental/parameterserverstrategy.html"><code translate="no" dir="ltr">tf.distribute.experimental.ParameterServerStrategy</code></a>). All replicas that want to operate on a variable retrieve it at the beginning of a step and send an update to be applied at the end of the step. These can in principle support either sync or async training, but right now we only have support for async training with parameter servers. Compare to <a href="../../distribute/experimental/centralstoragestrategy.html"><code translate="no" dir="ltr">tf.distribute.experimental.CentralStorageStrategy</code></a>, which puts all variables on a single device on the same machine (and does sync training), and <a href="../../distribute/mirroredstrategy.html"><code translate="no" dir="ltr">tf.distribute.MirroredStrategy</code></a>, which mirrors variables to multiple devices (see below).</p></li> <li>
<p><em>Replica context</em> vs. <em>Cross-replica context</em> vs <em>Update context</em></p> <p>A <em>replica context</em> applies when you execute the computation function that was called with <code translate="no" dir="ltr">strategy.run</code>. Conceptually, you're in replica context when executing the computation function that is being replicated.</p> <p>An <em>update context</em> is entered in a <a href="../../distribute/strategyextended.html#update"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.update</code></a> call.</p> <p>An <em>cross-replica context</em> is entered when you enter a <code translate="no" dir="ltr">strategy.scope</code>. This is useful for calling <a href="../../distribute/strategy.html"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> methods which operate across the replicas (like <code translate="no" dir="ltr">reduce_to()</code>). By default you start in a <em>replica context</em> (the "default single <em>replica context</em>") and then some methods can switch you back and forth.</p>
</li> <li>
<p><em>Distributed value</em>: Distributed value is represented by the base class <a href="../../distribute/distributedvalues.html"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a>. <a href="../../distribute/distributedvalues.html"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> is useful to represent values on multiple devices, and it contains a map from replica id to values. Two representative kinds of <a href="../../distribute/distributedvalues.html"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> are "PerReplica" and "Mirrored" values.</p> <p>"PerReplica" values exist on the worker devices, with a different value for each replica. They are produced by iterating through a distributed dataset returned by <a href="../../distribute/strategy.html#experimental_distribute_dataset"><code translate="no" dir="ltr">tf.distribute.Strategy.experimental_distribute_dataset</code></a> and <a href="../../distribute/strategy.html#distribute_datasets_from_function"><code translate="no" dir="ltr">tf.distribute.Strategy.distribute_datasets_from_function</code></a>. They are also the typical result returned by <a href="../../distribute/strategy.html#run"><code translate="no" dir="ltr">tf.distribute.Strategy.run</code></a>.</p> <p>"Mirrored" values are like "PerReplica" values, except we know that the value on all replicas are the same. We can safely read a "Mirrored" value in a cross-replica context by using the value on any replica.</p>
</li> <li><p><em>Unwrapping</em> and <em>merging</em>: Consider calling a function <code translate="no" dir="ltr">fn</code> on multiple replicas, like <code translate="no" dir="ltr">strategy.run(fn, args=[w])</code> with an argument <code translate="no" dir="ltr">w</code> that is a <a href="../../distribute/distributedvalues.html"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a>. This means <code translate="no" dir="ltr">w</code> will have a map taking replica id <code translate="no" dir="ltr">0</code> to <code translate="no" dir="ltr">w0</code>, replica id <code translate="no" dir="ltr">1</code> to <code translate="no" dir="ltr">w1</code>, etc. <code translate="no" dir="ltr">strategy.run()</code> unwraps <code translate="no" dir="ltr">w</code> before calling <code translate="no" dir="ltr">fn</code>, so it calls <code translate="no" dir="ltr">fn(w0)</code> on device <code translate="no" dir="ltr">d0</code>, <code translate="no" dir="ltr">fn(w1)</code> on device <code translate="no" dir="ltr">d1</code>, etc. It then merges the return values from <code translate="no" dir="ltr">fn()</code>, which leads to one common object if the returned values are the same object from every replica, or a <code translate="no" dir="ltr">DistributedValues</code> object otherwise.</p></li> <li><p><em>Reductions</em> and <em>all-reduce</em>: A <em>reduction</em> is a method of aggregating multiple values into one value, like "sum" or "mean". If a strategy is doing sync training, we will perform a reduction on the gradients to a parameter from all replicas before applying the update. <em>All-reduce</em> is an algorithm for performing a reduction on values from multiple devices and making the result available on all of those devices.</p></li> <li><p><em>Mirrored variables</em>: These are variables that are created on multiple devices, where we keep the variables in sync by applying the same updates to every copy. Mirrored variables are created with <a href="../../variable.html"><code translate="no" dir="ltr">tf.Variable(...synchronization=tf.VariableSynchronization.ON_WRITE...)</code></a>. Normally they are only used in synchronous training.</p></li> <li>
<p><em>SyncOnRead variables</em></p> <p><em>SyncOnRead variables</em> are created by <a href="../../variable.html"><code translate="no" dir="ltr">tf.Variable(...synchronization=tf.VariableSynchronization.ON_READ...)</code></a>, and they are created on multiple devices. In replica context, each component variable on the local replica can perform reads and writes without synchronization with each other. When the <em>SyncOnRead variable</em> is read in cross-replica context, the values from component variables are aggregated and returned.</p> <p><em>SyncOnRead variables</em> bring a lot of custom configuration difficulty to the underlying logic, so we do not encourage users to instantiate and use <em>SyncOnRead variable</em> on their own. We have mainly used <em>SyncOnRead variables</em> for use cases such as batch norm and metrics. For performance reasons, we often don't need to keep these statistics in sync every step and they can be accumulated on each replica independently. The only time we want to sync them is reporting or checkpointing, which typically happens in cross-replica context. <em>SyncOnRead variables</em> are also often used by advanced users who want to control when variable values are aggregated. For example, users sometimes want to maintain gradients independently on each replica for a couple of steps without aggregation.</p>
</li> <li>
<p><em>Distribute-aware layers</em></p> <p>Layers are generally called in a replica context, except when defining a Keras functional model. <a href="../../distribute/in_cross_replica_context.html"><code translate="no" dir="ltr">tf.distribute.in_cross_replica_context</code></a> will let you determine which case you are in. If in a replica context, the <a href="../../distribute/get_replica_context.html"><code translate="no" dir="ltr">tf.distribute.get_replica_context</code></a> function will return the default replica context outside a strategy scope, <code translate="no" dir="ltr">None</code> within a strategy scope, and a <a href="../../distribute/replicacontext.html"><code translate="no" dir="ltr">tf.distribute.ReplicaContext</code></a> object inside a strategy scope and within a <a href="../../distribute/strategy.html#run"><code translate="no" dir="ltr">tf.distribute.Strategy.run</code></a> function. The <code translate="no" dir="ltr">ReplicaContext</code> object has an <code translate="no" dir="ltr">all_reduce</code> method for aggregating across all replicas.</p>
</li> </ul> <p>Note that we provide a default version of <a href="../../distribute/strategy.html"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> that is used when no other strategy is in scope, that provides the same API with reasonable default behavior.</p> <h2 id="modules" data-text="Modules">Modules</h2> <p><a href="distribute/cluster_resolver.html"><code translate="no" dir="ltr">cluster_resolver</code></a> module: Library imports for ClusterResolvers.</p> <p><a href="distribute/experimental.html"><code translate="no" dir="ltr">experimental</code></a> module: Experimental Distribution Strategy library.</p> <h2 id="classes" data-text="Classes">Classes</h2> <p><a href="../../distribute/crossdeviceops.html"><code translate="no" dir="ltr">class CrossDeviceOps</code></a>: Base class for cross-device reduction and broadcasting algorithms.</p> <p><a href="../../distribute/hierarchicalcopyallreduce.html"><code translate="no" dir="ltr">class HierarchicalCopyAllReduce</code></a>: Hierarchical copy all-reduce implementation of CrossDeviceOps.</p> <p><a href="../../distribute/inputcontext.html"><code translate="no" dir="ltr">class InputContext</code></a>: A class wrapping information needed by an input function.</p> <p><a href="../../distribute/inputreplicationmode.html"><code translate="no" dir="ltr">class InputReplicationMode</code></a>: Replication mode for input function.</p> <p><a href="distribute/mirroredstrategy.html"><code translate="no" dir="ltr">class MirroredStrategy</code></a>: Synchronous training across multiple replicas on one machine.</p> <p><a href="../../distribute/ncclallreduce.html"><code translate="no" dir="ltr">class NcclAllReduce</code></a>: NCCL all-reduce implementation of CrossDeviceOps.</p> <p><a href="distribute/onedevicestrategy.html"><code translate="no" dir="ltr">class OneDeviceStrategy</code></a>: A distribution strategy for running on a single device.</p> <p><a href="../../distribute/reduceop.html"><code translate="no" dir="ltr">class ReduceOp</code></a>: Indicates how a set of values should be reduced.</p> <p><a href="../../distribute/reductiontoonedevice.html"><code translate="no" dir="ltr">class ReductionToOneDevice</code></a>: A CrossDeviceOps implementation that copies values to one device to reduce.</p> <p><a href="distribute/replicacontext.html"><code translate="no" dir="ltr">class ReplicaContext</code></a>: A class with a collection of APIs that can be called in a replica context.</p> <p><a href="../../distribute/runoptions.html"><code translate="no" dir="ltr">class RunOptions</code></a>: Run options for <code translate="no" dir="ltr">strategy.run</code>.</p> <p><a href="../../distribute/server.html"><code translate="no" dir="ltr">class Server</code></a>: An in-process TensorFlow server, for use in distributed training.</p> <p><a href="distribute/strategy.html"><code translate="no" dir="ltr">class Strategy</code></a>: A list of devices with a state &amp; compute distribution policy.</p> <p><a href="distribute/strategyextended.html"><code translate="no" dir="ltr">class StrategyExtended</code></a>: Additional APIs for algorithms that need to be distribution-aware.</p> <h2 id="functions" data-text="Functions">Functions</h2> <p><a href="../../distribute/experimental_set_strategy.html"><code translate="no" dir="ltr">experimental_set_strategy(...)</code></a>: Set a <a href="../../distribute/strategy.html"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> as current without <code translate="no" dir="ltr">with strategy.scope()</code>.</p> <p><a href="distribute/get_loss_reduction.html"><code translate="no" dir="ltr">get_loss_reduction(...)</code></a>: <a href="../../distribute/reduceop.html"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> corresponding to the last loss reduction.</p> <p><a href="../../distribute/get_replica_context.html"><code translate="no" dir="ltr">get_replica_context(...)</code></a>: Returns the current <a href="../../distribute/replicacontext.html"><code translate="no" dir="ltr">tf.distribute.ReplicaContext</code></a> or <code translate="no" dir="ltr">None</code>.</p> <p><a href="../../distribute/get_strategy.html"><code translate="no" dir="ltr">get_strategy(...)</code></a>: Returns the current <a href="../../distribute/strategy.html"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> object.</p> <p><a href="../../distribute/has_strategy.html"><code translate="no" dir="ltr">has_strategy(...)</code></a>: Return if there is a current non-default <a href="../../distribute/strategy.html"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a>.</p> <p><a href="../../distribute/in_cross_replica_context.html"><code translate="no" dir="ltr">in_cross_replica_context(...)</code></a>: Returns <code translate="no" dir="ltr">True</code> if in a cross-replica context.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/distribute" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/distribute</a>
  </p>
</div>
