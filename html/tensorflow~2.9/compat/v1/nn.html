<h1 class="devsite-page-title">Module: tf.compat.v1.nn</h1> <devsite-bookmark></devsite-bookmark>       <p>Primitive Neural Net (NN) Operations.</p> <h2 id="notes_on_padding" data-text="Notes on padding">Notes on padding</h2> <p>Several neural network operations, such as <a href="../../nn/conv2d.html"><code translate="no" dir="ltr">tf.nn.conv2d</code></a> and <a href="../../nn/max_pool2d.html"><code translate="no" dir="ltr">tf.nn.max_pool2d</code></a>, take a <code translate="no" dir="ltr">padding</code> parameter, which controls how the input is padded before running the operation. The input is padded by inserting values (typically zeros) before and after the tensor in each spatial dimension. The <code translate="no" dir="ltr">padding</code> parameter can either be the string <code translate="no" dir="ltr">'VALID'</code>, which means use no padding, or <code translate="no" dir="ltr">'SAME'</code> which adds padding according to a formula which is described below. Certain ops also allow the amount of padding per dimension to be explicitly specified by passing a list to <code translate="no" dir="ltr">padding</code>.</p> <p>In the case of convolutions, the input is padded with zeros. In case of pools, the padded input values are ignored. For example, in a max pool, the sliding window ignores padded values, which is equivalent to the padded values being <code translate="no" dir="ltr">-infinity</code>.</p> <h3 id="valid_padding" data-text="'VALID' padding">
<code translate="no" dir="ltr">'VALID'</code> padding</h3> <p>Passing <code translate="no" dir="ltr">padding='VALID'</code> to an op causes no padding to be used. This causes the output size to typically be smaller than the input size, even when the stride is one. In the 2D case, the output size is computed as:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">out_height = ceil((in_height - filter_height + 1) / stride_height)
out_width  = ceil((in_width - filter_width + 1) / stride_width)
</pre> <p>The 1D and 3D cases are similar. Note <code translate="no" dir="ltr">filter_height</code> and <code translate="no" dir="ltr">filter_width</code> refer to the filter size after dilations (if any) for convolutions, and refer to the window size for pools.</p> <h3 id="same_padding" data-text="'SAME' padding">
<code translate="no" dir="ltr">'SAME'</code> padding</h3> <p>With <code translate="no" dir="ltr">'SAME'</code> padding, padding is applied to each spatial dimension. When the strides are 1, the input is padded such that the output size is the same as the input size. In the 2D case, the output size is computed as:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">out_height = ceil(in_height / stride_height)
out_width  = ceil(in_width / stride_width)
</pre> <p>The amount of padding used is the smallest amount that results in the output size. The formula for the total amount of padding per dimension is:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">if (in_height % strides[1] == 0):
  pad_along_height = max(filter_height - stride_height, 0)
else:
  pad_along_height = max(filter_height - (in_height % stride_height), 0)
if (in_width % strides[2] == 0):
  pad_along_width = max(filter_width - stride_width, 0)
else:
  pad_along_width = max(filter_width - (in_width % stride_width), 0)
</pre> <p>Finally, the padding on the top, bottom, left and right are:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">pad_top = pad_along_height // 2
pad_bottom = pad_along_height - pad_top
pad_left = pad_along_width // 2
pad_right = pad_along_width - pad_left
</pre> <p>Note that the division by 2 means that there might be cases when the padding on both sides (top vs bottom, right vs left) are off by one. In this case, the bottom and right sides always get the one additional padded pixel. For example, when pad_along_height is 5, we pad 2 pixels at the top and 3 pixels at the bottom. Note that this is different from existing libraries such as PyTorch and Caffe, which explicitly specify the number of padded pixels and always pad the same number of pixels on both sides.</p> <p>Here is an example of <code translate="no" dir="ltr">'SAME'</code> padding:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
in_height = 5
filter_height = 3
stride_height = 2

in_width = 2
filter_width = 2
stride_width = 1

inp = tf.ones((2, in_height, in_width, 2))
filter = tf.ones((filter_height, filter_width, 2, 2))
strides = [stride_height, stride_width]
output = tf.nn.conv2d(inp, filter, strides, padding='SAME')
output.shape[1]  # output_height: ceil(5 / 2)
3
output.shape[2] # output_width: ceil(2 / 1)
2
</pre> <h3 id="explicit_padding" data-text="Explicit padding">Explicit padding</h3> <p>Certain ops, like <a href="../../nn/conv2d.html"><code translate="no" dir="ltr">tf.nn.conv2d</code></a>, also allow a list of explicit padding amounts to be passed to the <code translate="no" dir="ltr">padding</code> parameter. This list is in the same format as what is passed to <a href="../../pad.html"><code translate="no" dir="ltr">tf.pad</code></a>, except the padding must be a nested list, not a tensor. For example, in the 2D case, the list is in the format <code translate="no" dir="ltr">[[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]]</code> when <code translate="no" dir="ltr">data_format</code> is its default value of <code translate="no" dir="ltr">'NHWC'</code>. The two <code translate="no" dir="ltr">[0, 0]</code> pairs indicate the batch and channel dimensions have no padding, which is required, as only spatial dimensions can have padding.</p> <h4 id="for_example" data-text="For example:">For example:</h4> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
inp = tf.ones((1, 3, 3, 1))
filter = tf.ones((2, 2, 1, 1))
strides = [1, 1]
padding = [[0, 0], [1, 2], [0, 1], [0, 0]]
output = tf.nn.conv2d(inp, filter, strides, padding=padding)
tuple(output.shape)
(1, 5, 3, 1)
# Equivalently, tf.pad can be used, since convolutions pad with zeros.
inp = tf.pad(inp, padding)
# 'VALID' means to use no padding in conv2d (we already padded inp)
output2 = tf.nn.conv2d(inp, filter, strides, padding='VALID')
tf.debugging.assert_equal(output, output2)
</pre> <h2 id="modules" data-text="Modules">Modules</h2> <p><a href="nn/experimental.html"><code translate="no" dir="ltr">experimental</code></a> module: Public API for tf.nn.experimental namespace.</p> <p><a href="nn/rnn_cell.html"><code translate="no" dir="ltr">rnn_cell</code></a> module: Public API for tf.keras.<strong>internal</strong>.legacy.rnn_cell namespace.</p> <h2 id="functions" data-text="Functions">Functions</h2> <p><a href="../../random/all_candidate_sampler.html"><code translate="no" dir="ltr">all_candidate_sampler(...)</code></a>: Generate the set of all classes.</p> <p><a href="../../nn/atrous_conv2d.html"><code translate="no" dir="ltr">atrous_conv2d(...)</code></a>: Atrous convolution (a.k.a. convolution with holes or dilated convolution).</p> <p><a href="../../nn/atrous_conv2d_transpose.html"><code translate="no" dir="ltr">atrous_conv2d_transpose(...)</code></a>: The transpose of <code translate="no" dir="ltr">atrous_conv2d</code>.</p> <p><a href="nn/avg_pool.html"><code translate="no" dir="ltr">avg_pool(...)</code></a>: Performs the average pooling on the input.</p> <p><a href="../../nn/avg_pool1d.html"><code translate="no" dir="ltr">avg_pool1d(...)</code></a>: Performs the average pooling on the input.</p> <p><a href="nn/avg_pool.html"><code translate="no" dir="ltr">avg_pool2d(...)</code></a>: Performs the average pooling on the input.</p> <p><a href="../../nn/avg_pool3d.html"><code translate="no" dir="ltr">avg_pool3d(...)</code></a>: Performs the average pooling on the input.</p> <p><a href="../../nn/avg_pool.html"><code translate="no" dir="ltr">avg_pool_v2(...)</code></a>: Performs the avg pooling on the input.</p> <p><a href="nn/batch_norm_with_global_normalization.html"><code translate="no" dir="ltr">batch_norm_with_global_normalization(...)</code></a>: Batch normalization.</p> <p><a href="../../nn/batch_normalization.html"><code translate="no" dir="ltr">batch_normalization(...)</code></a>: Batch normalization.</p> <p><a href="../../nn/bias_add.html"><code translate="no" dir="ltr">bias_add(...)</code></a>: Adds <code translate="no" dir="ltr">bias</code> to <code translate="no" dir="ltr">value</code>.</p> <p><a href="nn/bidirectional_dynamic_rnn.html"><code translate="no" dir="ltr">bidirectional_dynamic_rnn(...)</code></a>: Creates a dynamic version of bidirectional recurrent neural network. (deprecated)</p> <p><a href="../../nn/collapse_repeated.html"><code translate="no" dir="ltr">collapse_repeated(...)</code></a>: Merge repeated labels into single labels.</p> <p><a href="../../nn/compute_accidental_hits.html"><code translate="no" dir="ltr">compute_accidental_hits(...)</code></a>: Compute the position ids in <code translate="no" dir="ltr">sampled_candidates</code> matching <code translate="no" dir="ltr">true_classes</code>.</p> <p><a href="../../nn/compute_average_loss.html"><code translate="no" dir="ltr">compute_average_loss(...)</code></a>: Scales per-example losses with sample_weights and computes their average.</p> <p><a href="nn/conv1d.html"><code translate="no" dir="ltr">conv1d(...)</code></a>: Computes a 1-D convolution of input with rank <code translate="no" dir="ltr">&gt;=3</code> and a <code translate="no" dir="ltr">3-D</code> filter. (deprecated argument values) (deprecated argument values)</p> <p><a href="../../nn/conv1d_transpose.html"><code translate="no" dir="ltr">conv1d_transpose(...)</code></a>: The transpose of <code translate="no" dir="ltr">conv1d</code>.</p> <p><a href="nn/conv2d.html"><code translate="no" dir="ltr">conv2d(...)</code></a>: Computes a 2-D convolution given 4-D <code translate="no" dir="ltr">input</code> and <code translate="no" dir="ltr">filter</code> tensors.</p> <p><a href="nn/conv2d_backprop_filter.html"><code translate="no" dir="ltr">conv2d_backprop_filter(...)</code></a>: Computes the gradients of convolution with respect to the filter.</p> <p><a href="nn/conv2d_backprop_input.html"><code translate="no" dir="ltr">conv2d_backprop_input(...)</code></a>: Computes the gradients of convolution with respect to the input.</p> <p><a href="nn/conv2d_transpose.html"><code translate="no" dir="ltr">conv2d_transpose(...)</code></a>: The transpose of <code translate="no" dir="ltr">conv2d</code>.</p> <p><a href="nn/conv3d.html"><code translate="no" dir="ltr">conv3d(...)</code></a>: Computes a 3-D convolution given 5-D <code translate="no" dir="ltr">input</code> and <code translate="no" dir="ltr">filter</code> tensors.</p> <p><a href="nn/conv3d_backprop_filter.html"><code translate="no" dir="ltr">conv3d_backprop_filter(...)</code></a>: Computes the gradients of 3-D convolution with respect to the filter.</p> <p><a href="nn/conv3d_backprop_filter.html"><code translate="no" dir="ltr">conv3d_backprop_filter_v2(...)</code></a>: Computes the gradients of 3-D convolution with respect to the filter.</p> <p><a href="nn/conv3d_transpose.html"><code translate="no" dir="ltr">conv3d_transpose(...)</code></a>: The transpose of <code translate="no" dir="ltr">conv3d</code>.</p> <p><a href="../../nn/conv_transpose.html"><code translate="no" dir="ltr">conv_transpose(...)</code></a>: The transpose of <code translate="no" dir="ltr">convolution</code>.</p> <p><a href="nn/convolution.html"><code translate="no" dir="ltr">convolution(...)</code></a>: Computes sums of N-D convolutions (actually cross-correlation).</p> <p><a href="nn/crelu.html"><code translate="no" dir="ltr">crelu(...)</code></a>: Computes Concatenated ReLU.</p> <p><a href="nn/ctc_beam_search_decoder.html"><code translate="no" dir="ltr">ctc_beam_search_decoder(...)</code></a>: Performs beam search decoding on the logits given in input.</p> <p><a href="../../nn/ctc_beam_search_decoder.html"><code translate="no" dir="ltr">ctc_beam_search_decoder_v2(...)</code></a>: Performs beam search decoding on the logits given in input.</p> <p><a href="../../nn/ctc_greedy_decoder.html"><code translate="no" dir="ltr">ctc_greedy_decoder(...)</code></a>: Performs greedy decoding on the logits given in input (best path).</p> <p><a href="nn/ctc_loss.html"><code translate="no" dir="ltr">ctc_loss(...)</code></a>: Computes the CTC (Connectionist Temporal Classification) Loss.</p> <p><a href="nn/ctc_loss_v2.html"><code translate="no" dir="ltr">ctc_loss_v2(...)</code></a>: Computes CTC (Connectionist Temporal Classification) loss.</p> <p><a href="../../nn/ctc_unique_labels.html"><code translate="no" dir="ltr">ctc_unique_labels(...)</code></a>: Get unique labels and indices for batched labels for <a href="../../nn/ctc_loss.html"><code translate="no" dir="ltr">tf.nn.ctc_loss</code></a>.</p> <p><a href="depth_to_space.html"><code translate="no" dir="ltr">depth_to_space(...)</code></a>: DepthToSpace for tensors of type T.</p> <p><a href="nn/depthwise_conv2d.html"><code translate="no" dir="ltr">depthwise_conv2d(...)</code></a>: Depthwise 2-D convolution.</p> <p><a href="../../nn/depthwise_conv2d_backprop_filter.html"><code translate="no" dir="ltr">depthwise_conv2d_backprop_filter(...)</code></a>: Computes the gradients of depthwise convolution with respect to the filter.</p> <p><a href="../../nn/depthwise_conv2d_backprop_input.html"><code translate="no" dir="ltr">depthwise_conv2d_backprop_input(...)</code></a>: Computes the gradients of depthwise convolution with respect to the input.</p> <p><a href="nn/depthwise_conv2d_native.html"><code translate="no" dir="ltr">depthwise_conv2d_native(...)</code></a>: Computes a 2-D depthwise convolution.</p> <p><a href="../../nn/depthwise_conv2d_backprop_filter.html"><code translate="no" dir="ltr">depthwise_conv2d_native_backprop_filter(...)</code></a>: Computes the gradients of depthwise convolution with respect to the filter.</p> <p><a href="../../nn/depthwise_conv2d_backprop_input.html"><code translate="no" dir="ltr">depthwise_conv2d_native_backprop_input(...)</code></a>: Computes the gradients of depthwise convolution with respect to the input.</p> <p><a href="nn/dilation2d.html"><code translate="no" dir="ltr">dilation2d(...)</code></a>: Computes the grayscale dilation of 4-D <code translate="no" dir="ltr">input</code> and 3-D <code translate="no" dir="ltr">filter</code> tensors.</p> <p><a href="nn/dropout.html"><code translate="no" dir="ltr">dropout(...)</code></a>: Computes dropout. (deprecated arguments)</p> <p><a href="nn/dynamic_rnn.html"><code translate="no" dir="ltr">dynamic_rnn(...)</code></a>: Creates a recurrent neural network specified by RNNCell <code translate="no" dir="ltr">cell</code>. (deprecated)</p> <p><a href="../../nn/elu.html"><code translate="no" dir="ltr">elu(...)</code></a>: Computes the exponential linear function.</p> <p><a href="nn/embedding_lookup.html"><code translate="no" dir="ltr">embedding_lookup(...)</code></a>: Looks up embeddings for the given <code translate="no" dir="ltr">ids</code> from a list of tensors.</p> <p><a href="nn/embedding_lookup_sparse.html"><code translate="no" dir="ltr">embedding_lookup_sparse(...)</code></a>: Looks up embeddings for the given ids and weights from a list of tensors.</p> <p><a href="nn/erosion2d.html"><code translate="no" dir="ltr">erosion2d(...)</code></a>: Computes the grayscale erosion of 4-D <code translate="no" dir="ltr">value</code> and 3-D <code translate="no" dir="ltr">kernel</code> tensors.</p> <p><a href="../../random/fixed_unigram_candidate_sampler.html"><code translate="no" dir="ltr">fixed_unigram_candidate_sampler(...)</code></a>: Samples a set of classes using the provided (fixed) base distribution.</p> <p><a href="nn/fractional_avg_pool.html"><code translate="no" dir="ltr">fractional_avg_pool(...)</code></a>: Performs fractional average pooling on the input. (deprecated)</p> <p><a href="nn/fractional_max_pool.html"><code translate="no" dir="ltr">fractional_max_pool(...)</code></a>: Performs fractional max pooling on the input. (deprecated)</p> <p><a href="nn/fused_batch_norm.html"><code translate="no" dir="ltr">fused_batch_norm(...)</code></a>: Batch normalization.</p> <p><a href="math/in_top_k.html"><code translate="no" dir="ltr">in_top_k(...)</code></a>: Says whether the targets are in the top <code translate="no" dir="ltr">K</code> predictions.</p> <p><a href="../../nn/l2_loss.html"><code translate="no" dir="ltr">l2_loss(...)</code></a>: L2 Loss.</p> <p><a href="../../math/l2_normalize.html"><code translate="no" dir="ltr">l2_normalize(...)</code></a>: Normalizes along dimension <code translate="no" dir="ltr">axis</code> using an L2 norm. (deprecated arguments)</p> <p><a href="../../nn/leaky_relu.html"><code translate="no" dir="ltr">leaky_relu(...)</code></a>: Compute the Leaky ReLU activation function.</p> <p><a href="../../random/learned_unigram_candidate_sampler.html"><code translate="no" dir="ltr">learned_unigram_candidate_sampler(...)</code></a>: Samples a set of classes from a distribution learned during training.</p> <p><a href="../../nn/local_response_normalization.html"><code translate="no" dir="ltr">local_response_normalization(...)</code></a>: Local Response Normalization.</p> <p><a href="../../nn/log_poisson_loss.html"><code translate="no" dir="ltr">log_poisson_loss(...)</code></a>: Computes log Poisson loss given <code translate="no" dir="ltr">log_input</code>.</p> <p><a href="math/log_softmax.html"><code translate="no" dir="ltr">log_softmax(...)</code></a>: Computes log softmax activations. (deprecated arguments)</p> <p><a href="../../random/log_uniform_candidate_sampler.html"><code translate="no" dir="ltr">log_uniform_candidate_sampler(...)</code></a>: Samples a set of classes using a log-uniform (Zipfian) base distribution.</p> <p><a href="../../nn/local_response_normalization.html"><code translate="no" dir="ltr">lrn(...)</code></a>: Local Response Normalization.</p> <p><a href="nn/max_pool.html"><code translate="no" dir="ltr">max_pool(...)</code></a>: Performs the max pooling on the input.</p> <p><a href="../../nn/max_pool1d.html"><code translate="no" dir="ltr">max_pool1d(...)</code></a>: Performs the max pooling on the input.</p> <p><a href="../../nn/max_pool2d.html"><code translate="no" dir="ltr">max_pool2d(...)</code></a>: Performs max pooling on 2D spatial data such as images.</p> <p><a href="../../nn/max_pool3d.html"><code translate="no" dir="ltr">max_pool3d(...)</code></a>: Performs the max pooling on the input.</p> <p><a href="../../nn/max_pool.html"><code translate="no" dir="ltr">max_pool_v2(...)</code></a>: Performs max pooling on the input.</p> <p><a href="nn/max_pool_with_argmax.html"><code translate="no" dir="ltr">max_pool_with_argmax(...)</code></a>: Performs max pooling on the input and outputs both max values and indices.</p> <p><a href="nn/moments.html"><code translate="no" dir="ltr">moments(...)</code></a>: Calculate the mean and variance of <code translate="no" dir="ltr">x</code>.</p> <p><a href="nn/nce_loss.html"><code translate="no" dir="ltr">nce_loss(...)</code></a>: Computes and returns the noise-contrastive estimation training loss.</p> <p><a href="../../nn/normalize_moments.html"><code translate="no" dir="ltr">normalize_moments(...)</code></a>: Calculate the mean and variance of based on the sufficient statistics.</p> <p><a href="nn/pool.html"><code translate="no" dir="ltr">pool(...)</code></a>: Performs an N-D pooling operation.</p> <p><a href="nn/quantized_avg_pool.html"><code translate="no" dir="ltr">quantized_avg_pool(...)</code></a>: Produces the average pool of the input tensor for quantized types.</p> <p><a href="nn/quantized_conv2d.html"><code translate="no" dir="ltr">quantized_conv2d(...)</code></a>: Computes a 2D convolution given quantized 4D input and filter tensors.</p> <p><a href="nn/quantized_max_pool.html"><code translate="no" dir="ltr">quantized_max_pool(...)</code></a>: Produces the max pool of the input tensor for quantized types.</p> <p><a href="nn/quantized_relu_x.html"><code translate="no" dir="ltr">quantized_relu_x(...)</code></a>: Computes Quantized Rectified Linear X: <code translate="no" dir="ltr">min(max(features, 0), max_value)</code></p> <p><a href="nn/raw_rnn.html"><code translate="no" dir="ltr">raw_rnn(...)</code></a>: Creates an <code translate="no" dir="ltr">RNN</code> specified by RNNCell <code translate="no" dir="ltr">cell</code> and loop function <code translate="no" dir="ltr">loop_fn</code>.</p> <p><a href="../../nn/relu.html"><code translate="no" dir="ltr">relu(...)</code></a>: Computes rectified linear: <code translate="no" dir="ltr">max(features, 0)</code>.</p> <p><a href="../../nn/relu6.html"><code translate="no" dir="ltr">relu6(...)</code></a>: Computes Rectified Linear 6: <code translate="no" dir="ltr">min(max(features, 0), 6)</code>.</p> <p><a href="nn/relu_layer.html"><code translate="no" dir="ltr">relu_layer(...)</code></a>: Computes Relu(x * weight + biases).</p> <p><a href="nn/safe_embedding_lookup_sparse.html"><code translate="no" dir="ltr">safe_embedding_lookup_sparse(...)</code></a>: Lookup embedding results, accounting for invalid IDs and empty features.</p> <p><a href="nn/sampled_softmax_loss.html"><code translate="no" dir="ltr">sampled_softmax_loss(...)</code></a>: Computes and returns the sampled softmax training loss.</p> <p><a href="../../nn/scale_regularization_loss.html"><code translate="no" dir="ltr">scale_regularization_loss(...)</code></a>: Scales the sum of the given regularization losses by number of replicas.</p> <p><a href="../../nn/selu.html"><code translate="no" dir="ltr">selu(...)</code></a>: Computes scaled exponential linear: <code translate="no" dir="ltr">scale * alpha * (exp(features) - 1)</code></p> <p><a href="nn/separable_conv2d.html"><code translate="no" dir="ltr">separable_conv2d(...)</code></a>: 2-D convolution with separable filters.</p> <p><a href="../../math/sigmoid.html"><code translate="no" dir="ltr">sigmoid(...)</code></a>: Computes sigmoid of <code translate="no" dir="ltr">x</code> element-wise.</p> <p><a href="nn/sigmoid_cross_entropy_with_logits.html"><code translate="no" dir="ltr">sigmoid_cross_entropy_with_logits(...)</code></a>: Computes sigmoid cross entropy given <code translate="no" dir="ltr">logits</code>.</p> <p><a href="../../nn/silu.html"><code translate="no" dir="ltr">silu(...)</code></a>: Computes the SiLU or Swish activation function: <code translate="no" dir="ltr">x * sigmoid(beta * x)</code>.</p> <p><a href="math/softmax.html"><code translate="no" dir="ltr">softmax(...)</code></a>: Computes softmax activations.</p> <p><a href="nn/softmax_cross_entropy_with_logits.html"><code translate="no" dir="ltr">softmax_cross_entropy_with_logits(...)</code></a>: Computes softmax cross entropy between <code translate="no" dir="ltr">logits</code> and <code translate="no" dir="ltr">labels</code>. (deprecated)</p> <p><a href="nn/softmax_cross_entropy_with_logits_v2.html"><code translate="no" dir="ltr">softmax_cross_entropy_with_logits_v2(...)</code></a>: Computes softmax cross entropy between <code translate="no" dir="ltr">logits</code> and <code translate="no" dir="ltr">labels</code>. (deprecated arguments)</p> <p><a href="../../math/softplus.html"><code translate="no" dir="ltr">softplus(...)</code></a>: Computes elementwise softplus: <code translate="no" dir="ltr">softplus(x) = log(exp(x) + 1)</code>.</p> <p><a href="../../nn/softsign.html"><code translate="no" dir="ltr">softsign(...)</code></a>: Computes softsign: <code translate="no" dir="ltr">features / (abs(features) + 1)</code>.</p> <p><a href="space_to_batch.html"><code translate="no" dir="ltr">space_to_batch(...)</code></a>: SpaceToBatch for 4-D tensors of type T.</p> <p><a href="space_to_depth.html"><code translate="no" dir="ltr">space_to_depth(...)</code></a>: SpaceToDepth for tensors of type T.</p> <p><a href="nn/sparse_softmax_cross_entropy_with_logits.html"><code translate="no" dir="ltr">sparse_softmax_cross_entropy_with_logits(...)</code></a>: Computes sparse softmax cross entropy between <code translate="no" dir="ltr">logits</code> and <code translate="no" dir="ltr">labels</code>.</p> <p><a href="nn/static_bidirectional_rnn.html"><code translate="no" dir="ltr">static_bidirectional_rnn(...)</code></a>: Creates a bidirectional recurrent neural network. (deprecated)</p> <p><a href="nn/static_rnn.html"><code translate="no" dir="ltr">static_rnn(...)</code></a>: Creates a recurrent neural network specified by RNNCell <code translate="no" dir="ltr">cell</code>. (deprecated)</p> <p><a href="nn/static_state_saving_rnn.html"><code translate="no" dir="ltr">static_state_saving_rnn(...)</code></a>: RNN that accepts a state saver for time-truncated RNN calculation. (deprecated)</p> <p><a href="nn/sufficient_statistics.html"><code translate="no" dir="ltr">sufficient_statistics(...)</code></a>: Calculate the sufficient statistics for the mean and variance of <code translate="no" dir="ltr">x</code>.</p> <p><a href="../../nn/silu.html"><code translate="no" dir="ltr">swish(...)</code></a>: Computes the SiLU or Swish activation function: <code translate="no" dir="ltr">x * sigmoid(beta * x)</code>.</p> <p><a href="../../math/tanh.html"><code translate="no" dir="ltr">tanh(...)</code></a>: Computes hyperbolic tangent of <code translate="no" dir="ltr">x</code> element-wise.</p> <p><a href="../../math/top_k.html"><code translate="no" dir="ltr">top_k(...)</code></a>: Finds values and indices of the <code translate="no" dir="ltr">k</code> largest entries for the last dimension.</p> <p><a href="../../random/uniform_candidate_sampler.html"><code translate="no" dir="ltr">uniform_candidate_sampler(...)</code></a>: Samples a set of classes using a uniform base distribution.</p> <p><a href="nn/weighted_cross_entropy_with_logits.html"><code translate="no" dir="ltr">weighted_cross_entropy_with_logits(...)</code></a>: Computes a weighted cross entropy. (deprecated arguments)</p> <p><a href="nn/weighted_moments.html"><code translate="no" dir="ltr">weighted_moments(...)</code></a>: Returns the frequency-weighted mean and variance of <code translate="no" dir="ltr">x</code>.</p> <p><a href="../../nn/with_space_to_batch.html"><code translate="no" dir="ltr">with_space_to_batch(...)</code></a>: Performs <code translate="no" dir="ltr">op</code> on the space-to-batch representation of <code translate="no" dir="ltr">input</code>.</p> <p><a href="nn/xw_plus_b.html"><code translate="no" dir="ltr">xw_plus_b(...)</code></a>: Computes matmul(x, weights) + biases.</p> <p><a href="../../math/zero_fraction.html"><code translate="no" dir="ltr">zero_fraction(...)</code></a>: Returns the fraction of zeros in <code translate="no" dir="ltr">value</code>.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/nn" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/nn</a>
  </p>
</div>
