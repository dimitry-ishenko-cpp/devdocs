<h1 class="devsite-page-title">tf.compat.v1.get_local_variable</h1> <devsite-bookmark></devsite-bookmark>       <p>Gets an existing <em>local</em> variable or creates a new one.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.get_local_variable(
    name,
    shape=None,
    dtype=None,
    initializer=None,
    regularizer=None,
    trainable=False,
    collections=None,
    caching_device=None,
    partitioner=None,
    validate_shape=True,
    use_resource=None,
    custom_getter=None,
    constraint=None,
    synchronization=tf.VariableSynchronization.AUTO,
    aggregation=tf.compat.v1.VariableAggregation.NONE
)
</pre> <p><section><devsite-expandable expanded> <h2 class="showalways" id="migrate-to-tf2" data-text="Migrate to TF2">Migrate to TF2</h2></devsite-expandable></section></p> <aside class="caution"><strong>Caution:</strong><span> This API was designed for TensorFlow v1. Continue reading for details on how to migrate from this API to a native TensorFlow v2 equivalent. See the <a href="https://www.tensorflow.org/guide/migrate">TensorFlow v1 to TensorFlow v2 migration guide</a> for instructions on how to migrate the rest of your code.</span></aside> <p>Although it is a legacy <a href="../v1.html"><code translate="no" dir="ltr">compat.v1</code></a> api, <a href="get_variable.html"><code translate="no" dir="ltr">tf.compat.v1.get_variable</code></a> is mostly compatible with eager execution and <a href="../../function.html"><code translate="no" dir="ltr">tf.function</code></a> but only if you combine it with the <a href="keras/utils/track_tf1_style_variables.html"><code translate="no" dir="ltr">tf.compat.v1.keras.utils.track_tf1_style_variables</code></a> decorator. (Though it will behave as if reuse is always set to <code translate="no" dir="ltr">AUTO_REUSE</code>.)</p> <p>See the <a href="https://www.tensorflow.org/guide/migrate/model_mapping">model migration guide</a> for more info.</p> <p>If you do not combine it with <a href="keras/utils/track_tf1_style_variables.html"><code translate="no" dir="ltr">tf.compat.v1.keras.utils.track_tf1_style_variables</code></a>, <code translate="no" dir="ltr">get_variable</code> will create a brand new variable every single time it is called and will never reuse variables, regardless of variable names or <code translate="no" dir="ltr">reuse</code> arguments.</p> <p>The TF2 equivalent of this symbol would be <a href="../../variable.html"><code translate="no" dir="ltr">tf.Variable</code></a>, but note that when using <a href="../../variable.html"><code translate="no" dir="ltr">tf.Variable</code></a> you must make sure you track your variables (and regularizer arguments) either manually or via <a href="../../module.html"><code translate="no" dir="ltr">tf.Module</code></a> or <a href="../../keras/layers/layer.html"><code translate="no" dir="ltr">tf.keras.layers.Layer</code></a> mechanisms.</p> <p>A section of the <a href="https://www.tensorflow.org/guide/migrate/model_mapping#incremental_migration_to_native_tf2">migration guide</a> provides more details on incrementally migrating these usages to <a href="../../variable.html"><code translate="no" dir="ltr">tf.Variable</code></a> as well.</p> <blockquote class="note">
<strong>Note:</strong><span> The <code translate="no" dir="ltr">partitioner</code> arg is not compatible with TF2 behaviors even when using <a href="keras/utils/track_tf1_style_variables.html"><code translate="no" dir="ltr">tf.compat.v1.keras.utils.track_tf1_style_variables</code></a>. It can be replaced by using <code translate="no" dir="ltr">ParameterServerStrategy</code> and its partitioners. See the <a href="https://www.tensorflow.org/guide/migrate/multi_worker_cpu_gpu_training">multi-gpu migration guide</a> and the ParameterServerStrategy guides it references for more info.</span>
</blockquote>  <h2 id="description" data-text="Description">Description</h2>  <p>Behavior is the same as in <code translate="no" dir="ltr">get_variable</code>, except that variables are added to the <code translate="no" dir="ltr">LOCAL_VARIABLES</code> collection and <code translate="no" dir="ltr">trainable</code> is set to <code translate="no" dir="ltr">False</code>. This function prefixes the name with the current variable scope and performs reuse checks. See the <a href="https://tensorflow.org/guide/variables">Variable Scope How To</a> for an extensive description of how reusing works. Here is a basic example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">def foo():
  with tf.variable_scope("foo", reuse=tf.AUTO_REUSE):
    v = tf.get_variable("v", [1])
  return v

v1 = foo()  # Creates v.
v2 = foo()  # Gets the same, existing v.
assert v1 == v2
</pre> <p>If initializer is <code translate="no" dir="ltr">None</code> (the default), the default initializer passed in the variable scope will be used. If that one is <code translate="no" dir="ltr">None</code> too, a <code translate="no" dir="ltr">glorot_uniform_initializer</code> will be used. The initializer can also be a Tensor, in which case the variable is initialized to this value and shape.</p> <p>Similarly, if the regularizer is <code translate="no" dir="ltr">None</code> (the default), the default regularizer passed in the variable scope will be used (if that is <code translate="no" dir="ltr">None</code> too, then by default no regularization is performed).</p> <p>If a partitioner is provided, a <code translate="no" dir="ltr">PartitionedVariable</code> is returned. Accessing this object as a <code translate="no" dir="ltr">Tensor</code> returns the shards concatenated along the partition axis.</p> <p>Some useful partitioners are available. See, e.g., <code translate="no" dir="ltr">variable_axis_size_partitioner</code> and <code translate="no" dir="ltr">min_max_variable_partitioner</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> The name of the new or existing variable. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">shape</code> </td> <td> Shape of the new or existing variable. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dtype</code> </td> <td> Type of the new or existing variable (defaults to <code translate="no" dir="ltr">DT_FLOAT</code>). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">initializer</code> </td> <td> Initializer for the variable if one is created. Can either be an initializer object or a Tensor. If it's a Tensor, its shape must be known unless validate_shape is False. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">regularizer</code> </td> <td> A (Tensor -&gt; Tensor or None) function; the result of applying it on a newly created variable will be added to the collection <code translate="no" dir="ltr">tf.GraphKeys.REGULARIZATION_LOSSES</code> and can be used for regularization. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">collections</code> </td> <td> List of graph collections keys to add the Variable to. Defaults to <code translate="no" dir="ltr">[GraphKeys.LOCAL_VARIABLES]</code> (see <a href="../../variable.html"><code translate="no" dir="ltr">tf.Variable</code></a>). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">caching_device</code> </td> <td> Optional device string or function describing where the Variable should be cached for reading. Defaults to the Variable's device. If not <code translate="no" dir="ltr">None</code>, caches on another device. Typical use is to cache on the device where the Ops using the Variable reside, to deduplicate copying through <code translate="no" dir="ltr">Switch</code> and other conditional statements. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">partitioner</code> </td> <td> Optional callable that accepts a fully defined <code translate="no" dir="ltr">TensorShape</code> and <code translate="no" dir="ltr">dtype</code> of the Variable to be created, and returns a list of partitions for each axis (currently only one axis can be partitioned). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">validate_shape</code> </td> <td> If False, allows the variable to be initialized with a value of unknown shape. If True, the default, the shape of initial_value must be known. For this to be used the initializer must be a Tensor and not an initializer object. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">use_resource</code> </td> <td> If False, creates a regular Variable. If true, creates an experimental ResourceVariable instead with well-defined semantics. Defaults to False (will later change to True). When eager execution is enabled this argument is always forced to be True. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">custom_getter</code> </td> <td> Callable that takes as a first argument the true getter, and allows overwriting the internal get_variable method. The signature of <code translate="no" dir="ltr">custom_getter</code> should match that of this method, but the most future-proof version will allow for changes: <code translate="no" dir="ltr">def custom_getter(getter, *args, **kwargs)</code>. Direct access to all <code translate="no" dir="ltr">get_variable</code> parameters is also allowed: <code translate="no" dir="ltr">def custom_getter(getter, name, *args, **kwargs)</code>. A simple identity custom getter that simply creates variables with modified names is: <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">def custom_getter(getter, name, *args, **kwargs):
  return getter(name + '_suffix', *args, **kwargs)
</pre> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">constraint</code> </td> <td> An optional projection function to be applied to the variable after being updated by an <code translate="no" dir="ltr">Optimizer</code> (e.g. used to implement norm constraints or value constraints for layer weights). The function must take as input the unprojected Tensor representing the value of the variable and return the Tensor for the projected value (which must have the same shape). Constraints are not safe to use when doing asynchronous distributed training. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">synchronization</code> </td> <td> Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class <a href="../../variablesynchronization.html"><code translate="no" dir="ltr">tf.VariableSynchronization</code></a>. By default the synchronization is set to <code translate="no" dir="ltr">AUTO</code> and the current <code translate="no" dir="ltr">DistributionStrategy</code> chooses when to synchronize. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">aggregation</code> </td> <td> Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class <a href="../../variableaggregation.html"><code translate="no" dir="ltr">tf.VariableAggregation</code></a>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> The created or existing <code translate="no" dir="ltr">Variable</code> (or <code translate="no" dir="ltr">PartitionedVariable</code>, if a partitioner was used). </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> when creating a new variable and shape is not declared, when violating reuse during variable creation, or when <code translate="no" dir="ltr">initializer</code> dtype and <code translate="no" dir="ltr">dtype</code> don't match. Reuse is set inside <code translate="no" dir="ltr">variable_scope</code>. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/get_local_variable" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/get_local_variable</a>
  </p>
</div>
