<h1 class="devsite-page-title">tf.compat.v1.nn.safe_embedding_lookup_sparse</h1> <devsite-bookmark></devsite-bookmark>       <p>Lookup embedding results, accounting for invalid IDs and empty features.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.nn.safe_embedding_lookup_sparse(
    embedding_weights,
    sparse_ids,
    sparse_weights=None,
    combiner='mean',
    default_id=None,
    name=None,
    partition_strategy='div',
    max_norm=None
)
</pre>  <p>The partitioned embedding in <code translate="no" dir="ltr">embedding_weights</code> must all be the same shape except for the first dimension. The first dimension is allowed to vary as the vocabulary size is not necessarily a multiple of <code translate="no" dir="ltr">P</code>. <code translate="no" dir="ltr">embedding_weights</code> may be a <code translate="no" dir="ltr">PartitionedVariable</code> as returned by using <a href="../get_variable.html"><code translate="no" dir="ltr">tf.compat.v1.get_variable()</code></a> with a partitioner.</p> <p>Invalid IDs (&lt; 0) are pruned from input IDs and weights, as well as any IDs with non-positive weight. For an entry with no features, the embedding vector for <code translate="no" dir="ltr">default_id</code> is returned, or the 0-vector if <code translate="no" dir="ltr">default_id</code> is not supplied.</p> <p>The ids and weights may be multi-dimensional. Embeddings are always aggregated along the last dimension.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">embedding_weights</code> </td> <td> A single tensor representing the complete embedding tensor, or a list tensors all of same shape except for the first dimension, representing sharded embedding tensors. Alternatively, a <code translate="no" dir="ltr">PartitionedVariable</code>, created by partitioning along dimension 0. Each element must be appropriately sized for the given <code translate="no" dir="ltr">partition_strategy</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">sparse_ids</code> </td> <td> <code translate="no" dir="ltr">SparseTensor</code> of shape <code translate="no" dir="ltr">[d_0, d_1, ..., d_n]</code> containing the ids. <code translate="no" dir="ltr">d_0</code> is typically batch size. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">sparse_weights</code> </td> <td> <code translate="no" dir="ltr">SparseTensor</code> of same shape as <code translate="no" dir="ltr">sparse_ids</code>, containing float weights corresponding to <code translate="no" dir="ltr">sparse_ids</code>, or <code translate="no" dir="ltr">None</code> if all weights are be assumed to be 1.0. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">combiner</code> </td> <td> A string specifying how to combine embedding results for each entry. Currently "mean", "sqrtn" and "sum" are supported, with "mean" the default. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">default_id</code> </td> <td> The id to use for an entry with no features. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> A name for this operation (optional). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">partition_strategy</code> </td> <td> A string specifying the partitioning strategy. Currently <code translate="no" dir="ltr">"div"</code> and <code translate="no" dir="ltr">"mod"</code> are supported. Default is <code translate="no" dir="ltr">"div"</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">max_norm</code> </td> <td> If not <code translate="no" dir="ltr">None</code>, all embeddings are l2-normalized to max_norm before combining. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A dense tensor representing the combined embeddings for the sparse ids. For each row in the dense tensor represented by <code translate="no" dir="ltr">sp_ids</code>, the op looks up the embeddings for all ids in that row, multiplies them by the corresponding weight, and combines these embeddings as specified. <p>In other words, if</p> <p><code translate="no" dir="ltr">shape(combined embedding_weights) = [p0, p1, ..., pm]</code></p> <p>and</p> <p><code translate="no" dir="ltr">shape(sparse_ids) = shape(sparse_weights) = [d0, d1, ..., dn]</code></p> <p>then</p> <p><code translate="no" dir="ltr">shape(output) = [d0, d1, ... dn-1, p1, ..., pm]</code>.</p> <p>For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">[0, 0]: id 1, weight 2.0
[0, 1]: id 3, weight 0.5
[1, 0]: id -1, weight 1.0
[2, 3]: id 1, weight 3.0
</pre> <p><code translate="no" dir="ltr">default_id</code> is 0.</p> <p>with <code translate="no" dir="ltr">combiner</code>="mean", then the output will be a 3x20 matrix where</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)
output[1, :] = (params[0, :] * 1.0) / 1.0
output[2, :] = (params[1, :] * 3.0) / 3.0
</pre> 
</td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if <code translate="no" dir="ltr">embedding_weights</code> is empty. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/nn/safe_embedding_lookup_sparse" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/nn/safe_embedding_lookup_sparse</a>
  </p>
</div>
