<h1 class="devsite-page-title">tf.compat.v1.nn.rnn_cell.DropoutWrapper</h1> <devsite-bookmark></devsite-bookmark>       <p>Operator adding dropout to inputs and outputs of the given cell.</p> <p>Inherits From: <a href="rnncell.html"><code translate="no" dir="ltr">RNNCell</code></a>, <a href="../../layers/layer.html"><code translate="no" dir="ltr">Layer</code></a>, <a href="../../../../keras/layers/layer.html"><code translate="no" dir="ltr">Layer</code></a>, <a href="../../../../module.html"><code translate="no" dir="ltr">Module</code></a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.nn.rnn_cell.DropoutWrapper(
    cell,
    input_keep_prob=1.0,
    output_keep_prob=1.0,
    state_keep_prob=1.0,
    variational_recurrent=False,
    input_size=None,
    dtype=None,
    seed=None,
    dropout_state_filter_visitor=None,
    **kwargs
)
</pre>   
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">cell</code> </td> <td> an RNNCell, a projection to output_size is added to it. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">input_keep_prob</code> </td> <td> unit Tensor or float between 0 and 1, input keep probability; if it is constant and 1, no input dropout will be added. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">output_keep_prob</code> </td> <td> unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">state_keep_prob</code> </td> <td> unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added. State dropout is performed on the outgoing states of the cell. <strong>Note</strong> the state components to which dropout is applied when <code translate="no" dir="ltr">state_keep_prob</code> is in <code translate="no" dir="ltr">(0, 1)</code> are also determined by the argument <code translate="no" dir="ltr">dropout_state_filter_visitor</code> (e.g. by default dropout is never applied to the <code translate="no" dir="ltr">c</code> component of an <code translate="no" dir="ltr">LSTMStateTuple</code>). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">variational_recurrent</code> </td> <td> Python bool. If <code translate="no" dir="ltr">True</code>, then the same dropout pattern is applied across all time steps per run call. If this parameter is set, <code translate="no" dir="ltr">input_size</code> <strong>must</strong> be provided. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">input_size</code> </td> <td> (optional) (possibly nested tuple of) <code translate="no" dir="ltr">TensorShape</code> objects containing the depth(s) of the input tensors expected to be passed in to the <code translate="no" dir="ltr">DropoutWrapper</code>. Required and used <strong>iff</strong> <code translate="no" dir="ltr">variational_recurrent = True</code> and <code translate="no" dir="ltr">input_keep_prob &lt; 1</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dtype</code> </td> <td> (optional) The <code translate="no" dir="ltr">dtype</code> of the input, state, and output tensors. Required and used <strong>iff</strong> <code translate="no" dir="ltr">variational_recurrent = True</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">seed</code> </td> <td> (optional) integer, the randomness seed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dropout_state_filter_visitor</code> </td> <td> (optional), default: (see below). Function that takes any hierarchical level of the state and returns a scalar or depth=1 structure of Python booleans describing which terms in the state should be dropped out. In addition, if the function returns <code translate="no" dir="ltr">True</code>, dropout is applied across this sublevel. If the function returns <code translate="no" dir="ltr">False</code>, dropout is not applied across this entire sublevel. Default behavior: perform dropout on all terms except the memory (<code translate="no" dir="ltr">c</code>) state of <code translate="no" dir="ltr">LSTMCellState</code> objects, and don't try to apply dropout to <code translate="no" dir="ltr">TensorArray</code> objects: <code translate="no" dir="ltr">def dropout_state_filter_visitor(s): if isinstance(s, LSTMCellState): # Never perform dropout on the c state. return LSTMCellState(c=False, h=True) elif isinstance(s, TensorArray): return False return True</code> </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> dict of keyword arguments for base layer. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if <code translate="no" dir="ltr">cell</code> is not an <code translate="no" dir="ltr">RNNCell</code>, or <code translate="no" dir="ltr">keep_state_fn</code> is provided but not <code translate="no" dir="ltr">callable</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if any of the keep_probs are not between 0 and 1. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">graph</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">output_size</code> </td> <td> Integer or TensorShape: size of outputs produced by this cell. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">scope_name</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">state_size</code> </td> <td> size(s) of state(s) used by this cell. <p>It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">wrapped_cell</code> </td> <td> 
</td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="apply" data-text="apply"><code translate="no" dir="ltr">apply</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/legacy_tf_layers/base.py#L239-L240">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
apply(
    *args, **kwargs
)
</pre> <h3 id="get_initial_state" data-text="get_initial_state"><code translate="no" dir="ltr">get_initial_state</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/layers/rnn/legacy_cells.py#L239-L266">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_initial_state(
    inputs=None, batch_size=None, dtype=None
)
</pre> <h3 id="get_losses_for" data-text="get_losses_for"><code translate="no" dir="ltr">get_losses_for</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_layer_v1.py#L1341-L1358">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_losses_for(
    inputs
)
</pre> <p>Retrieves losses relevant to a specific set of inputs.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">inputs</code> </td> <td> Input tensor or list/tuple of input tensors. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> List of loss tensors of the layer that depend on <code translate="no" dir="ltr">inputs</code>. </td> </tr> 
</table> <h3 id="get_updates_for" data-text="get_updates_for"><code translate="no" dir="ltr">get_updates_for</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_layer_v1.py#L1322-L1339">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_updates_for(
    inputs
)
</pre> <p>Retrieves updates relevant to a specific set of inputs.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">inputs</code> </td> <td> Input tensor or list/tuple of input tensors. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> List of update ops of the layer that depend on <code translate="no" dir="ltr">inputs</code>. </td> </tr> 
</table> <h3 id="zero_state" data-text="zero_state"><code translate="no" dir="ltr">zero_state</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/layers/rnn/legacy_cell_wrappers.py#L148-L150">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
zero_state(
    batch_size, dtype
)
</pre> <p>Return zero-filled state tensor(s).</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">batch_size</code> </td> <td> int, float, or unit Tensor representing the batch size. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dtype</code> </td> <td> the data type to use for the state. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> If <code translate="no" dir="ltr">state_size</code> is an int or TensorShape, then the return value is a <code translate="no" dir="ltr">N-D</code> tensor of shape <code translate="no" dir="ltr">[batch_size, state_size]</code> filled with zeros. <p>If <code translate="no" dir="ltr">state_size</code> is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of <code translate="no" dir="ltr">2-D</code> tensors with the shapes <code translate="no" dir="ltr">[batch_size, s]</code> for each s in <code translate="no" dir="ltr">state_size</code>. </p>
</td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/nn/rnn_cell/DropoutWrapper" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/nn/rnn_cell/DropoutWrapper</a>
  </p>
</div>
