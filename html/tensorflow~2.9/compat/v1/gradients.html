<h1 class="devsite-page-title">tf.compat.v1.gradients</h1> <devsite-bookmark></devsite-bookmark>       <p>Constructs symbolic derivatives of sum of <code translate="no" dir="ltr">ys</code> w.r.t. x in <code translate="no" dir="ltr">xs</code>.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.gradients(
    ys,
    xs,
    grad_ys=None,
    name='gradients',
    colocate_gradients_with_ops=False,
    gate_gradients=False,
    aggregation_method=None,
    stop_gradients=None,
    unconnected_gradients=tf.UnconnectedGradients.NONE
)
</pre>  <p><code translate="no" dir="ltr">ys</code> and <code translate="no" dir="ltr">xs</code> are each a <code translate="no" dir="ltr">Tensor</code> or a list of tensors. <code translate="no" dir="ltr">grad_ys</code> is a list of <code translate="no" dir="ltr">Tensor</code>, holding the gradients received by the <code translate="no" dir="ltr">ys</code>. The list must be the same length as <code translate="no" dir="ltr">ys</code>.</p> <p><code translate="no" dir="ltr">gradients()</code> adds ops to the graph to output the derivatives of <code translate="no" dir="ltr">ys</code> with respect to <code translate="no" dir="ltr">xs</code>. It returns a list of <code translate="no" dir="ltr">Tensor</code> of length <code translate="no" dir="ltr">len(xs)</code> where each tensor is the <code translate="no" dir="ltr">sum(dy/dx)</code> for y in <code translate="no" dir="ltr">ys</code> and for x in <code translate="no" dir="ltr">xs</code>.</p> <p><code translate="no" dir="ltr">grad_ys</code> is a list of tensors of the same length as <code translate="no" dir="ltr">ys</code> that holds the initial gradients for each y in <code translate="no" dir="ltr">ys</code>. When <code translate="no" dir="ltr">grad_ys</code> is None, we fill in a tensor of '1's of the shape of y for each y in <code translate="no" dir="ltr">ys</code>. A user can provide their own initial <code translate="no" dir="ltr">grad_ys</code> to compute the derivatives using a different initial gradient for each y (e.g., if one wanted to weight the gradient differently for each value in each y).</p> <p><code translate="no" dir="ltr">stop_gradients</code> is a <code translate="no" dir="ltr">Tensor</code> or a list of tensors to be considered constant with respect to all <code translate="no" dir="ltr">xs</code>. These tensors will not be backpropagated through, as though they had been explicitly disconnected using <code translate="no" dir="ltr">stop_gradient</code>. Among other things, this allows computation of partial derivatives as opposed to total derivatives. For example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">a = tf.constant(0.)
b = 2 * a
g = tf.gradients(a + b, [a, b], stop_gradients=[a, b])
</pre> <p>Here the partial derivatives <code translate="no" dir="ltr">g</code> evaluate to <code translate="no" dir="ltr">[1.0, 1.0]</code>, compared to the total derivatives <code translate="no" dir="ltr">tf.gradients(a + b, [a, b])</code>, which take into account the influence of <code translate="no" dir="ltr">a</code> on <code translate="no" dir="ltr">b</code> and evaluate to <code translate="no" dir="ltr">[3.0, 1.0]</code>. Note that the above is equivalent to:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">a = tf.stop_gradient(tf.constant(0.))
b = tf.stop_gradient(2 * a)
g = tf.gradients(a + b, [a, b])
</pre> <p><code translate="no" dir="ltr">stop_gradients</code> provides a way of stopping gradient after the graph has already been constructed, as compared to <a href="../../stop_gradient.html"><code translate="no" dir="ltr">tf.stop_gradient</code></a> which is used during graph construction. When the two approaches are combined, backpropagation stops at both <a href="../../stop_gradient.html"><code translate="no" dir="ltr">tf.stop_gradient</code></a> nodes and nodes in <code translate="no" dir="ltr">stop_gradients</code>, whichever is encountered first.</p> <p>All integer tensors are considered constant with respect to all <code translate="no" dir="ltr">xs</code>, as if they were included in <code translate="no" dir="ltr">stop_gradients</code>.</p> <p><code translate="no" dir="ltr">unconnected_gradients</code> determines the value returned for each x in xs if it is unconnected in the graph to ys. By default this is None to safeguard against errors. Mathematically these gradients are zero which can be requested using the <code translate="no" dir="ltr">'zero'</code> option. <code translate="no" dir="ltr">tf.UnconnectedGradients</code> provides the following options and behaviors:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">a = tf.ones([1, 2])
b = tf.ones([3, 1])
g1 = tf.gradients([b], [a], unconnected_gradients='none')
sess.run(g1)  # [None]

g2 = tf.gradients([b], [a], unconnected_gradients='zero')
sess.run(g2)  # [array([[0., 0.]], dtype=float32)]
</pre> <p>Let us take one practical example which comes during the back propogation phase. This function is used to evaluate the derivatives of the cost function with respect to Weights <code translate="no" dir="ltr">Ws</code> and Biases <code translate="no" dir="ltr">bs</code>. Below sample implementation provides the exaplantion of what it is actually used for :</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">Ws = tf.constant(0.)
bs = 2 * Ws
cost = Ws + bs  # This is just an example. So, please ignore the formulas.
g = tf.gradients(cost, [Ws, bs])
dCost_dW, dCost_db = g
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ys</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> or list of tensors to be differentiated. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">xs</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> or list of tensors to be used for differentiation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">grad_ys</code> </td> <td> Optional. A <code translate="no" dir="ltr">Tensor</code> or list of tensors the same size as <code translate="no" dir="ltr">ys</code> and holding the gradients computed for each y in <code translate="no" dir="ltr">ys</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name to use for grouping all the gradient ops together. defaults to 'gradients'. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">colocate_gradients_with_ops</code> </td> <td> If True, try colocating gradients with the corresponding op. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">gate_gradients</code> </td> <td> If True, add a tuple around the gradients returned for an operations. This avoids some race conditions. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">aggregation_method</code> </td> <td> Specifies the method used to combine gradient terms. Accepted values are constants defined in the class <code translate="no" dir="ltr">AggregationMethod</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">stop_gradients</code> </td> <td> Optional. A <code translate="no" dir="ltr">Tensor</code> or list of tensors not to differentiate through. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">unconnected_gradients</code> </td> <td> Optional. Specifies the gradient value returned when the given input tensors are unconnected. Accepted values are constants defined in the class <a href="../../unconnectedgradients.html"><code translate="no" dir="ltr">tf.UnconnectedGradients</code></a> and the default value is <code translate="no" dir="ltr">none</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of <code translate="no" dir="ltr">Tensor</code> of length <code translate="no" dir="ltr">len(xs)</code> where each tensor is the <code translate="no" dir="ltr">sum(dy/dx)</code> for y in <code translate="no" dir="ltr">ys</code> and for x in <code translate="no" dir="ltr">xs</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">LookupError</code> </td> <td> if one of the operations between <code translate="no" dir="ltr">x</code> and <code translate="no" dir="ltr">y</code> does not have a registered gradient function. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if the arguments are invalid. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> if called in Eager mode. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/gradients" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/gradients</a>
  </p>
</div>
