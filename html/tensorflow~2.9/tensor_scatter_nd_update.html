<h1 class="devsite-page-title">tf.tensor_scatter_nd_update</h1> <devsite-bookmark></devsite-bookmark>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/ops/array_ops.py#L5781-L6065">  View source on GitHub </a> </td> </table> <p>Scatter <code translate="no" dir="ltr">updates</code> into an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update"><code translate="no" dir="ltr">tf.compat.v1.tensor_scatter_nd_update</code></a>, <a href="https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update"><code translate="no" dir="ltr">tf.compat.v1.tensor_scatter_update</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.tensor_scatter_nd_update(
    tensor, indices, updates, name=None
)
</pre>  <p>This operation creates a new tensor by applying sparse <code translate="no" dir="ltr">updates</code> to the input <code translate="no" dir="ltr">tensor</code>. This is similar to an index assignment.</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp"># Not implemented: tensors cannot be updated inplace.
tensor[indices] = updates
</pre> <p>If an out of bound index is found on CPU, an error is returned.</p> <blockquote> <aside class="warning"><strong>Warning:</strong><span> There are some GPU specific semantics for this operation.</span></aside> <ul> <li>If an out of bound index is found, the index is ignored.</li> <li>The order in which updates are applied is nondeterministic, so the output will be nondeterministic if <code translate="no" dir="ltr">indices</code> contains duplicates.</li> </ul> </blockquote> <p>This operation is very similar to <a href="scatter_nd.html"><code translate="no" dir="ltr">tf.scatter_nd</code></a>, except that the updates are scattered onto an existing tensor (as opposed to a zero-tensor). If the memory for the existing tensor cannot be re-used, a copy is made and updated.</p> <h4 id="in_general" data-text="In general:">In general:</h4> <ul> <li>
<code translate="no" dir="ltr">indices</code> is an integer tensor - the indices to update in <code translate="no" dir="ltr">tensor</code>.</li> <li>
<code translate="no" dir="ltr">indices</code> has <strong>at least two</strong> axes, the last axis is the depth of the index vectors.</li> <li>For each index vector in <code translate="no" dir="ltr">indices</code> there is a corresponding entry in <code translate="no" dir="ltr">updates</code>.</li> <li>If the length of the index vectors matches the rank of the <code translate="no" dir="ltr">tensor</code>, then the index vectors each point to scalars in <code translate="no" dir="ltr">tensor</code> and each update is a scalar.</li> <li>If the length of the index vectors is less than the rank of <code translate="no" dir="ltr">tensor</code>, then the index vectors each point to slices of <code translate="no" dir="ltr">tensor</code> and shape of the updates must match that slice.</li> </ul> <p>Overall this leads to the following shape constraints:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">assert tf.rank(indices) &gt;= 2
index_depth = indices.shape[-1]
batch_shape = indices.shape[:-1]
assert index_depth &lt;= tf.rank(tensor)
outer_shape = tensor.shape[:index_depth]
inner_shape = tensor.shape[index_depth:]
assert updates.shape == batch_shape + inner_shape
</pre> <p>Typical usage is often much simpler than this general form, and it can be better understood starting with simple examples:</p> <h3 id="scalar_updates" data-text="Scalar updates">Scalar updates</h3> <p>The simplest usage inserts scalar elements into a tensor by index. In this case, the <code translate="no" dir="ltr">index_depth</code> must equal the rank of the input <code translate="no" dir="ltr">tensor</code>, slice each column of <code translate="no" dir="ltr">indices</code> is an index into an axis of the input <code translate="no" dir="ltr">tensor</code>.</p> <p>In this simplest case the shape constraints are:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">num_updates, index_depth = indices.shape.as_list()
assert updates.shape == [num_updates]
assert index_depth == tf.rank(tensor)`
</pre> <p>For example, to insert 4 scattered elements in a rank-1 tensor with 8 elements.</p> <div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;"> <img style="width:100%" src="data:image/png;base64,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"> </div> <p>This scatter operation would look like this:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
tensor = [0, 0, 0, 0, 0, 0, 0, 0]    # tf.rank(tensor) == 1
indices = [[1], [3], [4], [7]]       # num_updates == 4, index_depth == 1
updates = [9, 10, 11, 12]            # num_updates == 4
print(tf.tensor_scatter_nd_update(tensor, indices, updates))
tf.Tensor([ 0 9  0 10  11  0  0 12], shape=(8,), dtype=int32)
</pre> <p>The length (first axis) of <code translate="no" dir="ltr">updates</code> must equal the length of the <code translate="no" dir="ltr">indices</code>: <code translate="no" dir="ltr">num_updates</code>. This is the number of updates being inserted. Each scalar update is inserted into <code translate="no" dir="ltr">tensor</code> at the indexed location.</p> <p>For a higher rank input <code translate="no" dir="ltr">tensor</code> scalar updates can be inserted by using an <code translate="no" dir="ltr">index_depth</code> that matches <a href="rank.html"><code translate="no" dir="ltr">tf.rank(tensor)</code></a>:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
tensor = [[1, 1], [1, 1], [1, 1]]    # tf.rank(tensor) == 2
indices = [[0, 1], [2, 0]]           # num_updates == 2, index_depth == 2
updates = [5, 10]                    # num_updates == 2
print(tf.tensor_scatter_nd_update(tensor, indices, updates))
tf.Tensor(
    [[ 1  5]
     [ 1  1]
     [10  1]], shape=(3, 2), dtype=int32)
</pre> <h3 id="slice_updates" data-text="Slice updates">Slice updates</h3> <p>When the input <code translate="no" dir="ltr">tensor</code> has more than one axis scatter can be used to update entire slices.</p> <p>In this case it's helpful to think of the input <code translate="no" dir="ltr">tensor</code> as being a two level array-of-arrays. The shape of this two level array is split into the <code translate="no" dir="ltr">outer_shape</code> and the <code translate="no" dir="ltr">inner_shape</code>.</p> <p><code translate="no" dir="ltr">indices</code> indexes into the outer level of the input tensor (<code translate="no" dir="ltr">outer_shape</code>). and replaces the sub-array at that location with the corresponding item from the <code translate="no" dir="ltr">updates</code> list. The shape of each update is <code translate="no" dir="ltr">inner_shape</code>.</p> <p>When updating a list of slices the shape constraints are:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">num_updates, index_depth = indices.shape.as_list()
inner_shape = tensor.shape[:index_depth]
outer_shape = tensor.shape[index_depth:]
assert updates.shape == [num_updates, inner_shape]
</pre> <p>For example, to update rows of a <code translate="no" dir="ltr">(6, 3)</code> <code translate="no" dir="ltr">tensor</code>:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
tensor = tf.zeros([6, 3], dtype=tf.int32)
</pre> <p>Use an index depth of one.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
indices = tf.constant([[2], [4]])     # num_updates == 2, index_depth == 1
num_updates, index_depth = indices.shape.as_list()
</pre> <p>The <code translate="no" dir="ltr">outer_shape</code> is <code translate="no" dir="ltr">6</code>, the inner shape is <code translate="no" dir="ltr">3</code>:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
outer_shape = tensor.shape[:index_depth]
inner_shape = tensor.shape[index_depth:]
</pre> <p>2 rows are being indexed so 2 <code translate="no" dir="ltr">updates</code> must be supplied. Each update must be shaped to match the <code translate="no" dir="ltr">inner_shape</code>.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
# num_updates == 2, inner_shape==3
updates = tf.constant([[1, 2, 3],
                       [4, 5, 6]])
</pre> <h4 id="altogether_this_gives" data-text="Altogether this gives:">Altogether this gives:</h4> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
tf.tensor_scatter_nd_update(tensor, indices, updates).numpy()
array([[0, 0, 0],
       [0, 0, 0],
       [1, 2, 3],
       [0, 0, 0],
       [4, 5, 6],
       [0, 0, 0]], dtype=int32)
</pre> <h4 id="more_slice_update_examples" data-text="More slice update examples">More slice update examples</h4> <p>A tensor representing a batch of uniformly sized video clips naturally has 5 axes: <code translate="no" dir="ltr">[batch_size, time, width, height, channels]</code>.</p> <h4 id="for_example" data-text="For example:">For example:</h4> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
batch_size, time, width, height, channels = 13,11,7,5,3
video_batch = tf.zeros([batch_size, time, width, height, channels])
</pre> <p>To replace a selection of video clips:</p> <ul> <li>Use an <code translate="no" dir="ltr">index_depth</code> of 1 (indexing the <code translate="no" dir="ltr">outer_shape</code>: <code translate="no" dir="ltr">[batch_size]</code>)</li> <li>Provide updates each with a shape matching the <code translate="no" dir="ltr">inner_shape</code>: <code translate="no" dir="ltr">[time, width, height, channels]</code>.</li> </ul> <p>To replace the first two clips with ones:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
indices = [[0],[1]]
new_clips = tf.ones([2, time, width, height, channels])
tf.tensor_scatter_nd_update(video_batch, indices, new_clips)
</pre> <p>To replace a selection of frames in the videos:</p> <ul> <li>
<code translate="no" dir="ltr">indices</code> must have an <code translate="no" dir="ltr">index_depth</code> of 2 for the <code translate="no" dir="ltr">outer_shape</code>: <code translate="no" dir="ltr">[batch_size, time]</code>.</li> <li>
<code translate="no" dir="ltr">updates</code> must be shaped like a list of images. Each update must have a shape, matching the <code translate="no" dir="ltr">inner_shape</code>: <code translate="no" dir="ltr">[width, height, channels]</code>.</li> </ul> <p>To replace the first frame of the first three video clips:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
indices = [[0, 0], [1, 0], [2, 0]] # num_updates=3, index_depth=2
new_images = tf.ones([
  # num_updates=3, inner_shape=(width, height, channels)
  3, width, height, channels])
tf.tensor_scatter_nd_update(video_batch, indices, new_images)
</pre> <h3 id="folded_indices" data-text="Folded indices">Folded indices</h3> <p>In simple cases it's convenient to think of <code translate="no" dir="ltr">indices</code> and <code translate="no" dir="ltr">updates</code> as lists, but this is not a strict requirement. Instead of a flat <code translate="no" dir="ltr">num_updates</code>, the <code translate="no" dir="ltr">indices</code> and <code translate="no" dir="ltr">updates</code> can be folded into a <code translate="no" dir="ltr">batch_shape</code>. This <code translate="no" dir="ltr">batch_shape</code> is all axes of the <code translate="no" dir="ltr">indices</code>, except for the innermost <code translate="no" dir="ltr">index_depth</code> axis.</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">index_depth = indices.shape[-1]
batch_shape = indices.shape[:-1]
</pre>
<blockquote class="note">
<strong>Note:</strong><span> The one exception is that the <code translate="no" dir="ltr">batch_shape</code> cannot be <code translate="no" dir="ltr">[]</code>. You can't update a single index by passing indices with shape <code translate="no" dir="ltr">[index_depth]</code>.</span>
</blockquote> <p><code translate="no" dir="ltr">updates</code> must have a matching <code translate="no" dir="ltr">batch_shape</code> (the axes before <code translate="no" dir="ltr">inner_shape</code>).</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">assert updates.shape == batch_shape + inner_shape
</pre>
<blockquote class="note">
<strong>Note:</strong><span> The result is equivalent to flattening the <code translate="no" dir="ltr">batch_shape</code> axes of <code translate="no" dir="ltr">indices</code> and <code translate="no" dir="ltr">updates</code>. This generalization just avoids the need for reshapes when it is more natural to construct "folded" indices and updates.</span>
</blockquote> <p>With this generalization the full shape constraints are:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">assert tf.rank(indices) &gt;= 2
index_depth = indices.shape[-1]
batch_shape = indices.shape[:-1]
assert index_depth &lt;= tf.rank(tensor)
outer_shape = tensor.shape[:index_depth]
inner_shape = tensor.shape[index_depth:]
assert updates.shape == batch_shape + inner_shape
</pre> <p>For example, to draw an <code translate="no" dir="ltr">X</code> on a <code translate="no" dir="ltr">(5,5)</code> matrix start with these indices:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
tensor = tf.zeros([5,5])
indices = tf.constant([
 [[0,0],
  [1,1],
  [2,2],
  [3,3],
  [4,4]],
 [[0,4],
  [1,3],
  [2,2],
  [3,1],
  [4,0]],
])
indices.shape.as_list()  # batch_shape == [2, 5], index_depth == 2
[2, 5, 2]
</pre> <p>Here the <code translate="no" dir="ltr">indices</code> do not have a shape of <code translate="no" dir="ltr">[num_updates, index_depth]</code>, but a shape of <code translate="no" dir="ltr">batch_shape+[index_depth]</code>.</p> <p>Since the <code translate="no" dir="ltr">index_depth</code> is equal to the rank of <code translate="no" dir="ltr">tensor</code>:</p> <ul> <li>
<code translate="no" dir="ltr">outer_shape</code> is <code translate="no" dir="ltr">(5,5)</code>
</li> <li>
<code translate="no" dir="ltr">inner_shape</code> is <code translate="no" dir="ltr">()</code> - each update is scalar</li> <li>
<code translate="no" dir="ltr">updates.shape</code> is <code translate="no" dir="ltr">batch_shape + inner_shape == (5,2) + ()</code>
</li> </ul> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
updates = [
  [1,1,1,1,1],
  [1,1,1,1,1],
]
</pre> <p>Putting this together gives:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
tf.tensor_scatter_nd_update(tensor, indices, updates).numpy()
array([[1., 0., 0., 0., 1.],
       [0., 1., 0., 1., 0.],
       [0., 0., 1., 0., 0.],
       [0., 1., 0., 1., 0.],
       [1., 0., 0., 0., 1.]], dtype=float32)
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">tensor</code> </td> <td> Tensor to copy/update. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">indices</code> </td> <td> Indices to update. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">updates</code> </td> <td> Updates to apply at the indices. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the operation. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A new tensor with the given shape and updates applied according to the indices. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/tensor_scatter_nd_update" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/tensor_scatter_nd_update</a>
  </p>
</div>
