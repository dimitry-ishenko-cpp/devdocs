<h1 class="devsite-page-title">tf.distribute.InputOptions</h1> <devsite-bookmark></devsite-bookmark>       <p>Run options for <code translate="no" dir="ltr">experimental_distribute_dataset(s_from_function)</code>.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.distribute.InputOptions(
    experimental_fetch_to_device=None,
    experimental_replication_mode=tf.distribute.InputReplicationMode.PER_WORKER,
    experimental_place_dataset_on_device=False,
    experimental_per_replica_buffer_size=1
)
</pre>  <p>This can be used to hold some strategy specific configs.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># Setup TPUStrategy
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.TPUStrategy(resolver)

dataset = tf.data.Dataset.range(16)
distributed_dataset_on_host = (
    strategy.experimental_distribute_dataset(
        dataset,
        tf.distribute.InputOptions(
            experimental_replication_mode=
            experimental_replication_mode.PER_WORKER,
            experimental_place_dataset_on_device=False,
            experimental_per_replica_buffer_size=1)))
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">experimental_fetch_to_device</code> </td> <td> Boolean. If True, dataset elements will be prefetched to accelerator device memory. When False, dataset elements are prefetched to host device memory. Must be False when using TPUEmbedding API. experimental_fetch_to_device can only be used with experimental_replication_mode=PER_WORKER. Default behavior is same as setting it to True. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_replication_mode</code> </td> <td> Replication mode for the input function. Currently, the InputReplicationMode.PER_REPLICA is only supported with tf.distribute.MirroredStrategy. experimental_distribute_datasets_from_function. The default value is InputReplicationMode.PER_WORKER. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_place_dataset_on_device</code> </td> <td> Boolean. Default to False. When True, dataset will be placed on the device, otherwise it will remain on the host. experimental_place_dataset_on_device=True can only be used with experimental_replication_mode=PER_REPLICA </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_per_replica_buffer_size</code> </td> <td> Integer. Default to 1. Indicates the prefetch buffer size in the replica device memory. Users can set it to 0 to completely disable prefetching behavior, or a number greater than 1 to enable larger buffer size. Note that this option is still valid with <code translate="no" dir="ltr">experimental_fetch_to_device=False</code>. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/distribute/InputOptions" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/distribute/InputOptions</a>
  </p>
</div>
