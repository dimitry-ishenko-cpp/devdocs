<h1 class="devsite-page-title">tf.map_fn</h1> <devsite-bookmark></devsite-bookmark>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/ops/map_fn.py#L614-L645">  View source on GitHub </a> </td> </table> <p>Transforms <code translate="no" dir="ltr">elems</code> by applying <code translate="no" dir="ltr">fn</code> to each element unstacked on axis 0. (deprecated arguments)</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.map_fn(
    fn,
    elems,
    dtype=None,
    parallel_iterations=None,
    back_prop=True,
    swap_memory=False,
    infer_shape=True,
    name=None,
    fn_output_signature=None
)
</pre>  <aside class="deprecated"><strong>Deprecated:</strong><span> SOME ARGUMENTS ARE DEPRECATED: <code translate="no" dir="ltr">(dtype)</code>. They will be removed in a future version. Instructions for updating: Use fn_output_signature instead</span></aside> <p>See also <a href="scan.html"><code translate="no" dir="ltr">tf.scan</code></a>.</p> <p><code translate="no" dir="ltr">map_fn</code> unstacks <code translate="no" dir="ltr">elems</code> on axis 0 to obtain a sequence of elements; calls <code translate="no" dir="ltr">fn</code> to transform each element; and then stacks the transformed values back together.</p> <h4 id="mapping_functions_with_single-tensor_inputs_and_outputs" data-text="Mapping functions with single-Tensor inputs and outputs">Mapping functions with single-Tensor inputs and outputs</h4> <p>If <code translate="no" dir="ltr">elems</code> is a single tensor and <code translate="no" dir="ltr">fn</code>'s signature is <code translate="no" dir="ltr">tf.Tensor-&gt;tf.Tensor</code>, then <code translate="no" dir="ltr">map_fn(fn, elems)</code> is equivalent to <code translate="no" dir="ltr">tf.stack([fn(elem) for elem in tf.unstack(elems)])</code>. E.g.:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
tf.map_fn(fn=lambda t: tf.range(t, t + 3), elems=tf.constant([3, 5, 2]))
&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=
  array([[3, 4, 5],
         [5, 6, 7],
         [2, 3, 4]], dtype=int32)&gt;
</pre> <p><code translate="no" dir="ltr">map_fn(fn, elems).shape = [elems.shape[0]] + fn(elems[0]).shape</code>.</p> <h4 id="mapping_functions_with_multi-arity_inputs_and_outputs" data-text="Mapping functions with multi-arity inputs and outputs">Mapping functions with multi-arity inputs and outputs</h4> <p><code translate="no" dir="ltr">map_fn</code> also supports functions with multi-arity inputs and outputs:</p> <ul> <li><p>If <code translate="no" dir="ltr">elems</code> is a tuple (or nested structure) of tensors, then those tensors must all have the same outer-dimension size (<code translate="no" dir="ltr">num_elems</code>); and <code translate="no" dir="ltr">fn</code> is used to transform each tuple (or structure) of corresponding slices from <code translate="no" dir="ltr">elems</code>. E.g., if <code translate="no" dir="ltr">elems</code> is a tuple <code translate="no" dir="ltr">(t1, t2, t3)</code>, then <code translate="no" dir="ltr">fn</code> is used to transform each tuple of slices <code translate="no" dir="ltr">(t1[i], t2[i], t3[i])</code> (where <code translate="no" dir="ltr">0 &lt;= i &lt; num_elems</code>).</p></li> <li><p>If <code translate="no" dir="ltr">fn</code> returns a tuple (or nested structure) of tensors, then the result is formed by stacking corresponding elements from those structures.</p></li> </ul> <h4 id="specifying_fns_output_signature" data-text="Specifying fn's output signature">Specifying <code translate="no" dir="ltr">fn</code>'s output signature</h4> <p>If <code translate="no" dir="ltr">fn</code>'s input and output signatures are different, then the output signature must be specified using <code translate="no" dir="ltr">fn_output_signature</code>. (The input and output signatures are differ if their structures, dtypes, or tensor types do not match). E.g.:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
tf.map_fn(fn=tf.strings.length,  # input &amp; output have different dtypes
          elems=tf.constant(["hello", "moon"]),
          fn_output_signature=tf.int32)
&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([5, 4], dtype=int32)&gt;
tf.map_fn(fn=tf.strings.join,  # input &amp; output have different structures
          elems=[tf.constant(['The', 'A']), tf.constant(['Dog', 'Cat'])],
          fn_output_signature=tf.string)
&lt;tf.Tensor: shape=(2,), dtype=string,
 numpy=array([b'TheDog', b'ACat'], dtype=object)&gt;
</pre> <p><code translate="no" dir="ltr">fn_output_signature</code> can be specified using any of the following:</p> <ul> <li>A <a href="dtypes/dtype.html"><code translate="no" dir="ltr">tf.DType</code></a> or <a href="tensorspec.html"><code translate="no" dir="ltr">tf.TensorSpec</code></a> (to describe a <a href="tensor.html"><code translate="no" dir="ltr">tf.Tensor</code></a>)</li> <li>A <a href="raggedtensorspec.html"><code translate="no" dir="ltr">tf.RaggedTensorSpec</code></a> (to describe a <a href="raggedtensor.html"><code translate="no" dir="ltr">tf.RaggedTensor</code></a>)</li> <li>A <a href="sparsetensorspec.html"><code translate="no" dir="ltr">tf.SparseTensorSpec</code></a> (to describe a <a href="sparse/sparsetensor.html"><code translate="no" dir="ltr">tf.sparse.SparseTensor</code></a>)</li> <li>A (possibly nested) tuple, list, or dict containing the above types.</li> </ul> <h4 id="raggedtensors" data-text="RaggedTensors">RaggedTensors</h4> <p><code translate="no" dir="ltr">map_fn</code> supports <a href="raggedtensor.html"><code translate="no" dir="ltr">tf.RaggedTensor</code></a> inputs and outputs. In particular:</p> <ul> <li>
<p>If <code translate="no" dir="ltr">elems</code> is a <code translate="no" dir="ltr">RaggedTensor</code>, then <code translate="no" dir="ltr">fn</code> will be called with each row of that ragged tensor.</p> <ul> <li>If <code translate="no" dir="ltr">elems</code> has only one ragged dimension, then the values passed to <code translate="no" dir="ltr">fn</code> will be <a href="tensor.html"><code translate="no" dir="ltr">tf.Tensor</code></a>s.</li> <li>If <code translate="no" dir="ltr">elems</code> has multiple ragged dimensions, then the values passed to <code translate="no" dir="ltr">fn</code> will be <a href="raggedtensor.html"><code translate="no" dir="ltr">tf.RaggedTensor</code></a>s with one fewer ragged dimension.</li> </ul>
</li> <li>
<p>If the result of <code translate="no" dir="ltr">map_fn</code> should be a <code translate="no" dir="ltr">RaggedTensor</code>, then use a <a href="raggedtensorspec.html"><code translate="no" dir="ltr">tf.RaggedTensorSpec</code></a> to specify <code translate="no" dir="ltr">fn_output_signature</code>.</p> <ul> <li>If <code translate="no" dir="ltr">fn</code> returns <a href="tensor.html"><code translate="no" dir="ltr">tf.Tensor</code></a>s with varying sizes, then use a <a href="raggedtensorspec.html"><code translate="no" dir="ltr">tf.RaggedTensorSpec</code></a> with <code translate="no" dir="ltr">ragged_rank=0</code> to combine them into a single ragged tensor (which will have ragged_rank=1).</li> <li>If <code translate="no" dir="ltr">fn</code> returns <a href="raggedtensor.html"><code translate="no" dir="ltr">tf.RaggedTensor</code></a>s, then use a <a href="raggedtensorspec.html"><code translate="no" dir="ltr">tf.RaggedTensorSpec</code></a> with the same <code translate="no" dir="ltr">ragged_rank</code>.</li> </ul>
</li> </ul> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
# Example: RaggedTensor input
rt = tf.ragged.constant([[1, 2, 3], [], [4, 5], [6]])
tf.map_fn(tf.reduce_sum, rt, fn_output_signature=tf.int32)
&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([6, 0, 9, 6], dtype=int32)&gt;
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
# Example: RaggedTensor output
elems = tf.constant([3, 5, 0, 2])
tf.map_fn(tf.range, elems,
          fn_output_signature=tf.RaggedTensorSpec(shape=[None],
                                                  dtype=tf.int32))
&lt;tf.RaggedTensor [[0, 1, 2], [0, 1, 2, 3, 4], [], [0, 1]]&gt;
</pre> <blockquote class="note">
<strong>Note:</strong><span> <code translate="no" dir="ltr">map_fn</code> should only be used if you need to map a function over the <em>rows</em> of a <code translate="no" dir="ltr">RaggedTensor</code>. If you wish to map a function over the individual values, then you should use:</span>
</blockquote> <ul> <li>
<a href="ragged/map_flat_values.html"><code translate="no" dir="ltr">tf.ragged.map_flat_values(fn, rt)</code></a> (if fn is expressible as TensorFlow ops)</li> <li>
<code translate="no" dir="ltr">rt.with_flat_values(map_fn(fn, rt.flat_values))</code> (otherwise)</li> </ul> <p>E.g.:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
rt = tf.ragged.constant([[1, 2, 3], [], [4, 5], [6]])
tf.ragged.map_flat_values(lambda x: x + 2, rt)
&lt;tf.RaggedTensor [[3, 4, 5], [], [6, 7], [8]]&gt;
</pre> <h4 id="sparsetensors" data-text="SparseTensors">SparseTensors</h4> <p><code translate="no" dir="ltr">map_fn</code> supports <a href="sparse/sparsetensor.html"><code translate="no" dir="ltr">tf.sparse.SparseTensor</code></a> inputs and outputs. In particular:</p> <ul> <li><p>If <code translate="no" dir="ltr">elems</code> is a <code translate="no" dir="ltr">SparseTensor</code>, then <code translate="no" dir="ltr">fn</code> will be called with each row of that sparse tensor. In particular, the value passed to <code translate="no" dir="ltr">fn</code> will be a <a href="sparse/sparsetensor.html"><code translate="no" dir="ltr">tf.sparse.SparseTensor</code></a> with one fewer dimension than <code translate="no" dir="ltr">elems</code>.</p></li> <li><p>If the result of <code translate="no" dir="ltr">map_fn</code> should be a <code translate="no" dir="ltr">SparseTensor</code>, then use a <a href="sparsetensorspec.html"><code translate="no" dir="ltr">tf.SparseTensorSpec</code></a> to specify <code translate="no" dir="ltr">fn_output_signature</code>. The individual <code translate="no" dir="ltr">SparseTensor</code>s returned by <code translate="no" dir="ltr">fn</code> will be stacked into a single <code translate="no" dir="ltr">SparseTensor</code> with one more dimension.</p></li> </ul> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
# Example: SparseTensor input
st = tf.sparse.SparseTensor([[0, 0], [2, 0], [2, 1]], [2, 3, 4], [4, 4])
tf.map_fn(tf.sparse.reduce_sum, st, fn_output_signature=tf.int32)
&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([2, 0, 7, 0], dtype=int32)&gt;
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
# Example: SparseTensor output
tf.sparse.to_dense(
    tf.map_fn(tf.sparse.eye, tf.constant([2, 3]),
              fn_output_signature=tf.SparseTensorSpec(None, tf.float32)))
&lt;tf.Tensor: shape=(2, 3, 3), dtype=float32, numpy=
  array([[[1., 0., 0.],
          [0., 1., 0.],
          [0., 0., 0.]],
         [[1., 0., 0.],
          [0., 1., 0.],
          [0., 0., 1.]]], dtype=float32)&gt;
</pre> <blockquote class="note">
<strong>Note:</strong><span> <code translate="no" dir="ltr">map_fn</code> should only be used if you need to map a function over the <em>rows</em> of a <code translate="no" dir="ltr">SparseTensor</code>. If you wish to map a function over the nonzero values, then you should use:</span>
</blockquote> <ul> <li>
<p>If the function is expressible as TensorFlow ops, use:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">tf.sparse.SparseTensor(st.indices, fn(st.values), st.dense_shape)
</pre>
</li> <li>
<p>Otherwise, use:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">tf.sparse.SparseTensor(st.indices, tf.map_fn(fn, st.values),
                       st.dense_shape)
</pre>
</li> </ul> <h4 id="map_fn_vs_vectorized_operations" data-text="map_fn vs. vectorized operations">
<code translate="no" dir="ltr">map_fn</code> vs. vectorized operations</h4> <p><code translate="no" dir="ltr">map_fn</code> will apply the operations used by <code translate="no" dir="ltr">fn</code> to each element of <code translate="no" dir="ltr">elems</code>, resulting in <code translate="no" dir="ltr">O(elems.shape[0])</code> total operations. This is somewhat mitigated by the fact that <code translate="no" dir="ltr">map_fn</code> can process elements in parallel. However, a transform expressed using <code translate="no" dir="ltr">map_fn</code> is still typically less efficient than an equivalent transform expressed using vectorized operations.</p> <p><code translate="no" dir="ltr">map_fn</code> should typically only be used if one of the following is true:</p> <ul> <li>It is difficult or expensive to express the desired transform with vectorized operations.</li> <li>
<code translate="no" dir="ltr">fn</code> creates large intermediate values, so an equivalent vectorized transform would take too much memory.</li> <li>Processing elements in parallel is more efficient than an equivalent vectorized transform.</li> <li>Efficiency of the transform is not critical, and using <code translate="no" dir="ltr">map_fn</code> is more readable.</li> </ul> <p>E.g., the example given above that maps <code translate="no" dir="ltr">fn=lambda t: tf.range(t, t + 3)</code> across <code translate="no" dir="ltr">elems</code> could be rewritten more efficiently using vectorized ops:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
elems = tf.constant([3, 5, 2])
tf.range(3) + tf.expand_dims(elems, 1)
&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=
  array([[3, 4, 5],
         [5, 6, 7],
         [2, 3, 4]], dtype=int32)&gt;
</pre> <p>In some cases, <a href="vectorized_map.html"><code translate="no" dir="ltr">tf.vectorized_map</code></a> can be used to automatically convert a function to a vectorized equivalent.</p> <h4 id="eager_execution" data-text="Eager execution">Eager execution</h4> <p>When executing eagerly, <code translate="no" dir="ltr">map_fn</code> does not execute in parallel even if <code translate="no" dir="ltr">parallel_iterations</code> is set to a value &gt; 1. You can still get the performance benefits of running a function in parallel by using the <a href="function.html"><code translate="no" dir="ltr">tf.function</code></a> decorator:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
fn=lambda t: tf.range(t, t + 3)
@tf.function
def func(elems):
  return tf.map_fn(fn, elems, parallel_iterations=3)
func(tf.constant([3, 5, 2]))
&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=
  array([[3, 4, 5],
         [5, 6, 7],
         [2, 3, 4]], dtype=int32)&gt;
</pre> <blockquote class="note">
<strong>Note:</strong><span> if you use the <a href="function.html"><code translate="no" dir="ltr">tf.function</code></a> decorator, any non-TensorFlow Python code that you may have written in your function won't get executed. See <a href="function.html"><code translate="no" dir="ltr">tf.function</code></a> for more details. The recommendation would be to debug without <a href="function.html"><code translate="no" dir="ltr">tf.function</code></a> but switch to it to get performance benefits of running <code translate="no" dir="ltr">map_fn</code> in parallel.</span>
</blockquote>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">fn</code> </td> <td> The callable to be performed. It accepts one argument, which will have the same (possibly nested) structure as <code translate="no" dir="ltr">elems</code>. Its output must have the same structure as <code translate="no" dir="ltr">fn_output_signature</code> if one is provided; otherwise it must have the same structure as <code translate="no" dir="ltr">elems</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">elems</code> </td> <td> A tensor or (possibly nested) sequence of tensors, each of which will be unstacked along their first dimension. <code translate="no" dir="ltr">fn</code> will be applied to the nested sequence of the resulting slices. <code translate="no" dir="ltr">elems</code> may include ragged and sparse tensors. <code translate="no" dir="ltr">elems</code> must consist of at least one tensor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dtype</code> </td> <td> Deprecated: Equivalent to <code translate="no" dir="ltr">fn_output_signature</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">parallel_iterations</code> </td> <td> (optional) The number of iterations allowed to run in parallel. When graph building, the default value is 10. While executing eagerly, the default value is set to 1. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">back_prop</code> </td> <td> (optional) Deprecated: prefer using <a href="stop_gradient.html"><code translate="no" dir="ltr">tf.stop_gradient</code></a> instead. False disables support for back propagation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">swap_memory</code> </td> <td> (optional) True enables GPU-CPU memory swapping. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">infer_shape</code> </td> <td> (optional) False disables tests for consistent output shapes. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> (optional) Name prefix for the returned tensors. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">fn_output_signature</code> </td> <td> The output signature of <code translate="no" dir="ltr">fn</code>. Must be specified if <code translate="no" dir="ltr">fn</code>'s input and output signatures are different (i.e., if their structures, dtypes, or tensor types do not match). <code translate="no" dir="ltr">fn_output_signature</code> can be specified using any of the following: <ul> <li>A <a href="dtypes/dtype.html"><code translate="no" dir="ltr">tf.DType</code></a> or <a href="tensorspec.html"><code translate="no" dir="ltr">tf.TensorSpec</code></a> (to describe a <a href="tensor.html"><code translate="no" dir="ltr">tf.Tensor</code></a>)</li> <li>A <a href="raggedtensorspec.html"><code translate="no" dir="ltr">tf.RaggedTensorSpec</code></a> (to describe a <a href="raggedtensor.html"><code translate="no" dir="ltr">tf.RaggedTensor</code></a>)</li> <li>A <a href="sparsetensorspec.html"><code translate="no" dir="ltr">tf.SparseTensorSpec</code></a> (to describe a <a href="sparse/sparsetensor.html"><code translate="no" dir="ltr">tf.sparse.SparseTensor</code></a>)</li> <li>A (possibly nested) tuple, list, or dict containing the above types. </li>
</ul>
</td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A tensor or (possibly nested) sequence of tensors. Each tensor stacks the results of applying <code translate="no" dir="ltr">fn</code> to tensors unstacked from <code translate="no" dir="ltr">elems</code> along the first dimension, from first to last. The result may include ragged and sparse tensors. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if <code translate="no" dir="ltr">fn</code> is not callable or the structure of the output of <code translate="no" dir="ltr">fn</code> and <code translate="no" dir="ltr">fn_output_signature</code> do not match. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if the lengths of the output of <code translate="no" dir="ltr">fn</code> and <code translate="no" dir="ltr">fn_output_signature</code> do not match, or if the <code translate="no" dir="ltr">elems</code> does not contain any tensor. </td> </tr> </table> <h4 id="examples" data-text="Examples:">Examples:</h4> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
elems = np.array([1, 2, 3, 4, 5, 6])
tf.map_fn(lambda x: x * x, elems)
&lt;tf.Tensor: shape=(6,), dtype=int64, numpy=array([ 1,  4,  9, 16, 25, 36])&gt;
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
elems = (np.array([1, 2, 3]), np.array([-1, 1, -1]))
tf.map_fn(lambda x: x[0] * x[1], elems, fn_output_signature=tf.int64)
&lt;tf.Tensor: shape=(3,), dtype=int64, numpy=array([-1,  2, -3])&gt;
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
elems = np.array([1, 2, 3])
tf.map_fn(lambda x: (x, -x), elems,
         fn_output_signature=(tf.int64, tf.int64))
(&lt;tf.Tensor: shape=(3,), dtype=int64, numpy=array([1, 2, 3])&gt;,
 &lt;tf.Tensor: shape=(3,), dtype=int64, numpy=array([-1, -2, -3])&gt;)
</pre>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/map_fn" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/map_fn</a>
  </p>
</div>
