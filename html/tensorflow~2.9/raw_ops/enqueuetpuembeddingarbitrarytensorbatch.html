<h1 class="devsite-page-title">tf.raw_ops.EnqueueTPUEmbeddingArbitraryTensorBatch</h1> <devsite-bookmark></devsite-bookmark>       <p>Eases the porting of code that uses tf.nn.embedding_lookup_sparse().</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/raw_ops/EnqueueTPUEmbeddingArbitraryTensorBatch"><code translate="no" dir="ltr">tf.compat.v1.raw_ops.EnqueueTPUEmbeddingArbitraryTensorBatch</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.raw_ops.EnqueueTPUEmbeddingArbitraryTensorBatch(
    sample_indices_or_row_splits,
    embedding_indices,
    aggregation_weights,
    mode_override,
    device_ordinal=-1,
    combiners=[],
    name=None
)
</pre>  <p>embedding_indices[i] and aggregation_weights[i] correspond to the ith feature.</p> <p>The tensors at corresponding positions in the three input lists (sample_indices, embedding_indices and aggregation_weights) must have the same shape, i.e. rank 1 with dim_size() equal to the total number of lookups into the table described by the corresponding feature.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">sample_indices_or_row_splits</code> </td> <td> A list of at least 1 <code translate="no" dir="ltr">Tensor</code> objects with the same type in: <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">int64</code>. A list of rank 2 Tensors specifying the training example to which the corresponding embedding_indices and aggregation_weights values belong. If the size of its first dimension is 0, we assume each embedding_indices belongs to a different sample. Both int32 and int64 are allowed and will be converted to int32 internally. <p>Or a list of rank 1 Tensors specifying the row splits for splitting embedding_indices and aggregation_weights into rows. It corresponds to ids.row_splits in embedding_lookup(), when ids is a RaggedTensor. When enqueuing N-D ragged tensor, only the last dimension is allowed to be ragged. the row splits is 1-D dense tensor. When empty, we assume a dense tensor is passed to the op Both int32 and int64 are allowed and will be converted to int32 internally. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">embedding_indices</code> </td> <td> A list with the same length as <code translate="no" dir="ltr">sample_indices_or_row_splits</code> of <code translate="no" dir="ltr">Tensor</code> objects with the same type in: <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">int64</code>. A list of rank 1 Tensors, indices into the embedding tables. Both int32 and int64 are allowed and will be converted to int32 internally. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">aggregation_weights</code> </td> <td> A list with the same length as <code translate="no" dir="ltr">sample_indices_or_row_splits</code> of <code translate="no" dir="ltr">Tensor</code> objects with the same type in: <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>. A list of rank 1 Tensors containing per training example aggregation weights. Both float32 and float64 are allowed and will be converted to float32 internally. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">mode_override</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">string</code>. A string input that overrides the mode specified in the TPUEmbeddingConfiguration. Supported values are {'unspecified', 'inference', 'training', 'backward_pass_only'}. When set to 'unspecified', the mode set in TPUEmbeddingConfiguration is used, otherwise mode_override is used. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">device_ordinal</code> </td> <td> An optional <code translate="no" dir="ltr">int</code>. Defaults to <code translate="no" dir="ltr">-1</code>. The TPU device to use. Should be &gt;= 0 and less than the number of TPU cores in the task on which the node is placed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">combiners</code> </td> <td> An optional list of <code translate="no" dir="ltr">strings</code>. Defaults to <code translate="no" dir="ltr">[]</code>. A list of string scalars, one for each embedding table that specify how to normalize the embedding activations after weighted summation. Supported combiners are 'mean', 'sum', or 'sqrtn'. It is invalid to have the sum of the weights be 0 for 'mean' or the sum of the squared weights be 0 for 'sqrtn'. If combiners isn't passed, the default is to use 'sum' for all tables. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> The created Operation. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/raw_ops/EnqueueTPUEmbeddingArbitraryTensorBatch" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/raw_ops/EnqueueTPUEmbeddingArbitraryTensorBatch</a>
  </p>
</div>
