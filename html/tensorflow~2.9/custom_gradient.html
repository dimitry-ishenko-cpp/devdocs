<h1 class="devsite-page-title">tf.custom_gradient</h1> <devsite-bookmark></devsite-bookmark>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/ops/custom_gradient.py#L43-L297">  View source on GitHub </a> </td> </table> <p>Decorator to define a function with a custom gradient.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/custom_gradient"><code translate="no" dir="ltr">tf.compat.v1.custom_gradient</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.custom_gradient(
    f=None
)
</pre>  <p>This decorator allows fine grained control over the gradients of a sequence for operations. This may be useful for multiple reasons, including providing a more efficient or numerically stable gradient for a sequence of operations.</p> <p>For example, consider the following function that commonly occurs in the computation of cross entropy and log likelihoods:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">def log1pexp(x):
  return tf.math.log(1 + tf.exp(x))
</pre> <p>Due to numerical instability, the gradient of this function evaluated at x=100 is NaN. For example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">x = tf.constant(100.)
y = log1pexp(x)
dy_dx = tf.gradients(y, x) # Will be NaN when evaluated.
</pre> <p>The gradient expression can be analytically simplified to provide numerical stability:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">@tf.custom_gradient
def log1pexp(x):
  e = tf.exp(x)
  def grad(upstream):
    return upstream * (1 - 1 / (1 + e))
  return tf.math.log(1 + e), grad
</pre> <p>With this definition, the gradient <code translate="no" dir="ltr">dy_dx</code> at <code translate="no" dir="ltr">x = 100</code> will be correctly evaluated as 1.0.</p> <p>The variable <code translate="no" dir="ltr">upstream</code> is defined as the upstream gradient. i.e. the gradient from all the layers or functions originating from this layer. The above example has no upstream functions, therefore <code translate="no" dir="ltr">upstream = dy/dy = 1.0</code>.</p> <p>Assume that <code translate="no" dir="ltr">x_i</code> is <code translate="no" dir="ltr">log1pexp</code> in the forward pass <code translate="no" dir="ltr">x_1 = x_1(x_0)</code>, <code translate="no" dir="ltr">x_2 = x_2(x_1)</code>, ..., <code translate="no" dir="ltr">x_i = x_i(x_i-1)</code>, ..., <code translate="no" dir="ltr">x_n = x_n(x_n-1)</code>. By chain rule we know that <code translate="no" dir="ltr">dx_n/dx_0 = dx_n/dx_n-1 * dx_n-1/dx_n-2 * ... * dx_i/dx_i-1 * ... * dx_1/dx_0</code>.</p> <p>In this case the gradient of our current function defined as <code translate="no" dir="ltr">dx_i/dx_i-1 = (1 - 1 / (1 + e))</code>. The upstream gradient <code translate="no" dir="ltr">upstream</code> would be <code translate="no" dir="ltr">dx_n/dx_n-1 * dx_n-1/dx_n-2 * ... * dx_i+1/dx_i</code>. The upstream gradient multiplied by the current gradient is then passed downstream.</p> <p>In case the function takes multiple variables as input, the <code translate="no" dir="ltr">grad</code> function must also return the same number of variables. We take the function <code translate="no" dir="ltr">z = x * y</code> as an example.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
@tf.custom_gradient
def bar(x, y):
  def grad(upstream):
    dz_dx = y
    dz_dy = x
    return upstream * dz_dx, upstream * dz_dy
  z = x * y
  return z, grad
x = tf.constant(2.0, dtype=tf.float32)
y = tf.constant(3.0, dtype=tf.float32)
with tf.GradientTape(persistent=True) as tape:
  tape.watch(x)
  tape.watch(y)
  z = bar(x, y)
z
&lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0&gt;
tape.gradient(z, x)
&lt;tf.Tensor: shape=(), dtype=float32, numpy=3.0&gt;
tape.gradient(z, y)
&lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt;
</pre> <p>Nesting custom gradients can lead to unintuitive results. The default behavior does not correspond to n-th order derivatives. For example</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">@tf.custom_gradient
def op(x):
  y = op1(x)
  @tf.custom_gradient
  def grad_fn(dy):
    gdy = op2(x, y, dy)
    def grad_grad_fn(ddy):  # Not the 2nd order gradient of op w.r.t. x.
      return op3(x, y, dy, ddy)
    return gdy, grad_grad_fn
  return y, grad_fn
</pre> <p>The function <code translate="no" dir="ltr">grad_grad_fn</code> will be calculating the first order gradient of <code translate="no" dir="ltr">grad_fn</code> with respect to <code translate="no" dir="ltr">dy</code>, which is used to generate forward-mode gradient graphs from backward-mode gradient graphs, but is not the same as the second order gradient of <code translate="no" dir="ltr">op</code> with respect to <code translate="no" dir="ltr">x</code>.</p> <p>Instead, wrap nested <code translate="no" dir="ltr">@tf.custom_gradients</code> in another function:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">@tf.custom_gradient
def op_with_fused_backprop(x):
  y, x_grad = fused_op(x)
  def first_order_gradient(dy):
    @tf.custom_gradient
    def first_order_custom(unused_x):
      def second_order_and_transpose(ddy):
        return second_order_for_x(...), gradient_wrt_dy(...)
      return x_grad, second_order_and_transpose
    return dy * first_order_custom(x)
  return y, first_order_gradient
</pre> <p>Additional arguments to the inner <a href="custom_gradient.html"><code translate="no" dir="ltr">@tf.custom_gradient</code></a>-decorated function control the expected return values of the innermost function.</p> <p>The examples above illustrate how to specify custom gradients for functions which do not read from variables. The following example uses variables, which require special handling because they are effectively inputs of the forward function.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
weights = tf.Variable(tf.ones([2]))  # Trainable variable weights
@tf.custom_gradient
def linear_poly(x):
  # Creating polynomial
  poly = weights[1] * x + weights[0]

  def grad_fn(dpoly, variables):
    # dy/dx = weights[1] and we need to left multiply dpoly
    grad_xs = dpoly * weights[1]  # Scalar gradient

    grad_vars = []  # To store gradients of passed variables
    assert variables is not None
    assert len(variables) == 1
    assert variables[0] is weights
    # Manually computing dy/dweights
    dy_dw = dpoly * tf.stack([x ** 1, x ** 0])
    grad_vars.append(
        tf.reduce_sum(tf.reshape(dy_dw, [2, -1]), axis=1)
    )
    return grad_xs, grad_vars
  return poly, grad_fn
x = tf.constant([1., 2., 3.])
with tf.GradientTape(persistent=True) as tape:
  tape.watch(x)
  poly = linear_poly(x)
poly # poly = x + 1
&lt;tf.Tensor: shape=(3,),
  dtype=float32,
  numpy=array([2., 3., 4.], dtype=float32)&gt;
tape.gradient(poly, x)  # conventional scalar gradient dy/dx
&lt;tf.Tensor: shape=(3,),
  dtype=float32,
  numpy=array([1., 1., 1.], dtype=float32)&gt;
tape.gradient(poly, weights)
&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([6., 3.], dtype=float32)&gt;
</pre> <p>Above example illustrates usage of trainable variable <code translate="no" dir="ltr">weights</code>. In the example, the inner <code translate="no" dir="ltr">grad_fn</code> accepts an extra <code translate="no" dir="ltr">variables</code> input parameter and also returns an extra <code translate="no" dir="ltr">grad_vars</code> output. That extra argument is passed if the forward function reads any variables. You need to compute the gradient w.r.t. each of those <code translate="no" dir="ltr">variables</code> and output it as a list of <code translate="no" dir="ltr">grad_vars</code>. Note here that default value of <code translate="no" dir="ltr">variables</code> is set to <code translate="no" dir="ltr">None</code> when no variables are used in the forward function.</p> <p>It should be noted <a href="gradienttape.html"><code translate="no" dir="ltr">tf.GradientTape</code></a> is still watching the forward pass of a <a href="custom_gradient.html"><code translate="no" dir="ltr">tf.custom_gradient</code></a>, and will use the ops it watches. As a consequence, calling <a href="function.html"><code translate="no" dir="ltr">tf.function</code></a> while the tape is still watching leads to a gradient graph being built. If an op is used in <a href="function.html"><code translate="no" dir="ltr">tf.function</code></a> without registered gradient, a <code translate="no" dir="ltr">LookupError</code> will be raised.</p> <p>Users can insert <a href="stop_gradient.html"><code translate="no" dir="ltr">tf.stop_gradient</code></a> to customize this behavior. This is demonstrated in the example below. <a href="random/shuffle.html"><code translate="no" dir="ltr">tf.random.shuffle</code></a> does not have a registered gradient. As a result <a href="stop_gradient.html"><code translate="no" dir="ltr">tf.stop_gradient</code></a> is used to avoid the <code translate="no" dir="ltr">LookupError</code>.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">x = tf.constant([0.3, 0.5], dtype=tf.float32)

@tf.custom_gradient
def test_func_with_stop_grad(x):
  @tf.function
  def _inner_func():
    # Avoid exception during the forward pass
    return tf.stop_gradient(tf.random.shuffle(x))
    # return tf.random.shuffle(x)  # This will raise

  res = _inner_func()
  def grad(upstream):
    return upstream  # Arbitrarily defined custom gradient
  return res, grad

with tf.GradientTape() as g:
  g.watch(x)
  res = test_func_with_stop_grad(x)

g.gradient(res, x)
</pre> <p>See also <a href="registergradient.html"><code translate="no" dir="ltr">tf.RegisterGradient</code></a> which registers a gradient function for a primitive TensorFlow operation. <a href="custom_gradient.html"><code translate="no" dir="ltr">tf.custom_gradient</code></a> on the other hand allows for fine grained control over the gradient computation of a sequence of operations.</p> <p>Note that if the decorated function uses <code translate="no" dir="ltr">Variable</code>s, the enclosing variable scope must be using <code translate="no" dir="ltr">ResourceVariable</code>s.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">f</code> </td> <td> function <code translate="no" dir="ltr">f(*x)</code> that returns a tuple <code translate="no" dir="ltr">(y, grad_fn)</code> where: <ul> <li>
<code translate="no" dir="ltr">x</code> is a sequence of (nested structures of) <code translate="no" dir="ltr">Tensor</code> inputs to the function.</li> <li>
<code translate="no" dir="ltr">y</code> is a (nested structure of) <code translate="no" dir="ltr">Tensor</code> outputs of applying TensorFlow operations in <code translate="no" dir="ltr">f</code> to <code translate="no" dir="ltr">x</code>.</li> <li>
<p><code translate="no" dir="ltr">grad_fn</code> is a function with the signature <code translate="no" dir="ltr">g(*grad_ys)</code> which returns a list of <code translate="no" dir="ltr">Tensor</code>s the same size as (flattened) <code translate="no" dir="ltr">x</code> - the derivatives of <code translate="no" dir="ltr">Tensor</code>s in <code translate="no" dir="ltr">y</code> with respect to the <code translate="no" dir="ltr">Tensor</code>s in <code translate="no" dir="ltr">x</code>. <code translate="no" dir="ltr">grad_ys</code> is a sequence of <code translate="no" dir="ltr">Tensor</code>s the same size as (flattened) <code translate="no" dir="ltr">y</code> holding the initial value gradients for each <code translate="no" dir="ltr">Tensor</code> in <code translate="no" dir="ltr">y</code>.</p> <p>In a pure mathematical sense, a vector-argument vector-valued function <code translate="no" dir="ltr">f</code>'s derivatives should be its Jacobian matrix <code translate="no" dir="ltr">J</code>. Here we are expressing the Jacobian <code translate="no" dir="ltr">J</code> as a function <code translate="no" dir="ltr">grad_fn</code> which defines how <code translate="no" dir="ltr">J</code> will transform a vector <code translate="no" dir="ltr">grad_ys</code> when left-multiplied with it (<code translate="no" dir="ltr">grad_ys * J</code>, the vector-Jacobian product, or VJP). This functional representation of a matrix is convenient to use for chain-rule calculation (in e.g. the back-propagation algorithm).</p> <p>If <code translate="no" dir="ltr">f</code> uses <code translate="no" dir="ltr">Variable</code>s (that are not part of the inputs), i.e. through <code translate="no" dir="ltr">get_variable</code>, then <code translate="no" dir="ltr">grad_fn</code> should have signature <code translate="no" dir="ltr">g(*grad_ys, variables=None)</code>, where <code translate="no" dir="ltr">variables</code> is a list of the <code translate="no" dir="ltr">Variable</code>s, and return a 2-tuple <code translate="no" dir="ltr">(grad_xs, grad_vars)</code>, where <code translate="no" dir="ltr">grad_xs</code> is the same as above, and <code translate="no" dir="ltr">grad_vars</code> is a <code translate="no" dir="ltr">list&lt;Tensor&gt;</code> with the derivatives of <code translate="no" dir="ltr">Tensor</code>s in <code translate="no" dir="ltr">y</code> with respect to the variables (that is, grad_vars has one Tensor per variable in variables). </p>
</li>
</ul>
</td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A function <code translate="no" dir="ltr">h(x)</code> which returns the same value as <code translate="no" dir="ltr">f(x)[0]</code> and whose gradient (as calculated by <a href="gradients.html"><code translate="no" dir="ltr">tf.gradients</code></a>) is determined by <code translate="no" dir="ltr">f(x)[1]</code>. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/custom_gradient" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/custom_gradient</a>
  </p>
</div>
