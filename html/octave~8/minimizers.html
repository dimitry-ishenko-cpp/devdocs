<div class="section" id="Minimizers">   <h1 class="section">20.2 Minimizers</h1>   <p>Often it is useful to find the minimum value of a function rather than just the zeroes where it crosses the x-axis. <code>fminbnd</code> is designed for the simpler, but very common, case of a univariate function where the interval to search is bounded. For unbounded minimization of a function with potentially many variables use <code>fminunc</code> or <code>fminsearch</code>. The two functions use different internal algorithms and some knowledge of the objective function is required. For functions which can be differentiated, <code>fminunc</code> is appropriate. For functions with discontinuities, or for which a gradient search would fail, use <code>fminsearch</code>. See <a href="optimization.html">Optimization</a>, for minimization with the presence of constraint functions. Note that searches can be made for maxima by simply inverting the objective function (<code>Fto_max = -Fto_min</code>). </p> <dl class="def"> <dt id="index-fminbnd">
<span class="category">: </span><span><em><var>x</var> =</em> <strong>fminbnd</strong> <em>(<var>fcn</var>, <var>a</var>, <var>b</var>)</em><a href="#index-fminbnd" class="copiable-anchor"> ¶</a></span>
</dt> <dt id="index-fminbnd-1">
<span class="category">: </span><span><em><var>x</var> =</em> <strong>fminbnd</strong> <em>(<var>fcn</var>, <var>a</var>, <var>b</var>, <var>options</var>)</em><a href="#index-fminbnd-1" class="copiable-anchor"> ¶</a></span>
</dt> <dt id="index-fminbnd-2">
<span class="category">: </span><span><em>[<var>x</var>, <var>fval</var>, <var>info</var>, <var>output</var>] =</em> <strong>fminbnd</strong> <em>(…)</em><a href="#index-fminbnd-2" class="copiable-anchor"> ¶</a></span>
</dt> <dd>
<p>Find a minimum point of a univariate function. </p> <p><var>fcn</var> is a function handle, inline function, or string containing the name of the function to evaluate. </p> <p>The starting interval is specified by <var>a</var> (left boundary) and <var>b</var> (right boundary). The endpoints must be finite. </p> <p><var>options</var> is a structure specifying additional parameters which control the algorithm. Currently, <code>fminbnd</code> recognizes these options: <code>"Display"</code>, <code>"FunValCheck"</code>, <code>"MaxFunEvals"</code>, <code>"MaxIter"</code>, <code>"OutputFcn"</code>, <code>"TolX"</code>. </p> <p><code>"MaxFunEvals"</code> proscribes the maximum number of function evaluations before optimization is halted. The default value is 500. The value must be a positive integer. </p> <p><code>"MaxIter"</code> proscribes the maximum number of algorithm iterations before optimization is halted. The default value is 500. The value must be a positive integer. </p> <p><code>"TolX"</code> specifies the termination tolerance for the solution <var>x</var>. The default is <code>1e-4</code>. </p> <p>For a description of the other options, see <a href="linear-least-squares.html#XREFoptimset"><code>optimset</code></a>. To initialize an options structure with default values for <code>fminbnd</code> use <code>options = optimset ("fminbnd")</code>. </p> <p>On exit, the function returns <var>x</var>, the approximate minimum point, and <var>fval</var>, the function evaluated <var>x</var>. </p> <p>The third output <var>info</var> reports whether the algorithm succeeded and may take one of the following values: </p> <ul> <li> 1 The algorithm converged to a solution. </li>
<li> 0 Iteration limit (either <code>MaxIter</code> or <code>MaxFunEvals</code>) exceeded. </li>
<li> -1 The algorithm was terminated by a user <code>OutputFcn</code>. </li>
</ul> <p>Programming Notes: The search for a minimum is restricted to be in the finite interval bound by <var>a</var> and <var>b</var>. If you have only one initial point to begin searching from then you will need to use an unconstrained minimization algorithm such as <code>fminunc</code> or <code>fminsearch</code>. <code>fminbnd</code> internally uses a Golden Section search strategy. </p> <p><strong>See also:</strong> <a href="solvers.html#XREFfzero">fzero</a>, <a href="#XREFfminunc">fminunc</a>, <a href="#XREFfminsearch">fminsearch</a>, <a href="linear-least-squares.html#XREFoptimset">optimset</a>. </p>
</dd>
</dl> <dl class="def"> <dt id="index-fminunc">
<span class="category">: </span><span><em><var>x</var> =</em> <strong>fminunc</strong> <em>(<var>fcn</var>, <var>x0</var>)</em><a href="#index-fminunc" class="copiable-anchor"> ¶</a></span>
</dt> <dt id="index-fminunc-1">
<span class="category">: </span><span><em><var>x</var> =</em> <strong>fminunc</strong> <em>(<var>fcn</var>, <var>x0</var>, <var>options</var>)</em><a href="#index-fminunc-1" class="copiable-anchor"> ¶</a></span>
</dt> <dt id="index-fminunc-2">
<span class="category">: </span><span><em>[<var>x</var>, <var>fval</var>] =</em> <strong>fminunc</strong> <em>(<var>fcn</var>, …)</em><a href="#index-fminunc-2" class="copiable-anchor"> ¶</a></span>
</dt> <dt id="index-fminunc-3">
<span class="category">: </span><span><em>[<var>x</var>, <var>fval</var>, <var>info</var>] =</em> <strong>fminunc</strong> <em>(<var>fcn</var>, …)</em><a href="#index-fminunc-3" class="copiable-anchor"> ¶</a></span>
</dt> <dt id="index-fminunc-4">
<span class="category">: </span><span><em>[<var>x</var>, <var>fval</var>, <var>info</var>, <var>output</var>] =</em> <strong>fminunc</strong> <em>(<var>fcn</var>, …)</em><a href="#index-fminunc-4" class="copiable-anchor"> ¶</a></span>
</dt> <dt id="index-fminunc-5">
<span class="category">: </span><span><em>[<var>x</var>, <var>fval</var>, <var>info</var>, <var>output</var>, <var>grad</var>] =</em> <strong>fminunc</strong> <em>(<var>fcn</var>, …)</em><a href="#index-fminunc-5" class="copiable-anchor"> ¶</a></span>
</dt> <dt id="index-fminunc-6">
<span class="category">: </span><span><em>[<var>x</var>, <var>fval</var>, <var>info</var>, <var>output</var>, <var>grad</var>, <var>hess</var>] =</em> <strong>fminunc</strong> <em>(<var>fcn</var>, …)</em><a href="#index-fminunc-6" class="copiable-anchor"> ¶</a></span>
</dt> <dd>
<p>Solve an unconstrained optimization problem defined by the function <var>fcn</var>. </p> <p><code>fminunc</code> attempts to determine a vector <var>x</var> such that <code><var>fcn</var> (<var>x</var>)</code> is a local minimum. </p> <p><var>fcn</var> is a function handle, inline function, or string containing the name of the function to evaluate. <var>fcn</var> should accept a vector (array) defining the unknown variables, and return the objective function value, optionally with gradient. </p> <p><var>x0</var> determines a starting guess. The shape of <var>x0</var> is preserved in all calls to <var>fcn</var>, but otherwise is treated as a column vector. </p> <p><var>options</var> is a structure specifying additional parameters which control the algorithm. Currently, <code>fminunc</code> recognizes these options: <code>"AutoScaling"</code>, <code>"FinDiffType"</code>, <code>"FunValCheck"</code>, <code>"GradObj"</code>, <code>"MaxFunEvals"</code>, <code>"MaxIter"</code>, <code>"OutputFcn"</code>, <code>"TolFun"</code>, <code>"TolX"</code>, <code>"TypicalX"</code>. </p> <p>If <code>"AutoScaling"</code> is <code>"on"</code>, the variables will be automatically scaled according to the column norms of the (estimated) Jacobian. As a result, <code>"TolFun"</code> becomes scaling-independent. By default, this option is <code>"off"</code> because it may sometimes deliver unexpected (though mathematically correct) results. </p> <p>If <code>"GradObj"</code> is <code>"on"</code>, it specifies that <var>fcn</var>—when called with two output arguments—also returns the Jacobian matrix of partial first derivatives at the requested point. </p> <p><code>"MaxFunEvals"</code> proscribes the maximum number of function evaluations before optimization is halted. The default value is <code>100 * number_of_variables</code>, i.e., <code>100 * length (<var>x0</var>)</code>. The value must be a positive integer. </p> <p><code>"MaxIter"</code> proscribes the maximum number of algorithm iterations before optimization is halted. The default value is 400. The value must be a positive integer. </p> <p><code>"TolX"</code> specifies the termination tolerance for the unknown variables <var>x</var>, while <code>"TolFun"</code> is a tolerance for the objective function value <var>fval</var>. The default is <code>1e-6</code> for both options. </p> <p>For a description of the other options, see <a href="linear-least-squares.html#XREFoptimset"><code>optimset</code></a>. </p> <p>On return, <var>x</var> is the location of the minimum and <var>fval</var> contains the value of the objective function at <var>x</var>. </p> <p><var>info</var> may be one of the following values: </p> <dl compact> <dt><span>1</span></dt> <dd>
<p>Converged to a solution point. Relative gradient error is less than specified by <code>TolFun</code>. </p> </dd> <dt><span>2</span></dt> <dd>
<p>Last relative step size was less than <code>TolX</code>. </p> </dd> <dt><span>3</span></dt> <dd>
<p>Last relative change in function value was less than <code>TolFun</code>. </p> </dd> <dt><span>0</span></dt> <dd>
<p>Iteration limit exceeded—either maximum number of algorithm iterations <code>MaxIter</code> or maximum number of function evaluations <code>MaxFunEvals</code>. </p> </dd> <dt><span>-1</span></dt> <dd>
<p>Algorithm terminated by <code>OutputFcn</code>. </p> </dd> <dt><span>-3</span></dt> <dd><p>The trust region radius became excessively small. </p></dd> </dl> <p>Optionally, <code>fminunc</code> can return a structure with convergence statistics (<var>output</var>), the output gradient (<var>grad</var>) at the solution <var>x</var>, and approximate Hessian (<var>hess</var>) at the solution <var>x</var>. </p> <p>Application Notes: If the objective function is a single nonlinear equation of one variable then using <code>fminbnd</code> is usually a better choice. </p> <p>The algorithm used by <code>fminunc</code> is a gradient search which depends on the objective function being differentiable. If the function has discontinuities it may be better to use a derivative-free algorithm such as <code>fminsearch</code>. </p> <p><strong>See also:</strong> <a href="#XREFfminbnd">fminbnd</a>, <a href="#XREFfminsearch">fminsearch</a>, <a href="linear-least-squares.html#XREFoptimset">optimset</a>. </p>
</dd>
</dl> <dl class="def"> <dt id="index-fminsearch">
<span class="category">: </span><span><em><var>x</var> =</em> <strong>fminsearch</strong> <em>(<var>fcn</var>, <var>x0</var>)</em><a href="#index-fminsearch" class="copiable-anchor"> ¶</a></span>
</dt> <dt id="index-fminsearch-1">
<span class="category">: </span><span><em><var>x</var> =</em> <strong>fminsearch</strong> <em>(<var>fcn</var>, <var>x0</var>, <var>options</var>)</em><a href="#index-fminsearch-1" class="copiable-anchor"> ¶</a></span>
</dt> <dt id="index-fminsearch-2">
<span class="category">: </span><span><em><var>x</var> =</em> <strong>fminsearch</strong> <em>(<var>problem</var>)</em><a href="#index-fminsearch-2" class="copiable-anchor"> ¶</a></span>
</dt> <dt id="index-fminsearch-3">
<span class="category">: </span><span><em>[<var>x</var>, <var>fval</var>, <var>exitflag</var>, <var>output</var>] =</em> <strong>fminsearch</strong> <em>(…)</em><a href="#index-fminsearch-3" class="copiable-anchor"> ¶</a></span>
</dt> <dd> <p>Find a value of <var>x</var> which minimizes the multi-variable function <var>fcn</var>. </p> <p><var>fcn</var> is a function handle, inline function, or string containing the name of the function to evaluate. </p> <p>The search begins at the point <var>x0</var> and iterates using the Nelder &amp; Mead Simplex algorithm (a derivative-free method). This algorithm is better-suited to functions which have discontinuities or for which a gradient-based search such as <code>fminunc</code> fails. </p> <p>Options for the search are provided in the parameter <var>options</var> using the function <code>optimset</code>. Currently, <code>fminsearch</code> accepts the options: <code>"Display"</code>, <code>"FunValCheck"</code>,<code>"MaxFunEvals"</code>, <code>"MaxIter"</code>, <code>"OutputFcn"</code>, <code>"TolFun"</code>, <code>"TolX"</code>. </p> <p><code>"MaxFunEvals"</code> proscribes the maximum number of function evaluations before optimization is halted. The default value is <code>200 * number_of_variables</code>, i.e., <code>200 * length (<var>x0</var>)</code>. The value must be a positive integer. </p> <p><code>"MaxIter"</code> proscribes the maximum number of algorithm iterations before optimization is halted. The default value is <code>200 * number_of_variables</code>, i.e., <code>200 * length (<var>x0</var>)</code>. The value must be a positive integer. </p> <p>For a description of the other options, see <a href="linear-least-squares.html#XREFoptimset"><code>optimset</code></a>. To initialize an options structure with default values for <code>fminsearch</code> use <code>options = optimset ("fminsearch")</code>. </p> <p><code>fminsearch</code> may also be called with a single structure argument with the following fields: </p> <dl compact> <dt><span><code>objective</code></span></dt> <dd>
<p>The objective function. </p> </dd> <dt><span><code>x0</code></span></dt> <dd>
<p>The initial point. </p> </dd> <dt><span><code>solver</code></span></dt> <dd>
<p>Must be set to <code>"fminsearch"</code>. </p> </dd> <dt><span><code>options</code></span></dt> <dd><p>A structure returned from <code>optimset</code> or an empty matrix to indicate that defaults should be used. </p></dd> </dl> <p>The field <code>options</code> is optional. All others are required. </p> <p>On exit, the function returns <var>x</var>, the minimum point, and <var>fval</var>, the function value at the minimum. </p> <p>The third output <var>exitflag</var> reports whether the algorithm succeeded and may take one of the following values: </p> <dl compact> <dt><span>1</span></dt> <dd>
<p>if the algorithm converged (size of the simplex is smaller than <code>TolX</code> <strong>AND</strong> the step in function value between iterations is smaller than <code>TolFun</code>). </p> </dd> <dt><span>0</span></dt> <dd>
<p>if the maximum number of iterations or the maximum number of function evaluations are exceeded. </p> </dd> <dt><span>-1</span></dt> <dd><p>if the iteration is stopped by the <code>"OutputFcn"</code>. </p></dd> </dl> <p>The fourth output is a structure <var>output</var> containing runtime about the algorithm. Fields in the structure are <code>funcCount</code> containing the number of function calls to <var>fcn</var>, <code>iterations</code> containing the number of iteration steps, <code>algorithm</code> with the name of the search algorithm (always: <code>"Nelder-Mead simplex direct search"</code>), and <code>message</code> with the exit message. </p> <p>Example: </p> <pre class="example" data-language="matlab">fminsearch (@(x) (x(1)-5).^2+(x(2)-8).^4, [0;0])</pre> <p>Note: If you need to find the minimum of a single variable function it is probably better to use <code>fminbnd</code>. </p> <p><strong>See also:</strong> <a href="#XREFfminbnd">fminbnd</a>, <a href="#XREFfminunc">fminunc</a>, <a href="linear-least-squares.html#XREFoptimset">optimset</a>. </p>
</dd>
</dl> <p>The function <code>humps</code> is a useful function for testing zero and extrema finding functions. </p> <dl class="def"> <dt id="index-humps">
<span class="category">: </span><span><em><var>y</var> =</em> <strong>humps</strong> <em>(<var>x</var>)</em><a href="#index-humps" class="copiable-anchor"> ¶</a></span>
</dt> <dt id="index-humps-1">
<span class="category">: </span><span><em>[<var>x</var>, <var>y</var>] =</em> <strong>humps</strong> <em>(<var>x</var>)</em><a href="#index-humps-1" class="copiable-anchor"> ¶</a></span>
</dt> <dd>
<p>Evaluate a function with multiple minima, maxima, and zero crossings. </p> <p>The output <var>y</var> is the evaluation of the rational function: </p> <pre class="example" data-language="matlab">1200*x^4 - 2880*x^3 + 2036*x^2 - 348*x - 88
 y = - ---------------------------------------------
         200*x^4 - 480*x^3 + 406*x^2 - 138*x + 17</pre> <p><var>x</var> may be a scalar, vector or array. If <var>x</var> is omitted, the default range [0:0.05:1] is used. </p> <p>When called with two output arguments, [<var>x</var>, <var>y</var>], <var>x</var> will contain the input values, and <var>y</var> will contain the output from <code>humps</code>. </p> <p>Programming Notes: <code>humps</code> has two local maxima located near <var>x</var> = 0.300 and 0.893, a local minimum near <var>x</var> = 0.637, and zeros near <var>x</var> = -0.132 and 1.300. <code>humps</code> is a useful function for testing algorithms which find zeros or local minima and maxima. </p> <p>Try <code>demo humps</code> to see a plot of the <code>humps</code> function. </p> <p><strong>See also:</strong> <a href="solvers.html#XREFfzero">fzero</a>, <a href="#XREFfminbnd">fminbnd</a>, <a href="#XREFfminunc">fminunc</a>, <a href="#XREFfminsearch">fminsearch</a>. </p>
</dd>
</dl> </div><div class="_attribution">
  <p class="_attribution-p">
    &copy; 1996–2023 The Octave Project Developers<br>Permission is granted to make and distribute verbatim copies of this manual provided the copyright notice and this permission notice are preserved on all copies.<br/>Permission is granted to copy and distribute modified versions of this manual under the conditions for verbatim copying, provided that the entire resulting derived work is distributed under the terms of a permission notice identical to this one.</br>Permission is granted to copy and distribute translations of this manual into another language, under the above conditions for modified versions.<br>
    <a href="https://docs.octave.org/v8.1.0/Minimizers.html" class="_attribution-link">https://docs.octave.org/v8.1.0/Minimizers.html</a>
  </p>
</div>
