<section id="linear-and-quadratic-discriminant-analysis"> <h1 id="lda-qda">1.2. Linear and Quadratic Discriminant Analysis</h1> <p>Linear Discriminant Analysis (<a class="reference internal" href="generated/sklearn.discriminant_analysis.lineardiscriminantanalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code>LinearDiscriminantAnalysis</code></a>) and Quadratic Discriminant Analysis (<a class="reference internal" href="generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis" title="sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis"><code>QuadraticDiscriminantAnalysis</code></a>) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively.</p> <p>These classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no hyperparameters to tune.</p> <p class="centered"> <strong><a class="reference external" href="../auto_examples/classification/plot_lda_qda.html"><img alt="ldaqda" src="https://scikit-learn.org/1.6/_images/sphx_glr_plot_lda_qda_001.png" style="width: 640.0px; height: 960.0px;"></a></strong></p>
<p>The plot shows decision boundaries for Linear Discriminant Analysis and Quadratic Discriminant Analysis. The bottom row demonstrates that Linear Discriminant Analysis can only learn linear boundaries, while Quadratic Discriminant Analysis can learn quadratic boundaries and is therefore more flexible.</p> <h4 class="rubric">Examples</h4> <ul class="simple"> <li>
<a class="reference internal" href="../auto_examples/classification/plot_lda_qda.html#sphx-glr-auto-examples-classification-plot-lda-qda-py"><span class="std std-ref">Linear and Quadratic Discriminant Analysis with covariance ellipsoid</span></a>: Comparison of LDA and QDA on synthetic data.</li> </ul> <section id="dimensionality-reduction-using-linear-discriminant-analysis"> <h2>
<span class="section-number">1.2.1. </span>Dimensionality reduction using Linear Discriminant Analysis</h2> <p><a class="reference internal" href="generated/sklearn.discriminant_analysis.lineardiscriminantanalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code>LinearDiscriminantAnalysis</code></a> can be used to perform supervised dimensionality reduction, by projecting the input data to a linear subspace consisting of the directions which maximize the separation between classes (in a precise sense discussed in the mathematics section below). The dimension of the output is necessarily less than the number of classes, so this is in general a rather strong dimensionality reduction, and only makes sense in a multiclass setting.</p> <p>This is implemented in the <code>transform</code> method. The desired dimensionality can be set using the <code>n_components</code> parameter. This parameter has no influence on the <code>fit</code> and <code>predict</code> methods.</p> <h4 class="rubric">Examples</h4> <ul class="simple"> <li>
<a class="reference internal" href="../auto_examples/decomposition/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py"><span class="std std-ref">Comparison of LDA and PCA 2D projection of Iris dataset</span></a>: Comparison of LDA and PCA for dimensionality reduction of the Iris dataset</li> </ul> </section> <section id="mathematical-formulation-of-the-lda-and-qda-classifiers"> <h2 id="lda-qda-math">
<span class="section-number">1.2.2. </span>Mathematical formulation of the LDA and QDA classifiers</h2> <p>Both LDA and QDA can be derived from simple probabilistic models which model the class conditional distribution of the data <span class="math notranslate nohighlight">\(P(X|y=k)\)</span> for each class <span class="math notranslate nohighlight">\(k\)</span>. Predictions can then be obtained by using Bayes’ rule, for each training sample <span class="math notranslate nohighlight">\(x \in \mathcal{R}^d\)</span>:</p> <div class="math notranslate nohighlight"> \[P(y=k | x) = \frac{P(x | y=k) P(y=k)}{P(x)} = \frac{P(x | y=k) P(y = k)}{ \sum_{l} P(x | y=l) \cdot P(y=l)}\]</div> <p>and we select the class <span class="math notranslate nohighlight">\(k\)</span> which maximizes this posterior probability.</p> <p>More specifically, for linear and quadratic discriminant analysis, <span class="math notranslate nohighlight">\(P(x|y)\)</span> is modeled as a multivariate Gaussian distribution with density:</p> <div class="math notranslate nohighlight"> \[P(x | y=k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} (x-\mu_k)^t \Sigma_k^{-1} (x-\mu_k)\right)\]</div> <p>where <span class="math notranslate nohighlight">\(d\)</span> is the number of features.</p> <section id="qda"> <h3>
<span class="section-number">1.2.2.1. </span>QDA</h3> <p>According to the model above, the log of the posterior is:</p> <div class="math notranslate nohighlight"> \[\begin{split}\log P(y=k | x) &amp;= \log P(x | y=k) + \log P(y = k) + Cst \\ &amp;= -\frac{1}{2} \log |\Sigma_k| -\frac{1}{2} (x-\mu_k)^t \Sigma_k^{-1} (x-\mu_k) + \log P(y = k) + Cst,\end{split}\]</div> <p>where the constant term <span class="math notranslate nohighlight">\(Cst\)</span> corresponds to the denominator <span class="math notranslate nohighlight">\(P(x)\)</span>, in addition to other constant terms from the Gaussian. The predicted class is the one that maximises this log-posterior.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p><strong>Relation with Gaussian Naive Bayes</strong></p> <p>If in the QDA model one assumes that the covariance matrices are diagonal, then the inputs are assumed to be conditionally independent in each class, and the resulting classifier is equivalent to the Gaussian Naive Bayes classifier <a class="reference internal" href="generated/sklearn.naive_bayes.gaussiannb.html#sklearn.naive_bayes.GaussianNB" title="sklearn.naive_bayes.GaussianNB"><code>naive_bayes.GaussianNB</code></a>.</p> </div> </section> <section id="lda"> <h3>
<span class="section-number">1.2.2.2. </span>LDA</h3> <p>LDA is a special case of QDA, where the Gaussians for each class are assumed to share the same covariance matrix: <span class="math notranslate nohighlight">\(\Sigma_k = \Sigma\)</span> for all <span class="math notranslate nohighlight">\(k\)</span>. This reduces the log posterior to:</p> <div class="math notranslate nohighlight"> \[\log P(y=k | x) = -\frac{1}{2} (x-\mu_k)^t \Sigma^{-1} (x-\mu_k) + \log P(y = k) + Cst.\]</div> <p>The term <span class="math notranslate nohighlight">\((x-\mu_k)^t \Sigma^{-1} (x-\mu_k)\)</span> corresponds to the <a class="reference external" href="https://en.wikipedia.org/wiki/Mahalanobis_distance">Mahalanobis Distance</a> between the sample <span class="math notranslate nohighlight">\(x\)</span> and the mean <span class="math notranslate nohighlight">\(\mu_k\)</span>. The Mahalanobis distance tells how close <span class="math notranslate nohighlight">\(x\)</span> is from <span class="math notranslate nohighlight">\(\mu_k\)</span>, while also accounting for the variance of each feature. We can thus interpret LDA as assigning <span class="math notranslate nohighlight">\(x\)</span> to the class whose mean is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities.</p> <p>The log-posterior of LDA can also be written <a class="footnote-reference brackets" href="#id7" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> as:</p> <div class="math notranslate nohighlight"> \[\log P(y=k | x) = \omega_k^t x + \omega_{k0} + Cst.\]</div> <p>where <span class="math notranslate nohighlight">\(\omega_k = \Sigma^{-1} \mu_k\)</span> and <span class="math notranslate nohighlight">\(\omega_{k0} = -\frac{1}{2} \mu_k^t\Sigma^{-1}\mu_k + \log P (y = k)\)</span>. These quantities correspond to the <code>coef_</code> and <code>intercept_</code> attributes, respectively.</p> <p>From the above formula, it is clear that LDA has a linear decision surface. In the case of QDA, there are no assumptions on the covariance matrices <span class="math notranslate nohighlight">\(\Sigma_k\)</span> of the Gaussians, leading to quadratic decision surfaces. See <a class="footnote-reference brackets" href="#id5" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> for more details.</p> </section> </section> <section id="mathematical-formulation-of-lda-dimensionality-reduction"> <h2>
<span class="section-number">1.2.3. </span>Mathematical formulation of LDA dimensionality reduction</h2> <p>First note that the K means <span class="math notranslate nohighlight">\(\mu_k\)</span> are vectors in <span class="math notranslate nohighlight">\(\mathcal{R}^d\)</span>, and they lie in an affine subspace <span class="math notranslate nohighlight">\(H\)</span> of dimension at most <span class="math notranslate nohighlight">\(K - 1\)</span> (2 points lie on a line, 3 points lie on a plane, etc.).</p> <p>As mentioned above, we can interpret LDA as assigning <span class="math notranslate nohighlight">\(x\)</span> to the class whose mean <span class="math notranslate nohighlight">\(\mu_k\)</span> is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities. Alternatively, LDA is equivalent to first <em>sphering</em> the data so that the covariance matrix is the identity, and then assigning <span class="math notranslate nohighlight">\(x\)</span> to the closest mean in terms of Euclidean distance (still accounting for the class priors).</p> <p>Computing Euclidean distances in this d-dimensional space is equivalent to first projecting the data points into <span class="math notranslate nohighlight">\(H\)</span>, and computing the distances there (since the other dimensions will contribute equally to each class in terms of distance). In other words, if <span class="math notranslate nohighlight">\(x\)</span> is closest to <span class="math notranslate nohighlight">\(\mu_k\)</span> in the original space, it will also be the case in <span class="math notranslate nohighlight">\(H\)</span>. This shows that, implicit in the LDA classifier, there is a dimensionality reduction by linear projection onto a <span class="math notranslate nohighlight">\(K-1\)</span> dimensional space.</p> <p>We can reduce the dimension even more, to a chosen <span class="math notranslate nohighlight">\(L\)</span>, by projecting onto the linear subspace <span class="math notranslate nohighlight">\(H_L\)</span> which maximizes the variance of the <span class="math notranslate nohighlight">\(\mu^*_k\)</span> after projection (in effect, we are doing a form of PCA for the transformed class means <span class="math notranslate nohighlight">\(\mu^*_k\)</span>). This <span class="math notranslate nohighlight">\(L\)</span> corresponds to the <code>n_components</code> parameter used in the <a class="reference internal" href="generated/sklearn.discriminant_analysis.lineardiscriminantanalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform"><code>transform</code></a> method. See <a class="footnote-reference brackets" href="#id5" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> for more details.</p> </section> <section id="shrinkage-and-covariance-estimator"> <h2>
<span class="section-number">1.2.4. </span>Shrinkage and Covariance Estimator</h2> <p>Shrinkage is a form of regularization used to improve the estimation of covariance matrices in situations where the number of training samples is small compared to the number of features. In this scenario, the empirical sample covariance is a poor estimator, and shrinkage helps improving the generalization performance of the classifier. Shrinkage LDA can be used by setting the <code>shrinkage</code> parameter of the <a class="reference internal" href="generated/sklearn.discriminant_analysis.lineardiscriminantanalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code>LinearDiscriminantAnalysis</code></a> class to ‘auto’. This automatically determines the optimal shrinkage parameter in an analytic way following the lemma introduced by Ledoit and Wolf <a class="footnote-reference brackets" href="#id6" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>. Note that currently shrinkage only works when setting the <code>solver</code> parameter to ‘lsqr’ or ‘eigen’.</p> <p>The <code>shrinkage</code> parameter can also be manually set between 0 and 1. In particular, a value of 0 corresponds to no shrinkage (which means the empirical covariance matrix will be used) and a value of 1 corresponds to complete shrinkage (which means that the diagonal matrix of variances will be used as an estimate for the covariance matrix). Setting this parameter to a value between these two extrema will estimate a shrunk version of the covariance matrix.</p> <p>The shrunk Ledoit and Wolf estimator of covariance may not always be the best choice. For example if the distribution of the data is normally distributed, the Oracle Approximating Shrinkage estimator <a class="reference internal" href="generated/sklearn.covariance.oas.html#sklearn.covariance.OAS" title="sklearn.covariance.OAS"><code>sklearn.covariance.OAS</code></a> yields a smaller Mean Squared Error than the one given by Ledoit and Wolf’s formula used with shrinkage=”auto”. In LDA, the data are assumed to be gaussian conditionally to the class. If these assumptions hold, using LDA with the OAS estimator of covariance will yield a better classification accuracy than if Ledoit and Wolf or the empirical covariance estimator is used.</p> <p>The covariance estimator can be chosen using with the <code>covariance_estimator</code> parameter of the <a class="reference internal" href="generated/sklearn.discriminant_analysis.lineardiscriminantanalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code>discriminant_analysis.LinearDiscriminantAnalysis</code></a> class. A covariance estimator should have a <a class="reference internal" href="https://scikit-learn.org/1.6/glossary.html#term-fit"><span class="xref std std-term">fit</span></a> method and a <code>covariance_</code> attribute like all covariance estimators in the <a class="reference internal" href="https://scikit-learn.org/1.6/api/sklearn.covariance.html#module-sklearn.covariance" title="sklearn.covariance"><code>sklearn.covariance</code></a> module.</p> <p class="centered"> <strong><a class="reference external" href="../auto_examples/classification/plot_lda.html"><img alt="shrinkage" src="data:image/png;base64,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" style="width: 480.0px; height: 360.0px;"></a></strong></p>
<h4 class="rubric">Examples</h4> <ul class="simple"> <li>
<a class="reference internal" href="../auto_examples/classification/plot_lda.html#sphx-glr-auto-examples-classification-plot-lda-py"><span class="std std-ref">Normal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification</span></a>: Comparison of LDA classifiers with Empirical, Ledoit Wolf and OAS covariance estimator.</li> </ul> </section> <section id="estimation-algorithms"> <h2>
<span class="section-number">1.2.5. </span>Estimation algorithms</h2> <p>Using LDA and QDA requires computing the log-posterior which depends on the class priors <span class="math notranslate nohighlight">\(P(y=k)\)</span>, the class means <span class="math notranslate nohighlight">\(\mu_k\)</span>, and the covariance matrices.</p> <p>The ‘svd’ solver is the default solver used for <a class="reference internal" href="generated/sklearn.discriminant_analysis.lineardiscriminantanalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code>LinearDiscriminantAnalysis</code></a>, and it is the only available solver for <a class="reference internal" href="generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis" title="sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis"><code>QuadraticDiscriminantAnalysis</code></a>. It can perform both classification and transform (for LDA). As it does not rely on the calculation of the covariance matrix, the ‘svd’ solver may be preferable in situations where the number of features is large. The ‘svd’ solver cannot be used with shrinkage. For QDA, the use of the SVD solver relies on the fact that the covariance matrix <span class="math notranslate nohighlight">\(\Sigma_k\)</span> is, by definition, equal to <span class="math notranslate nohighlight">\(\frac{1}{n - 1} X_k^tX_k = \frac{1}{n - 1} V S^2 V^t\)</span> where <span class="math notranslate nohighlight">\(V\)</span> comes from the SVD of the (centered) matrix: <span class="math notranslate nohighlight">\(X_k = U S V^t\)</span>. It turns out that we can compute the log-posterior above without having to explicitly compute <span class="math notranslate nohighlight">\(\Sigma\)</span>: computing <span class="math notranslate nohighlight">\(S\)</span> and <span class="math notranslate nohighlight">\(V\)</span> via the SVD of <span class="math notranslate nohighlight">\(X\)</span> is enough. For LDA, two SVDs are computed: the SVD of the centered input matrix <span class="math notranslate nohighlight">\(X\)</span> and the SVD of the class-wise mean vectors.</p> <p>The ‘lsqr’ solver is an efficient algorithm that only works for classification. It needs to explicitly compute the covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span>, and supports shrinkage and custom covariance estimators. This solver computes the coefficients <span class="math notranslate nohighlight">\(\omega_k = \Sigma^{-1}\mu_k\)</span> by solving for <span class="math notranslate nohighlight">\(\Sigma \omega = \mu_k\)</span>, thus avoiding the explicit computation of the inverse <span class="math notranslate nohighlight">\(\Sigma^{-1}\)</span>.</p> <p>The ‘eigen’ solver is based on the optimization of the between class scatter to within class scatter ratio. It can be used for both classification and transform, and it supports shrinkage. However, the ‘eigen’ solver needs to compute the covariance matrix, so it might not be suitable for situations with a high number of features.</p> <h4 class="rubric">References</h4> <aside class="footnote-list brackets"> <aside class="footnote brackets" id="id5" role="doc-footnote"> <span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span> <span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id3">2</a>)</span> <p>“The Elements of Statistical Learning”, Hastie T., Tibshirani R., Friedman J., Section 4.3, p.106-119, 2008.</p> </aside> <aside class="footnote brackets" id="id6" role="doc-footnote"> <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">2</a><span class="fn-bracket">]</span></span> <p>Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix. The Journal of Portfolio Management 30(4), 110-119, 2004.</p> </aside> <aside class="footnote brackets" id="id7" role="doc-footnote"> <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">3</a><span class="fn-bracket">]</span></span> <p>R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification (Second Edition), section 2.6.2.</p> </aside> </aside> </section> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2025 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.6/modules/lda_qda.html" class="_attribution-link">https://scikit-learn.org/1.6/modules/lda_qda.html</a>
  </p>
</div>
