<section id="load-svmlight-file"> <h1>load_svmlight_file</h1> <dl class="py function"> <dt class="sig sig-object py" id="sklearn.datasets.load_svmlight_file"> <span class="sig-prename descclassname">sklearn.datasets.</span><span class="sig-name descname">load_svmlight_file</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">f</span></em>, <em class="sig-param"><span class="n">*</span></em>, <em class="sig-param"><span class="n">n_features=None</span></em>, <em class="sig-param"><span class="n">dtype=&lt;class 'numpy.float64'&gt;</span></em>, <em class="sig-param"><span class="n">multilabel=False</span></em>, <em class="sig-param"><span class="n">zero_based='auto'</span></em>, <em class="sig-param"><span class="n">query_id=False</span></em>, <em class="sig-param"><span class="n">offset=0</span></em>, <em class="sig-param"><span class="n">length=-1</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/99bf3d8e4/sklearn/datasets/_svmlight_format_io.py#L32"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Load datasets in the svmlight / libsvm format into sparse CSR matrix.</p> <p>This format is a text-based format, with one sample per line. It does not store zero valued features hence is suitable for sparse dataset.</p> <p>The first element of each line can be used to store a target variable to predict.</p> <p>This format is used as the default format for both svmlight and the libsvm command line programs.</p> <p>Parsing a text based source can be expensive. When repeatedly working on the same dataset, it is recommended to wrap this loader with joblib.Memory.cache to store a memmapped backup of the CSR results of the first call and benefit from the near instantaneous loading of memmapped structures for the subsequent calls.</p> <p>In case the file contains a pairwise preference constraint (known as “qid” in the svmlight format) these are ignored unless the query_id parameter is set to True. These pairwise preference constraints can be used to constraint the combination of samples when using pairwise loss functions (as is the case in some learning to rank problems) so that only pairs with the same query_id value are considered.</p> <p>This implementation is written in Cython and is reasonably fast. However, a faster API-compatible loader is also available at: <a class="github reference external" href="https://github.com/mblondel/svmlight-loader">mblondel/svmlight-loader</a></p> <dl class="field-list"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl> <dt>
<strong>f</strong><span class="classifier">str, path-like, file-like or int</span>
</dt>
<dd>
<p>(Path to) a file to load. If a path ends in “.gz” or “.bz2”, it will be uncompressed on the fly. If an integer is passed, it is assumed to be a file descriptor. A file-like or file descriptor will not be closed by this function. A file-like object must be opened in binary mode.</p> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 1.2: </span>Path-like objects are now accepted.</p> </div> </dd> <dt>
<strong>n_features</strong><span class="classifier">int, default=None</span>
</dt>
<dd>
<p>The number of features to use. If None, it will be inferred. This argument is useful to load several files that are subsets of a bigger sliced dataset: each subset might not have examples of every feature, hence the inferred shape might vary from one slice to another. n_features is only required if <code>offset</code> or <code>length</code> are passed a non-default value.</p> </dd> <dt>
<strong>dtype</strong><span class="classifier">numpy data type, default=np.float64</span>
</dt>
<dd>
<p>Data type of dataset to be loaded. This will be the data type of the output numpy arrays <code>X</code> and <code>y</code>.</p> </dd> <dt>
<strong>multilabel</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>Samples may have several labels each (see <a class="reference external" href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html">https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html</a>).</p> </dd> <dt>
<strong>zero_based</strong><span class="classifier">bool or “auto”, default=”auto”</span>
</dt>
<dd>
<p>Whether column indices in f are zero-based (True) or one-based (False). If column indices are one-based, they are transformed to zero-based to match Python/NumPy conventions. If set to “auto”, a heuristic check is applied to determine this from the file contents. Both kinds of files occur “in the wild”, but they are unfortunately not self-identifying. Using “auto” or True should always be safe when no <code>offset</code> or <code>length</code> is passed. If <code>offset</code> or <code>length</code> are passed, the “auto” mode falls back to <code>zero_based=True</code> to avoid having the heuristic check yield inconsistent results on different segments of the file.</p> </dd> <dt>
<strong>query_id</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>If True, will return the query_id array for each file.</p> </dd> <dt>
<strong>offset</strong><span class="classifier">int, default=0</span>
</dt>
<dd>
<p>Ignore the offset first bytes by seeking forward, then discarding the following bytes up until the next new line character.</p> </dd> <dt>
<strong>length</strong><span class="classifier">int, default=-1</span>
</dt>
<dd>
<p>If strictly positive, stop reading any new line of data once the position in the file has reached the (offset + length) bytes threshold.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>X</strong><span class="classifier">scipy.sparse matrix of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>The data matrix.</p> </dd> <dt>
<strong>y</strong><span class="classifier">ndarray of shape (n_samples,), or a list of tuples of length n_samples</span>
</dt>
<dd>
<p>The target. It is a list of tuples when <code>multilabel=True</code>, else a ndarray.</p> </dd> <dt>
<strong>query_id</strong><span class="classifier">array of shape (n_samples,)</span>
</dt>
<dd>
<p>The query_id for each sample. Only returned when query_id is set to True.</p> </dd> </dl> </dd> </dl> <div class="admonition seealso"> <p class="admonition-title">See also</p> <dl class="simple"> <dt><a class="reference internal" href="sklearn.datasets.load_svmlight_files.html#sklearn.datasets.load_svmlight_files" title="sklearn.datasets.load_svmlight_files"><code>load_svmlight_files</code></a></dt>
<dd>
<p>Similar function for loading multiple files in this format, enforcing the same number of features/columns on all of them.</p> </dd> </dl> </div> <h4 class="rubric">Examples</h4> <p>To use joblib.Memory to cache the svmlight file:</p> <pre data-language="python">from joblib import Memory
from sklearn.datasets import load_svmlight_file
mem = Memory("./mycache")

@mem.cache
def get_data():
    data = load_svmlight_file("mysvmlightfile")
    return data[0], data[1]

X, y = get_data()
</pre> </dd>
</dl> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2025 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.6/modules/generated/sklearn.datasets.load_svmlight_file.html" class="_attribution-link">https://scikit-learn.org/1.6/modules/generated/sklearn.datasets.load_svmlight_file.html</a>
  </p>
</div>
