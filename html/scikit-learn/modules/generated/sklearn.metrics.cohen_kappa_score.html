<section id="cohen-kappa-score"> <h1>cohen_kappa_score</h1> <dl class="py function"> <dt class="sig sig-object py" id="sklearn.metrics.cohen_kappa_score"> <span class="sig-prename descclassname">sklearn.metrics.</span><span class="sig-name descname">cohen_kappa_score</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">y1</span></em>, <em class="sig-param"><span class="n">y2</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">labels</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weights</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/99bf3d8e4/sklearn/metrics/_classification.py#L667"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Compute Cohen’s kappa: a statistic that measures inter-annotator agreement.</p> <p>This function computes Cohen’s kappa <a class="reference internal" href="#r219a3b9132e1-1" id="id1">[1]</a>, a score that expresses the level of agreement between two annotators on a classification problem. It is defined as</p> <div class="math notranslate nohighlight"> \[\kappa = (p_o - p_e) / (1 - p_e)\]</div> <p>where <span class="math notranslate nohighlight">\(p_o\)</span> is the empirical probability of agreement on the label assigned to any sample (the observed agreement ratio), and <span class="math notranslate nohighlight">\(p_e\)</span> is the expected agreement when both annotators assign labels randomly. <span class="math notranslate nohighlight">\(p_e\)</span> is estimated using a per-annotator empirical prior over the class labels <a class="reference internal" href="#r219a3b9132e1-2" id="id2">[2]</a>.</p> <p>Read more in the <a class="reference internal" href="../model_evaluation.html#cohen-kappa"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>y1</strong><span class="classifier">array-like of shape (n_samples,)</span>
</dt>
<dd>
<p>Labels assigned by the first annotator.</p> </dd> <dt>
<strong>y2</strong><span class="classifier">array-like of shape (n_samples,)</span>
</dt>
<dd>
<p>Labels assigned by the second annotator. The kappa statistic is symmetric, so swapping <code>y1</code> and <code>y2</code> doesn’t change the value.</p> </dd> <dt>
<strong>labels</strong><span class="classifier">array-like of shape (n_classes,), default=None</span>
</dt>
<dd>
<p>List of labels to index the matrix. This may be used to select a subset of labels. If <code>None</code>, all labels that appear at least once in <code>y1</code> or <code>y2</code> are used.</p> </dd> <dt>
<strong>weights</strong><span class="classifier">{‘linear’, ‘quadratic’}, default=None</span>
</dt>
<dd>
<p>Weighting type to calculate the score. <code>None</code> means not weighted; “linear” means linear weighting; “quadratic” means quadratic weighting.</p> </dd> <dt>
<strong>sample_weight</strong><span class="classifier">array-like of shape (n_samples,), default=None</span>
</dt>
<dd>
<p>Sample weights.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>kappa</strong><span class="classifier">float</span>
</dt>
<dd>
<p>The kappa statistic, which is a number between -1 and 1. The maximum value means complete agreement; zero or lower means chance agreement.</p> </dd> </dl> </dd> </dl> <h4 class="rubric">References</h4> <div role="list" class="citation-list"> <div class="citation" id="r219a3b9132e1-1" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span> <p><a class="reference external" href="https://doi.org/10.1177/001316446002000104">J. Cohen (1960). “A coefficient of agreement for nominal scales”. Educational and Psychological Measurement 20(1):37-46.</a></p> </div> <div class="citation" id="r219a3b9132e1-2" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span> <p><a class="reference external" href="https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2">R. Artstein and M. Poesio (2008). “Inter-coder agreement for computational linguistics”. Computational Linguistics 34(4):555-596</a>.</p> </div> <div class="citation" id="r219a3b9132e1-3" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span> <p><a class="reference external" href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Wikipedia entry for the Cohen’s kappa</a>.</p> </div> </div> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from sklearn.metrics import cohen_kappa_score
&gt;&gt;&gt; y1 = ["negative", "positive", "negative", "neutral", "positive"]
&gt;&gt;&gt; y2 = ["negative", "positive", "negative", "neutral", "negative"]
&gt;&gt;&gt; cohen_kappa_score(y1, y2)
np.float64(0.6875)
</pre> </dd>
</dl> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2025 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.6/modules/generated/sklearn.metrics.cohen_kappa_score.html" class="_attribution-link">https://scikit-learn.org/1.6/modules/generated/sklearn.metrics.cohen_kappa_score.html</a>
  </p>
</div>
