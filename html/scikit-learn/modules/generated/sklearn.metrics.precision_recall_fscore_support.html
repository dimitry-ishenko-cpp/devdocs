<section id="sklearn-metrics-precision-recall-fscore-support"> <h1>sklearn.metrics.precision_recall_fscore_support</h1> <dl class="py function"> <dt class="sig sig-object py" id="sklearn.metrics.precision_recall_fscore_support"> <span class="sig-prename descclassname">sklearn.metrics.</span><span class="sig-name descname">precision_recall_fscore_support</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">y_pred</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">beta</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">labels</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pos_label</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">average</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">warn_for</span><span class="o">=</span><span class="default_value">('precision', 'recall', 'f-score')</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">zero_division</span><span class="o">=</span><span class="default_value">'warn'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/metrics/_classification.py#L1396"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Compute precision, recall, F-measure and support for each class.</p> <p>The precision is the ratio <code>tp / (tp + fp)</code> where <code>tp</code> is the number of true positives and <code>fp</code> the number of false positives. The precision is intuitively the ability of the classifier not to label a negative sample as positive.</p> <p>The recall is the ratio <code>tp / (tp + fn)</code> where <code>tp</code> is the number of true positives and <code>fn</code> the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.</p> <p>The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.</p> <p>The F-beta score weights recall more than precision by a factor of <code>beta</code>. <code>beta == 1.0</code> means recall and precision are equally important.</p> <p>The support is the number of occurrences of each class in <code>y_true</code>.</p> <p>If <code>pos_label is None</code> and in binary classification, this function returns the average precision, recall and F-measure if <code>average</code> is one of <code>'micro'</code>, <code>'macro'</code>, <code>'weighted'</code> or <code>'samples'</code>.</p> <p>Read more in the <a class="reference internal" href="../model_evaluation.html#precision-recall-f-measure-metrics"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl> <dt>
<strong>y_true</strong><span class="classifier">1d array-like, or label indicator array / sparse matrix</span>
</dt>
<dd>
<p>Ground truth (correct) target values.</p> </dd> <dt>
<strong>y_pred</strong><span class="classifier">1d array-like, or label indicator array / sparse matrix</span>
</dt>
<dd>
<p>Estimated targets as returned by a classifier.</p> </dd> <dt>
<strong>beta</strong><span class="classifier">float, default=1.0</span>
</dt>
<dd>
<p>The strength of recall versus precision in the F-score.</p> </dd> <dt>
<strong>labels</strong><span class="classifier">array-like, default=None</span>
</dt>
<dd>
<p>The set of labels to include when <code>average != 'binary'</code>, and their order if <code>average is None</code>. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in <code>y_true</code> and <code>y_pred</code> are used in sorted order.</p> </dd> <dt>
<strong>pos_label</strong><span class="classifier">str or int, default=1</span>
</dt>
<dd>
<p>The class to report if <code>average='binary'</code> and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting <code>labels=[pos_label]</code> and <code>average != 'binary'</code> will report scores for that label only.</p> </dd> <dt>
<strong>average</strong><span class="classifier">{‘binary’, ‘micro’, ‘macro’, ‘samples’, ‘weighted’}, default=None</span>
</dt>
<dd>
<p>If <code>None</code>, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:</p> <dl class="simple"> <dt>
<code>'binary'</code>:</dt>
<dd>
<p>Only report results for the class specified by <code>pos_label</code>. This is applicable only if targets (<code>y_{true,pred}</code>) are binary.</p> </dd> <dt>
<code>'micro'</code>:</dt>
<dd>
<p>Calculate metrics globally by counting the total true positives, false negatives and false positives.</p> </dd> <dt>
<code>'macro'</code>:</dt>
<dd>
<p>Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.</p> </dd> <dt>
<code>'weighted'</code>:</dt>
<dd>
<p>Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.</p> </dd> <dt>
<code>'samples'</code>:</dt>
<dd>
<p>Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from <a class="reference internal" href="sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code>accuracy_score</code></a>).</p> </dd> </dl> </dd> <dt>
<strong>warn_for</strong><span class="classifier">tuple or set, for internal use</span>
</dt>
<dd>
<p>This determines which warnings will be made in the case that this function is being used to return only one of its metrics.</p> </dd> <dt>
<strong>sample_weight</strong><span class="classifier">array-like of shape (n_samples,), default=None</span>
</dt>
<dd>
<p>Sample weights.</p> </dd> <dt>
<strong>zero_division</strong><span class="classifier">“warn”, 0 or 1, default=”warn”</span>
</dt>
<dd>
<dl class="simple"> <dt>Sets the value to return when there is a zero division:</dt>
<dd>
<ul class="simple"> <li>recall: when there are no positive labels</li> <li>precision: when there are no positive predictions</li> <li>f-score: both</li> </ul> </dd> </dl> <p>If set to “warn”, this acts as 0, but warnings are also raised.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>precision</strong><span class="classifier">float (if average is not None) or array of float, shape = [n_unique_labels]</span>
</dt>
<dd>
<p>Precision score.</p> </dd> <dt>
<strong>recall</strong><span class="classifier">float (if average is not None) or array of float, shape = [n_unique_labels]</span>
</dt>
<dd>
<p>Recall score.</p> </dd> <dt>
<strong>fbeta_score</strong><span class="classifier">float (if average is not None) or array of float, shape = [n_unique_labels]</span>
</dt>
<dd>
<p>F-beta score.</p> </dd> <dt>
<strong>support</strong><span class="classifier">None (if average is not None) or array of int, shape = [n_unique_labels]</span>
</dt>
<dd>
<p>The number of occurrences of each label in <code>y_true</code>.</p> </dd> </dl> </dd> </dl> <h4 class="rubric">Notes</h4> <p>When <code>true positive + false positive == 0</code>, precision is undefined. When <code>true positive + false negative == 0</code>, recall is undefined. In such cases, by default the metric will be set to 0, as will f-score, and <code>UndefinedMetricWarning</code> will be raised. This behavior can be modified with <code>zero_division</code>.</p> <h4 class="rubric">References</h4> <div role="list" class="citation-list"> <div class="citation" id="r623484488881-1" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span> <p><a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">Wikipedia entry for the Precision and recall</a>.</p> </div> <div class="citation" id="r623484488881-2" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span> <p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a>.</p> </div> <div class="citation" id="r623484488881-3" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span> <p><a class="reference external" href="http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf">Discriminative Methods for Multi-labeled Classification Advances in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu Godbole, Sunita Sarawagi</a>.</p> </div> </div> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.metrics import precision_recall_fscore_support
&gt;&gt;&gt; y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])
&gt;&gt;&gt; y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])
&gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average='macro')
(0.22..., 0.33..., 0.26..., None)
&gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average='micro')
(0.33..., 0.33..., 0.33..., None)
&gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average='weighted')
(0.22..., 0.33..., 0.26..., None)
</pre> <p>It is possible to compute per-label precisions, recalls, F1-scores and supports instead of averaging:</p> <pre data-language="python">&gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average=None,
... labels=['pig', 'dog', 'cat'])
(array([0.        , 0.        , 0.66...]),
 array([0., 0., 1.]), array([0. , 0. , 0.8]),
 array([2, 2, 2]))
</pre> </dd>
</dl> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2022 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.1/modules/generated/sklearn.metrics.precision_recall_fscore_support.html" class="_attribution-link">https://scikit-learn.org/1.1/modules/generated/sklearn.metrics.precision_recall_fscore_support.html</a>
  </p>
</div>
