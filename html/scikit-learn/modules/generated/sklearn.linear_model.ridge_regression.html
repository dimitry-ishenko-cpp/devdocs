<section id="ridge-regression"> <h1>ridge_regression</h1> <dl class="py function"> <dt class="sig sig-object py" id="sklearn.linear_model.ridge_regression"> <span class="sig-prename descclassname">sklearn.linear_model.</span><span class="sig-name descname">ridge_regression</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">alpha</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">solver</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">max_iter</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">positive</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_n_iter</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_intercept</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">check_input</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/99bf3d8e4/sklearn/linear_model/_ridge.py#L377"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Solve the ridge equation by the method of normal equations.</p> <p>Read more in the <a class="reference internal" href="../linear_model.html#ridge-regression"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl> <dt>
<strong>X</strong><span class="classifier">{array-like, sparse matrix, LinearOperator} of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>Training data.</p> </dd> <dt>
<strong>y</strong><span class="classifier">array-like of shape (n_samples,) or (n_samples, n_targets)</span>
</dt>
<dd>
<p>Target values.</p> </dd> <dt>
<strong>alpha</strong><span class="classifier">float or array-like of shape (n_targets,)</span>
</dt>
<dd>
<p>Constant that multiplies the L2 term, controlling regularization strength. <code>alpha</code> must be a non-negative float i.e. in <code>[0, inf)</code>.</p> <p>When <code>alpha = 0</code>, the objective is equivalent to ordinary least squares, solved by the <a class="reference internal" href="sklearn.linear_model.linearregression.html#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code>LinearRegression</code></a> object. For numerical reasons, using <code>alpha = 0</code> with the <code>Ridge</code> object is not advised. Instead, you should use the <a class="reference internal" href="sklearn.linear_model.linearregression.html#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code>LinearRegression</code></a> object.</p> <p>If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number.</p> </dd> <dt>
<strong>sample_weight</strong><span class="classifier">float or array-like of shape (n_samples,), default=None</span>
</dt>
<dd>
<p>Individual weights for each sample. If given a float, every sample will have the same weight. If sample_weight is not None and solver=’auto’, the solver will be set to ‘cholesky’.</p> <div class="versionadded"> <p><span class="versionmodified added">Added in version 0.17.</span></p> </div> </dd> <dt>
<strong>solver</strong><span class="classifier">{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’, ‘lbfgs’}, default=’auto’</span>
</dt>
<dd>
<p>Solver to use in the computational routines:</p> <ul class="simple"> <li>‘auto’ chooses the solver automatically based on the type of data.</li> <li>‘svd’ uses a Singular Value Decomposition of X to compute the Ridge coefficients. It is the most stable solver, in particular more stable for singular matrices than ‘cholesky’ at the cost of being slower.</li> <li>‘cholesky’ uses the standard scipy.linalg.solve function to obtain a closed-form solution via a Cholesky decomposition of dot(X.T, X)</li> <li>‘sparse_cg’ uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than ‘cholesky’ for large-scale data (possibility to set <code>tol</code> and <code>max_iter</code>).</li> <li>‘lsqr’ uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative procedure.</li> <li>‘sag’ uses a Stochastic Average Gradient descent, and ‘saga’ uses its improved, unbiased version named SAGA. Both methods also use an iterative procedure, and are often faster than other solvers when both n_samples and n_features are large. Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.</li> <li>‘lbfgs’ uses L-BFGS-B algorithm implemented in <code>scipy.optimize.minimize</code>. It can be used only when <code>positive</code> is True.</li> </ul> <p>All solvers except ‘svd’ support both dense and sparse data. However, only ‘lsqr’, ‘sag’, ‘sparse_cg’, and ‘lbfgs’ support sparse input when <code>fit_intercept</code> is True.</p> <div class="versionadded"> <p><span class="versionmodified added">Added in version 0.17: </span>Stochastic Average Gradient descent solver.</p> </div> <div class="versionadded"> <p><span class="versionmodified added">Added in version 0.19: </span>SAGA solver.</p> </div> </dd> <dt>
<strong>max_iter</strong><span class="classifier">int, default=None</span>
</dt>
<dd>
<p>Maximum number of iterations for conjugate gradient solver. For the ‘sparse_cg’ and ‘lsqr’ solvers, the default value is determined by scipy.sparse.linalg. For ‘sag’ and saga solver, the default value is 1000. For ‘lbfgs’ solver, the default value is 15000.</p> </dd> <dt>
<strong>tol</strong><span class="classifier">float, default=1e-4</span>
</dt>
<dd>
<p>Precision of the solution. Note that <code>tol</code> has no effect for solvers ‘svd’ and ‘cholesky’.</p> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 1.2: </span>Default value changed from 1e-3 to 1e-4 for consistency with other linear models.</p> </div> </dd> <dt>
<strong>verbose</strong><span class="classifier">int, default=0</span>
</dt>
<dd>
<p>Verbosity level. Setting verbose &gt; 0 will display additional information depending on the solver used.</p> </dd> <dt>
<strong>positive</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>When set to <code>True</code>, forces the coefficients to be positive. Only ‘lbfgs’ solver is supported in this case.</p> </dd> <dt>
<strong>random_state</strong><span class="classifier">int, RandomState instance, default=None</span>
</dt>
<dd>
<p>Used when <code>solver</code> == ‘sag’ or ‘saga’ to shuffle the data. See <a class="reference internal" href="https://scikit-learn.org/1.6/glossary.html#term-random_state"><span class="xref std std-term">Glossary</span></a> for details.</p> </dd> <dt>
<strong>return_n_iter</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>If True, the method also returns <code>n_iter</code>, the actual number of iteration performed by the solver.</p> <div class="versionadded"> <p><span class="versionmodified added">Added in version 0.17.</span></p> </div> </dd> <dt>
<strong>return_intercept</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>If True and if X is sparse, the method also returns the intercept, and the solver is automatically changed to ‘sag’. This is only a temporary fix for fitting the intercept with sparse data. For dense data, use sklearn.linear_model._preprocess_data before your regression.</p> <div class="versionadded"> <p><span class="versionmodified added">Added in version 0.17.</span></p> </div> </dd> <dt>
<strong>check_input</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>If False, the input arrays X and y will not be checked.</p> <div class="versionadded"> <p><span class="versionmodified added">Added in version 0.21.</span></p> </div> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>coef</strong><span class="classifier">ndarray of shape (n_features,) or (n_targets, n_features)</span>
</dt>
<dd>
<p>Weight vector(s).</p> </dd> <dt>
<strong>n_iter</strong><span class="classifier">int, optional</span>
</dt>
<dd>
<p>The actual number of iteration performed by the solver. Only returned if <code>return_n_iter</code> is True.</p> </dd> <dt>
<strong>intercept</strong><span class="classifier">float or ndarray of shape (n_targets,)</span>
</dt>
<dd>
<p>The intercept of the model. Only returned if <code>return_intercept</code> is True and if X is a scipy sparse array.</p> </dd> </dl> </dd> </dl> <h4 class="rubric">Notes</h4> <p>This function won’t compute the intercept.</p> <p>Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to <code>1 / (2C)</code> in other linear models such as <a class="reference internal" href="sklearn.linear_model.logisticregression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code>LogisticRegression</code></a> or <a class="reference internal" href="sklearn.svm.linearsvc.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code>LinearSVC</code></a>. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number.</p> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.datasets import make_regression
&gt;&gt;&gt; from sklearn.linear_model import ridge_regression
&gt;&gt;&gt; rng = np.random.RandomState(0)
&gt;&gt;&gt; X = rng.randn(100, 4)
&gt;&gt;&gt; y = 2.0 * X[:, 0] - 1.0 * X[:, 1] + 0.1 * rng.standard_normal(100)
&gt;&gt;&gt; coef, intercept = ridge_regression(X, y, alpha=1.0, return_intercept=True)
&gt;&gt;&gt; list(coef)
[np.float64(1.9...), np.float64(-1.0...), np.float64(-0.0...), np.float64(-0.0...)]
&gt;&gt;&gt; intercept
np.float64(-0.0...)
</pre> </dd>
</dl> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2025 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.ridge_regression.html" class="_attribution-link">https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.ridge_regression.html</a>
  </p>
</div>
