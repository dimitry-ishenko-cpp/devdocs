<section id="sklearn-preprocessing-power-transform"> <h1>sklearn.preprocessing.power_transform</h1> <dl class="py function"> <dt class="sig sig-object py" id="sklearn.preprocessing.power_transform"> <span class="sig-prename descclassname">sklearn.preprocessing.</span><span class="sig-name descname">power_transform</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">method</span><span class="o">=</span><span class="default_value">'yeo-johnson'</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">standardize</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">copy</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/preprocessing/_data.py#L3326"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Parametric, monotonic transformation to make data more Gaussian-like.</p> <p>Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired.</p> <p>Currently, power_transform supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood.</p> <p>Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data.</p> <p>By default, zero-mean, unit-variance normalization is applied to the transformed data.</p> <p>Read more in the <a class="reference internal" href="../preprocessing.html#preprocessing-transformer"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl> <dt>
<strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>The data to be transformed using a power transformation.</p> </dd> <dt>
<strong>method</strong><span class="classifier">{‘yeo-johnson’, ‘box-cox’}, default=’yeo-johnson’</span>
</dt>
<dd>
<p>The power transform method. Available methods are:</p> <ul class="simple"> <li>‘yeo-johnson’ <a class="reference internal" href="#r742a88cfa144-1" id="id1">[1]</a>, works with positive and negative values</li> <li>‘box-cox’ <a class="reference internal" href="#r742a88cfa144-2" id="id2">[2]</a>, only works with strictly positive values</li> </ul> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 0.23: </span>The default value of the <code>method</code> parameter changed from ‘box-cox’ to ‘yeo-johnson’ in 0.23.</p> </div> </dd> <dt>
<strong>standardize</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>Set to True to apply zero-mean, unit-variance normalization to the transformed output.</p> </dd> <dt>
<strong>copy</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>Set to False to perform inplace computation during transformation.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>X_trans</strong><span class="classifier">ndarray of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>The transformed data.</p> </dd> </dl> </dd> </dl> <div class="admonition seealso"> <p class="admonition-title">See also</p> <dl class="simple"> <dt><a class="reference internal" href="sklearn.preprocessing.powertransformer.html#sklearn.preprocessing.PowerTransformer" title="sklearn.preprocessing.PowerTransformer"><code>PowerTransformer</code></a></dt>
<dd>
<p>Equivalent transformation with the Transformer API (e.g. as part of a preprocessing <a class="reference internal" href="sklearn.pipeline.pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code>Pipeline</code></a>).</p> </dd> <dt><a class="reference internal" href="sklearn.preprocessing.quantile_transform.html#sklearn.preprocessing.quantile_transform" title="sklearn.preprocessing.quantile_transform"><code>quantile_transform</code></a></dt>
<dd>
<p>Maps data to a standard normal distribution with the parameter <code>output_distribution='normal'</code>.</p> </dd> </dl> </div> <h4 class="rubric">Notes</h4> <p>NaNs are treated as missing values: disregarded in <code>fit</code>, and maintained in <code>transform</code>.</p> <p>For a comparison of the different scalers, transformers, and normalizers, see <a class="reference internal" href="../../auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py"><span class="std std-ref">examples/preprocessing/plot_all_scaling.py</span></a>.</p> <h4 class="rubric">References</h4> <div role="list" class="citation-list"> <div class="citation" id="r742a88cfa144-1" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span> <p>I.K. Yeo and R.A. Johnson, “A new family of power transformations to improve normality or symmetry.” Biometrika, 87(4), pp.954-959, (2000).</p> </div> <div class="citation" id="r742a88cfa144-2" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span> <p>G.E.P. Box and D.R. Cox, “An Analysis of Transformations”, Journal of the Royal Statistical Society B, 26, 211-252 (1964).</p> </div> </div> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.preprocessing import power_transform
&gt;&gt;&gt; data = [[1, 2], [3, 2], [4, 5]]
&gt;&gt;&gt; print(power_transform(data, method='box-cox'))
[[-1.332... -0.707...]
 [ 0.256... -0.707...]
 [ 1.076...  1.414...]]
</pre> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Risk of data leak. Do not use <a class="reference internal" href="#sklearn.preprocessing.power_transform" title="sklearn.preprocessing.power_transform"><code>power_transform</code></a> unless you know what you are doing. A common mistake is to apply it to the entire data <em>before</em> splitting into training and test sets. This will bias the model evaluation because information would have leaked from the test set to the training set. In general, we recommend using <a class="reference internal" href="sklearn.preprocessing.powertransformer.html#sklearn.preprocessing.PowerTransformer" title="sklearn.preprocessing.PowerTransformer"><code>PowerTransformer</code></a> within a <a class="reference internal" href="../compose.html#pipeline"><span class="std std-ref">Pipeline</span></a> in order to prevent most risks of data leaking, e.g.: <code>pipe = make_pipeline(PowerTransformer(),
LogisticRegression())</code>.</p> </div> </dd>
</dl> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2022 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.1/modules/generated/sklearn.preprocessing.power_transform.html" class="_attribution-link">https://scikit-learn.org/1.1/modules/generated/sklearn.preprocessing.power_transform.html</a>
  </p>
</div>
