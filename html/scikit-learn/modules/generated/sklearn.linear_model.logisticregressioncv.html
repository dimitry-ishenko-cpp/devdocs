<section id="sklearn-linear-model-logisticregressioncv"> <h1>sklearn.linear_model.LogisticRegressionCV</h1> <dl class="py class"> <dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV"> <em class="property">class</em><span class="sig-prename descclassname">sklearn.linear_model.</span><span class="sig-name descname">LogisticRegressionCV</span><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">Cs</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">fit_intercept</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">cv</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dual</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">penalty</span><span class="o">=</span><span class="default_value">'l2'</span></em>, <em class="sig-param"><span class="n">scoring</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">solver</span><span class="o">=</span><span class="default_value">'lbfgs'</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">max_iter</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">class_weight</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_jobs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">refit</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">intercept_scaling</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">multi_class</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">l1_ratios</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/linear_model/_logistic.py#L1344"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Logistic Regression CV (aka logit, MaxEnt) classifier.</p> <p>See glossary entry for <a class="reference internal" href="https://scikit-learn.org/1.1/glossary.html#term-cross-validation-estimator"><span class="xref std std-term">cross-validation estimator</span></a>.</p> <p>This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. Elastic-Net penalty is only supported by the saga solver.</p> <p>For the grid of <code>Cs</code> values and <code>l1_ratios</code> values, the best hyperparameter is selected by the cross-validator <a class="reference internal" href="sklearn.model_selection.stratifiedkfold.html#sklearn.model_selection.StratifiedKFold" title="sklearn.model_selection.StratifiedKFold"><code>StratifiedKFold</code></a>, but it can be changed using the <a class="reference internal" href="https://scikit-learn.org/1.1/glossary.html#term-cv"><span class="xref std std-term">cv</span></a> parameter. The ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ solvers can warm-start the coefficients (see <a class="reference internal" href="https://scikit-learn.org/1.1/glossary.html#term-warm_start"><span class="xref std std-term">Glossary</span></a>).</p> <p>Read more in the <a class="reference internal" href="../linear_model.html#logistic-regression"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl> <dt>
<strong>Cs</strong><span class="classifier">int or list of floats, default=10</span>
</dt>
<dd>
<p>Each of the values in Cs describes the inverse of regularization strength. If Cs is as an int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4. Like in support vector machines, smaller values specify stronger regularization.</p> </dd> <dt>
<strong>fit_intercept</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function.</p> </dd> <dt>
<strong>cv</strong><span class="classifier">int or cross-validation generator, default=None</span>
</dt>
<dd>
<p>The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module <a class="reference internal" href="../classes.html#module-sklearn.model_selection" title="sklearn.model_selection"><code>sklearn.model_selection</code></a> module for the list of possible cross-validation objects.</p> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 0.22: </span><code>cv</code> default value if None changed from 3-fold to 5-fold.</p> </div> </dd> <dt>
<strong>dual</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples &gt; n_features.</p> </dd> <dt>
<strong>penalty</strong><span class="classifier">{‘l1’, ‘l2’, ‘elasticnet’}, default=’l2’</span>
</dt>
<dd>
<p>Specify the norm of the penalty:</p> <ul class="simple"> <li>
<code>'l2'</code>: add a L2 penalty term (used by default);</li> <li>
<code>'l1'</code>: add a L1 penalty term;</li> <li>
<code>'elasticnet'</code>: both L1 and L2 penalty terms are added.</li> </ul> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Some penalties may not work with some solvers. See the parameter <code>solver</code> below, to know the compatibility between the penalty and solver.</p> </div> </dd> <dt>
<strong>scoring</strong><span class="classifier">str or callable, default=None</span>
</dt>
<dd>
<p>A string (see model evaluation documentation) or a scorer callable object / function with signature <code>scorer(estimator, X, y)</code>. For a list of scoring functions that can be used, look at <a class="reference internal" href="../classes.html#module-sklearn.metrics" title="sklearn.metrics"><code>sklearn.metrics</code></a>. The default scoring option used is ‘accuracy’.</p> </dd> <dt>
<strong>solver</strong><span class="classifier">{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’</span>
</dt>
<dd>
<p>Algorithm to use in the optimization problem. Default is ‘lbfgs’. To choose a solver, you might want to consider the following aspects:</p>  <ul class="simple"> <li>For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones;</li> <li>For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss;</li> <li>‘liblinear’ might be slower in <a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV" title="sklearn.linear_model.LogisticRegressionCV"><code>LogisticRegressionCV</code></a> because it does not handle warm-starting. ‘liblinear’ is limited to one-versus-rest schemes.</li> </ul>  <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>The choice of the algorithm depends on the penalty chosen:</p> <ul class="simple"> <li>‘newton-cg’ - [‘l2’]</li> <li>‘lbfgs’ - [‘l2’]</li> <li>‘liblinear’ - [‘l1’, ‘l2’]</li> <li>‘sag’ - [‘l2’]</li> <li>‘saga’ - [‘elasticnet’, ‘l1’, ‘l2’]</li> </ul> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>‘sag’ and ‘saga’ fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from <a class="reference internal" href="../classes.html#module-sklearn.preprocessing" title="sklearn.preprocessing"><code>sklearn.preprocessing</code></a>.</p> </div> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.17: </span>Stochastic Average Gradient descent solver.</p> </div> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.19: </span>SAGA solver.</p> </div> </dd> <dt>
<strong>tol</strong><span class="classifier">float, default=1e-4</span>
</dt>
<dd>
<p>Tolerance for stopping criteria.</p> </dd> <dt>
<strong>max_iter</strong><span class="classifier">int, default=100</span>
</dt>
<dd>
<p>Maximum number of iterations of the optimization algorithm.</p> </dd> <dt>
<strong>class_weight</strong><span class="classifier">dict or ‘balanced’, default=None</span>
</dt>
<dd>
<p>Weights associated with classes in the form <code>{class_label: weight}</code>. If not given, all classes are supposed to have weight one.</p> <p>The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as <code>n_samples / (n_classes * np.bincount(y))</code>.</p> <p>Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.17: </span>class_weight == ‘balanced’</p> </div> </dd> <dt>
<strong>n_jobs</strong><span class="classifier">int, default=None</span>
</dt>
<dd>
<p>Number of CPU cores used during the cross-validation loop. <code>None</code> means 1 unless in a <a class="reference external" href="https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend" title="(in joblib v1.3.0.dev0)"><code>joblib.parallel_backend</code></a> context. <code>-1</code> means using all processors. See <a class="reference internal" href="https://scikit-learn.org/1.1/glossary.html#term-n_jobs"><span class="xref std std-term">Glossary</span></a> for more details.</p> </dd> <dt>
<strong>verbose</strong><span class="classifier">int, default=0</span>
</dt>
<dd>
<p>For the ‘liblinear’, ‘sag’ and ‘lbfgs’ solvers set verbose to any positive number for verbosity.</p> </dd> <dt>
<strong>refit</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>If set to True, the scores are averaged across all folds, and the coefs and the C that corresponds to the best score is taken, and a final refit is done using these parameters. Otherwise the coefs, intercepts and C that correspond to the best scores across folds are averaged.</p> </dd> <dt>
<strong>intercept_scaling</strong><span class="classifier">float, default=1</span>
</dt>
<dd>
<p>Useful only when the solver ‘liblinear’ is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a “synthetic” feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes <code>intercept_scaling * synthetic_feature_weight</code>.</p> <p>Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased.</p> </dd> <dt>
<strong>multi_class</strong><span class="classifier">{‘auto, ‘ovr’, ‘multinomial’}, default=’auto’</span>
</dt>
<dd>
<p>If the option chosen is ‘ovr’, then a binary problem is fit for each label. For ‘multinomial’ the loss minimised is the multinomial loss fit across the entire probability distribution, <em>even when the data is binary</em>. ‘multinomial’ is unavailable when solver=’liblinear’. ‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’, and otherwise selects ‘multinomial’.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.18: </span>Stochastic Average Gradient descent solver for ‘multinomial’ case.</p> </div> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 0.22: </span>Default changed from ‘ovr’ to ‘auto’ in 0.22.</p> </div> </dd> <dt>
<strong>random_state</strong><span class="classifier">int, RandomState instance, default=None</span>
</dt>
<dd>
<p>Used when <code>solver='sag'</code>, ‘saga’ or ‘liblinear’ to shuffle the data. Note that this only applies to the solver and not the cross-validation generator. See <a class="reference internal" href="https://scikit-learn.org/1.1/glossary.html#term-random_state"><span class="xref std std-term">Glossary</span></a> for details.</p> </dd> <dt>
<strong>l1_ratios</strong><span class="classifier">list of float, default=None</span>
</dt>
<dd>
<p>The list of Elastic-Net mixing parameter, with <code>0 &lt;= l1_ratio &lt;= 1</code>. Only used if <code>penalty='elasticnet'</code>. A value of 0 is equivalent to using <code>penalty='l2'</code>, while 1 is equivalent to using <code>penalty='l1'</code>. For <code>0 &lt; l1_ratio &lt;1</code>, the penalty is a combination of L1 and L2.</p> </dd> </dl> </dd> <dt class="field-even">Attributes<span class="colon">:</span>
</dt> <dd class="field-even">
<dl> <dt>
<strong>classes_</strong><span class="classifier">ndarray of shape (n_classes, )</span>
</dt>
<dd>
<p>A list of class labels known to the classifier.</p> </dd> <dt>
<strong>coef_</strong><span class="classifier">ndarray of shape (1, n_features) or (n_classes, n_features)</span>
</dt>
<dd>
<p>Coefficient of the features in the decision function.</p> <p><code>coef_</code> is of shape (1, n_features) when the given problem is binary.</p> </dd> <dt>
<strong>intercept_</strong><span class="classifier">ndarray of shape (1,) or (n_classes,)</span>
</dt>
<dd>
<p>Intercept (a.k.a. bias) added to the decision function.</p> <p>If <code>fit_intercept</code> is set to False, the intercept is set to zero. <code>intercept_</code> is of shape(1,) when the problem is binary.</p> </dd> <dt>
<strong>Cs_</strong><span class="classifier">ndarray of shape (n_cs)</span>
</dt>
<dd>
<p>Array of C i.e. inverse of regularization parameter values used for cross-validation.</p> </dd> <dt>
<strong>l1_ratios_</strong><span class="classifier">ndarray of shape (n_l1_ratios)</span>
</dt>
<dd>
<p>Array of l1_ratios used for cross-validation. If no l1_ratio is used (i.e. penalty is not ‘elasticnet’), this is set to <code>[None]</code></p> </dd> <dt>
<strong>coefs_paths_</strong><span class="classifier">ndarray of shape (n_folds, n_cs, n_features) or (n_folds, n_cs, n_features + 1)</span>
</dt>
<dd>
<p>dict with classes as the keys, and the path of coefficients obtained during cross-validating across each fold and then across each Cs after doing an OvR for the corresponding class as values. If the ‘multi_class’ option is set to ‘multinomial’, then the coefs_paths are the coefficients corresponding to each class. Each dict value has shape <code>(n_folds, n_cs, n_features)</code> or <code>(n_folds, n_cs, n_features + 1)</code> depending on whether the intercept is fit or not. If <code>penalty='elasticnet'</code>, the shape is <code>(n_folds, n_cs, n_l1_ratios_, n_features)</code> or <code>(n_folds, n_cs, n_l1_ratios_, n_features + 1)</code>.</p> </dd> <dt>
<strong>scores_</strong><span class="classifier">dict</span>
</dt>
<dd>
<p>dict with classes as the keys, and the values as the grid of scores obtained during cross-validating each fold, after doing an OvR for the corresponding class. If the ‘multi_class’ option given is ‘multinomial’ then the same scores are repeated across all classes, since this is the multinomial class. Each dict value has shape <code>(n_folds, n_cs</code> or <code>(n_folds, n_cs, n_l1_ratios)</code> if <code>penalty='elasticnet'</code>.</p> </dd> <dt>
<strong>C_</strong><span class="classifier">ndarray of shape (n_classes,) or (n_classes - 1,)</span>
</dt>
<dd>
<p>Array of C that maps to the best scores across every class. If refit is set to False, then for each class, the best C is the average of the C’s that correspond to the best scores for each fold. <code>C_</code> is of shape(n_classes,) when the problem is binary.</p> </dd> <dt>
<strong>l1_ratio_</strong><span class="classifier">ndarray of shape (n_classes,) or (n_classes - 1,)</span>
</dt>
<dd>
<p>Array of l1_ratio that maps to the best scores across every class. If refit is set to False, then for each class, the best l1_ratio is the average of the l1_ratio’s that correspond to the best scores for each fold. <code>l1_ratio_</code> is of shape(n_classes,) when the problem is binary.</p> </dd> <dt>
<strong>n_iter_</strong><span class="classifier">ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)</span>
</dt>
<dd>
<p>Actual number of iterations for all classes, folds and Cs. In the binary or multinomial cases, the first dimension is equal to 1. If <code>penalty='elasticnet'</code>, the shape is <code>(n_classes, n_folds,
n_cs, n_l1_ratios)</code> or <code>(1, n_folds, n_cs, n_l1_ratios)</code>.</p> </dd> <dt>
<strong>n_features_in_</strong><span class="classifier">int</span>
</dt>
<dd>
<p>Number of features seen during <a class="reference internal" href="https://scikit-learn.org/1.1/glossary.html#term-fit"><span class="xref std std-term">fit</span></a>.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.24.</span></p> </div> </dd> <dt>
<strong>feature_names_in_</strong><span class="classifier">ndarray of shape (<code>n_features_in_</code>,)</span>
</dt>
<dd>
<p>Names of features seen during <a class="reference internal" href="https://scikit-learn.org/1.1/glossary.html#term-fit"><span class="xref std std-term">fit</span></a>. Defined only when <code>X</code> has feature names that are all strings.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 1.0.</span></p> </div> </dd> </dl> </dd> </dl> <div class="admonition seealso"> <p class="admonition-title">See also</p> <dl class="simple"> <dt><a class="reference internal" href="sklearn.linear_model.logisticregression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code>LogisticRegression</code></a></dt>
<dd>
<p>Logistic regression without tuning the hyperparameter <code>C</code>.</p> </dd> </dl> </div> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from sklearn.datasets import load_iris
&gt;&gt;&gt; from sklearn.linear_model import LogisticRegressionCV
&gt;&gt;&gt; X, y = load_iris(return_X_y=True)
&gt;&gt;&gt; clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)
&gt;&gt;&gt; clf.predict(X[:2, :])
array([0, 0])
&gt;&gt;&gt; clf.predict_proba(X[:2, :]).shape
(2, 3)
&gt;&gt;&gt; clf.score(X, y)
0.98...
</pre> <h4 class="rubric">Methods</h4> <table class="autosummary longtable docutils align-default">  <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.decision_function" title="sklearn.linear_model.LogisticRegressionCV.decision_function"><code>decision_function</code></a>(X)</p></td> <td><p>Predict confidence scores for samples.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.densify" title="sklearn.linear_model.LogisticRegressionCV.densify"><code>densify</code></a>()</p></td> <td><p>Convert coefficient matrix to dense array format.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.fit" title="sklearn.linear_model.LogisticRegressionCV.fit"><code>fit</code></a>(X, y[, sample_weight])</p></td> <td><p>Fit the model according to the given training data.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.get_params" title="sklearn.linear_model.LogisticRegressionCV.get_params"><code>get_params</code></a>([deep])</p></td> <td><p>Get parameters for this estimator.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.predict" title="sklearn.linear_model.LogisticRegressionCV.predict"><code>predict</code></a>(X)</p></td> <td><p>Predict class labels for samples in X.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.predict_log_proba" title="sklearn.linear_model.LogisticRegressionCV.predict_log_proba"><code>predict_log_proba</code></a>(X)</p></td> <td><p>Predict logarithm of probability estimates.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.predict_proba" title="sklearn.linear_model.LogisticRegressionCV.predict_proba"><code>predict_proba</code></a>(X)</p></td> <td><p>Probability estimates.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.score" title="sklearn.linear_model.LogisticRegressionCV.score"><code>score</code></a>(X, y[, sample_weight])</p></td> <td><p>Score using the <code>scoring</code> option on the given test data and labels.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.set_params" title="sklearn.linear_model.LogisticRegressionCV.set_params"><code>set_params</code></a>(**params)</p></td> <td><p>Set the parameters of this estimator.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV.sparsify" title="sklearn.linear_model.LogisticRegressionCV.sparsify"><code>sparsify</code></a>()</p></td> <td><p>Convert coefficient matrix to sparse format.</p></td> </tr>  </table> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.decision_function"> <span class="sig-name descname">decision_function</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/linear_model/_base.py#L408"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Predict confidence scores for samples.</p> <p>The confidence score for a sample is proportional to the signed distance of that sample to the hyperplane.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>X</strong><span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>The data matrix for which we want to get the confidence scores.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>scores</strong><span class="classifier">ndarray of shape (n_samples,) or (n_samples, n_classes)</span>
</dt>
<dd>
<p>Confidence scores per <code>(n_samples, n_classes)</code> combination. In the binary case, confidence score for <code>self.classes_[1]</code> where &gt;0 means this class would be predicted.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.densify"> <span class="sig-name descname">densify</span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/linear_model/_base.py#L477"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Convert coefficient matrix to dense array format.</p> <p>Converts the <code>coef_</code> member (back) to a numpy.ndarray. This is the default format of <code>coef_</code> and is required for fitting, so calling this method is only required on models that have previously been sparsified; otherwise, it is a no-op.</p> <dl class="field-list simple"> <dt class="field-odd">Returns<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>self</dt>
<dd>
<p>Fitted estimator.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.fit"> <span class="sig-name descname">fit</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/linear_model/_logistic.py#L1651"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fit the model according to the given training data.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>X</strong><span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>Training vector, where <code>n_samples</code> is the number of samples and <code>n_features</code> is the number of features.</p> </dd> <dt>
<strong>y</strong><span class="classifier">array-like of shape (n_samples,)</span>
</dt>
<dd>
<p>Target vector relative to X.</p> </dd> <dt>
<strong>sample_weight</strong><span class="classifier">array-like of shape (n_samples,) default=None</span>
</dt>
<dd>
<p>Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>self</strong><span class="classifier">object</span>
</dt>
<dd>
<p>Fitted LogisticRegressionCV estimator.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.get_params"> <span class="sig-name descname">get_params</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">deep</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/base.py#L194"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Get parameters for this estimator.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>deep</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>If True, will return the parameters for this estimator and contained subobjects that are estimators.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>params</strong><span class="classifier">dict</span>
</dt>
<dd>
<p>Parameter names mapped to their values.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.predict"> <span class="sig-name descname">predict</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/linear_model/_base.py#L433"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Predict class labels for samples in X.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>X</strong><span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>The data matrix for which we want to get the predictions.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>y_pred</strong><span class="classifier">ndarray of shape (n_samples,)</span>
</dt>
<dd>
<p>Vector containing the class labels for each sample.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.predict_log_proba"> <span class="sig-name descname">predict_log_proba</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/linear_model/_logistic.py#L1322"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Predict logarithm of probability estimates.</p> <p>The returned estimates for all classes are ordered by the label of classes.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>Vector to be scored, where <code>n_samples</code> is the number of samples and <code>n_features</code> is the number of features.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>T</strong><span class="classifier">array-like of shape (n_samples, n_classes)</span>
</dt>
<dd>
<p>Returns the log-probability of the sample for each class in the model, where classes are ordered as they are in <code>self.classes_</code>.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.predict_proba"> <span class="sig-name descname">predict_proba</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/linear_model/_logistic.py#L1278"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Probability estimates.</p> <p>The returned estimates for all classes are ordered by the label of classes.</p> <p>For a multi_class problem, if multi_class is set to be “multinomial” the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>Vector to be scored, where <code>n_samples</code> is the number of samples and <code>n_features</code> is the number of features.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>T</strong><span class="classifier">array-like of shape (n_samples, n_classes)</span>
</dt>
<dd>
<p>Returns the probability of the sample for each class in the model, where classes are ordered as they are in <code>self.classes_</code>.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.score"> <span class="sig-name descname">score</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/linear_model/_logistic.py#L1999"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Score using the <code>scoring</code> option on the given test data and labels.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>Test samples.</p> </dd> <dt>
<strong>y</strong><span class="classifier">array-like of shape (n_samples,)</span>
</dt>
<dd>
<p>True labels for X.</p> </dd> <dt>
<strong>sample_weight</strong><span class="classifier">array-like of shape (n_samples,), default=None</span>
</dt>
<dd>
<p>Sample weights.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>score</strong><span class="classifier">float</span>
</dt>
<dd>
<p>Score of self.predict(X) wrt. y.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.set_params"> <span class="sig-name descname">set_params</span><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">params</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/base.py#L218"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as <a class="reference internal" href="sklearn.pipeline.pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code>Pipeline</code></a>). The latter have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it’s possible to update each component of a nested object.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>**params</strong><span class="classifier">dict</span>
</dt>
<dd>
<p>Estimator parameters.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>self</strong><span class="classifier">estimator instance</span>
</dt>
<dd>
<p>Estimator instance.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.sparsify"> <span class="sig-name descname">sparsify</span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/linear_model/_base.py#L497"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Convert coefficient matrix to sparse format.</p> <p>Converts the <code>coef_</code> member to a scipy.sparse matrix, which for L1-regularized models can be much more memory- and storage-efficient than the usual numpy.ndarray representation.</p> <p>The <code>intercept_</code> member is not converted.</p> <dl class="field-list simple"> <dt class="field-odd">Returns<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>self</dt>
<dd>
<p>Fitted estimator.</p> </dd> </dl> </dd> </dl> <h4 class="rubric">Notes</h4> <p>For non-sparse models, i.e. when there are not many zeros in <code>coef_</code>, this may actually <em>increase</em> memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with <code>(coef_ == 0).sum()</code>, must be more than 50% for this to provide significant benefits.</p> <p>After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.</p> </dd>
</dl> </dd>
</dl> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2022 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.1/modules/generated/sklearn.linear_model.LogisticRegressionCV.html" class="_attribution-link">https://scikit-learn.org/1.1/modules/generated/sklearn.linear_model.LogisticRegressionCV.html</a>
  </p>
</div>
