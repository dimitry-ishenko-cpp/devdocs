<section id="randomized-svd"> <h1>randomized_svd</h1> <dl class="py function"> <dt class="sig sig-object py" id="sklearn.utils.extmath.randomized_svd"> <span class="sig-prename descclassname">sklearn.utils.extmath.</span><span class="sig-name descname">randomized_svd</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">M</span></em>, <em class="sig-param"><span class="n">n_components</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">n_oversamples</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">n_iter</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">power_iteration_normalizer</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">transpose</span><span class="o">=</span><span class="default_value">'auto'</span></em>, <em class="sig-param"><span class="n">flip_sign</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">svd_lapack_driver</span><span class="o">=</span><span class="default_value">'gesdd'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/99bf3d8e4/sklearn/utils/extmath.py#L345"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Compute a truncated randomized SVD.</p> <p>This method solves the fixed-rank approximation problem described in <a class="reference internal" href="#rf38c2b656ebc-1" id="id1">[1]</a> (problem (1.5), p5).</p> <p>Refer to <a class="reference internal" href="../../auto_examples/applications/wikipedia_principal_eigenvector.html#sphx-glr-auto-examples-applications-wikipedia-principal-eigenvector-py"><span class="std std-ref">Wikipedia principal eigenvector</span></a> for a typical example where the power iteration algorithm is used to rank web pages. This algorithm is also known to be used as a building block in Google’s PageRank algorithm.</p> <dl class="field-list"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl> <dt>
<strong>M</strong><span class="classifier">{ndarray, sparse matrix}</span>
</dt>
<dd>
<p>Matrix to decompose.</p> </dd> <dt>
<strong>n_components</strong><span class="classifier">int</span>
</dt>
<dd>
<p>Number of singular values and vectors to extract.</p> </dd> <dt>
<strong>n_oversamples</strong><span class="classifier">int, default=10</span>
</dt>
<dd>
<p>Additional number of random vectors to sample the range of <code>M</code> so as to ensure proper conditioning. The total number of random vectors used to find the range of <code>M</code> is <code>n_components + n_oversamples</code>. Smaller number can improve speed but can negatively impact the quality of approximation of singular vectors and singular values. Users might wish to increase this parameter up to <code>2*k - n_components</code> where k is the effective rank, for large matrices, noisy problems, matrices with slowly decaying spectrums, or to increase precision accuracy. See <a class="reference internal" href="#rf38c2b656ebc-1" id="id2">[1]</a> (pages 5, 23 and 26).</p> </dd> <dt>
<strong>n_iter</strong><span class="classifier">int or ‘auto’, default=’auto’</span>
</dt>
<dd>
<p>Number of power iterations. It can be used to deal with very noisy problems. When ‘auto’, it is set to 4, unless <code>n_components</code> is small (&lt; .1 * min(X.shape)) in which case <code>n_iter</code> is set to 7. This improves precision with few components. Note that in general users should rather increase <code>n_oversamples</code> before increasing <code>n_iter</code> as the principle of the randomized method is to avoid usage of these more costly power iterations steps. When <code>n_components</code> is equal or greater to the effective matrix rank and the spectrum does not present a slow decay, <code>n_iter=0</code> or <code>1</code> should even work fine in theory (see <a class="reference internal" href="#rf38c2b656ebc-1" id="id3">[1]</a> page 9).</p> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 0.18.</span></p> </div> </dd> <dt>
<strong>power_iteration_normalizer</strong><span class="classifier">{‘auto’, ‘QR’, ‘LU’, ‘none’}, default=’auto’</span>
</dt>
<dd>
<p>Whether the power iterations are normalized with step-by-step QR factorization (the slowest but most accurate), ‘none’ (the fastest but numerically unstable when <code>n_iter</code> is large, e.g. typically 5 or larger), or ‘LU’ factorization (numerically stable but can lose slightly in accuracy). The ‘auto’ mode applies no normalization if <code>n_iter</code> &lt;= 2 and switches to LU otherwise.</p> <div class="versionadded"> <p><span class="versionmodified added">Added in version 0.18.</span></p> </div> </dd> <dt>
<strong>transpose</strong><span class="classifier">bool or ‘auto’, default=’auto’</span>
</dt>
<dd>
<p>Whether the algorithm should be applied to M.T instead of M. The result should approximately be the same. The ‘auto’ mode will trigger the transposition if M.shape[1] &gt; M.shape[0] since this implementation of randomized SVD tend to be a little faster in that case.</p> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 0.18.</span></p> </div> </dd> <dt>
<strong>flip_sign</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>The output of a singular value decomposition is only unique up to a permutation of the signs of the singular vectors. If <code>flip_sign</code> is set to <code>True</code>, the sign ambiguity is resolved by making the largest loadings for each component in the left singular vectors positive.</p> </dd> <dt>
<strong>random_state</strong><span class="classifier">int, RandomState instance or None, default=’warn’</span>
</dt>
<dd>
<p>The seed of the pseudo random number generator to use when shuffling the data, i.e. getting the random vectors to initialize the algorithm. Pass an int for reproducible results across multiple function calls. See <a class="reference internal" href="https://scikit-learn.org/1.6/glossary.html#term-random_state"><span class="xref std std-term">Glossary</span></a>.</p> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 1.2: </span>The default value changed from 0 to None.</p> </div> </dd> <dt>
<strong>svd_lapack_driver</strong><span class="classifier">{“gesdd”, “gesvd”}, default=”gesdd”</span>
</dt>
<dd>
<p>Whether to use the more efficient divide-and-conquer approach (<code>"gesdd"</code>) or more general rectangular approach (<code>"gesvd"</code>) to compute the SVD of the matrix B, which is the projection of M into a low dimensional subspace, as described in <a class="reference internal" href="#rf38c2b656ebc-1" id="id4">[1]</a>.</p> <div class="versionadded"> <p><span class="versionmodified added">Added in version 1.2.</span></p> </div> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>u</strong><span class="classifier">ndarray of shape (n_samples, n_components)</span>
</dt>
<dd>
<p>Unitary matrix having left singular vectors with signs flipped as columns.</p> </dd> <dt>
<strong>s</strong><span class="classifier">ndarray of shape (n_components,)</span>
</dt>
<dd>
<p>The singular values, sorted in non-increasing order.</p> </dd> <dt>
<strong>vh</strong><span class="classifier">ndarray of shape (n_components, n_features)</span>
</dt>
<dd>
<p>Unitary matrix having right singular vectors with signs flipped as rows.</p> </dd> </dl> </dd> </dl> <h4 class="rubric">Notes</h4> <p>This algorithm finds a (usually very good) approximate truncated singular value decomposition using randomization to speed up the computations. It is particularly fast on large matrices on which you wish to extract only a small number of components. In order to obtain further speed up, <code>n_iter</code> can be set &lt;=2 (at the cost of loss of precision). To increase the precision it is recommended to increase <code>n_oversamples</code>, up to <code>2*k-n_components</code> where k is the effective rank. Usually, <code>n_components</code> is chosen to be greater than k so increasing <code>n_oversamples</code> up to <code>n_components</code> should be enough.</p> <h4 class="rubric">References</h4> <div role="list" class="citation-list"> <div class="citation" id="rf38c2b656ebc-1" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span> <span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id2">2</a>,<a role="doc-backlink" href="#id3">3</a>,<a role="doc-backlink" href="#id4">4</a>)</span> <p><a class="reference external" href="https://arxiv.org/abs/0909.4061">“Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions”</a> Halko, et al. (2009)</p> </div> <div class="citation" id="rf38c2b656ebc-2" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span> <p>A randomized algorithm for the decomposition of matrices Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert</p> </div> <div class="citation" id="rf38c2b656ebc-3" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span> <p>An implementation of a randomized algorithm for principal component analysis A. Szlam et al. 2014</p> </div> </div> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.utils.extmath import randomized_svd
&gt;&gt;&gt; a = np.array([[1, 2, 3, 5],
...               [3, 4, 5, 6],
...               [7, 8, 9, 10]])
&gt;&gt;&gt; U, s, Vh = randomized_svd(a, n_components=2, random_state=0)
&gt;&gt;&gt; U.shape, s.shape, Vh.shape
((3, 2), (2,), (2, 4))
</pre> </dd>
</dl> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2025 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.6/modules/generated/sklearn.utils.extmath.randomized_svd.html" class="_attribution-link">https://scikit-learn.org/1.6/modules/generated/sklearn.utils.extmath.randomized_svd.html</a>
  </p>
</div>
