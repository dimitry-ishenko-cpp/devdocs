<section id="smacof"> <h1>smacof</h1> <dl class="py function"> <dt class="sig sig-object py" id="sklearn.manifold.smacof"> <span class="sig-prename descclassname">sklearn.manifold.</span><span class="sig-name descname">smacof</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">dissimilarities</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">metric</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">n_components</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">init</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_init</span><span class="o">=</span><span class="default_value">8</span></em>, <em class="sig-param"><span class="n">n_jobs</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_iter</span><span class="o">=</span><span class="default_value">300</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_n_iter</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">normalized_stress</span><span class="o">=</span><span class="default_value">'auto'</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/99bf3d8e4/sklearn/manifold/_mds.py#L171"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Compute multidimensional scaling using the SMACOF algorithm.</p> <p>The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a multidimensional scaling algorithm which minimizes an objective function (the <em>stress</em>) using a majorization technique. Stress majorization, also known as the Guttman Transform, guarantees a monotone convergence of stress, and is more powerful than traditional techniques such as gradient descent.</p> <p>The SMACOF algorithm for metric MDS can be summarized by the following steps:</p> <ol class="arabic simple"> <li>Set an initial start configuration, randomly or not.</li> <li>Compute the stress</li> <li>Compute the Guttman Transform</li> <li>Iterate 2 and 3 until convergence.</li> </ol> <p>The nonmetric algorithm adds a monotonic regression step before computing the stress.</p> <dl class="field-list"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl> <dt>
<strong>dissimilarities</strong><span class="classifier">array-like of shape (n_samples, n_samples)</span>
</dt>
<dd>
<p>Pairwise dissimilarities between the points. Must be symmetric.</p> </dd> <dt>
<strong>metric</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>Compute metric or nonmetric SMACOF algorithm. When <code>False</code> (i.e. non-metric MDS), dissimilarities with 0 are considered as missing values.</p> </dd> <dt>
<strong>n_components</strong><span class="classifier">int, default=2</span>
</dt>
<dd>
<p>Number of dimensions in which to immerse the dissimilarities. If an <code>init</code> array is provided, this option is overridden and the shape of <code>init</code> is used to determine the dimensionality of the embedding space.</p> </dd> <dt>
<strong>init</strong><span class="classifier">array-like of shape (n_samples, n_components), default=None</span>
</dt>
<dd>
<p>Starting configuration of the embedding to initialize the algorithm. By default, the algorithm is initialized with a randomly chosen array.</p> </dd> <dt>
<strong>n_init</strong><span class="classifier">int, default=8</span>
</dt>
<dd>
<p>Number of times the SMACOF algorithm will be run with different initializations. The final results will be the best output of the runs, determined by the run with the smallest final stress. If <code>init</code> is provided, this option is overridden and a single run is performed.</p> </dd> <dt>
<strong>n_jobs</strong><span class="classifier">int, default=None</span>
</dt>
<dd>
<p>The number of jobs to use for the computation. If multiple initializations are used (<code>n_init</code>), each run of the algorithm is computed in parallel.</p> <p><code>None</code> means 1 unless in a <a class="reference external" href="https://joblib.readthedocs.io/en/latest/generated/joblib.parallel_backend.html#joblib.parallel_backend" title="(in joblib v1.5.dev0)"><code>joblib.parallel_backend</code></a> context. <code>-1</code> means using all processors. See <a class="reference internal" href="https://scikit-learn.org/1.6/glossary.html#term-n_jobs"><span class="xref std std-term">Glossary</span></a> for more details.</p> </dd> <dt>
<strong>max_iter</strong><span class="classifier">int, default=300</span>
</dt>
<dd>
<p>Maximum number of iterations of the SMACOF algorithm for a single run.</p> </dd> <dt>
<strong>verbose</strong><span class="classifier">int, default=0</span>
</dt>
<dd>
<p>Level of verbosity.</p> </dd> <dt>
<strong>eps</strong><span class="classifier">float, default=1e-3</span>
</dt>
<dd>
<p>Relative tolerance with respect to stress at which to declare convergence. The value of <code>eps</code> should be tuned separately depending on whether or not <code>normalized_stress</code> is being used.</p> </dd> <dt>
<strong>random_state</strong><span class="classifier">int, RandomState instance or None, default=None</span>
</dt>
<dd>
<p>Determines the random number generator used to initialize the centers. Pass an int for reproducible results across multiple function calls. See <a class="reference internal" href="https://scikit-learn.org/1.6/glossary.html#term-random_state"><span class="xref std std-term">Glossary</span></a>.</p> </dd> <dt>
<strong>return_n_iter</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>Whether or not to return the number of iterations.</p> </dd> <dt>
<strong>normalized_stress</strong><span class="classifier">bool or “auto” default=”auto”</span>
</dt>
<dd>
<p>Whether use and return normed stress value (Stress-1) instead of raw stress calculated by default. Only supported in non-metric MDS.</p> <div class="versionadded"> <p><span class="versionmodified added">Added in version 1.2.</span></p> </div> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 1.4: </span>The default value changed from <code>False</code> to <code>"auto"</code> in version 1.4.</p> </div> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>X</strong><span class="classifier">ndarray of shape (n_samples, n_components)</span>
</dt>
<dd>
<p>Coordinates of the points in a <code>n_components</code>-space.</p> </dd> <dt>
<strong>stress</strong><span class="classifier">float</span>
</dt>
<dd>
<p>The final value of the stress (sum of squared distance of the disparities and the distances for all constrained points). If <code>normalized_stress=True</code>, and <code>metric=False</code> returns Stress-1. A value of 0 indicates “perfect” fit, 0.025 excellent, 0.05 good, 0.1 fair, and 0.2 poor <a class="reference internal" href="#r8e3dcaa71efe-1" id="id1">[1]</a>.</p> </dd> <dt>
<strong>n_iter</strong><span class="classifier">int</span>
</dt>
<dd>
<p>The number of iterations corresponding to the best stress. Returned only if <code>return_n_iter</code> is set to <code>True</code>.</p> </dd> </dl> </dd> </dl> <h4 class="rubric">References</h4> <div role="list" class="citation-list"> <div class="citation" id="r8e3dcaa71efe-1" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span> <p>“Nonmetric multidimensional scaling: a numerical method” Kruskal, J. Psychometrika, 29 (1964)</p> </div> <div class="citation" id="r8e3dcaa71efe-2" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span> <p>“Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis” Kruskal, J. Psychometrika, 29, (1964)</p> </div> <div class="citation" id="r8e3dcaa71efe-3" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span> <p>“Modern Multidimensional Scaling - Theory and Applications” Borg, I.; Groenen P. Springer Series in Statistics (1997)</p> </div> </div> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.manifold import smacof
&gt;&gt;&gt; from sklearn.metrics import euclidean_distances
&gt;&gt;&gt; X = np.array([[0, 1, 2], [1, 0, 3],[2, 3, 0]])
&gt;&gt;&gt; dissimilarities = euclidean_distances(X)
&gt;&gt;&gt; mds_result, stress = smacof(dissimilarities, n_components=2, random_state=42)
&gt;&gt;&gt; mds_result
array([[ 0.05... -1.07... ],
       [ 1.74..., -0.75...],
       [-1.79...,  1.83...]])
&gt;&gt;&gt; stress
np.float64(0.0012...)
</pre> </dd>
</dl> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2025 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.6/modules/generated/sklearn.manifold.smacof.html" class="_attribution-link">https://scikit-learn.org/1.6/modules/generated/sklearn.manifold.smacof.html</a>
  </p>
</div>
