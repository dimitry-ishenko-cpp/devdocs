<section id="sklearn-metrics-hinge-loss"> <h1>sklearn.metrics.hinge_loss</h1> <dl class="py function"> <dt class="sig sig-object py" id="sklearn.metrics.hinge_loss"> <span class="sig-prename descclassname">sklearn.metrics.</span><span class="sig-name descname">hinge_loss</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">y_true</span></em>, <em class="sig-param"><span class="n">pred_decision</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">labels</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sample_weight</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/metrics/_classification.py#L2470"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Average hinge loss (non-regularized).</p> <p>In binary class case, assuming labels in y_true are encoded with +1 and -1, when a prediction mistake is made, <code>margin = y_true * pred_decision</code> is always negative (since the signs disagree), implying <code>1 - margin</code> is always greater than 1. The cumulated hinge loss is therefore an upper bound of the number of mistakes made by the classifier.</p> <p>In multiclass case, the function expects that either all the labels are included in y_true or an optional labels argument is provided which contains all the labels. The multilabel margin is calculated according to Crammer-Singerâ€™s method. As in the binary case, the cumulated hinge loss is an upper bound of the number of mistakes made by the classifier.</p> <p>Read more in the <a class="reference internal" href="../model_evaluation.html#hinge-loss"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>y_true</strong><span class="classifier">array of shape (n_samples,)</span>
</dt>
<dd>
<p>True target, consisting of integers of two values. The positive label must be greater than the negative label.</p> </dd> <dt>
<strong>pred_decision</strong><span class="classifier">array of shape (n_samples,) or (n_samples, n_classes)</span>
</dt>
<dd>
<p>Predicted decisions, as output by decision_function (floats).</p> </dd> <dt>
<strong>labels</strong><span class="classifier">array-like, default=None</span>
</dt>
<dd>
<p>Contains all the labels for the problem. Used in multiclass hinge loss.</p> </dd> <dt>
<strong>sample_weight</strong><span class="classifier">array-like of shape (n_samples,), default=None</span>
</dt>
<dd>
<p>Sample weights.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>loss</strong><span class="classifier">float</span>
</dt>
<dd>
<p>Average hinge loss.</p> </dd> </dl> </dd> </dl> <h4 class="rubric">References</h4> <div role="list" class="citation-list"> <div class="citation" id="rf22d8d20ab3d-1" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span> <p><a class="reference external" href="https://en.wikipedia.org/wiki/Hinge_loss">Wikipedia entry on the Hinge loss</a>.</p> </div> <div class="citation" id="rf22d8d20ab3d-2" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span> <p>Koby Crammer, Yoram Singer. On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines. Journal of Machine Learning Research 2, (2001), 265-292.</p> </div> <div class="citation" id="rf22d8d20ab3d-3" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span> <p><a class="reference external" href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37362.pdf">L1 AND L2 Regularization for Multiclass Hinge Loss Models by Robert C. Moore, John DeNero</a>.</p> </div> </div> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from sklearn import svm
&gt;&gt;&gt; from sklearn.metrics import hinge_loss
&gt;&gt;&gt; X = [[0], [1]]
&gt;&gt;&gt; y = [-1, 1]
&gt;&gt;&gt; est = svm.LinearSVC(random_state=0)
&gt;&gt;&gt; est.fit(X, y)
LinearSVC(random_state=0)
&gt;&gt;&gt; pred_decision = est.decision_function([[-2], [3], [0.5]])
&gt;&gt;&gt; pred_decision
array([-2.18...,  2.36...,  0.09...])
&gt;&gt;&gt; hinge_loss([-1, 1, 1], pred_decision)
0.30...
</pre> <p>In the multiclass case:</p> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; X = np.array([[0], [1], [2], [3]])
&gt;&gt;&gt; Y = np.array([0, 1, 2, 3])
&gt;&gt;&gt; labels = np.array([0, 1, 2, 3])
&gt;&gt;&gt; est = svm.LinearSVC()
&gt;&gt;&gt; est.fit(X, Y)
LinearSVC()
&gt;&gt;&gt; pred_decision = est.decision_function([[-1], [2], [3]])
&gt;&gt;&gt; y_true = [0, 2, 3]
&gt;&gt;&gt; hinge_loss(y_true, pred_decision, labels=labels)
0.56...
</pre> </dd>
</dl> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2022 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.1/modules/generated/sklearn.metrics.hinge_loss.html" class="_attribution-link">https://scikit-learn.org/1.1/modules/generated/sklearn.metrics.hinge_loss.html</a>
  </p>
</div>
