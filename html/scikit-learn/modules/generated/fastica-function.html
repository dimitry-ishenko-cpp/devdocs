<section id="fastica"> <h1>fastica</h1> <dl class="py function"> <dt class="sig sig-object py" id="sklearn.decomposition.fastica"> <span class="sig-prename descclassname">sklearn.decomposition.</span><span class="sig-name descname">fastica</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">n_components</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">algorithm</span><span class="o">=</span><span class="default_value">'parallel'</span></em>, <em class="sig-param"><span class="n">whiten</span><span class="o">=</span><span class="default_value">'unit-variance'</span></em>, <em class="sig-param"><span class="n">fun</span><span class="o">=</span><span class="default_value">'logcosh'</span></em>, <em class="sig-param"><span class="n">fun_args</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">max_iter</span><span class="o">=</span><span class="default_value">200</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">w_init</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">whiten_solver</span><span class="o">=</span><span class="default_value">'svd'</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_X_mean</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">compute_sources</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">return_n_iter</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/99bf3d8e4/sklearn/decomposition/_fastica.py#L163"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Perform Fast Independent Component Analysis.</p> <p>The implementation is based on <a class="reference internal" href="#r4ef46ec4ecf2-1" id="id1">[1]</a>.</p> <p>Read more in the <a class="reference internal" href="../decomposition.html#ica"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl> <dt>
<strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>Training vector, where <code>n_samples</code> is the number of samples and <code>n_features</code> is the number of features.</p> </dd> <dt>
<strong>n_components</strong><span class="classifier">int, default=None</span>
</dt>
<dd>
<p>Number of components to use. If None is passed, all are used.</p> </dd> <dt>
<strong>algorithm</strong><span class="classifier">{‘parallel’, ‘deflation’}, default=’parallel’</span>
</dt>
<dd>
<p>Specify which algorithm to use for FastICA.</p> </dd> <dt>
<strong>whiten</strong><span class="classifier">str or bool, default=’unit-variance’</span>
</dt>
<dd>
<p>Specify the whitening strategy to use.</p> <ul class="simple"> <li>If ‘arbitrary-variance’, a whitening with variance arbitrary is used.</li> <li>If ‘unit-variance’, the whitening matrix is rescaled to ensure that each recovered source has unit variance.</li> <li>If False, the data is already considered to be whitened, and no whitening is performed.</li> </ul> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 1.3: </span>The default value of <code>whiten</code> changed to ‘unit-variance’ in 1.3.</p> </div> </dd> <dt>
<strong>fun</strong><span class="classifier">{‘logcosh’, ‘exp’, ‘cube’} or callable, default=’logcosh’</span>
</dt>
<dd>
<p>The functional form of the G function used in the approximation to neg-entropy. Could be either ‘logcosh’, ‘exp’, or ‘cube’. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. The derivative should be averaged along its last dimension. Example:</p> <pre data-language="python">def my_g(x):
    return x ** 3, (3 * x ** 2).mean(axis=-1)
</pre> </dd> <dt>
<strong>fun_args</strong><span class="classifier">dict, default=None</span>
</dt>
<dd>
<p>Arguments to send to the functional form. If empty or None and if fun=’logcosh’, fun_args will take value {‘alpha’ : 1.0}.</p> </dd> <dt>
<strong>max_iter</strong><span class="classifier">int, default=200</span>
</dt>
<dd>
<p>Maximum number of iterations to perform.</p> </dd> <dt>
<strong>tol</strong><span class="classifier">float, default=1e-4</span>
</dt>
<dd>
<p>A positive scalar giving the tolerance at which the un-mixing matrix is considered to have converged.</p> </dd> <dt>
<strong>w_init</strong><span class="classifier">ndarray of shape (n_components, n_components), default=None</span>
</dt>
<dd>
<p>Initial un-mixing array. If <code>w_init=None</code>, then an array of values drawn from a normal distribution is used.</p> </dd> <dt>
<strong>whiten_solver</strong><span class="classifier">{“eigh”, “svd”}, default=”svd”</span>
</dt>
<dd>
<p>The solver to use for whitening.</p> <ul class="simple"> <li>“svd” is more stable numerically if the problem is degenerate, and often faster when <code>n_samples &lt;= n_features</code>.</li> <li>“eigh” is generally more memory efficient when <code>n_samples &gt;= n_features</code>, and can be faster when <code>n_samples &gt;= 50 * n_features</code>.</li> </ul> <div class="versionadded"> <p><span class="versionmodified added">Added in version 1.2.</span></p> </div> </dd> <dt>
<strong>random_state</strong><span class="classifier">int, RandomState instance or None, default=None</span>
</dt>
<dd>
<p>Used to initialize <code>w_init</code> when not specified, with a normal distribution. Pass an int, for reproducible results across multiple function calls. See <a class="reference internal" href="https://scikit-learn.org/1.6/glossary.html#term-random_state"><span class="xref std std-term">Glossary</span></a>.</p> </dd> <dt>
<strong>return_X_mean</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>If True, X_mean is returned too.</p> </dd> <dt>
<strong>compute_sources</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>If False, sources are not computed, but only the rotation matrix. This can save memory when working with big data. Defaults to True.</p> </dd> <dt>
<strong>return_n_iter</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>Whether or not to return the number of iterations.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>K</strong><span class="classifier">ndarray of shape (n_components, n_features) or None</span>
</dt>
<dd>
<p>If whiten is ‘True’, K is the pre-whitening matrix that projects data onto the first n_components principal components. If whiten is ‘False’, K is ‘None’.</p> </dd> <dt>
<strong>W</strong><span class="classifier">ndarray of shape (n_components, n_components)</span>
</dt>
<dd>
<p>The square matrix that unmixes the data after whitening. The mixing matrix is the pseudo-inverse of matrix <code>W K</code> if K is not None, else it is the inverse of W.</p> </dd> <dt>
<strong>S</strong><span class="classifier">ndarray of shape (n_samples, n_components) or None</span>
</dt>
<dd>
<p>Estimated source matrix.</p> </dd> <dt>
<strong>X_mean</strong><span class="classifier">ndarray of shape (n_features,)</span>
</dt>
<dd>
<p>The mean over features. Returned only if return_X_mean is True.</p> </dd> <dt>
<strong>n_iter</strong><span class="classifier">int</span>
</dt>
<dd>
<p>If the algorithm is “deflation”, n_iter is the maximum number of iterations run across all components. Else they are just the number of iterations taken to converge. This is returned only when return_n_iter is set to <code>True</code>.</p> </dd> </dl> </dd> </dl> <h4 class="rubric">Notes</h4> <p>The data matrix X is considered to be a linear combination of non-Gaussian (independent) components i.e. X = AS where columns of S contain the independent components and A is a linear mixing matrix. In short ICA attempts to <code>un-mix' the data by estimating an
un-mixing matrix W where ``S = W K X.`</code> While FastICA was proposed to estimate as many sources as features, it is possible to estimate less by setting n_components &lt; n_features. It this case K is not a square matrix and the estimated A is the pseudo-inverse of <code>W K</code>.</p> <p>This implementation was originally made for data of shape [n_features, n_samples]. Now the input is transposed before the algorithm is applied. This makes it slightly faster for Fortran-ordered input.</p> <h4 class="rubric">References</h4> <div role="list" class="citation-list"> <div class="citation" id="r4ef46ec4ecf2-1" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span> <p>A. Hyvarinen and E. Oja, “Fast Independent Component Analysis”, Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430.</p> </div> </div> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from sklearn.datasets import load_digits
&gt;&gt;&gt; from sklearn.decomposition import fastica
&gt;&gt;&gt; X, _ = load_digits(return_X_y=True)
&gt;&gt;&gt; K, W, S = fastica(X, n_components=7, random_state=0, whiten='unit-variance')
&gt;&gt;&gt; K.shape
(7, 64)
&gt;&gt;&gt; W.shape
(7, 7)
&gt;&gt;&gt; S.shape
(1797, 7)
</pre> </dd>
</dl> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2025 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.6/modules/generated/fastica-function.html" class="_attribution-link">https://scikit-learn.org/1.6/modules/generated/fastica-function.html</a>
  </p>
</div>
