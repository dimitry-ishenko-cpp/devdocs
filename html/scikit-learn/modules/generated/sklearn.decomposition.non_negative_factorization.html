<section id="sklearn-decomposition-non-negative-factorization"> <h1>sklearn.decomposition.non_negative_factorization</h1> <dl class="py function"> <dt class="sig sig-object py" id="sklearn.decomposition.non_negative_factorization"> <span class="sig-prename descclassname">sklearn.decomposition.</span><span class="sig-name descname">non_negative_factorization</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">W</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">H</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_components</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">init</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">update_H</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">solver</span><span class="o">=</span><span class="default_value">'cd'</span></em>, <em class="sig-param"><span class="n">beta_loss</span><span class="o">=</span><span class="default_value">'frobenius'</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">max_iter</span><span class="o">=</span><span class="default_value">200</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">'deprecated'</span></em>, <em class="sig-param"><span class="n">alpha_W</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">alpha_H</span><span class="o">=</span><span class="default_value">'same'</span></em>, <em class="sig-param"><span class="n">l1_ratio</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">regularization</span><span class="o">=</span><span class="default_value">'deprecated'</span></em>, <em class="sig-param"><span class="n">random_state</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">shuffle</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/decomposition/_nmf.py#L910"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Compute Non-negative Matrix Factorization (NMF).</p> <p>Find two non-negative matrices (W, H) whose product approximates the non- negative matrix X. This factorization can be used for example for dimensionality reduction, source separation or topic extraction.</p> <p>The objective function is:</p>  <div class="math notranslate nohighlight"> \[ \begin{align}\begin{aligned}L(W, H) &amp;= 0.5 * ||X - WH||_{loss}^2\\&amp;+ alpha\_W * l1\_ratio * n\_features * ||vec(W)||_1\\&amp;+ alpha\_H * l1\_ratio * n\_samples * ||vec(H)||_1\\&amp;+ 0.5 * alpha\_W * (1 - l1\_ratio) * n\_features * ||W||_{Fro}^2\\&amp;+ 0.5 * alpha\_H * (1 - l1\_ratio) * n\_samples * ||H||_{Fro}^2\end{aligned}\end{align} \]</div>  <p>Where:</p> <p><span class="math notranslate nohighlight">\(||A||_{Fro}^2 = \sum_{i,j} A_{ij}^2\)</span> (Frobenius norm)</p> <p><span class="math notranslate nohighlight">\(||vec(A)||_1 = \sum_{i,j} abs(A_{ij})\)</span> (Elementwise L1 norm)</p> <p>The generic norm <span class="math notranslate nohighlight">\(||X - WH||_{loss}^2\)</span> may represent the Frobenius norm or another supported beta-divergence loss. The choice between options is controlled by the <code>beta_loss</code> parameter.</p> <p>The regularization terms are scaled by <code>n_features</code> for <code>W</code> and by <code>n_samples</code> for <code>H</code> to keep their impact balanced with respect to one another and to the data fit term as independent as possible of the size <code>n_samples</code> of the training set.</p> <p>The objective function is minimized with an alternating minimization of W and H. If H is given and update_H=False, it solves for W only.</p> <p>Note that the transformed data is named W and the components matrix is named H. In the NMF literature, the naming convention is usually the opposite since the data matrix X is transposed.</p> <dl class="field-list"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl> <dt>
<strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>Constant matrix.</p> </dd> <dt>
<strong>W</strong><span class="classifier">array-like of shape (n_samples, n_components), default=None</span>
</dt>
<dd>
<p>If init=’custom’, it is used as initial guess for the solution.</p> </dd> <dt>
<strong>H</strong><span class="classifier">array-like of shape (n_components, n_features), default=None</span>
</dt>
<dd>
<p>If init=’custom’, it is used as initial guess for the solution. If update_H=False, it is used as a constant, to solve for W only.</p> </dd> <dt>
<strong>n_components</strong><span class="classifier">int, default=None</span>
</dt>
<dd>
<p>Number of components, if n_components is not set all features are kept.</p> </dd> <dt>
<strong>init</strong><span class="classifier">{‘random’, ‘nndsvd’, ‘nndsvda’, ‘nndsvdar’, ‘custom’}, default=None</span>
</dt>
<dd>
<p>Method used to initialize the procedure.</p> <p>Valid options:</p> <ul class="simple"> <li>None: ‘nndsvda’ if n_components &lt; n_features, otherwise ‘random’.</li> <li>
<dl class="simple"> <dt>‘random’: non-negative random matrices, scaled with:</dt>
<dd>
<p>sqrt(X.mean() / n_components)</p> </dd> </dl> </li> <li>
<dl class="simple"> <dt>‘nndsvd’: Nonnegative Double Singular Value Decomposition (NNDSVD)</dt>
<dd>
<p>initialization (better for sparseness)</p> </dd> </dl> </li> <li>
<dl class="simple"> <dt>‘nndsvda’: NNDSVD with zeros filled with the average of X</dt>
<dd>
<p>(better when sparsity is not desired)</p> </dd> </dl> </li> <li>
<dl class="simple"> <dt>‘nndsvdar’: NNDSVD with zeros filled with small random values</dt>
<dd>
<p>(generally faster, less accurate alternative to NNDSVDa for when sparsity is not desired)</p> </dd> </dl> </li> <li>‘custom’: use custom matrices W and H if <code>update_H=True</code>. If <code>update_H=False</code>, then only custom matrix H is used.</li> </ul> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 0.23: </span>The default value of <code>init</code> changed from ‘random’ to None in 0.23.</p> </div> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 1.1: </span>When <code>init=None</code> and n_components is less than n_samples and n_features defaults to <code>nndsvda</code> instead of <code>nndsvd</code>.</p> </div> </dd> <dt>
<strong>update_H</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>Set to True, both W and H will be estimated from initial guesses. Set to False, only W will be estimated.</p> </dd> <dt>
<strong>solver</strong><span class="classifier">{‘cd’, ‘mu’}, default=’cd’</span>
</dt>
<dd>
<p>Numerical solver to use:</p> <ul class="simple"> <li>
<dl class="simple"> <dt>‘cd’ is a Coordinate Descent solver that uses Fast Hierarchical</dt>
<dd>
<p>Alternating Least Squares (Fast HALS).</p> </dd> </dl> </li> <li>‘mu’ is a Multiplicative Update solver.</li> </ul> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.17: </span>Coordinate Descent solver.</p> </div> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.19: </span>Multiplicative Update solver.</p> </div> </dd> <dt>
<strong>beta_loss</strong><span class="classifier">float or {‘frobenius’, ‘kullback-leibler’, ‘itakura-saito’}, default=’frobenius’</span>
</dt>
<dd>
<p>Beta divergence to be minimized, measuring the distance between X and the dot product WH. Note that values different from ‘frobenius’ (or 2) and ‘kullback-leibler’ (or 1) lead to significantly slower fits. Note that for beta_loss &lt;= 0 (or ‘itakura-saito’), the input matrix X cannot contain zeros. Used only in ‘mu’ solver.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.19.</span></p> </div> </dd> <dt>
<strong>tol</strong><span class="classifier">float, default=1e-4</span>
</dt>
<dd>
<p>Tolerance of the stopping condition.</p> </dd> <dt>
<strong>max_iter</strong><span class="classifier">int, default=200</span>
</dt>
<dd>
<p>Maximum number of iterations before timing out.</p> </dd> <dt>
<strong>alpha</strong><span class="classifier">float, default=0.0</span>
</dt>
<dd>
<p>Constant that multiplies the regularization terms. Set it to zero to have no regularization. When using <code>alpha</code> instead of <code>alpha_W</code> and <code>alpha_H</code>, the regularization terms are not scaled by the <code>n_features</code> (resp. <code>n_samples</code>) factors for <code>W</code> (resp. <code>H</code>).</p> <div class="deprecated"> <p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>The <code>alpha</code> parameter is deprecated in 1.0 and will be removed in 1.2. Use <code>alpha_W</code> and <code>alpha_H</code> instead.</p> </div> </dd> <dt>
<strong>alpha_W</strong><span class="classifier">float, default=0.0</span>
</dt>
<dd>
<p>Constant that multiplies the regularization terms of <code>W</code>. Set it to zero (default) to have no regularization on <code>W</code>.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 1.0.</span></p> </div> </dd> <dt>
<strong>alpha_H</strong><span class="classifier">float or “same”, default=”same”</span>
</dt>
<dd>
<p>Constant that multiplies the regularization terms of <code>H</code>. Set it to zero to have no regularization on <code>H</code>. If “same” (default), it takes the same value as <code>alpha_W</code>.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 1.0.</span></p> </div> </dd> <dt>
<strong>l1_ratio</strong><span class="classifier">float, default=0.0</span>
</dt>
<dd>
<p>The regularization mixing parameter, with 0 &lt;= l1_ratio &lt;= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2.</p> </dd> <dt>
<strong>regularization</strong><span class="classifier">{‘both’, ‘components’, ‘transformation’}, default=None</span>
</dt>
<dd>
<p>Select whether the regularization affects the components (H), the transformation (W), both or none of them.</p> <div class="deprecated"> <p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>The <code>regularization</code> parameter is deprecated in 1.0 and will be removed in 1.2. Use <code>alpha_W</code> and <code>alpha_H</code> instead.</p> </div> </dd> <dt>
<strong>random_state</strong><span class="classifier">int, RandomState instance or None, default=None</span>
</dt>
<dd>
<p>Used for NMF initialisation (when <code>init</code> == ‘nndsvdar’ or ‘random’), and in Coordinate Descent. Pass an int for reproducible results across multiple function calls. See <a class="reference internal" href="https://scikit-learn.org/1.1/glossary.html#term-random_state"><span class="xref std std-term">Glossary</span></a>.</p> </dd> <dt>
<strong>verbose</strong><span class="classifier">int, default=0</span>
</dt>
<dd>
<p>The verbosity level.</p> </dd> <dt>
<strong>shuffle</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>If true, randomize the order of coordinates in the CD solver.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>W</strong><span class="classifier">ndarray of shape (n_samples, n_components)</span>
</dt>
<dd>
<p>Solution to the non-negative least squares problem.</p> </dd> <dt>
<strong>H</strong><span class="classifier">ndarray of shape (n_components, n_features)</span>
</dt>
<dd>
<p>Solution to the non-negative least squares problem.</p> </dd> <dt>
<strong>n_iter</strong><span class="classifier">int</span>
</dt>
<dd>
<p>Actual number of iterations.</p> </dd> </dl> </dd> </dl> <h4 class="rubric">References</h4> <div role="list" class="citation-list"> <div class="citation" id="r409e25584b3c-1" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span> <p><a class="reference external" href="https://doi.org/10.1587/transfun.E92.A.708">“Fast local algorithms for large scale nonnegative matrix and tensor factorizations”</a> Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals of electronics, communications and computer sciences 92.3: 708-721, 2009.</p> </div> <div class="citation" id="r409e25584b3c-2" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span> <p><a class="reference external" href="https://doi.org/10.1162/NECO_a_00168">“Algorithms for nonnegative matrix factorization with the beta-divergence”</a> Fevotte, C., &amp; Idier, J. (2011). Neural Computation, 23(9).</p> </div> </div> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])
&gt;&gt;&gt; from sklearn.decomposition import non_negative_factorization
&gt;&gt;&gt; W, H, n_iter = non_negative_factorization(X, n_components=2,
... init='random', random_state=0)
</pre> </dd>
</dl> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2022 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.1/modules/generated/sklearn.decomposition.non_negative_factorization.html" class="_attribution-link">https://scikit-learn.org/1.1/modules/generated/sklearn.decomposition.non_negative_factorization.html</a>
  </p>
</div>
