<section id="sklearn-neighbors-kdtree"> <h1>sklearn.neighbors.KDTree</h1> <dl class="py class"> <dt class="sig sig-object py" id="sklearn.neighbors.KDTree"> <em class="property">class</em><span class="sig-prename descclassname">sklearn.neighbors.</span><span class="sig-name descname">KDTree</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">leaf_size</span><span class="o">=</span><span class="default_value">40</span></em>, <em class="sig-param"><span class="n">metric</span><span class="o">=</span><span class="default_value">'minkowski'</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span>
</dt> <dd>
<p>KDTree for fast generalized N-point problems</p> <p>Read more in the <a class="reference internal" href="../neighbors.html#unsupervised-neighbors"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>n_samples is the number of points in the data set, and n_features is the dimension of the parameter space. Note: if X is a C-contiguous array of doubles then data will not be copied. Otherwise, an internal copy will be made.</p> </dd> <dt>
<strong>leaf_size</strong><span class="classifier">positive int, default=40</span>
</dt>
<dd>
<p>Number of points at which to switch to brute-force. Changing leaf_size will not affect the results of a query, but can significantly impact the speed of a query and the memory required to store the constructed tree. The amount of memory needed to store the tree scales as approximately n_samples / leaf_size. For a specified <code>leaf_size</code>, a leaf node is guaranteed to satisfy <code>leaf_size &lt;= n_points &lt;= 2 * leaf_size</code>, except in the case that <code>n_samples &lt; leaf_size</code>.</p> </dd> <dt>
<strong>metric</strong><span class="classifier">str or DistanceMetric object, default=’minkowski’</span>
</dt>
<dd>
<p>Metric to use for distance computation. Default is “minkowski”, which results in the standard Euclidean distance when p = 2. kd_tree.valid_metrics gives a list of the metrics which are valid for KDTree. See the documentation of <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/spatial.distance.html">scipy.spatial.distance</a> and the metrics listed in <a class="reference internal" href="sklearn.metrics.pairwise.distance_metrics.html#sklearn.metrics.pairwise.distance_metrics" title="sklearn.metrics.pairwise.distance_metrics"><code>distance_metrics</code></a> for more information.</p> </dd> <dt><strong>Additional keywords are passed to the distance metric class.</strong></dt>
 <dt><strong>Note: Callable functions in the metric parameter are NOT supported for KDTree</strong></dt>
 <dt><strong>and Ball Tree. Function call overhead will result in very poor performance.</strong></dt>
 </dl> </dd> <dt class="field-even">Attributes<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>data</strong><span class="classifier">memory view</span>
</dt>
<dd>
<p>The training data</p> </dd> </dl> </dd> </dl> <h4 class="rubric">Examples</h4> <p>Query for k-nearest neighbors</p> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.neighbors import KDTree
&gt;&gt;&gt; rng = np.random.RandomState(0)
&gt;&gt;&gt; X = rng.random_sample((10, 3))  # 10 points in 3 dimensions
&gt;&gt;&gt; tree = KDTree(X, leaf_size=2)              
&gt;&gt;&gt; dist, ind = tree.query(X[:1], k=3)                
&gt;&gt;&gt; print(ind)  # indices of 3 closest neighbors
[0 3 1]
&gt;&gt;&gt; print(dist)  # distances to 3 closest neighbors
[ 0.          0.19662693  0.29473397]
</pre> <p>Pickle and Unpickle a tree. Note that the state of the tree is saved in the pickle operation: the tree needs not be rebuilt upon unpickling.</p> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import pickle
&gt;&gt;&gt; rng = np.random.RandomState(0)
&gt;&gt;&gt; X = rng.random_sample((10, 3))  # 10 points in 3 dimensions
&gt;&gt;&gt; tree = KDTree(X, leaf_size=2)        
&gt;&gt;&gt; s = pickle.dumps(tree)                     
&gt;&gt;&gt; tree_copy = pickle.loads(s)                
&gt;&gt;&gt; dist, ind = tree_copy.query(X[:1], k=3)     
&gt;&gt;&gt; print(ind)  # indices of 3 closest neighbors
[0 3 1]
&gt;&gt;&gt; print(dist)  # distances to 3 closest neighbors
[ 0.          0.19662693  0.29473397]
</pre> <p>Query for neighbors within a given radius</p> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; rng = np.random.RandomState(0)
&gt;&gt;&gt; X = rng.random_sample((10, 3))  # 10 points in 3 dimensions
&gt;&gt;&gt; tree = KDTree(X, leaf_size=2)     
&gt;&gt;&gt; print(tree.query_radius(X[:1], r=0.3, count_only=True))
3
&gt;&gt;&gt; ind = tree.query_radius(X[:1], r=0.3)  
&gt;&gt;&gt; print(ind)  # indices of neighbors within distance 0.3
[3 0 1]
</pre> <p>Compute a gaussian kernel density estimate:</p> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; rng = np.random.RandomState(42)
&gt;&gt;&gt; X = rng.random_sample((100, 3))
&gt;&gt;&gt; tree = KDTree(X)                
&gt;&gt;&gt; tree.kernel_density(X[:3], h=0.1, kernel='gaussian')
array([ 6.94114649,  7.83281226,  7.2071716 ])
</pre> <p>Compute a two-point auto-correlation function</p> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; rng = np.random.RandomState(0)
&gt;&gt;&gt; X = rng.random_sample((30, 3))
&gt;&gt;&gt; r = np.linspace(0, 1, 5)
&gt;&gt;&gt; tree = KDTree(X)                
&gt;&gt;&gt; tree.two_point_correlation(X, r)
array([ 30,  62, 278, 580, 820])
</pre> <h4 class="rubric">Methods</h4> <table class="autosummary longtable docutils align-default">  <tr>
<td><p><a class="reference internal" href="#sklearn.neighbors.KDTree.get_arrays" title="sklearn.neighbors.KDTree.get_arrays"><code>get_arrays</code></a>()</p></td> <td><p>Get data and node arrays.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.neighbors.KDTree.get_n_calls" title="sklearn.neighbors.KDTree.get_n_calls"><code>get_n_calls</code></a>()</p></td> <td><p>Get number of calls.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.neighbors.KDTree.get_tree_stats" title="sklearn.neighbors.KDTree.get_tree_stats"><code>get_tree_stats</code></a>()</p></td> <td><p>Get tree status.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.neighbors.KDTree.kernel_density" title="sklearn.neighbors.KDTree.kernel_density"><code>kernel_density</code></a>(X, h[, kernel, atol, rtol, ...])</p></td> <td><p>Compute the kernel density estimate at points X with the given kernel, using the distance metric specified at tree creation.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.neighbors.KDTree.query" title="sklearn.neighbors.KDTree.query"><code>query</code></a>(X[, k, return_distance, dualtree, ...])</p></td> <td><p>query the tree for the k nearest neighbors</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.neighbors.KDTree.query_radius" title="sklearn.neighbors.KDTree.query_radius"><code>query_radius</code></a>(X, r[, return_distance, ...])</p></td> <td><p>query the tree for neighbors within a radius r</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.neighbors.KDTree.reset_n_calls" title="sklearn.neighbors.KDTree.reset_n_calls"><code>reset_n_calls</code></a>()</p></td> <td><p>Reset number of calls to 0.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.neighbors.KDTree.two_point_correlation" title="sklearn.neighbors.KDTree.two_point_correlation"><code>two_point_correlation</code></a>(X, r[, dualtree])</p></td> <td><p>Compute the two-point correlation function</p></td> </tr>  </table> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.neighbors.KDTree.get_arrays"> <span class="sig-name descname">get_arrays</span><span class="sig-paren">(</span><span class="sig-paren">)</span>
</dt> <dd>
<p>Get data and node arrays.</p> <dl class="field-list simple"> <dt class="field-odd">Returns<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>arrays: tuple of array</dt>
<dd>
<p>Arrays for storing tree data, index, node data and node bounds.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.neighbors.KDTree.get_n_calls"> <span class="sig-name descname">get_n_calls</span><span class="sig-paren">(</span><span class="sig-paren">)</span>
</dt> <dd>
<p>Get number of calls.</p> <dl class="field-list simple"> <dt class="field-odd">Returns<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>n_calls: int</dt>
<dd>
<p>number of distance computation calls</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.neighbors.KDTree.get_tree_stats"> <span class="sig-name descname">get_tree_stats</span><span class="sig-paren">(</span><span class="sig-paren">)</span>
</dt> <dd>
<p>Get tree status.</p> <dl class="field-list simple"> <dt class="field-odd">Returns<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>tree_stats: tuple of int</dt>
<dd>
<p>(number of trims, number of leaves, number of splits)</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.neighbors.KDTree.kernel_density"> <span class="sig-name descname">kernel_density</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">h</span></em>, <em class="sig-param"><span class="n">kernel</span><span class="o">=</span><span class="default_value">'gaussian'</span></em>, <em class="sig-param"><span class="n">atol</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">rtol</span><span class="o">=</span><span class="default_value">1E-8</span></em>, <em class="sig-param"><span class="n">breadth_first</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">return_log</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span>
</dt> <dd>
<p>Compute the kernel density estimate at points X with the given kernel, using the distance metric specified at tree creation.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>An array of points to query. Last dimension should match dimension of training data.</p> </dd> <dt>
<strong>h</strong><span class="classifier">float</span>
</dt>
<dd>
<p>the bandwidth of the kernel</p> </dd> <dt>
<strong>kernel</strong><span class="classifier">str, default=”gaussian”</span>
</dt>
<dd>
<p>specify the kernel to use. Options are - ‘gaussian’ - ‘tophat’ - ‘epanechnikov’ - ‘exponential’ - ‘linear’ - ‘cosine’ Default is kernel = ‘gaussian’</p> </dd> <dt>
<strong>atol</strong><span class="classifier">float, default=0</span>
</dt>
<dd>
<p>Specify the desired absolute tolerance of the result. If the true result is <code>K_true</code>, then the returned result <code>K_ret</code> satisfies <code>abs(K_true - K_ret) &lt; atol + rtol * K_ret</code> The default is zero (i.e. machine precision).</p> </dd> <dt>
<strong>rtol</strong><span class="classifier">float, default=1e-8</span>
</dt>
<dd>
<p>Specify the desired relative tolerance of the result. If the true result is <code>K_true</code>, then the returned result <code>K_ret</code> satisfies <code>abs(K_true - K_ret) &lt; atol + rtol * K_ret</code> The default is <code>1e-8</code> (i.e. machine precision).</p> </dd> <dt>
<strong>breadth_first</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>If True, use a breadth-first search. If False (default) use a depth-first search. Breadth-first is generally faster for compact kernels and/or high tolerances.</p> </dd> <dt>
<strong>return_log</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>Return the logarithm of the result. This can be more accurate than returning the result itself for narrow kernels.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>density</strong><span class="classifier">ndarray of shape X.shape[:-1]</span>
</dt>
<dd>
<p>The array of (log)-density evaluations</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.neighbors.KDTree.query"> <span class="sig-name descname">query</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">k</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">return_distance</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">dualtree</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">breadth_first</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span>
</dt> <dd>
<p>query the tree for the k nearest neighbors</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>An array of points to query</p> </dd> <dt>
<strong>k</strong><span class="classifier">int, default=1</span>
</dt>
<dd>
<p>The number of nearest neighbors to return</p> </dd> <dt>
<strong>return_distance</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>if True, return a tuple (d, i) of distances and indices if False, return array i</p> </dd> <dt>
<strong>dualtree</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>if True, use the dual tree formalism for the query: a tree is built for the query points, and the pair of trees is used to efficiently search this space. This can lead to better performance as the number of points grows large.</p> </dd> <dt>
<strong>breadth_first</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>if True, then query the nodes in a breadth-first manner. Otherwise, query the nodes in a depth-first manner.</p> </dd> <dt>
<strong>sort_results</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>if True, then distances and indices of each point are sorted on return, so that the first column contains the closest points. Otherwise, neighbors are returned in an arbitrary order.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>i</strong><span class="classifier">if return_distance == False</span>
</dt>
 <dt>
<strong>(d,i)</strong><span class="classifier">if return_distance == True</span>
</dt>
 <dt>
<strong>d</strong><span class="classifier">ndarray of shape X.shape[:-1] + (k,), dtype=double</span>
</dt>
<dd>
<p>Each entry gives the list of distances to the neighbors of the corresponding point.</p> </dd> <dt>
<strong>i</strong><span class="classifier">ndarray of shape X.shape[:-1] + (k,), dtype=int</span>
</dt>
<dd>
<p>Each entry gives the list of indices of neighbors of the corresponding point.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.neighbors.KDTree.query_radius"> <span class="sig-name descname">query_radius</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">r</span></em>, <em class="sig-param"><span class="n">return_distance</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">count_only</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">sort_results</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span>
</dt> <dd>
<p>query the tree for neighbors within a radius r</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>An array of points to query</p> </dd> <dt>
<strong>r</strong><span class="classifier">distance within which neighbors are returned</span>
</dt>
<dd>
<p>r can be a single value, or an array of values of shape x.shape[:-1] if different radii are desired for each point.</p> </dd> <dt>
<strong>return_distance</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>if True, return distances to neighbors of each point if False, return only neighbors Note that unlike the query() method, setting return_distance=True here adds to the computation time. Not all distances need to be calculated explicitly for return_distance=False. Results are not sorted by default: see <code>sort_results</code> keyword.</p> </dd> <dt>
<strong>count_only</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>if True, return only the count of points within distance r if False, return the indices of all points within distance r If return_distance==True, setting count_only=True will result in an error.</p> </dd> <dt>
<strong>sort_results</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>if True, the distances and indices will be sorted before being returned. If False, the results will not be sorted. If return_distance == False, setting sort_results = True will result in an error.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>count</strong><span class="classifier">if count_only == True</span>
</dt>
 <dt>
<strong>ind</strong><span class="classifier">if count_only == False and return_distance == False</span>
</dt>
 <dt>
<strong>(ind, dist)</strong><span class="classifier">if count_only == False and return_distance == True</span>
</dt>
 <dt>
<strong>count</strong><span class="classifier">ndarray of shape X.shape[:-1], dtype=int</span>
</dt>
<dd>
<p>Each entry gives the number of neighbors within a distance r of the corresponding point.</p> </dd> <dt>
<strong>ind</strong><span class="classifier">ndarray of shape X.shape[:-1], dtype=object</span>
</dt>
<dd>
<p>Each element is a numpy integer array listing the indices of neighbors of the corresponding point. Note that unlike the results of a k-neighbors query, the returned neighbors are not sorted by distance by default.</p> </dd> <dt>
<strong>dist</strong><span class="classifier">ndarray of shape X.shape[:-1], dtype=object</span>
</dt>
<dd>
<p>Each element is a numpy double array listing the distances corresponding to indices in i.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.neighbors.KDTree.reset_n_calls"> <span class="sig-name descname">reset_n_calls</span><span class="sig-paren">(</span><span class="sig-paren">)</span>
</dt> <dd>
<p>Reset number of calls to 0.</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.neighbors.KDTree.two_point_correlation"> <span class="sig-name descname">two_point_correlation</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">r</span></em>, <em class="sig-param"><span class="n">dualtree</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span>
</dt> <dd>
<p>Compute the two-point correlation function</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>An array of points to query. Last dimension should match dimension of training data.</p> </dd> <dt>
<strong>r</strong><span class="classifier">array-like</span>
</dt>
<dd>
<p>A one-dimensional array of distances</p> </dd> <dt>
<strong>dualtree</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>If True, use a dualtree algorithm. Otherwise, use a single-tree algorithm. Dual tree algorithms can have better scaling for large N.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>counts</strong><span class="classifier">ndarray</span>
</dt>
<dd>
<p>counts[i] contains the number of pairs of points with distance less than or equal to r[i]</p> </dd> </dl> </dd> </dl> </dd>
</dl> </dd>
</dl> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2022 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.1/modules/generated/sklearn.neighbors.KDTree.html" class="_attribution-link">https://scikit-learn.org/1.1/modules/generated/sklearn.neighbors.KDTree.html</a>
  </p>
</div>
