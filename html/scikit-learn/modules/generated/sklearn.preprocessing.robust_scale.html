<section id="robust-scale"> <h1>robust_scale</h1> <dl class="py function"> <dt class="sig sig-object py" id="sklearn.preprocessing.robust_scale"> <span class="sig-prename descclassname">sklearn.preprocessing.</span><span class="sig-name descname">robust_scale</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">axis</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">with_centering</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">with_scaling</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">quantile_range</span><span class="o">=</span><span class="default_value">(25.0, 75.0)</span></em>, <em class="sig-param"><span class="n">copy</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">unit_variance</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/99bf3d8e4/sklearn/preprocessing/_data.py#L1747"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Standardize a dataset along any axis.</p> <p>Center to the median and component wise scale according to the interquartile range.</p> <p>Read more in the <a class="reference internal" href="../preprocessing.html#preprocessing-scaler"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl> <dt>
<strong>X</strong><span class="classifier">{array-like, sparse matrix} of shape (n_sample, n_features)</span>
</dt>
<dd>
<p>The data to center and scale.</p> </dd> <dt>
<strong>axis</strong><span class="classifier">int, default=0</span>
</dt>
<dd>
<p>Axis used to compute the medians and IQR along. If 0, independently scale each feature, otherwise (if 1) scale each sample.</p> </dd> <dt>
<strong>with_centering</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>If <code>True</code>, center the data before scaling.</p> </dd> <dt>
<strong>with_scaling</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>If <code>True</code>, scale the data to unit variance (or equivalently, unit standard deviation).</p> </dd> <dt>
<strong>quantile_range</strong><span class="classifier">tuple (q_min, q_max), 0.0 &lt; q_min &lt; q_max &lt; 100.0, default=(25.0, 75.0)</span>
</dt>
<dd>
<p>Quantile range used to calculate <code>scale_</code>. By default this is equal to the IQR, i.e., <code>q_min</code> is the first quantile and <code>q_max</code> is the third quantile.</p> <div class="versionadded"> <p><span class="versionmodified added">Added in version 0.18.</span></p> </div> </dd> <dt>
<strong>copy</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>If False, try to avoid a copy and scale in place. This is not guaranteed to always work in place; e.g. if the data is a numpy array with an int dtype, a copy will be returned even with copy=False.</p> </dd> <dt>
<strong>unit_variance</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>If <code>True</code>, scale data so that normally distributed features have a variance of 1. In general, if the difference between the x-values of <code>q_max</code> and <code>q_min</code> for a standard normal distribution is greater than 1, the dataset will be scaled down. If less than 1, the dataset will be scaled up.</p> <div class="versionadded"> <p><span class="versionmodified added">Added in version 0.24.</span></p> </div> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>X_tr</strong><span class="classifier">{ndarray, sparse matrix} of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>The transformed data.</p> </dd> </dl> </dd> </dl> <div class="admonition seealso"> <p class="admonition-title">See also</p> <dl class="simple"> <dt><a class="reference internal" href="sklearn.preprocessing.robustscaler.html#sklearn.preprocessing.RobustScaler" title="sklearn.preprocessing.RobustScaler"><code>RobustScaler</code></a></dt>
<dd>
<p>Performs centering and scaling using the Transformer API (e.g. as part of a preprocessing <a class="reference internal" href="sklearn.pipeline.pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code>Pipeline</code></a>).</p> </dd> </dl> </div> <h4 class="rubric">Notes</h4> <p>This implementation will refuse to center scipy.sparse matrices since it would make them non-sparse and would potentially crash the program with memory exhaustion problems.</p> <p>Instead the caller is expected to either set explicitly <code>with_centering=False</code> (in that case, only variance scaling will be performed on the features of the CSR matrix) or to call <code>X.toarray()</code> if he/she expects the materialized dense array to fit in memory.</p> <p>To avoid memory copy the caller should pass a CSR matrix.</p> <p>For a comparison of the different scalers, transformers, and normalizers, see: <a class="reference internal" href="../../auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py"><span class="std std-ref">Compare the effect of different scalers on data with outliers</span></a>.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Risk of data leak</p> <p>Do not use <a class="reference internal" href="#sklearn.preprocessing.robust_scale" title="sklearn.preprocessing.robust_scale"><code>robust_scale</code></a> unless you know what you are doing. A common mistake is to apply it to the entire data <em>before</em> splitting into training and test sets. This will bias the model evaluation because information would have leaked from the test set to the training set. In general, we recommend using <a class="reference internal" href="sklearn.preprocessing.robustscaler.html#sklearn.preprocessing.RobustScaler" title="sklearn.preprocessing.RobustScaler"><code>RobustScaler</code></a> within a <a class="reference internal" href="../compose.html#pipeline"><span class="std std-ref">Pipeline</span></a> in order to prevent most risks of data leaking: <code>pipe = make_pipeline(RobustScaler(), LogisticRegression())</code>.</p> </div> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from sklearn.preprocessing import robust_scale
&gt;&gt;&gt; X = [[-2, 1, 2], [-1, 0, 1]]
&gt;&gt;&gt; robust_scale(X, axis=0)  # scale each column independently
array([[-1.,  1.,  1.],
       [ 1., -1., -1.]])
&gt;&gt;&gt; robust_scale(X, axis=1)  # scale each row independently
array([[-1.5,  0. ,  0.5],
       [-1. ,  0. ,  1. ]])
</pre> </dd>
</dl> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2025 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.robust_scale.html" class="_attribution-link">https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.robust_scale.html</a>
  </p>
</div>
