<section id="pairwise-metrics-affinities-and-kernels"> <h1 id="metrics">6.8. Pairwise metrics, Affinities and Kernels</h1> <p>The <a class="reference internal" href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code>sklearn.metrics.pairwise</code></a> submodule implements utilities to evaluate pairwise distances or affinity of sets of samples.</p> <p>This module contains both distance metrics and kernels. A brief summary is given on the two here.</p> <p>Distance metrics are functions <code>d(a, b)</code> such that <code>d(a, b) &lt; d(a, c)</code> if objects <code>a</code> and <code>b</code> are considered “more similar” than objects <code>a</code> and <code>c</code>. Two objects exactly alike would have a distance of zero. One of the most popular examples is Euclidean distance. To be a ‘true’ metric, it must obey the following four conditions:</p> <pre data-language="python">1. d(a, b) &gt;= 0, for all a and b
2. d(a, b) == 0, if and only if a = b, positive definiteness
3. d(a, b) == d(b, a), symmetry
4. d(a, c) &lt;= d(a, b) + d(b, c), the triangle inequality
</pre> <p>Kernels are measures of similarity, i.e. <code>s(a, b) &gt; s(a, c)</code> if objects <code>a</code> and <code>b</code> are considered “more similar” than objects <code>a</code> and <code>c</code>. A kernel must also be positive semi-definite.</p> <p>There are a number of ways to convert between a distance metric and a similarity measure, such as a kernel. Let <code>D</code> be the distance, and <code>S</code> be the kernel:</p>  <ol class="arabic simple"> <li>
<code>S = np.exp(-D * gamma)</code>, where one heuristic for choosing <code>gamma</code> is <code>1 / num_features</code>
</li> <li><code>S = 1. / (D / np.max(D))</code></li> </ol>  <p>The distances between the row vectors of <code>X</code> and the row vectors of <code>Y</code> can be evaluated using <a class="reference internal" href="generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances" title="sklearn.metrics.pairwise_distances"><code>pairwise_distances</code></a>. If <code>Y</code> is omitted the pairwise distances of the row vectors of <code>X</code> are calculated. Similarly, <a class="reference internal" href="generated/sklearn.metrics.pairwise.pairwise_kernels.html#sklearn.metrics.pairwise.pairwise_kernels" title="sklearn.metrics.pairwise.pairwise_kernels"><code>pairwise.pairwise_kernels</code></a> can be used to calculate the kernel between <code>X</code> and <code>Y</code> using different kernel functions. See the API reference for more details.</p> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.metrics import pairwise_distances
&gt;&gt;&gt; from sklearn.metrics.pairwise import pairwise_kernels
&gt;&gt;&gt; X = np.array([[2, 3], [3, 5], [5, 8]])
&gt;&gt;&gt; Y = np.array([[1, 0], [2, 1]])
&gt;&gt;&gt; pairwise_distances(X, Y, metric='manhattan')
array([[ 4.,  2.],
       [ 7.,  5.],
       [12., 10.]])
&gt;&gt;&gt; pairwise_distances(X, metric='manhattan')
array([[0., 3., 8.],
       [3., 0., 5.],
       [8., 5., 0.]])
&gt;&gt;&gt; pairwise_kernels(X, Y, metric='linear')
array([[ 2.,  7.],
       [ 3., 11.],
       [ 5., 18.]])
</pre> <section id="cosine-similarity"> <h2 id="id1">
<span class="section-number">6.8.1. </span>Cosine similarity</h2> <p><a class="reference internal" href="generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn.metrics.pairwise.cosine_similarity" title="sklearn.metrics.pairwise.cosine_similarity"><code>cosine_similarity</code></a> computes the L2-normalized dot product of vectors. That is, if <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are row vectors, their cosine similarity <span class="math notranslate nohighlight">\(k\)</span> is defined as:</p> <div class="math notranslate nohighlight"> \[k(x, y) = \frac{x y^\top}{\|x\| \|y\|}\]</div> <p>This is called cosine similarity, because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors.</p> <p>This kernel is a popular choice for computing the similarity of documents represented as tf-idf vectors. <a class="reference internal" href="generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn.metrics.pairwise.cosine_similarity" title="sklearn.metrics.pairwise.cosine_similarity"><code>cosine_similarity</code></a> accepts <code>scipy.sparse</code> matrices. (Note that the tf-idf functionality in <code>sklearn.feature_extraction.text</code> can produce normalized vectors, in which case <a class="reference internal" href="generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn.metrics.pairwise.cosine_similarity" title="sklearn.metrics.pairwise.cosine_similarity"><code>cosine_similarity</code></a> is equivalent to <a class="reference internal" href="generated/sklearn.metrics.pairwise.linear_kernel.html#sklearn.metrics.pairwise.linear_kernel" title="sklearn.metrics.pairwise.linear_kernel"><code>linear_kernel</code></a>, only slower.)</p> <aside class="topic"> <p class="topic-title">References:</p> <ul class="simple"> <li>C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to Information Retrieval. Cambridge University Press. <a class="reference external" href="https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html">https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html</a>
</li> </ul> </aside> </section> <section id="linear-kernel"> <h2 id="id2">
<span class="section-number">6.8.2. </span>Linear kernel</h2> <p>The function <a class="reference internal" href="generated/sklearn.metrics.pairwise.linear_kernel.html#sklearn.metrics.pairwise.linear_kernel" title="sklearn.metrics.pairwise.linear_kernel"><code>linear_kernel</code></a> computes the linear kernel, that is, a special case of <a class="reference internal" href="generated/sklearn.metrics.pairwise.polynomial_kernel.html#sklearn.metrics.pairwise.polynomial_kernel" title="sklearn.metrics.pairwise.polynomial_kernel"><code>polynomial_kernel</code></a> with <code>degree=1</code> and <code>coef0=0</code> (homogeneous). If <code>x</code> and <code>y</code> are column vectors, their linear kernel is:</p> <div class="math notranslate nohighlight"> \[k(x, y) = x^\top y\]</div> </section> <section id="polynomial-kernel"> <h2 id="id3">
<span class="section-number">6.8.3. </span>Polynomial kernel</h2> <p>The function <a class="reference internal" href="generated/sklearn.metrics.pairwise.polynomial_kernel.html#sklearn.metrics.pairwise.polynomial_kernel" title="sklearn.metrics.pairwise.polynomial_kernel"><code>polynomial_kernel</code></a> computes the degree-d polynomial kernel between two vectors. The polynomial kernel represents the similarity between two vectors. Conceptually, the polynomial kernels considers not only the similarity between vectors under the same dimension, but also across dimensions. When used in machine learning algorithms, this allows to account for feature interaction.</p> <p>The polynomial kernel is defined as:</p> <div class="math notranslate nohighlight"> \[k(x, y) = (\gamma x^\top y +c_0)^d\]</div> <p>where:</p>  <ul class="simple"> <li>
<code>x</code>, <code>y</code> are the input vectors</li> <li>
<code>d</code> is the kernel degree</li> </ul>  <p>If <span class="math notranslate nohighlight">\(c_0 = 0\)</span> the kernel is said to be homogeneous.</p> </section> <section id="sigmoid-kernel"> <h2 id="id4">
<span class="section-number">6.8.4. </span>Sigmoid kernel</h2> <p>The function <a class="reference internal" href="generated/sklearn.metrics.pairwise.sigmoid_kernel.html#sklearn.metrics.pairwise.sigmoid_kernel" title="sklearn.metrics.pairwise.sigmoid_kernel"><code>sigmoid_kernel</code></a> computes the sigmoid kernel between two vectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer Perceptron (because, in the neural network field, it is often used as neuron activation function). It is defined as:</p> <div class="math notranslate nohighlight"> \[k(x, y) = \tanh( \gamma x^\top y + c_0)\]</div> <p>where:</p>  <ul class="simple"> <li>
<code>x</code>, <code>y</code> are the input vectors</li> <li>
<span class="math notranslate nohighlight">\(\gamma\)</span> is known as slope</li> <li>
<span class="math notranslate nohighlight">\(c_0\)</span> is known as intercept</li> </ul>  </section> <section id="rbf-kernel"> <h2 id="id5">
<span class="section-number">6.8.5. </span>RBF kernel</h2> <p>The function <a class="reference internal" href="generated/sklearn.metrics.pairwise.rbf_kernel.html#sklearn.metrics.pairwise.rbf_kernel" title="sklearn.metrics.pairwise.rbf_kernel"><code>rbf_kernel</code></a> computes the radial basis function (RBF) kernel between two vectors. This kernel is defined as:</p> <div class="math notranslate nohighlight"> \[k(x, y) = \exp( -\gamma \| x-y \|^2)\]</div> <p>where <code>x</code> and <code>y</code> are the input vectors. If <span class="math notranslate nohighlight">\(\gamma = \sigma^{-2}\)</span> the kernel is known as the Gaussian kernel of variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p> </section> <section id="laplacian-kernel"> <h2 id="id6">
<span class="section-number">6.8.6. </span>Laplacian kernel</h2> <p>The function <a class="reference internal" href="generated/sklearn.metrics.pairwise.laplacian_kernel.html#sklearn.metrics.pairwise.laplacian_kernel" title="sklearn.metrics.pairwise.laplacian_kernel"><code>laplacian_kernel</code></a> is a variant on the radial basis function kernel defined as:</p> <div class="math notranslate nohighlight"> \[k(x, y) = \exp( -\gamma \| x-y \|_1)\]</div> <p>where <code>x</code> and <code>y</code> are the input vectors and <span class="math notranslate nohighlight">\(\|x-y\|_1\)</span> is the Manhattan distance between the input vectors.</p> <p>It has proven useful in ML applied to noiseless data. See e.g. <a class="reference external" href="https://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/">Machine learning for quantum mechanics in a nutshell</a>.</p> </section> <section id="chi-squared-kernel"> <h2 id="chi2-kernel">
<span class="section-number">6.8.7. </span>Chi-squared kernel</h2> <p>The chi-squared kernel is a very popular choice for training non-linear SVMs in computer vision applications. It can be computed using <a class="reference internal" href="generated/sklearn.metrics.pairwise.chi2_kernel.html#sklearn.metrics.pairwise.chi2_kernel" title="sklearn.metrics.pairwise.chi2_kernel"><code>chi2_kernel</code></a> and then passed to an <a class="reference internal" href="generated/sklearn.svm.svc.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code>SVC</code></a> with <code>kernel="precomputed"</code>:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.svm import SVC
&gt;&gt;&gt; from sklearn.metrics.pairwise import chi2_kernel
&gt;&gt;&gt; X = [[0, 1], [1, 0], [.2, .8], [.7, .3]]
&gt;&gt;&gt; y = [0, 1, 0, 1]
&gt;&gt;&gt; K = chi2_kernel(X, gamma=.5)
&gt;&gt;&gt; K
array([[1.        , 0.36787944, 0.89483932, 0.58364548],
       [0.36787944, 1.        , 0.51341712, 0.83822343],
       [0.89483932, 0.51341712, 1.        , 0.7768366 ],
       [0.58364548, 0.83822343, 0.7768366 , 1.        ]])

&gt;&gt;&gt; svm = SVC(kernel='precomputed').fit(K, y)
&gt;&gt;&gt; svm.predict(K)
array([0, 1, 0, 1])
</pre> <p>It can also be directly used as the <code>kernel</code> argument:</p> <pre data-language="python">&gt;&gt;&gt; svm = SVC(kernel=chi2_kernel).fit(X, y)
&gt;&gt;&gt; svm.predict(X)
array([0, 1, 0, 1])
</pre> <p>The chi squared kernel is given by</p> <div class="math notranslate nohighlight"> \[k(x, y) = \exp \left (-\gamma \sum_i \frac{(x[i] - y[i]) ^ 2}{x[i] + y[i]} \right )\]</div> <p>The data is assumed to be non-negative, and is often normalized to have an L1-norm of one. The normalization is rationalized with the connection to the chi squared distance, which is a distance between discrete probability distributions.</p> <p>The chi squared kernel is most commonly used on histograms (bags) of visual words.</p> <aside class="topic"> <p class="topic-title">References:</p> <ul class="simple"> <li>Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C. Local features and kernels for classification of texture and object categories: A comprehensive study International Journal of Computer Vision 2007 <a class="reference external" href="https://hal.archives-ouvertes.fr/hal-00171412/document">https://hal.archives-ouvertes.fr/hal-00171412/document</a>
</li> </ul> </aside> </section> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2022 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.1/modules/metrics.html" class="_attribution-link">https://scikit-learn.org/1.1/modules/metrics.html</a>
  </p>
</div>
