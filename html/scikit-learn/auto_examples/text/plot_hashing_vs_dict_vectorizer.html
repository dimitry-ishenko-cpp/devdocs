<div class="sphx-glr-download-link-note admonition note"> <p class="admonition-title">Note</p> <p>Click <a class="reference internal" href="#sphx-glr-download-auto-examples-text-plot-hashing-vs-dict-vectorizer-py"><span class="std std-ref">here</span></a> to download the full example code or to run this example in your browser via Binder</p> </div> <section class="sphx-glr-example-title" id="featurehasher-and-dictvectorizer-comparison"> <h1 id="sphx-glr-auto-examples-text-plot-hashing-vs-dict-vectorizer-py">FeatureHasher and DictVectorizer Comparison</h1> <p>In this example we illustrate text vectorization, which is the process of representing non-numerical input data (such as dictionaries or text documents) as vectors of real numbers.</p> <p>We first compare <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.featurehasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code>FeatureHasher</code></a> and <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.dictvectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code>DictVectorizer</code></a> by using both methods to vectorize text documents that are preprocessed (tokenized) with the help of a custom Python function.</p> <p>Later we introduce and analyze the text-specific vectorizers <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.hashingvectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code>HashingVectorizer</code></a>, <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.countvectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code>CountVectorizer</code></a> and <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.tfidfvectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code>TfidfVectorizer</code></a> that handle both the tokenization and the assembling of the feature matrix within a single class.</p> <p>The objective of the example is to demonstrate the usage of text vectorization API and to compare their processing time. See the example scripts <a class="reference internal" href="plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py"><span class="std std-ref">Classification of text documents using sparse features</span></a> and <a class="reference internal" href="plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py"><span class="std std-ref">Clustering text documents using k-means</span></a> for actual learning on text documents.</p> <pre data-language="python"># Author: Lars Buitinck
#         Olivier Grisel &lt;olivier.grisel@ensta.org&gt;
#         Arturo Amor &lt;david-arturo.amor-quiroz@inria.fr&gt;
# License: BSD 3 clause
</pre> <section id="load-data"> <h2>Load Data</h2> <p>We load data from <a class="reference internal" href="https://scikit-learn.org/1.1/datasets/real_world.html#newsgroups-dataset"><span class="std std-ref">The 20 newsgroups text dataset</span></a>, which comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training and one for testing. For the sake of simplicity and reducing the computational cost, we select a subset of 7 topics and use the training set only.</p> <pre data-language="python">from sklearn.datasets import fetch_20newsgroups

categories = [
    "alt.atheism",
    "comp.graphics",
    "comp.sys.ibm.pc.hardware",
    "misc.forsale",
    "rec.autos",
    "sci.space",
    "talk.religion.misc",
]

print("Loading 20 newsgroups training data")
raw_data, _ = fetch_20newsgroups(subset="train", categories=categories, return_X_y=True)
data_size_mb = sum(len(s.encode("utf-8")) for s in raw_data) / 1e6
print(f"{len(raw_data)} documents - {data_size_mb:.3f}MB")
</pre> <pre data-language="none">Loading 20 newsgroups training data
3803 documents - 6.245MB
</pre> </section> <section id="define-preprocessing-functions"> <h2>Define preprocessing functions</h2> <p>A token may be a word, part of a word or anything comprised between spaces or symbols in a string. Here we define a function that extracts the tokens using a simple regular expression (regex) that matches Unicode word characters. This includes most characters that can be part of a word in any language, as well as numbers and the underscore:</p> <pre data-language="python">import re


def tokenize(doc):
    """Extract tokens from doc.

    This uses a simple regex that matches word characters to break strings
    into tokens. For a more principled approach, see CountVectorizer or
    TfidfVectorizer.
    """
    return (tok.lower() for tok in re.findall(r"\w+", doc))


list(tokenize("This is a simple example, isn't it?"))
</pre> <pre data-language="none">['this', 'is', 'a', 'simple', 'example', 'isn', 't', 'it']
</pre> <p>We define an additional function that counts the (frequency of) occurrence of each token in a given document. It returns a frequency dictionary to be used by the vectorizers.</p> <pre data-language="python">from collections import defaultdict


def token_freqs(doc):
    """Extract a dict mapping tokens from doc to their occurrences."""

    freq = defaultdict(int)
    for tok in tokenize(doc):
        freq[tok] += 1
    return freq


token_freqs("That is one example, but this is another one")
</pre> <pre data-language="none">defaultdict(&lt;class 'int'&gt;, {'that': 1, 'is': 2, 'one': 2, 'example': 1, 'but': 1, 'this': 1, 'another': 1})
</pre> <p>Observe in particular that the repeated token <code>"is"</code> is counted twice for instance.</p> <p>Breaking a text document into word tokens, potentially losing the order information between the words in a sentence is often called a <a class="reference external" href="https://en.wikipedia.org/wiki/Bag-of-words_model">Bag of Words representation</a>.</p> </section> <section id="dictvectorizer"> <h2>DictVectorizer</h2> <p>First we benchmark the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.dictvectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code>DictVectorizer</code></a>, then we compare it to <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.featurehasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code>FeatureHasher</code></a> as both of them receive dictionaries as input.</p> <pre data-language="python">from time import time
from sklearn.feature_extraction import DictVectorizer

dict_count_vectorizers = defaultdict(list)

t0 = time()
vectorizer = DictVectorizer()
vectorizer.fit_transform(token_freqs(d) for d in raw_data)
duration = time() - t0
dict_count_vectorizers["vectorizer"].append(
    vectorizer.__class__.__name__ + "\non freq dicts"
)
dict_count_vectorizers["speed"].append(data_size_mb / duration)
print(f"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s")
print(f"Found {len(vectorizer.get_feature_names_out())} unique terms")
</pre> <pre data-language="none">done in 0.989 s at 6.3 MB/s
Found 47928 unique terms
</pre> <p>The actual mapping from text token to column index is explicitly stored in the <code>.vocabulary_</code> attribute which is a potentially very large Python dictionary:</p> <pre data-language="python">type(vectorizer.vocabulary_)
</pre> <pre data-language="python">len(vectorizer.vocabulary_)
</pre> <pre data-language="none">47928
</pre> <pre data-language="python">vectorizer.vocabulary_["example"]
</pre> <pre data-language="none">19145
</pre> </section> <section id="featurehasher"> <h2>FeatureHasher</h2> <p>Dictionaries take up a large amount of storage space and grow in size as the training set grows. Instead of growing the vectors along with a dictionary, feature hashing builds a vector of pre-defined length by applying a hash function <code>h</code> to the features (e.g., tokens), then using the hash values directly as feature indices and updating the resulting vector at those indices. When the feature space is not large enough, hashing functions tend to map distinct values to the same hash code (hash collisions). As a result, it is impossible to determine what object generated any particular hash code.</p> <p>Because of the above it is impossible to recover the original tokens from the feature matrix and the best approach to estimate the number of unique terms in the original dictionary is to count the number of active columns in the encoded feature matrix. For such a purpose we define the following function:</p> <pre data-language="python">import numpy as np


def n_nonzero_columns(X):
    """Number of columns with at least one non-zero value in a CSR matrix.

    This is useful to count the number of features columns that are effectively
    active when using the FeatureHasher.
    """
    return len(np.unique(X.nonzero()[1]))
</pre> <p>The default number of features for the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.featurehasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code>FeatureHasher</code></a> is 2**20. Here we set <code>n_features = 2**18</code> to illustrate hash collisions.</p> <p><strong>FeatureHasher on frequency dictionaries</strong></p> <pre data-language="python">from sklearn.feature_extraction import FeatureHasher

t0 = time()
hasher = FeatureHasher(n_features=2**18)
X = hasher.transform(token_freqs(d) for d in raw_data)
duration = time() - t0
dict_count_vectorizers["vectorizer"].append(
    hasher.__class__.__name__ + "\non freq dicts"
)
dict_count_vectorizers["speed"].append(data_size_mb / duration)
print(f"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s")
print(f"Found {n_nonzero_columns(X)} unique tokens")
</pre> <pre data-language="none">done in 0.544 s at 11.5 MB/s
Found 43873 unique tokens
</pre> <p>The number of unique tokens when using the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.featurehasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code>FeatureHasher</code></a> is lower than those obtained using the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.dictvectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code>DictVectorizer</code></a>. This is due to hash collisions.</p> <p>The number of collisions can be reduced by increasing the feature space. Notice that the speed of the vectorizer does not change significantly when setting a large number of features, though it causes larger coefficient dimensions and then requires more memory usage to store them, even if a majority of them is inactive.</p> <pre data-language="python">t0 = time()
hasher = FeatureHasher(n_features=2**22)
X = hasher.transform(token_freqs(d) for d in raw_data)
duration = time() - t0

print(f"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s")
print(f"Found {n_nonzero_columns(X)} unique tokens")
</pre> <pre data-language="none">done in 0.546 s at 11.4 MB/s
Found 47668 unique tokens
</pre> <p>We confirm that the number of unique tokens gets closer to the number of unique terms found by the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.dictvectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code>DictVectorizer</code></a>.</p> <p><strong>FeatureHasher on raw tokens</strong></p> <p>Alternatively, one can set <code>input_type="string"</code> in the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.featurehasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code>FeatureHasher</code></a> to vectorize the strings output directly from the customized <code>tokenize</code> function. This is equivalent to passing a dictionary with an implied frequency of 1 for each feature name.</p> <pre data-language="python">t0 = time()
hasher = FeatureHasher(n_features=2**18, input_type="string")
X = hasher.transform(tokenize(d) for d in raw_data)
duration = time() - t0
dict_count_vectorizers["vectorizer"].append(
    hasher.__class__.__name__ + "\non raw tokens"
)
dict_count_vectorizers["speed"].append(data_size_mb / duration)
print(f"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s")
print(f"Found {n_nonzero_columns(X)} unique tokens")
</pre> <pre data-language="none">done in 0.483 s at 12.9 MB/s
Found 43873 unique tokens
</pre> <p>We now plot the speed of the above methods for vectorizing.</p> <pre data-language="python">import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(12, 6))

y_pos = np.arange(len(dict_count_vectorizers["vectorizer"]))
ax.barh(y_pos, dict_count_vectorizers["speed"], align="center")
ax.set_yticks(y_pos)
ax.set_yticklabels(dict_count_vectorizers["vectorizer"])
ax.invert_yaxis()
_ = ax.set_xlabel("speed (MB/s)")
</pre> <img src="data:image/png;base64,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" srcset="../../_images/sphx_glr_plot_hashing_vs_dict_vectorizer_001.png" alt="plot hashing vs dict vectorizer" class="sphx-glr-single-img"><p>In both cases <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.featurehasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code>FeatureHasher</code></a> is approximately twice as fast as <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.dictvectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code>DictVectorizer</code></a>. This is handy when dealing with large amounts of data, with the downside of losing the invertibility of the transformation, which in turn makes the interpretation of a model a more complex task.</p> <p>The <code>FeatureHeasher</code> with <code>input_type="string"</code> is slightly faster than the variant that works on frequency dict because it does not count repeated tokens: each token is implicitly counted once, even if it was repeated. Depending on the downstream machine learning task, it can be a limitation or not.</p> </section> <section id="comparison-with-special-purpose-text-vectorizers"> <h2>Comparison with special purpose text vectorizers</h2> <p><a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.countvectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code>CountVectorizer</code></a> accepts raw data as it internally implements tokenization and occurrence counting. It is similar to the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.dictvectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code>DictVectorizer</code></a> when used along with the customized function <code>token_freqs</code> as done in the previous section. The difference being that <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.countvectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code>CountVectorizer</code></a> is more flexible. In particular it accepts various regex patterns through the <code>token_pattern</code> parameter.</p> <pre data-language="python">from sklearn.feature_extraction.text import CountVectorizer

t0 = time()
vectorizer = CountVectorizer()
vectorizer.fit_transform(raw_data)
duration = time() - t0
dict_count_vectorizers["vectorizer"].append(vectorizer.__class__.__name__)
dict_count_vectorizers["speed"].append(data_size_mb / duration)
print(f"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s")
print(f"Found {len(vectorizer.get_feature_names_out())} unique terms")
</pre> <pre data-language="none">done in 0.626 s at 10.0 MB/s
Found 47885 unique terms
</pre> <p>We see that using the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.countvectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code>CountVectorizer</code></a> implementation is approximately twice as fast as using the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.dictvectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code>DictVectorizer</code></a> along with the simple function we defined for mapping the tokens. The reason is that <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.countvectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code>CountVectorizer</code></a> is optimized by reusing a compiled regular expression for the full training set instead of creating one per document as done in our naive tokenize function.</p> <p>Now we make a similar experiment with the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.hashingvectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code>HashingVectorizer</code></a>, which is equivalent to combining the “hashing trick” implemented by the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.featurehasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code>FeatureHasher</code></a> class and the text preprocessing and tokenization of the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.countvectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code>CountVectorizer</code></a>.</p> <pre data-language="python">from sklearn.feature_extraction.text import HashingVectorizer

t0 = time()
vectorizer = HashingVectorizer(n_features=2**18)
vectorizer.fit_transform(raw_data)
duration = time() - t0
dict_count_vectorizers["vectorizer"].append(vectorizer.__class__.__name__)
dict_count_vectorizers["speed"].append(data_size_mb / duration)
print(f"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s")
</pre> <pre data-language="none">done in 0.458 s at 13.6 MB/s
</pre> <p>We can observe that this is the fastest text tokenization strategy so far, assuming that the downstream machine learning task can tolerate a few collisions.</p> </section> <section id="tfidfvectorizer"> <h2>TfidfVectorizer</h2> <p>In a large text corpus, some words appear with higher frequency (e.g. “the”, “a”, “is” in English) and do not carry meaningful information about the actual contents of a document. If we were to feed the word count data directly to a classifier, those very common terms would shadow the frequencies of rarer yet more informative terms. In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform as implemented by the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.tfidftransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code>TfidfTransformer</code></a>. TF stands for “term-frequency” while “tf–idf” means term-frequency times inverse document-frequency.</p> <p>We now benchmark the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.tfidfvectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code>TfidfVectorizer</code></a>, which is equivalent to combining the tokenization and occurrence counting of the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.countvectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code>CountVectorizer</code></a> along with the normalizing and weighting from a <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.tfidftransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code>TfidfTransformer</code></a>.</p> <pre data-language="python">from sklearn.feature_extraction.text import TfidfVectorizer

t0 = time()
vectorizer = TfidfVectorizer()
vectorizer.fit_transform(raw_data)
duration = time() - t0
dict_count_vectorizers["vectorizer"].append(vectorizer.__class__.__name__)
dict_count_vectorizers["speed"].append(data_size_mb / duration)
print(f"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s")
print(f"Found {len(vectorizer.get_feature_names_out())} unique terms")
</pre> <pre data-language="none">done in 0.637 s at 9.8 MB/s
Found 47885 unique terms
</pre> </section> <section id="summary"> <h2>Summary</h2> <p>Let’s conclude this notebook by summarizing all the recorded processing speeds in a single plot:</p> <pre data-language="python">fig, ax = plt.subplots(figsize=(12, 6))

y_pos = np.arange(len(dict_count_vectorizers["vectorizer"]))
ax.barh(y_pos, dict_count_vectorizers["speed"], align="center")
ax.set_yticks(y_pos)
ax.set_yticklabels(dict_count_vectorizers["vectorizer"])
ax.invert_yaxis()
_ = ax.set_xlabel("speed (MB/s)")
</pre> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABLAAAAJYCAMAAABFOO8oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAAS1BMVEX///+urq6fn58dHR3d3d0dcKoJCQkfd7Tx8fEAAAARERHS0tI/Pz+AgIBPT09fX1+Pj4/m5ua9vb1vb28wMDD6+vru7u7IyMgmJibJy586AAAdLUlEQVR42uzda3PaygGAYd261R10Q/r/v7QrATY4yfi0hZ5k+jwfYmyBJ97JvqMV8iZJAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAICfWoYS+H0NKvVgqIDfmWI9KCunWPAbn2BVpUw9BstwgBlqOAAz1HCAGWo4ADPUcABmqOEAM9RwAGao4QDMUMMBZqjhAMxQwwGYoYYDzFDDAZihhgMwQw0HmKGGAzBDDQdghhoOMEMNB2CG/g3D8Y9/8lvyjxPBEizBQrAEC8FCsARLsBAsBEuwECzBQrAQLMESLATLcAiWYCFYgoVgIViCJVgIlmAhWAiWYCFYCJZgCRaCJVgIFoIlWAgWgiVYgoVg/R6q9L94cTsJlmAhWO92qqqqbrP4aFg+vtjNSbLU3fVxvXx5yX70q0spWIKFYL09WP0wNF3olx+SNG3H4+2Hc6cfg7X88tsvgiVYCNYLg7X/OVbZsSQc+jrMaxbPuqqsqcbjSJNkW9jOyfPRpOhD6Ic9X1le7UvCcf96dUqSdA55F0tVnfvQCZZgIVgvDlYy93uwyq1di3Rdpm2IC8T5tB+fkyxPi7TOvh5t13VuY7BC2zR7sJZhGMaQJWOdFWMeS1XFB4VgCRaC9epg9dserCxcHhZ951AmZSxQvl/h6trno2MVz66aak26/cHtovtlXz+2+1lVlsdgTa5hCRaC9YZgnY5gTe3jVao9Vlkohyrs6uej53x/GM+7uuNS1xGspW3jSvB4dog/fJUJlmAhWG9bEj4HKzm1SXtKhiqu7KJfBWv+CNYp30/Bwvn69C/3SQiWYCFYL73ontWPS8L41XS/8J7frpw/Hf1cEn4E6xya4+Hp9q0FS7AQrFcHaxjW47aG2Jflelk9LgSby35Dwlbv670snjM12fnL0Y+L7vdgxegNw1DGj13TpJ1gCRaC9fJgfdw4uvelOG5cSJa+Pi5AddVxcpXNoW6/Hv28reEWrO5+W8PYhnrOBEuwEKw/eDgES7AQrD/AUpblIFiChWD9Ca6rRcESLATLGRaChWC5hiVYCJZgIVgI1ls0c5i/fVJ+/n7/P8ESLATr3U5tcflLwfqy/59gCRaC9T8333e0Wr4J1gPBEiwE682WqQ7tuv+W4DiH9raR1X47QldUabvv3HDbx2+Nq8S0aq5PGPqQZ/cl4dMOf10e8kmwBAvBeocpH5tTfYnBmsemve3MMGzTUBZVnhbDxz5+dd+k2z1Y/byubbgG62mHv7QeizUTLMFCsN7y1411WeK50rjv0JBWy+eSsKj2E6uPffzq+HOdb8EqqnXftOEarKcd/s7bYkkoWAjWezTVvgrsT/eNY4rHYMUqPe/j19yCdQ1bfQ3W04ZZQ56f0kWwBAvBem+wLj8Eq0m+7OP3GazkF8FKlnTKW/9rjmAhWO/464aPJeFPg/Wwj1+sUPa4JCzuS8Ln/f9uRwVLsBCsN150/3mw7vv4lfXpFxfdn3b4y7Km6MJFsAQLwXqDz9safh6sj338fn1bw+MOf+kcv93oGpZgIVh/u+IerH9jOARLsBAswUKwECzBEiwE6/92OARLsBAswUKwECzBEiwES7AQLARLsBAsBEuwBAvBEiwEC8ESLAQLwRIswUKwBAvBQrAEC8FCsAwHmKGGAzBDDQdghhoOMEMNB2CGGg7ADDUcYIYaDsAMNRyAGWo4wAw1HIAZajgAM9RwgBlqOAAz9O3DYXsZ7NYjWIIFgiVYggWCJVggWIIlWCBYgiVYIFiCBYIlWIIFgiVYggWCJVggWIIlWCBYgiVYIFiCBYIlWIIFgiVYggWCJVggWIIlWCBYgpUkVSpYIFhvc6p2xdcvZ/W3WTr1ggWC9b8NVj9Ey18I1vKfBmsRLARLsF4UrONDOoe8i2U5x49TmYz7aVd37U+dJUWVtiFLsi1s56dgjW1d9/H0bJny+Pp4JOvDth9t+lCfLknSTlPdChaCJVgvDNZYZ8W4F+c8FuM2Jcu5HobyIVh5WgxZ/CONnz0EK02Lpp/jy/KxWPcjeVZM4ZIMddc0bSxVG6aiESwES7BedA0rhNC33b4MzK9fSuv7kvAzWPuJVR6LlHSxQvtLQqhuS8KhapLpdhYVz8rizzwez4oHiqSdXcNCsATrZcFqi6IYwjVBcS3Y5sfHr8Fa9/4cT4oHqnN8TdHHYBWn/elpstbbND68oD+eG8vVngQLwRKs1y4Jw5GgIinCtBZZdXkMVtiD1ezByq5PelgSbu3YNPunZXqq+4dg9cdTy6SdBAvBEqzXBut+IpRW+6pvD1bYP63jSrCobsFK9mtcz+8SXuI5VLLe3hoc48vuweq221uDgoVgCdbLL7pXXdOkXdLE1V6Wx/Ks1Xgpk9O2ru1HsLJ4HtZk58czrPpUjHP89Jw2cXX4eYY11P1ajKdFsBAswXr9bQ1jG+o529/uC+2+JEymuuqSoQ3bWN+DlWRzqNv06baGLcxj/PQ40nwGKyn6OmyTMywES7D+SEtZloNgIViC9Sfojt8BEiwES7CcYYFgCdarh0OwECzBEiwQLME6NHOYv31Sfv52dxnBQrAE6/23PrTF5S8F62Gbmm4WLARLsP4G8/0W9+WbYD2+KShYCJZgvfmtvKkO7brf+T7Oob3tQ3rsjfVlS6w1rhLT6rZxzNCHPLsvCYe+DvOa7S/Kki4P+SRYCJZgvcOUj82pvsRgzeOxn9XRo20ayuctscq6b9LtHqx+Xtc2XINVbu1apOsybcOwpPV1myzBQrAE6w1/3ViXJZ4rjftvNafV8rkkfN4SK6vjz3W+BevYfKaprsHKwuVzSXjeFktCBEuw3vR24PG/UfSnGKx/sXdny23jCABFIZLFEsGd1ML//9IBqMXKNpl0y9Pp7nMfEiuSXQ6qcAqgKXh+PHqC9Xok1n5gX3sH6wZbvIH1OMpvB2suy/FwBpaABazPBevyDVivR2J9BVb4AVjhfFjLzi+hELCA9Rnfbv3cEn4XrOeRWEVMChWvW8LqsSWMly9/Srg/CywBC1ifeNH9+2A9jsQ6xvEHF93Pt4vu6ZXt5VwUbdXXF2AJWMD61Nsavg/W80isH9/WUO23NYTzEJvicEpfbnENS8AC1l9e1bS/PBzAErCABSwJWMAClgSsf2nAErCABSwJWMAClgQsYEnAAhawJGABC1gSsIAlAQtYwJKABSxgScAClgQsYAFLAhawDIdkhhoOSWao4ZBkhhoOyQw1HJLMUMMhyQw1HJIZajgkmaGGQ5IZajgkM9RwSDJDDYckM9RwSGao4ZBkhhoOyQzVx3A4D0v6fU/bAhawJGABSwIWsIAlAUvAkoAFLAlYwAKWBCxgSQIWsCRgAQtYErCAJQlYwJKABSxgScACliRgAUsCFrCAJQHrH1BzAJYErE9rbHLV1/9cxJ+yNA7AkoD1/wVrmFPn/wGs8x8F6wwsCVhvAmv/63Cqyz7JMqW/12NY8rKrv/kTi1A1h64uQrHV2/QFWEsX45CWZ+e1TJ+fnimGesvPtkMdx0sI3brGDlgSsN4I1hKLasniTEu1bGs4T3Gejy9glYdqLtIfh/ToBazDoWqHU/q0cqmu+ZmyqNb6EubYt22XpOrqtWqBJQHrTdew6roeuj5vA8vbPx3iY0v4AVZeWJVJpNAnhfKn1HVz3xLOTRvW+yoqrcrS/3nZX5WeqEJ3cg1LAtbbwOqqqprrG0FpL9iV+99fg3XN/uwvSk80U/qcakhgVWN++SFc47YuL58w7K9NcnUjsCRgvXdLWO8EVaGq12tVNJdXsOoMVpvBKm4vetkSbt3Stvnh8TDG4QWsYX/pMXQrsCRgvResx0Lo0ORdXwarzg9j2glWzR2skK9xfflTwktaQ4Xr/UeDS/q0B1j9dv/RILAkYL39onvTt+2hD23a7RVlkufaLJdjGLfrtXuCVaR1WFtMryusOFbLKT2cDm3aHX6ssOY4XKtlPANLAtb7b2tYujqeivzjvrrLW8KwxqYPc1dvS3yAFYpTHbvDF7c1bPVpSQ/3Z9oPsEI1xHpbrbAkYHkvoQQsYH1m5+PxOANLAtbfoX5/0yKwJGBZYUnAApZrWBKw/mn95FgZYEnA+n2IApYErL/+whOwJGD9Ziytse6u+X735VR399NHb2dgXcayzjeU5hMc2mYNYby9iadsmqYMYdqarbiD1cc2XLv9OK1Q9mOdD3d4nJMFLAlYb2otl3aMlwTWadlPsbqBlc/Amqe2mppr+o9cwxRPIewn+O3vhJ7ncGim9Oyyg7WWVWjrqbqeEmllnKq+aZ/nZAFLAta7vsm8FiqnvMLK738+38GaHi8Y0srqNIWhr49z075sAvf3TA/7KQ3jNt+XX9f0Bcr8QfqKa2dLKAHrnbX776AYxgTW/HgUbmdghXN/ivtRfesQYntanme9P981mFZR+aTR7ZI+3G7HarXZqmRc/zwnC1gSsN4O1uUVrLyWmmLR7kf1HWIbw7o+f/PEC1hbejTW+cNt3Y/BOj/BepyTBSwJWG/6JuvnlvAbsIa8t9uG/D9JVh1O2xS+vyU81Id8fOn92SdY4XZOFrAkYL39ovs3YK3ltb2tkU7NFC7N4xJWWkzNly8uumex2nptq8P6AdbjnCxgScB6+20N34B1GerY7/vANT8+PX9d4WH7+raGLNa1q+u8sHqA9TgnC1gSsLyXUAIWsIAlAQtYwJKABSwJWMAClgQsYEkCFrAkYAELWBKwgCUJWMCSgAUsYEnAApYkYAFLAhawgCUBC1iSgPVbgGU4JDPUcEgyQw2HZIYaDklmqOGQZIYaDskMNRySzFDDIckMNRySGWo4JJmhhkOSGWo4JDPUcEgyQw2HJDP0V4bD8TL6Jx/PAixgScACFrAELAELWAIWsIAlAQtYwBKwBCxgCVjAApYELGABS8ACloAlYAELWBKwgAUsAQtYApaABSxgScACFrAELGABSwIWsIAlAQtYwBKwgAUsCVjAApYELGABS8ACFrAkYAELWBKwgAUsAQtYf+eW5gIsAQtYn9C8lnU5LP/lFc0hhCnu3/85Tt979qvOsxWWgAWsT6gqt0PVTttPwJqbIn9YNPPPwTr/6AudgSVgAevPNJT7N3YJ1VDXQ+JoHNLDtQuhW9cY+xDKpmnKMHT5Zd0Q5iHGocp4bXW53p8N09ZsmbRmGuo+bwm7JleF4xjrrg2hPxVlAywBC1h/okvT3z86ddfrqXsFK/ZV0Sx5cTXP4ZDwCVWzHLexbcftHKZ6qq7T89mpmtJrQxOLqspgXeZ5HtLLuuFarfES+sRWe1tnHY/HGVgCFrB+uetjR7fkzV7bXF/Aymuq03rf9J3LJFtfnou8eTzXSyj7ly1hN+bFWvrMZv246D7FKiwx7wPT4qt/bib7fe0FLAELWH8YrKnMf8bidUuYERofV6n69IKE1NrUqWaam+UFrFjcv8R+qesG1pJUC9Pt5Wvon1fJrLAELGD9yS3hT8FK28ElbQvXU5U7Hr8D1nZ/tIPV7j9Q7Mv95WlLePJTQgELWG+66P7cEq5Zlu5bsNK2b0z/WsT7f+QHW8IHWJdtvO00q/tGEFgCFrDedlvD46L70hRVH1/B2tY5b/GKuk7rqOPWLdWyzunhftH99uzHRfcHWN1WzfN8Dt1pqa79FVgCFrDe0OPG0cdtDaGPcf1iS3jY8o0L4Rz3C+jzGOttPObbGppyfTz7vK3hAVbzuK1hLZtynIElYAHr7zscwBKwgAUsCVjAApaABSxgScACFrAkYAELWAIWsIAlAQtYwJKABSxgCVjAApYELGABSwIWsIAlYAELWBKwgAUsCVjAApaABSxgScACFrAELAELWAIWsIAlAQtYwBKwBCxgCVjAApYELGABS8ASsAyHZIYaDklmqOGQzFAZDskMNRySzFDDIZmhhsMgSGao4ZBkhhoOyQw1HJLMUMMhyQw1HJIZajgkmaGGQ5IZajgkM/TfMRzOw5J+3+O/gAUsCVjAkgQsYEnAErAkYAFLErCAJQELWJKABSxJwALWf9i71+22UQWAwkiwWAKBEEK393/SAWw56YlzpplJi7tmfz+SNLETDTU7oMoegGARLAAEi2ABIFgECyBYBAsAwSJYAAgWwQIIFsECQLAIFkCwCBbBAggWwQJAsAgWQLAIFsECCBbBAkCw3gnT/QM5/8vvpMxIsACC9W1dWsz+abD2J4faGVXfn+l/v/Ikb1vcCBZAsH5LsJ7ZrC/vjo9rp4/B+rRWG8ECCNY/C9YepF77snpatXVD/sIsbdpuCTL9pM+ufPHUrs91S2e98yqGYLUb73ebhDNZvVktl/TBhrIlDOXzZhFbyj9lEaK33XlfpREsgGB9NVhxHtVsDhHNrMY5B8umsdP9PViyV0nvQpmkOpmDNeb4iEHPwk2HSnbPm0Q/jl7s0seYV15e9fW+dlaqBGuIMSYbRXCLmrUSvXGHGggWQLC+FCyji/uWcEo5NreFT5B5dTVN92D5csyLSGv+ii83XkNeJul9sWVjd/bChXdbwuDym7IIk9PbSfdOH/nDmD9yPgfr2kxuwzBEggUQrJ8JlstLIJV3eZtfrTaT2Jye+v2+9EruHqyyHbS9mEI9476XWA0lUnPtnUmirKcewVp9vdmWt4SPYI3lFt3t5lO++3UAvm4WCRZAsL6wJZxtP6qyohJHTpf6u2ANuld5yTXL0ju1ly8+C9Z8BSvKdPvXxSKK3gpWWADB+qfBqi06b/80uOXQPA3W25YwL838We54P3N+bQnPH7eEV7C21W31w+V2s7dgcQ4LIFhfDlaSxxjslNdXh+r08jxY95PuQ72koV7a4NZF5bvk7+DHMd/STXF/d9L9ClaQY4xxy+87dcwdwQII1r+6rGHS1uc/jc7qskx6Gqx6WcNs6hVUZz2BPiRpZIjXZQ3iWHW9rMHcLmu4giWvyxp8vvk0EiyAYP0eXvJcQoBg/QHmQ/W3q9wJFkCwXlyS+vQbwQIIFi8vA4BgESwABItgAQSLYAEgWAQLAMEiWADBIlgACBbBAggWwSJYAMEiWAAIFsECCBbBIlgAwSJYAAgWwQIIFsEiWADBIlgACBbBAggWwSJYAMEiWAAIFsECCBbBIlgAwfqTg8VwAMxQhgMAM5ThAJihDAcAZijDAYAZynAAzFCGAwAzlOEAwAxlOABmKMMBgBnKcABghjIcADOU4QDADGU4ADBDvzIcvLwMXv9FVggWCBYIFsEiWADBIlgECwSLYBEsgGARLIIFECyCRbBAsAgWwQIIFsEiWADBIlgECwSLYBEsgGARLIIFECyCRbBAsAgWwQIIFsEiWADBIlgECwSLYBEsgGARLIIFggWCRbBAsAgWwQIIFsEiWCBYIFgECwSLYBEsgGARLIIFgoU/MVhq1asy4+0Pi9mF6E6TvvQdrnsTLBAsgvUrmLsgJqf2LW7vgmVT7I2qfz4/lEvOH77X494ECwSLYP0KMcbZ5jeDWP27T5dgDWYRm62fPT6unT4G69NabQQLBItgfZPeXkstXzd13aldb/alfGZJZ/liWMUQrHYlW92q7SRc+Wq9bS2X9MGGcu9Q12u5dEnqdSnfPG8sFcECwSJY3xmseKY4lOREnVRvzb4p08VtzPERg56Fmw6V7C4648fRi136GPPKy6te9zlYdlaq3HvIq7VkowhuUbNWojfuUHUMtmEYIsECwSJY3xCsuiUsyfFlTeXzlnAvrRJryLfQ+2LLxu7shQvvtoTB5TdlESant5PunT7yhzF/5HwO1rWZ9HXtRbBAsAjWNwZrKkXqHsHq9VAiNRudmSTKeuoRrHreqzNb3hI+gjWWW3S3m0/57o8zWaywQLAI1i8O1qB7lT+YpSp2YZ8Ha76CFWWqn6s3j9c35xwWCBbB+g1bwrztK59ZrjPn15bw/HFLeAVrW91WP1x++OYECwSLYP2KYKly0l2+Besw9dIGty7q8Eculx/HnCc3xf3dSfcrWEGOMcYtv+/UMXcECwSLYP3KYF2XNVzBEmc9gT4kaWSI12UN4lh1vazB3C5ruIIlr8safL75NBIsECyC9ecOB8ECwSJYBAsgWASLYIFgESyCBRAsgkWwAIJFsAgWCBbBIlgAwSJYBAsgWASLYIFgESyCBRAsgkWwAIJFsAgWCBbBIlgAwSJYBAsECwSLYIFgESyCBRAsgkWwQLBAsAgWCBbBIlgAwSJYBAsECwSLYIFgESyCBRAsgkWwQLBAsBgOgBnKcABghjIcADMUDAfADGU4ADBDGQ6AGcpwMAgAM5ThAMAMZTgAZijDAYAZynAAYIYyHAAzlOEAwAxlOAAwQxkOgBnKcABghjIcAJihDAfADGU4ADBDGQ4AzNAvDEccALyqSLDeUwbAK4tk6s3efIUVOYL2R8AQvO4R0KvX2iFzBC9wBAwBR0CwOAKCxREQLP6WOAKGgCP4b9r8xhFwBAwBRwAAAAAAAAAAAP5LZqnXpeHP96u2k2o9Ct6klj8+BqvXo+EBbF5q6Vv99GWSpqt/DVK7seERbGnVMsSmYyBEMDNd+kxn+jHphsFw/ThOsvHVJ4dcWwZrl+FQS8tqe9upTreaJ4vv6mSdddfosXA/gsF16ljXlmOQp+QqCdan6kQ9fduDiGZp+vOHc3Etg5Vc64fBFK43jdTJavNE3Wzf8Ajqby+jWh5BlCPB+nwrUAep9YRRZmz680MSTYN1psmufcsR8FKJ0TY8hPI4vD0MWmXzEayl1dXmtyNwsyBY/2dtU86c+LPx7/e2wezXrW2wtPZjr9sWyxjTcp1dJutRX0YluJa5yL/D19BwDITP//kE67WDlWTT18+INv9ibxos41ovczvZjX3rFdZLBGub1qHlGNhIsF58S5hk238k7O6vktbuWVyy/E6fZcMxqFOk5e+tV9kS5l7tLcdgvj0WpcBz7U+6t+6VGMZsDQ1Po4XmKyz7CsGq2Wx70n2bzth0DPbyYJRJCXy2vGh8WUOyS4yx+XPUm24JD+NV23NYQXaqs63GIP/KMPOoxGy7MTS5rOF+BNskx0aPxmsMHutdPNf6wtHbErhvPQxNgyW6VZ9Nh2BIUp/NXtpkqY+B0PLC0dsR3P//BkvLMSBYAAAAAAAAAAAAAAAAAADgp7xd/up+5jrUlBgyAM2D1ZWnBd5fc/cszyOQ5Rm1oT6vd3k8szZqnrMGoHmwXHk6u5Tl+dSHLc9SlD7G5azP/EhvL4cwscQC8P3+au9OdhyFoSgMX4hlgUdwYeD9n7RthkAWJfWCUrVa/7fIoBAvsji6dsx1M2njOonBGx3rPX/1HtBaQNmgTa2cuqhfwxlY89ao5eX1WNup1v4Ht/4xa3OOJi1tTAA8buwHZYcSWDrYxviaPUm1OslovLXO1S49qWTXEViNro+vIXjpjD0Da9x6aVqdz9HE9swJATztPBghmq4eLiOdrp1iY9w66pY4U52uzZTOwBrWPbCaVdpJtsDSWvdbbzofrmMWul8+/QPAf8np0M5Hq79SFy293hJIwv4i7bXSeViZPwIrm+SGPbC8UmlyZTI5tZKP0SQTWAB+osbyk1G3wEqqGCWE+qy6z8Da16bKNPDL6Vnea1jlW6Uam9+jHS39AeBpuYRONKVGauuUsD1rqXxM7u5TwmVLpXK97YPcA6uR1l2jSeozvyuAx+urRTU6SdTRpm3R3bTKDq2MJiwqxfy56J7N2SJ9zkdg+XFcnJklDNdo+woYADxKOaPXEjW3bQ1rb1zJHBXKJ1/btgbz3tYgPsrVadccG0dNsKU0U9dodYcDAPyQGP7uutF8s1+hWT/eMCME8OuBJc03f/+le03VsOQO4B8ILAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADA8/4AUezOXxJSH4kAAAAASUVORK5CYII=" srcset="../../_images/sphx_glr_plot_hashing_vs_dict_vectorizer_002.png" alt="plot hashing vs dict vectorizer" class="sphx-glr-single-img"><p>Notice from the plot that <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.tfidfvectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code>TfidfVectorizer</code></a> is slightly slower than <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.countvectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code>CountVectorizer</code></a> because of the extra operation induced by the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.tfidftransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code>TfidfTransformer</code></a>.</p> <p>Also notice that, by setting the number of features <code>n_features = 2**18</code>, the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.hashingvectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code>HashingVectorizer</code></a> performs better than the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.countvectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code>CountVectorizer</code></a> at the expense of inversibility of the transformation due to hash collisions.</p> <p>We highlight that <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.countvectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code>CountVectorizer</code></a> and <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.hashingvectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code>HashingVectorizer</code></a> perform better than their equivalent <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.dictvectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code>DictVectorizer</code></a> and <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.featurehasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code>FeatureHasher</code></a> on manually tokenized documents since the internal tokenization step of the former vectorizers compiles a regular expression once and then reuses it for all the documents.</p> <p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes 4.738 seconds)</p> <div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-text-plot-hashing-vs-dict-vectorizer-py"> <div class="binder-badge docutils container"> <a class="reference external image-reference" href="https://mybinder.org/v2/gh/scikit-learn/scikit-learn/1.1.X?urlpath=lab/tree/notebooks/auto_examples/text/plot_hashing_vs_dict_vectorizer.ipynb"><img alt="Launch binder" src="data:image/svg+xml;base64,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" width="150px"></a> </div> <div class="sphx-glr-download sphx-glr-download-python docutils container"> <p><a class="reference download internal" download="" href="https://scikit-learn.org/1.1/_downloads/5775388ede077a05b00514ecbaa17f32/plot_hashing_vs_dict_vectorizer.py"><code>Download Python source code: plot_hashing_vs_dict_vectorizer.py</code></a></p> </div> <div class="sphx-glr-download sphx-glr-download-jupyter docutils container"> <p><a class="reference download internal" download="" href="https://scikit-learn.org/1.1/_downloads/06cfc926acb27652fb2aa5bfc583e7cb/plot_hashing_vs_dict_vectorizer.ipynb"><code>Download Jupyter notebook: plot_hashing_vs_dict_vectorizer.ipynb</code></a></p> </div> </div>  </section> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2022 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.1/auto_examples/text/plot_hashing_vs_dict_vectorizer.html" class="_attribution-link">https://scikit-learn.org/1.1/auto_examples/text/plot_hashing_vs_dict_vectorizer.html</a>
  </p>
</div>
