<h1 class="break-words">robots.txt</h1>
<p>Add or generate a <code>robots.txt</code> file that matches the <a href="https://en.wikipedia.org/wiki/Robots.txt#Standard" rel="noopener noreferrer nofollow" target="_blank">Robots Exclusion Standard</a> in the <strong>root</strong> of <code>app</code> directory to tell search engine crawlers which URLs they can access on your site.</p> <h2 id="static-robotstxt" data-docs-heading="">Static robots.txt     </h2> <div class="code-block_wrapper__t6FCO code-block_hasFileName__ezlBD not-prose"><pre class="code-block_pre___OLfy" data-language="typescript">User-Agent: *
Allow: /
Disallow: /private/

Sitemap: https://acme.com/sitemap.xml</pre></div> <h2 id="generate-a-robots-file" data-docs-heading="">Generate a Robots file     </h2> <p>Add a <code>robots.js</code> or <code>robots.ts</code> file that returns a <a href="#robots-object"><code>Robots</code> object</a>.</p> <div class="code-block_wrapper__t6FCO code-block_hasFileName__ezlBD not-prose"><pre class="code-block_pre___OLfy" data-language="typescript">import { MetadataRoute } from 'next'
 
export default function robots(): MetadataRoute.Robots {
  return {
    rules: {
      userAgent: '*',
      allow: '/',
      disallow: '/private/',
    },
    sitemap: 'https://acme.com/sitemap.xml',
  }
}</pre></div>  <p>Output:</p> <div class="code-block_wrapper__t6FCO not-prose">
<pre class="code-block_pre___OLfy" data-language="typescript">User-Agent: *
Allow: /
Disallow: /private/

Sitemap: https://acme.com/sitemap.xml</pre>
</div> <h3 id="customizing-specific-user-agents" data-docs-heading="">Customizing specific user agents     </h3> <p>You can customise how individual search engine bots crawl your site by passing an array of user agents to the <code>rules</code> property. For example:</p> <div class="code-block_wrapper__t6FCO code-block_hasFileName__ezlBD not-prose"><pre class="code-block_pre___OLfy" data-language="typescript">import type { MetadataRoute } from 'next'
 
export default function robots(): MetadataRoute.Robots {
  return {
    rules: [
      {
        userAgent: 'Googlebot',
        allow: ['/'],
        disallow: '/private/',
      },
      {
        userAgent: ['Applebot', 'Bingbot'],
        disallow: ['/'],
      },
    ],
    sitemap: 'https://acme.com/sitemap.xml',
  }
}</pre></div>  <p>Output:</p> <div class="code-block_wrapper__t6FCO not-prose">
<pre class="code-block_pre___OLfy" data-language="typescript">User-Agent: Googlebot
Allow: /
Disallow: /private/

User-Agent: Applebot
Disallow: /

User-Agent: Bingbot
Disallow: /

Sitemap: https://acme.com/sitemap.xml</pre>
</div> <h3 id="robots-object" data-docs-heading="">Robots object     </h3> <div class="code-block_wrapper__t6FCO not-prose">
<pre class="code-block_pre___OLfy" data-language="typescript">type Robots = {
  rules:
    | {
        userAgent?: string | string[]
        allow?: string | string[]
        disallow?: string | string[]
        crawlDelay?: number
      }
    | Array&lt;{
        userAgent: string | string[]
        allow?: string | string[]
        disallow?: string | string[]
        crawlDelay?: number
      }&gt;
  sitemap?: string | string[]
  host?: string
}</pre>
</div> <h2 id="version-history" data-docs-heading="">Version History     </h2> <div class="overflow-x-auto"><table class="w-full table-auto">
<thead><tr>
<th>Version</th>
<th>Changes</th>
</tr></thead>
<tbody><tr>
<td><code>v13.3.0</code></td>
<td>
<code>robots</code> introduced.</td>
</tr></tbody>
</table></div><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024 Vercel, Inc.<br>Licensed under the MIT License.<br>
    <a href="https://nextjs.org/docs/app/api-reference/file-conventions/metadata/robots" class="_attribution-link">https://nextjs.org/docs/app/api-reference/file-conventions/metadata/robots</a>
  </p>
</div>
