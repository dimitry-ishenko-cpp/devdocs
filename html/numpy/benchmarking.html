<section id="numpy-benchmarks"> <h1>NumPy benchmarks</h1> <p>Benchmarking NumPy with Airspeed Velocity.</p> <section id="usage"> <h2>Usage</h2> <p>Airspeed Velocity manages building and Python virtualenvs by itself, unless told otherwise. To run the benchmarks, you do not need to install a development version of NumPy to your current Python environment.</p> <p>Before beginning, ensure that <em>airspeed velocity</em> is installed. By default, <code>asv</code> ships with support for anaconda and virtualenv:</p> <pre data-language="python">pip install asv
pip install virtualenv
</pre> <p>After contributing new benchmarks, you should test them locally before submitting a pull request.</p> <p>To run all benchmarks, navigate to the root NumPy directory at the command line and execute:</p> <pre data-language="python">spin bench
</pre> <p>This builds NumPy and runs all available benchmarks defined in <code>benchmarks/</code>. (Note: this could take a while. Each benchmark is run multiple times to measure the distribution in execution times.)</p> <p>For <strong>testing</strong> benchmarks locally, it may be better to run these without replications:</p> <pre data-language="python">cd benchmarks/
export REGEXP="bench.*Ufunc"
asv run --dry-run --show-stderr --python=same --quick -b $REGEXP
</pre> <p>Where the regular expression used to match benchmarks is stored in <code>$REGEXP</code>, and <code>–quick</code> is used to avoid repetitions.</p> <p>To run benchmarks from a particular benchmark module, such as <code>bench_core.py</code>, simply append the filename without the extension:</p> <pre data-language="python">spin bench -t bench_core
</pre> <p>To run a benchmark defined in a class, such as <code>MeshGrid</code> from <code>bench_creation.py</code>:</p> <pre data-language="python">spin bench -t bench_creation.MeshGrid
</pre> <p>Compare changes in benchmark results to another version/commit/branch, use the <code>--compare</code> option (or the equivalent <code>-c</code>):</p> <pre data-language="python">spin bench --compare v1.6.2 -t bench_core
spin bench --compare 20d03bcfd -t bench_core
spin bench -c main -t bench_core
</pre> <p>All of the commands above display the results in plain text in the console, and the results are not saved for comparison with future commits. For greater control, a graphical view, and to have results saved for future comparison you can run ASV commands (record results and generate HTML):</p> <pre data-language="python">cd benchmarks
asv run -n -e --python=same
asv publish
asv preview
</pre> <p>More on how to use <code>asv</code> can be found in <a class="reference external" href="https://asv.readthedocs.io/">ASV documentation</a> Command-line help is available as usual via <code>asv --help</code> and <code>asv run --help</code>.</p> </section> <section id="benchmarking-versions"> <h2>Benchmarking versions</h2> <p>To benchmark or visualize only releases on different machines locally, the tags with their commits can be generated, before being run with <code>asv</code>, that is:</p> <pre data-language="python">cd benchmarks
# Get commits for tags
# delete tag_commits.txt before re-runs
for gtag in $(git tag --list --sort taggerdate | grep "^v"); do
git log $gtag --oneline -n1 --decorate=no | awk '{print $1;}' &gt;&gt; tag_commits.txt
done
# Use the last 20
tail --lines=20 tag_commits.txt &gt; 20_vers.txt
asv run HASHFILE:20_vers.txt
# Publish and view
asv publish
asv preview
</pre> <p>For details on contributing these, see the <a class="reference external" href="https://github.com/HaoZeke/asv-numpy">benchmark results repository</a>.</p> </section> <section id="writing-benchmarks"> <h2>Writing benchmarks</h2> <p>See <a class="reference external" href="https://asv.readthedocs.io/">ASV documentation</a> for basics on how to write benchmarks.</p> <p>Some things to consider:</p> <ul class="simple"> <li>The benchmark suite should be importable with any NumPy version.</li> <li>The benchmark parameters etc. should not depend on which NumPy version is installed.</li> <li>Try to keep the runtime of the benchmark reasonable.</li> <li>Prefer ASV’s <code>time_</code> methods for benchmarking times rather than cooking up time measurements via <code>time.clock</code>, even if it requires some juggling when writing the benchmark.</li> <li>Preparing arrays etc. should generally be put in the <code>setup</code> method rather than the <code>time_</code> methods, to avoid counting preparation time together with the time of the benchmarked operation.</li> <li>Be mindful that large arrays created with <code>np.empty</code> or <code>np.zeros</code> might not be allocated in physical memory until the memory is accessed. If this is desired behaviour, make sure to comment it in your setup function. If you are benchmarking an algorithm, it is unlikely that a user will be executing said algorithm on a newly created empty/zero array. One can force pagefaults to occur in the setup phase either by calling <code>np.ones</code> or <code>arr.fill(value)</code> after creating the array,</li> </ul> </section> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2005&ndash;2024 NumPy Developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://numpy.org/doc/2.0/benchmarking.html" class="_attribution-link">https://numpy.org/doc/2.0/benchmarking.html</a>
  </p>
</div>
