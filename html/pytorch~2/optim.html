<h1 id="torch-optim">torch.optim</h1> <p id="module-torch.optim"><a class="reference internal" href="#module-torch.optim" title="torch.optim"><code>torch.optim</code></a> is a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can also be easily integrated in the future.</p>  <h2 id="how-to-use-an-optimizer">How to use an optimizer</h2> <p>To use <a class="reference internal" href="#module-torch.optim" title="torch.optim"><code>torch.optim</code></a> you have to construct an optimizer object that will hold the current state and will update the parameters based on the computed gradients.</p>  <h3 id="constructing-it">Constructing it</h3> <p>To construct an <a class="reference internal" href="#torch.optim.Optimizer" title="torch.optim.Optimizer"><code>Optimizer</code></a> you have to give it an iterable containing the parameters (all should be <code>Variable</code> s) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.</p> <p>Example:</p> <pre data-language="python">optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = optim.Adam([var1, var2], lr=0.0001)
</pre>   <h3 id="per-parameter-options">Per-parameter options</h3> <p><a class="reference internal" href="#torch.optim.Optimizer" title="torch.optim.Optimizer"><code>Optimizer</code></a> s also support specifying per-parameter options. To do this, instead of passing an iterable of <code>Variable</code> s, pass in an iterable of <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)"><code>dict</code></a> s. Each of them will define a separate parameter group, and should contain a <code>params</code> key, containing a list of parameters belonging to it. Other keys should match the keyword arguments accepted by the optimizers, and will be used as optimization options for this group.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>You can still pass options as keyword arguments. They will be used as defaults, in the groups that didn’t override them. This is useful when you only want to vary a single option, while keeping all others consistent between parameter groups.</p> </div> <p>For example, this is very useful when one wants to specify per-layer learning rates:</p> <pre data-language="python">optim.SGD([
                {'params': model.base.parameters()},
                {'params': model.classifier.parameters(), 'lr': 1e-3}
            ], lr=1e-2, momentum=0.9)
</pre> <p>This means that <code>model.base</code>’s parameters will use the default learning rate of <code>1e-2</code>, <code>model.classifier</code>’s parameters will use a learning rate of <code>1e-3</code>, and a momentum of <code>0.9</code> will be used for all parameters.</p>   <h3 id="taking-an-optimization-step">Taking an optimization step</h3> <p>All optimizers implement a <a class="reference internal" href="generated/torch.optim.optimizer.step.html#torch.optim.Optimizer.step" title="torch.optim.Optimizer.step"><code>step()</code></a> method, that updates the parameters. It can be used in two ways:</p>  <h4 id="optimizer-step"><code>optimizer.step()</code></h4> <p>This is a simplified version supported by most optimizers. The function can be called once the gradients are computed using e.g. <code>backward()</code>.</p> <p>Example:</p> <pre data-language="python">for input, target in dataset:
    optimizer.zero_grad()
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optimizer.step()
</pre>   <h4 id="optimizer-step-closure"><code>optimizer.step(closure)</code></h4> <p>Some optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute your model. The closure should clear the gradients, compute the loss, and return it.</p> <p>Example:</p> <pre data-language="python">for input, target in dataset:
    def closure():
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        return loss
    optimizer.step(closure)
</pre>     <h2 id="optimizer-algorithms">Base class</h2> <dl class="py class" id="base-class"> <dt class="sig sig-object py" id="torch.optim.Optimizer">
<code>class torch.optim.Optimizer(params, defaults)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/optim/optimizer.html#Optimizer"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Base class for all optimizers.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Parameters need to be specified as collections that have a deterministic ordering that is consistent between runs. Examples of objects that don’t satisfy those properties are sets and iterators over values of dictionaries.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>params</strong> (<em>iterable</em>) – an iterable of <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> s or <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)"><code>dict</code></a> s. Specifies what Tensors should be optimized.</li> <li>
<strong>defaults</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.12)">Dict</a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">Any</a><em>]</em>) – (dict): a dict containing default values of optimization options (used when a parameter group doesn’t specify them).</li> </ul> </dd> </dl> </dd>
</dl> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td><p><a class="reference internal" href="generated/torch.optim.optimizer.add_param_group.html#torch.optim.Optimizer.add_param_group" title="torch.optim.Optimizer.add_param_group"><code>Optimizer.add_param_group</code></a></p></td> <td><p>Add a param group to the <a class="reference internal" href="#torch.optim.Optimizer" title="torch.optim.Optimizer"><code>Optimizer</code></a> s <code>param_groups</code>.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.optimizer.load_state_dict.html#torch.optim.Optimizer.load_state_dict" title="torch.optim.Optimizer.load_state_dict"><code>Optimizer.load_state_dict</code></a></p></td> <td><p>Loads the optimizer state.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.optimizer.state_dict.html#torch.optim.Optimizer.state_dict" title="torch.optim.Optimizer.state_dict"><code>Optimizer.state_dict</code></a></p></td> <td><p>Returns the state of the optimizer as a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)"><code>dict</code></a>.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.optimizer.step.html#torch.optim.Optimizer.step" title="torch.optim.Optimizer.step"><code>Optimizer.step</code></a></p></td> <td><p>Performs a single optimization step (parameter update).</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad" title="torch.optim.Optimizer.zero_grad"><code>Optimizer.zero_grad</code></a></p></td> <td><p>Resets the gradients of all optimized <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> s.</p></td> </tr>  </table>   <h2 id="algorithms">Algorithms</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.optim.adadelta.html#torch.optim.Adadelta" title="torch.optim.Adadelta"><code>Adadelta</code></a>
</td> <td><p>Implements Adadelta algorithm.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.adagrad.html#torch.optim.Adagrad" title="torch.optim.Adagrad"><code>Adagrad</code></a>
</td> <td><p>Implements Adagrad algorithm.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.adam.html#torch.optim.Adam" title="torch.optim.Adam"><code>Adam</code></a>
</td> <td><p>Implements Adam algorithm.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.adamw.html#torch.optim.AdamW" title="torch.optim.AdamW"><code>AdamW</code></a>
</td> <td><p>Implements AdamW algorithm.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.sparseadam.html#torch.optim.SparseAdam" title="torch.optim.SparseAdam"><code>SparseAdam</code></a>
</td> <td><p>SparseAdam implements a masked version of the Adam algorithm suitable for sparse gradients.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.adamax.html#torch.optim.Adamax" title="torch.optim.Adamax"><code>Adamax</code></a>
</td> <td><p>Implements Adamax algorithm (a variant of Adam based on infinity norm).</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.asgd.html#torch.optim.ASGD" title="torch.optim.ASGD"><code>ASGD</code></a>
</td> <td><p>Implements Averaged Stochastic Gradient Descent.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.lbfgs.html#torch.optim.LBFGS" title="torch.optim.LBFGS"><code>LBFGS</code></a>
</td> <td><p>Implements L-BFGS algorithm, heavily inspired by <a class="reference external" href="https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html">minFunc</a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.nadam.html#torch.optim.NAdam" title="torch.optim.NAdam"><code>NAdam</code></a>
</td> <td><p>Implements NAdam algorithm.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.radam.html#torch.optim.RAdam" title="torch.optim.RAdam"><code>RAdam</code></a>
</td> <td><p>Implements RAdam algorithm.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.rmsprop.html#torch.optim.RMSprop" title="torch.optim.RMSprop"><code>RMSprop</code></a>
</td> <td><p>Implements RMSprop algorithm.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.rprop.html#torch.optim.Rprop" title="torch.optim.Rprop"><code>Rprop</code></a>
</td> <td><p>Implements the resilient backpropagation algorithm.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.sgd.html#torch.optim.SGD" title="torch.optim.SGD"><code>SGD</code></a>
</td> <td><p>Implements stochastic gradient descent (optionally with momentum).</p></td> </tr>  </table> <p>Many of our algorithms have various implementations optimized for performance, readability and/or generality, so we attempt to default to the generally fastest implementation for the current device if no particular implementation has been specified by the user.</p> <p>We have 3 major categories of implementations: for-loop, foreach (multi-tensor), and fused. The most straightforward implementations are for-loops over the parameters with big chunks of computation. For-looping is usually slower than our foreach implementations, which combine parameters into a multi-tensor and run the big chunks of computation all at once, thereby saving many sequential kernel calls. A few of our optimizers have even faster fused implementations, which fuse the big chunks of computation into one kernel. We can think of foreach implementations as fusing horizontally and fused implementations as fusing vertically on top of that.</p> <p>In general, the performance ordering of the 3 implementations is fused &gt; foreach &gt; for-loop. So when applicable, we default to foreach over for-loop. Applicable means the foreach implementation is available, the user has not specified any implementation-specific kwargs (e.g., fused, foreach, differentiable), and all tensors are native and on CUDA. Note that while fused should be even faster than foreach, the implementations are newer and we would like to give them more bake-in time before flipping the switch everywhere. You are welcome to try them out though!</p> <p>Below is a table showing the available and default implementations of each algorithm:</p> <table class="colwidths-given docutils colwidths-auto align-default">  <thead> <tr>
<th class="head"><p>Algorithm</p></th> <th class="head"><p>Default</p></th> <th class="head"><p>Has foreach?</p></th> <th class="head"><p>Has fused?</p></th> </tr> </thead>  <tr>
<td>


<a class="reference internal" href="generated/torch.optim.adadelta.html#torch.optim.Adadelta" title="torch.optim.Adadelta"><code>Adadelta</code></a>
</td> <td><p>foreach</p></td> <td><p>yes</p></td> <td><p>no</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.adagrad.html#torch.optim.Adagrad" title="torch.optim.Adagrad"><code>Adagrad</code></a>
</td> <td><p>foreach</p></td> <td><p>yes</p></td> <td><p>no</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.adam.html#torch.optim.Adam" title="torch.optim.Adam"><code>Adam</code></a>
</td> <td><p>foreach</p></td> <td><p>yes</p></td> <td><p>yes</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.adamw.html#torch.optim.AdamW" title="torch.optim.AdamW"><code>AdamW</code></a>
</td> <td><p>foreach</p></td> <td><p>yes</p></td> <td><p>yes</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.sparseadam.html#torch.optim.SparseAdam" title="torch.optim.SparseAdam"><code>SparseAdam</code></a>
</td> <td><p>for-loop</p></td> <td><p>no</p></td> <td><p>no</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.adamax.html#torch.optim.Adamax" title="torch.optim.Adamax"><code>Adamax</code></a>
</td> <td><p>foreach</p></td> <td><p>yes</p></td> <td><p>no</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.asgd.html#torch.optim.ASGD" title="torch.optim.ASGD"><code>ASGD</code></a>
</td> <td><p>foreach</p></td> <td><p>yes</p></td> <td><p>no</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.lbfgs.html#torch.optim.LBFGS" title="torch.optim.LBFGS"><code>LBFGS</code></a>
</td> <td><p>for-loop</p></td> <td><p>no</p></td> <td><p>no</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.nadam.html#torch.optim.NAdam" title="torch.optim.NAdam"><code>NAdam</code></a>
</td> <td><p>foreach</p></td> <td><p>yes</p></td> <td><p>no</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.radam.html#torch.optim.RAdam" title="torch.optim.RAdam"><code>RAdam</code></a>
</td> <td><p>foreach</p></td> <td><p>yes</p></td> <td><p>no</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.rmsprop.html#torch.optim.RMSprop" title="torch.optim.RMSprop"><code>RMSprop</code></a>
</td> <td><p>foreach</p></td> <td><p>yes</p></td> <td><p>no</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.rprop.html#torch.optim.Rprop" title="torch.optim.Rprop"><code>Rprop</code></a>
</td> <td><p>foreach</p></td> <td><p>yes</p></td> <td><p>no</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.optim.sgd.html#torch.optim.SGD" title="torch.optim.SGD"><code>SGD</code></a>
</td> <td><p>foreach</p></td> <td><p>yes</p></td> <td><p>no</p></td> </tr>  </table>   <h2 id="how-to-adjust-learning-rate">How to adjust learning rate</h2> <p><code>torch.optim.lr_scheduler</code> provides several methods to adjust the learning rate based on the number of epochs. <a class="reference internal" href="generated/torch.optim.lr_scheduler.reducelronplateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau" title="torch.optim.lr_scheduler.ReduceLROnPlateau"><code>torch.optim.lr_scheduler.ReduceLROnPlateau</code></a> allows dynamic learning rate reducing based on some validation measurements.</p> <p>Learning rate scheduling should be applied after optimizer’s update; e.g., you should write your code this way:</p> <p>Example:</p> <pre data-language="python">optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
scheduler = ExponentialLR(optimizer, gamma=0.9)

for epoch in range(20):
    for input, target in dataset:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
    scheduler.step()
</pre> <p>Most learning rate schedulers can be called back-to-back (also referred to as chaining schedulers). The result is that each scheduler is applied one after the other on the learning rate obtained by the one preceding it.</p> <p>Example:</p> <pre data-language="python">optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
scheduler1 = ExponentialLR(optimizer, gamma=0.9)
scheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)

for epoch in range(20):
    for input, target in dataset:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
    scheduler1.step()
    scheduler2.step()
</pre> <p>In many places in the documentation, we will use the following template to refer to schedulers algorithms.</p> <pre data-language="python">&gt;&gt;&gt; scheduler = ...
&gt;&gt;&gt; for epoch in range(100):
&gt;&gt;&gt;     train(...)
&gt;&gt;&gt;     validate(...)
&gt;&gt;&gt;     scheduler.step()
</pre> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before the optimizer’s update; 1.1.0 changed this behavior in a BC-breaking way. If you use the learning rate scheduler (calling <code>scheduler.step()</code>) before the optimizer’s update (calling <code>optimizer.step()</code>), this will skip the first value of the learning rate schedule. If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check if you are calling <code>scheduler.step()</code> at the wrong time.</p> </div> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td><p><a class="reference internal" href="generated/torch.optim.lr_scheduler.lambdalr.html#torch.optim.lr_scheduler.LambdaLR" title="torch.optim.lr_scheduler.LambdaLR"><code>lr_scheduler.LambdaLR</code></a></p></td> <td><p>Sets the learning rate of each parameter group to the initial lr times a given function.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.lr_scheduler.multiplicativelr.html#torch.optim.lr_scheduler.MultiplicativeLR" title="torch.optim.lr_scheduler.MultiplicativeLR"><code>lr_scheduler.MultiplicativeLR</code></a></p></td> <td><p>Multiply the learning rate of each parameter group by the factor given in the specified function.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.lr_scheduler.steplr.html#torch.optim.lr_scheduler.StepLR" title="torch.optim.lr_scheduler.StepLR"><code>lr_scheduler.StepLR</code></a></p></td> <td><p>Decays the learning rate of each parameter group by gamma every step_size epochs.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.lr_scheduler.multisteplr.html#torch.optim.lr_scheduler.MultiStepLR" title="torch.optim.lr_scheduler.MultiStepLR"><code>lr_scheduler.MultiStepLR</code></a></p></td> <td><p>Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.lr_scheduler.constantlr.html#torch.optim.lr_scheduler.ConstantLR" title="torch.optim.lr_scheduler.ConstantLR"><code>lr_scheduler.ConstantLR</code></a></p></td> <td><p>Decays the learning rate of each parameter group by a small constant factor until the number of epoch reaches a pre-defined milestone: total_iters.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.lr_scheduler.linearlr.html#torch.optim.lr_scheduler.LinearLR" title="torch.optim.lr_scheduler.LinearLR"><code>lr_scheduler.LinearLR</code></a></p></td> <td><p>Decays the learning rate of each parameter group by linearly changing small multiplicative factor until the number of epoch reaches a pre-defined milestone: total_iters.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.lr_scheduler.exponentiallr.html#torch.optim.lr_scheduler.ExponentialLR" title="torch.optim.lr_scheduler.ExponentialLR"><code>lr_scheduler.ExponentialLR</code></a></p></td> <td><p>Decays the learning rate of each parameter group by gamma every epoch.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.lr_scheduler.polynomiallr.html#torch.optim.lr_scheduler.PolynomialLR" title="torch.optim.lr_scheduler.PolynomialLR"><code>lr_scheduler.PolynomialLR</code></a></p></td> <td><p>Decays the learning rate of each parameter group using a polynomial function in the given total_iters.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.lr_scheduler.cosineannealinglr.html#torch.optim.lr_scheduler.CosineAnnealingLR" title="torch.optim.lr_scheduler.CosineAnnealingLR"><code>lr_scheduler.CosineAnnealingLR</code></a></p></td> <td><p>Set the learning rate of each parameter group using a cosine annealing schedule, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>η</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\eta_{max}</annotation></semantics></math></span></span></span> is set to the initial lr and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mrow><mi>c</mi><mi>u</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{cur}</annotation></semantics></math></span></span></span> is the number of epochs since the last restart in SGDR:</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.lr_scheduler.chainedscheduler.html#torch.optim.lr_scheduler.ChainedScheduler" title="torch.optim.lr_scheduler.ChainedScheduler"><code>lr_scheduler.ChainedScheduler</code></a></p></td> <td><p>Chains list of learning rate schedulers.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.lr_scheduler.sequentiallr.html#torch.optim.lr_scheduler.SequentialLR" title="torch.optim.lr_scheduler.SequentialLR"><code>lr_scheduler.SequentialLR</code></a></p></td> <td><p>Receives the list of schedulers that is expected to be called sequentially during optimization process and milestone points that provides exact intervals to reflect which scheduler is supposed to be called at a given epoch.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.lr_scheduler.reducelronplateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau" title="torch.optim.lr_scheduler.ReduceLROnPlateau"><code>lr_scheduler.ReduceLROnPlateau</code></a></p></td> <td><p>Reduce learning rate when a metric has stopped improving.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.lr_scheduler.cycliclr.html#torch.optim.lr_scheduler.CyclicLR" title="torch.optim.lr_scheduler.CyclicLR"><code>lr_scheduler.CyclicLR</code></a></p></td> <td><p>Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR).</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.lr_scheduler.onecyclelr.html#torch.optim.lr_scheduler.OneCycleLR" title="torch.optim.lr_scheduler.OneCycleLR"><code>lr_scheduler.OneCycleLR</code></a></p></td> <td><p>Sets the learning rate of each parameter group according to the 1cycle learning rate policy.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.optim.lr_scheduler.cosineannealingwarmrestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts" title="torch.optim.lr_scheduler.CosineAnnealingWarmRestarts"><code>lr_scheduler.CosineAnnealingWarmRestarts</code></a></p></td> <td><p>Set the learning rate of each parameter group using a cosine annealing schedule, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>η</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\eta_{max}</annotation></semantics></math></span></span></span> is set to the initial lr, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mrow><mi>c</mi><mi>u</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">T_{cur}</annotation></semantics></math></span></span></span> is the number of epochs since the last restart and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">T_{i}</annotation></semantics></math></span></span></span> is the number of epochs between two warm restarts in SGDR:</p></td> </tr>  </table>   <h2 id="weight-averaging-swa-and-ema">Weight Averaging (SWA and EMA)</h2> <p><code>torch.optim.swa_utils</code> implements Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA). In particular, the <code>torch.optim.swa_utils.AveragedModel</code> class implements SWA and EMA models, <code>torch.optim.swa_utils.SWALR</code> implements the SWA learning rate scheduler and <code>torch.optim.swa_utils.update_bn()</code> is a utility function used to update SWA/EMA batch normalization statistics at the end of training.</p> <p>SWA has been proposed in <a class="reference external" href="https://arxiv.org/abs/1803.05407">Averaging Weights Leads to Wider Optima and Better Generalization</a>.</p> <p>EMA is a widely known technique to reduce the training time by reducing the number of weight updates needed. It is a variation of <a class="reference external" href="https://paperswithcode.com/method/polyak-averaging">Polyak averaging</a>, but using exponential weights instead of equal weights across iterations.</p>  <h3 id="constructing-averaged-models">Constructing averaged models</h3> <p>The <code>AveragedModel</code> class serves to compute the weights of the SWA or EMA model.</p> <p>You can create an SWA averaged model by running:</p> <pre data-language="python">&gt;&gt;&gt; averaged_model = AveragedModel(model)
</pre> <p>EMA models are constructed by specifying the <code>multi_avg_fn</code> argument as follows:</p> <pre data-language="python">&gt;&gt;&gt; decay = 0.999
&gt;&gt;&gt; averaged_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(decay))
</pre> <p>Decay is a parameter between 0 and 1 that controls how fast the averaged parameters are decayed. If not provided to <code>get_ema_multi_avg_fn</code>, the default is 0.999.</p> <p><code>get_ema_multi_avg_fn</code> returns a function that applies the following EMA equation to the weights:</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>W</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mtext>EMA</mtext></msubsup><mo>=</mo><mi>α</mi><msubsup><mi>W</mi><mi>t</mi><mtext>EMA</mtext></msubsup><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="false">)</mo><msubsup><mi>W</mi><mi>t</mi><mtext>model</mtext></msubsup></mrow><annotation encoding="application/x-tex">W^\textrm{EMA}_{t+1} = \alpha W^\textrm{EMA}_{t} + (1 - \alpha) W^\textrm{model}_t </annotation></semantics></math></span></span></span>
</div>
<p>where alpha is the EMA decay.</p> <p>Here the model <code>model</code> can be an arbitrary <a class="reference internal" href="generated/torch.nn.module.html#torch.nn.Module" title="torch.nn.Module"><code>torch.nn.Module</code></a> object. <code>averaged_model</code> will keep track of the running averages of the parameters of the <code>model</code>. To update these averages, you should use the <code>update_parameters()</code> function after the <code>optimizer.step()</code>:</p> <pre data-language="python">&gt;&gt;&gt; averaged_model.update_parameters(model)
</pre> <p>For SWA and EMA, this call is usually done right after the optimizer <code>step()</code>. In the case of SWA, this is usually skipped for some numbers of steps at the beginning of the training.</p>   <h3 id="custom-averaging-strategies">Custom averaging strategies</h3> <p>By default, <code>torch.optim.swa_utils.AveragedModel</code> computes a running equal average of the parameters that you provide, but you can also use custom averaging functions with the <code>avg_fn</code> or <code>multi_avg_fn</code> parameters:</p> <ul class="simple"> <li>
<code>avg_fn</code> allows defining a function operating on each parameter tuple (averaged parameter, model parameter) and should return the new averaged parameter.</li> <li>
<code>multi_avg_fn</code> allows defining more efficient operations acting on a tuple of parameter lists, (averaged parameter list, model parameter list), at the same time, for example using the <code>torch._foreach*</code> functions. This function must update the averaged parameters in-place.</li> </ul> <p>In the following example <code>ema_model</code> computes an exponential moving average using the <code>avg_fn</code> parameter:</p> <pre data-language="python">&gt;&gt;&gt; ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\
&gt;&gt;&gt;         0.9 * averaged_model_parameter + 0.1 * model_parameter
&gt;&gt;&gt; ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg)
</pre> <p>In the following example <code>ema_model</code> computes an exponential moving average using the more efficient <code>multi_avg_fn</code> parameter:</p> <pre data-language="python">&gt;&gt;&gt; ema_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(0.9))
</pre>   <h3 id="swa-learning-rate-schedules">SWA learning rate schedules</h3> <p>Typically, in SWA the learning rate is set to a high constant value. <code>SWALR</code> is a learning rate scheduler that anneals the learning rate to a fixed value, and then keeps it constant. For example, the following code creates a scheduler that linearly anneals the learning rate from its initial value to 0.05 in 5 epochs within each parameter group:</p> <pre data-language="python">&gt;&gt;&gt; swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \
&gt;&gt;&gt;         anneal_strategy="linear", anneal_epochs=5, swa_lr=0.05)
</pre> <p>You can also use cosine annealing to a fixed value instead of linear annealing by setting <code>anneal_strategy="cos"</code>.</p>   <h3 id="taking-care-of-batch-normalization">Taking care of batch normalization</h3> <p><code>update_bn()</code> is a utility function that allows to compute the batchnorm statistics for the SWA model on a given dataloader <code>loader</code> at the end of training:</p> <pre data-language="python">&gt;&gt;&gt; torch.optim.swa_utils.update_bn(loader, swa_model)
</pre> <p><code>update_bn()</code> applies the <code>swa_model</code> to every element in the dataloader and computes the activation statistics for each batch normalization layer in the model.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p><code>update_bn()</code> assumes that each batch in the dataloader <code>loader</code> is either a tensors or a list of tensors where the first element is the tensor that the network <code>swa_model</code> should be applied to. If your dataloader has a different structure, you can update the batch normalization statistics of the <code>swa_model</code> by doing a forward pass with the <code>swa_model</code> on each element of the dataset.</p> </div>   <h3 id="putting-it-all-together-swa">Putting it all together: SWA</h3> <p>In the example below, <code>swa_model</code> is the SWA model that accumulates the averages of the weights. We train the model for a total of 300 epochs and we switch to the SWA learning rate schedule and start to collect SWA averages of the parameters at epoch 160:</p> <pre data-language="python">&gt;&gt;&gt; loader, optimizer, model, loss_fn = ...
&gt;&gt;&gt; swa_model = torch.optim.swa_utils.AveragedModel(model)
&gt;&gt;&gt; scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)
&gt;&gt;&gt; swa_start = 160
&gt;&gt;&gt; swa_scheduler = SWALR(optimizer, swa_lr=0.05)
&gt;&gt;&gt;
&gt;&gt;&gt; for epoch in range(300):
&gt;&gt;&gt;       for input, target in loader:
&gt;&gt;&gt;           optimizer.zero_grad()
&gt;&gt;&gt;           loss_fn(model(input), target).backward()
&gt;&gt;&gt;           optimizer.step()
&gt;&gt;&gt;       if epoch &gt; swa_start:
&gt;&gt;&gt;           swa_model.update_parameters(model)
&gt;&gt;&gt;           swa_scheduler.step()
&gt;&gt;&gt;       else:
&gt;&gt;&gt;           scheduler.step()
&gt;&gt;&gt;
&gt;&gt;&gt; # Update bn statistics for the swa_model at the end
&gt;&gt;&gt; torch.optim.swa_utils.update_bn(loader, swa_model)
&gt;&gt;&gt; # Use swa_model to make predictions on test data
&gt;&gt;&gt; preds = swa_model(test_input)
</pre>   <h3 id="putting-it-all-together-ema">Putting it all together: EMA</h3> <p>In the example below, <code>ema_model</code> is the EMA model that accumulates the exponentially-decayed averages of the weights with a decay rate of 0.999. We train the model for a total of 300 epochs and start to collect EMA averages immediately.</p> <pre data-language="python">&gt;&gt;&gt; loader, optimizer, model, loss_fn = ...
&gt;&gt;&gt; ema_model = torch.optim.swa_utils.AveragedModel(model, \
&gt;&gt;&gt;             multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.999))
&gt;&gt;&gt;
&gt;&gt;&gt; for epoch in range(300):
&gt;&gt;&gt;       for input, target in loader:
&gt;&gt;&gt;           optimizer.zero_grad()
&gt;&gt;&gt;           loss_fn(model(input), target).backward()
&gt;&gt;&gt;           optimizer.step()
&gt;&gt;&gt;           ema_model.update_parameters(model)
&gt;&gt;&gt;
&gt;&gt;&gt; # Update bn statistics for the ema_model at the end
&gt;&gt;&gt; torch.optim.swa_utils.update_bn(loader, ema_model)
&gt;&gt;&gt; # Use ema_model to make predictions on test data
&gt;&gt;&gt; preds = ema_model(test_input)
</pre><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/optim.html" class="_attribution-link">https://pytorch.org/docs/2.1/optim.html</a>
  </p>
</div>
