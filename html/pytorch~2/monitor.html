<h1 id="torch-monitor">torch.monitor</h1> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This module is a prototype release, and its interfaces and functionality may change without warning in future PyTorch releases.</p> </div> <p><code>torch.monitor</code> provides an interface for logging events and counters from PyTorch.</p> <p>The stat interfaces are designed to be used for tracking high level metrics that are periodically logged out to be used for monitoring system performance. Since the stats aggregate with a specific window size you can log to them from critical loops with minimal performance impact.</p> <p>For more infrequent events or values such as loss, accuracy, usage tracking the event interface can be directly used.</p> <p>Event handlers can be registered to handle the events and pass them to an external event sink.</p>  <h2 id="api-reference">API Reference</h2> <dl class="py class" id="module-torch.monitor"> <dt class="sig sig-object py" id="torch.monitor.Aggregation">
<code>class torch.monitor.Aggregation</code> </dt> <dd> <p>These are types of aggregations that can be used to accumulate stats.</p>  <p>Members:</p>  <dl class="simple"> <dt>VALUE :</dt>
<dd>
<p>VALUE returns the last value to be added.</p> </dd> <dt>MEAN :</dt>
<dd>
<p>MEAN computes the arithmetic mean of all the added values.</p> </dd> <dt>COUNT :</dt>
<dd>
<p>COUNT returns the total number of added values.</p> </dd> <dt>SUM :</dt>
<dd>
<p>SUM returns the sum of the added values.</p> </dd> <dt>MAX :</dt>
<dd>
<p>MAX returns the max of the added values.</p> </dd> <dt>MIN :</dt>
<dd>
<p>MIN returns the min of the added values.</p> </dd> </dl>  <dl class="py property"> <dt class="sig sig-object py" id="torch.monitor.Aggregation.name">
<code>property name</code> </dt> 
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.monitor.Stat">
<code>class torch.monitor.Stat</code> </dt> <dd>
<p>Stat is used to compute summary statistics in a performant way over fixed intervals. Stat logs the statistics as an Event once every <code>window_size</code> duration. When the window closes the stats are logged via the event handlers as a <code>torch.monitor.Stat</code> event.</p> <p><code>window_size</code> should be set to something relatively high to avoid a huge number of events being logged. Ex: 60s. Stat uses millisecond precision.</p> <p>If <code>max_samples</code> is set, the stat will cap the number of samples per window by discarding <code>add</code> calls once <code>max_samples</code> adds have occurred. If it’s not set, all <code>add</code> calls during the window will be included. This is an optional field to make aggregations more directly comparable across windows when the number of samples might vary.</p> <p>When the Stat is destructed it will log any remaining data even if the window hasn’t elapsed.</p> <dl class="py method"> <dt class="sig sig-object py" id="torch.monitor.Stat.__init__">
<code>__init__(self: torch._C._monitor.Stat, name: str, aggregations: List[torch._C._monitor.Aggregation], window_size: datetime.timedelta, max_samples: int = 9223372036854775807) → None</code> </dt> <dd>
<p>Constructs the <code>Stat</code>.</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.monitor.Stat.add">
<code>add(self: torch._C._monitor.Stat, v: float) → None</code> </dt> <dd>
<p>Adds a value to the stat to be aggregated according to the configured stat type and aggregations.</p> </dd>
</dl> <dl class="py property"> <dt class="sig sig-object py" id="torch.monitor.Stat.count">
<code>property count</code> </dt> <dd>
<p>Number of data points that have currently been collected. Resets once the event has been logged.</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.monitor.Stat.get">
<code>get(self: torch._C._monitor.Stat) → Dict[torch._C._monitor.Aggregation, float]</code> </dt> <dd>
<p>Returns the current value of the stat, primarily for testing purposes. If the stat has logged and no additional values have been added this will be zero.</p> </dd>
</dl> <dl class="py property"> <dt class="sig sig-object py" id="torch.monitor.Stat.name">
<code>property name</code> </dt> <dd>
<p>The name of the stat that was set during creation.</p> </dd>
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.monitor.data_value_t">
<code>class torch.monitor.data_value_t</code> </dt> <dd>
<p>data_value_t is one of <code>str</code>, <code>float</code>, <code>int</code>, <code>bool</code>.</p> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.monitor.Event">
<code>class torch.monitor.Event</code> </dt> <dd>
<p>Event represents a specific typed event to be logged. This can represent high-level data points such as loss or accuracy per epoch or more low-level aggregations such as through the Stats provided through this library.</p> <p>All Events of the same type should have the same name so downstream handlers can correctly process them.</p> <dl class="py method"> <dt class="sig sig-object py" id="torch.monitor.Event.__init__">
<code>__init__(self: torch._C._monitor.Event, name: str, timestamp: datetime.datetime, data: Dict[str, data_value_t]) → None</code> </dt> <dd>
<p>Constructs the <code>Event</code>.</p> </dd>
</dl> <dl class="py property"> <dt class="sig sig-object py" id="torch.monitor.Event.data">
<code>property data</code> </dt> <dd>
<p>The structured data contained within the <code>Event</code>.</p> </dd>
</dl> <dl class="py property"> <dt class="sig sig-object py" id="torch.monitor.Event.name">
<code>property name</code> </dt> <dd>
<p>The name of the <code>Event</code>.</p> </dd>
</dl> <dl class="py property"> <dt class="sig sig-object py" id="torch.monitor.Event.timestamp">
<code>property timestamp</code> </dt> <dd>
<p>The timestamp when the <code>Event</code> happened.</p> </dd>
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.monitor.EventHandlerHandle">
<code>class torch.monitor.EventHandlerHandle</code> </dt> <dd>
<p>EventHandlerHandle is a wrapper type returned by <code>register_event_handler</code> used to unregister the handler via <code>unregister_event_handler</code>. This cannot be directly initialized.</p> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.monitor.log_event">
<code>torch.monitor.log_event(event: torch._C._monitor.Event) → None</code> </dt> <dd>
<p>log_event logs the specified event to all of the registered event handlers. It’s up to the event handlers to log the event out to the corresponding event sink.</p> <p>If there are no event handlers registered this method is a no-op.</p> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.monitor.register_event_handler">
<code>torch.monitor.register_event_handler(callback: Callable[[torch._C._monitor.Event], None]) → torch._C._monitor.EventHandlerHandle</code> </dt> <dd>
<p>register_event_handler registers a callback to be called whenever an event is logged via <code>log_event</code>. These handlers should avoid blocking the main thread since that may interfere with training as they run during the <code>log_event</code> call.</p> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.monitor.unregister_event_handler">
<code>torch.monitor.unregister_event_handler(handler: torch._C._monitor.EventHandlerHandle) → None</code> </dt> <dd>
<p>unregister_event_handler unregisters the <code>EventHandlerHandle</code> returned after calling <code>register_event_handler</code>. After this returns the event handler will no longer receive events.</p> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.monitor.TensorboardEventHandler">
<code>class torch.monitor.TensorboardEventHandler(writer)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/monitor.html#TensorboardEventHandler"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>TensorboardEventHandler is an event handler that will write known events to the provided SummaryWriter.</p> <p>This currently only supports <code>torch.monitor.Stat</code> events which are logged as scalars.</p> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; from torch.utils.tensorboard import SummaryWriter
&gt;&gt;&gt; from torch.monitor import TensorboardEventHandler, register_event_handler
&gt;&gt;&gt; writer = SummaryWriter("log_dir")
&gt;&gt;&gt; register_event_handler(TensorboardEventHandler(writer))
</pre>  <dl class="py method"> <dt class="sig sig-object py" id="torch.monitor.TensorboardEventHandler.__init__">
<code>__init__(writer)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/monitor.html#TensorboardEventHandler.__init__"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Constructs the <code>TensorboardEventHandler</code>.</p>  </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/monitor.html" class="_attribution-link">https://pytorch.org/docs/2.1/monitor.html</a>
  </p>
</div>
