<h1 id="id1">Pipeline Parallelism</h1> <p id="pipeline-parallelism">Pipeline parallelism was original introduced in the <a class="reference external" href="https://arxiv.org/abs/1811.06965">Gpipe</a> paper and is an efficient technique to train large models on multiple GPUs.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Pipeline Parallelism is experimental and subject to change.</p> </div>  <h2 id="model-parallelism-using-multiple-gpus">Model Parallelism using multiple GPUs</h2> <p>Typically for large models which don’t fit on a single GPU, model parallelism is employed where certain parts of the model are placed on different GPUs. Although, if this is done naively for sequential models, the training process suffers from GPU under utilization since only one GPU is active at one time as shown in the figure below:</p> <div class="figure align-default" id="id2"> <img alt="_images/no_pipe.png" src="data:image/png;base64,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"> <p class="caption"><span class="caption-text">The figure represents a model with 4 layers placed on 4 different GPUs (vertical axis). The horizontal axis represents training this model through time demonstrating that only 1 GPU is utilized at a time (<a class="reference external" href="https://arxiv.org/abs/1811.06965">image source</a>).</span></p> </div>   <h2 id="pipelined-execution">Pipelined Execution</h2> <p>To alleviate this problem, pipeline parallelism splits the input minibatch into multiple microbatches and pipelines the execution of these microbatches across multiple GPUs. This is outlined in the figure below:</p> <div class="figure align-default" id="id3"> <img alt="_images/pipe.png" src="data:image/png;base64,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"> <p class="caption"><span class="caption-text">The figure represents a model with 4 layers placed on 4 different GPUs (vertical axis). The horizontal axis represents training this model through time demonstrating that the GPUs are utilized much more efficiently. However, there still exists a bubble (as demonstrated in the figure) where certain GPUs are not utilized. (<a class="reference external" href="https://arxiv.org/abs/1811.06965">image source</a>).</span></p> </div>   <h2 id="pipe-apis-in-pytorch">Pipe APIs in PyTorch</h2> <dl class="py class"> <dt class="sig sig-object py" id="torch.distributed.pipeline.sync.Pipe">
<code>class torch.distributed.pipeline.sync.Pipe(module, chunks=1, checkpoint='except_last', deferred_batch_norm=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/distributed/pipeline/sync/pipe.html#Pipe"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Wraps an arbitrary <a class="reference internal" href="generated/torch.nn.sequential.html#torch.nn.Sequential" title="torch.nn.Sequential"><code>nn.Sequential</code></a> module to train on using synchronous pipeline parallelism. If the module requires lots of memory and doesn’t fit on a single GPU, pipeline parallelism is a useful technique to employ for training.</p> <p>The implementation is based on the <a class="reference external" href="https://arxiv.org/abs/2004.09910">torchgpipe</a> paper.</p> <p>Pipe combines pipeline parallelism with checkpointing to reduce peak memory required to train while minimizing device under-utilization.</p> <p>You should place all the modules on the appropriate devices and wrap them into an <a class="reference internal" href="generated/torch.nn.sequential.html#torch.nn.Sequential" title="torch.nn.Sequential"><code>nn.Sequential</code></a> module defining the desired order of execution. If a module does not contain any parameters/buffers, it is assumed this module should be executed on CPU and appropriate input tensors to the module are moved to CPU before execution. This behavior can be overridden by the <code>WithDevice</code> wrapper which can be used to explicitly specify which device a module should run on.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>module</strong> (<a class="reference internal" href="generated/torch.nn.sequential.html#torch.nn.Sequential" title="torch.nn.Sequential"><code>nn.Sequential</code></a>) – sequential module to be parallelized using pipelining. Each module in the sequence has to have all of its parameters on a single device. Each module in the sequence has to either be an nn.Module or <a class="reference internal" href="generated/torch.nn.sequential.html#torch.nn.Sequential" title="torch.nn.Sequential"><code>nn.Sequential</code></a> (to combine multiple sequential modules on a single device)</li> <li>
<strong>chunks</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – number of micro-batches (default: <code>1</code>)</li> <li>
<strong>checkpoint</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – when to enable checkpointing, one of <code>'always'</code>, <code>'except_last'</code>, or <code>'never'</code> (default: <code>'except_last'</code>). <code>'never'</code> disables checkpointing completely, <code>'except_last'</code> enables checkpointing for all micro-batches except the last one and <code>'always'</code> enables checkpointing for all micro-batches.</li> <li>
<strong>deferred_batch_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – whether to use deferred <code>BatchNorm</code> moving statistics (default: <a class="reference external" href="https://docs.python.org/3/library/constants.html#False" title="(in Python v3.12)"><code>False</code></a>). If set to <a class="reference external" href="https://docs.python.org/3/library/constants.html#True" title="(in Python v3.12)"><code>True</code></a>, we track statistics across multiple micro-batches to update the running statistics per mini-batch.</li> </ul> </dd> <dt class="field-even">Raises</dt> <dd class="field-even">
<ul class="simple"> <li>
<a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><strong>TypeError</strong></a> – the module is not a <a class="reference internal" href="generated/torch.nn.sequential.html#torch.nn.Sequential" title="torch.nn.Sequential"><code>nn.Sequential</code></a>.</li> <li>
<a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)"><strong>ValueError</strong></a> – invalid arguments</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>
<dd>
<p>Pipeline of two FC layers across GPUs 0 and 1.</p> <pre data-language="python">&gt;&gt;&gt; # Need to initialize RPC framework first.
&gt;&gt;&gt; os.environ['MASTER_ADDR'] = 'localhost'
&gt;&gt;&gt; os.environ['MASTER_PORT'] = '29500'
&gt;&gt;&gt; torch.distributed.rpc.init_rpc('worker', rank=0, world_size=1)
&gt;&gt;&gt;
&gt;&gt;&gt; # Build pipe.
&gt;&gt;&gt; fc1 = nn.Linear(16, 8).cuda(0)
&gt;&gt;&gt; fc2 = nn.Linear(8, 4).cuda(1)
&gt;&gt;&gt; model = nn.Sequential(fc1, fc2)
&gt;&gt;&gt; model = Pipe(model, chunks=8)
&gt;&gt;&gt; input = torch.rand(16, 16).cuda(0)
&gt;&gt;&gt; output_rref = model(input)
</pre> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>You can wrap a <a class="reference internal" href="#torch.distributed.pipeline.sync.Pipe" title="torch.distributed.pipeline.sync.Pipe"><code>Pipe</code></a> model with <a class="reference internal" href="generated/torch.nn.parallel.distributeddataparallel.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code>torch.nn.parallel.DistributedDataParallel</code></a> only when the checkpoint parameter of <a class="reference internal" href="#torch.distributed.pipeline.sync.Pipe" title="torch.distributed.pipeline.sync.Pipe"><code>Pipe</code></a> is <code>'never'</code>.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p><a class="reference internal" href="#torch.distributed.pipeline.sync.Pipe" title="torch.distributed.pipeline.sync.Pipe"><code>Pipe</code></a> only supports intra-node pipelining currently, but will be expanded to support inter-node pipelining in the future. The forward function returns an <code>RRef</code> to allow for inter-node pipelining in the future, where the output might be on a remote host. For intra-node pipelining you can use <code>local_value()</code> to retrieve the output locally.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p><a class="reference internal" href="#torch.distributed.pipeline.sync.Pipe" title="torch.distributed.pipeline.sync.Pipe"><code>Pipe</code></a> is experimental and subject to change.</p> </div> <dl class="py method"> <dt class="sig sig-object py" id="torch.distributed.pipeline.sync.Pipe.forward">
<code>forward(*inputs)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/distributed/pipeline/sync/pipe.html#Pipe.forward"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Processes a single input mini-batch through the pipe and returns an <code>RRef</code> pointing to the output. <a class="reference internal" href="#torch.distributed.pipeline.sync.Pipe" title="torch.distributed.pipeline.sync.Pipe"><code>Pipe</code></a> is a fairly transparent module wrapper. It doesn’t modify the input and output signature of the underlying module. But there’s type restriction. Input and output have to contain at least one tensor. This restriction is applied at partition boundaries too.</p> <p>The sequence of inputs are fed into the first stage of the pipeline as <code>*inputs</code>. As a result the positional args for this function should match the positional args for the first stage of the pipeline. The same condition applies for output of one stage of the pipeline which is the input for the next stage.</p> <p>The input tensor is split into multiple micro-batches based on the <code>chunks</code> parameter used to initialize <a class="reference internal" href="#torch.distributed.pipeline.sync.Pipe" title="torch.distributed.pipeline.sync.Pipe"><code>Pipe</code></a>. The batch size is assumed to be the first dimension of the tensor and if the batch size is less than <code>chunks</code>, the number of micro-batches is equal to the batch size.</p> <p>Only tensors are split into multiple micro-batches, non-Tensor inputs are just replicated as-is in each micro-batch. For non-Tensor outputs in the last stage of the pipeline, they are aggregated as a <code>List</code> and returned the user. For example, if you have 2 micro-batches returning the integer 5, the user would receive the consolidated output of <code>[5, 5]</code></p> <p>All the input tensors need to be on the same device as the first partition of the pipeline.</p> <p>If a tensor is wrapped with the <code>NoChunk</code> wrapper, the tensor is not split across micro-batches and is replicated as-is similar to non-tensors.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>inputs</strong> – input mini-batch</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p><code>RRef</code> to the output of the mini-batch</p> </dd> <dt class="field-odd">Raises</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><strong>TypeError</strong></a> – input doesn’t contain at least one tensor</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><em>RRef</em></p> </dd> </dl> </dd>
</dl> </dd>
</dl>  <h3 id="skip-connections">Skip connections</h3> <p>Certain models like <a class="reference external" href="https://pytorch.org/hub/pytorch_vision_resnext/">ResNeXt</a> are not completely sequential and have skip connections between layers. Naively implementing as part of pipeline parallelism would imply that we need to copy outputs for certain layers through multiple GPUs till we eventually reach the GPU where the layer for the skip connection resides. To avoid this copy overhead, we provide APIs below to stash and pop Tensors in different layers of the model.</p> <dl class="py function"> <dt class="sig sig-object py" id="torch.distributed.pipeline.sync.skip.skippable.skippable">
<code>torch.distributed.pipeline.sync.skip.skippable.skippable(stash=(), pop=())</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/distributed/pipeline/sync/skip/skippable.html#skippable"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>The decorator to define a <a class="reference internal" href="generated/torch.nn.module.html#torch.nn.Module" title="torch.nn.Module"><code>nn.Module</code></a> with skip connections. Decorated modules are called “skippable”. This functionality works perfectly fine even when the module is not wrapped by <a class="reference internal" href="#torch.distributed.pipeline.sync.Pipe" title="torch.distributed.pipeline.sync.Pipe"><code>Pipe</code></a>.</p> <p>Each skip tensor is managed by its name. Before manipulating skip tensors, a skippable module must statically declare the names for skip tensors by <code>stash</code> and/or <code>pop</code> parameters. Skip tensors with pre-declared name can be stashed by <code>yield stash(name, tensor)</code> or popped by <code>tensor = yield
pop(name)</code>.</p> <p>Here is an example with three layers. A skip tensor named “1to3” is stashed and popped at the first and last layer, respectively:</p> <pre data-language="python">@skippable(stash=['1to3'])
class Layer1(nn.Module):
    def forward(self, input):
        yield stash('1to3', input)
        return f1(input)

class Layer2(nn.Module):
    def forward(self, input):
        return f2(input)

@skippable(pop=['1to3'])
class Layer3(nn.Module):
    def forward(self, input):
        skip_1to3 = yield pop('1to3')
        return f3(input) + skip_1to3

model = nn.Sequential(Layer1(), Layer2(), Layer3())
</pre> <p>One skippable module can stash or pop multiple skip tensors:</p> <pre data-language="python">@skippable(stash=['alice', 'bob'], pop=['carol'])
class StashStashPop(nn.Module):
    def forward(self, input):
        yield stash('alice', f_alice(input))
        yield stash('bob', f_bob(input))
        carol = yield pop('carol')
        return input + carol
</pre> <p>Every skip tensor must be associated with exactly one pair of <code>stash</code> and <code>pop</code>. <a class="reference internal" href="#torch.distributed.pipeline.sync.Pipe" title="torch.distributed.pipeline.sync.Pipe"><code>Pipe</code></a> checks this restriction automatically when wrapping a module. You can also check the restriction by <a class="reference internal" href="#torch.distributed.pipeline.sync.skip.skippable.verify_skippables" title="torch.distributed.pipeline.sync.skip.skippable.verify_skippables"><code>verify_skippables()</code></a> without <a class="reference internal" href="#torch.distributed.pipeline.sync.Pipe" title="torch.distributed.pipeline.sync.Pipe"><code>Pipe</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.12)">Callable</a>[[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Type" title="(in Python v3.12)">Type</a>[<a class="reference internal" href="generated/torch.nn.module.html#torch.nn.Module" title="torch.nn.modules.module.Module">Module</a>]], <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Type" title="(in Python v3.12)">Type</a>[<em>Skippable</em>]]</p> </dd> </dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.distributed.pipeline.sync.skip.skippable.stash">
<code>class torch.distributed.pipeline.sync.skip.skippable.stash(name, tensor)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/distributed/pipeline/sync/skip/skippable.html#stash"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>The command to stash a skip tensor.</p> <pre data-language="python">def forward(self, input):
    yield stash('name', input)
    return f(input)
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – name of skip tensor</li> <li>
<strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a><em> or </em><em>None</em>) – tensor to pass to the skip connection</li> </ul> </dd> </dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.distributed.pipeline.sync.skip.skippable.pop">
<code>class torch.distributed.pipeline.sync.skip.skippable.pop(name)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/distributed/pipeline/sync/skip/skippable.html#pop"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>The command to pop a skip tensor.</p> <pre data-language="python">def forward(self, input):
    skip = yield pop('name')
    return f(input) + skip
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – name of skip tensor</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>the skip tensor previously stashed by another layer under the same name</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p>None</p> </dd> </dl> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.distributed.pipeline.sync.skip.skippable.verify_skippables">
<code>torch.distributed.pipeline.sync.skip.skippable.verify_skippables(module)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/distributed/pipeline/sync/skip/skippable.html#verify_skippables"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Verifies if the underlying skippable modules satisfy integrity.</p> <p>Every skip tensor must have only one pair of <code>stash</code> and <code>pop</code>. If there are one or more unmatched pairs, it will raise <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><code>TypeError</code></a> with the detailed messages.</p> <p>Here are a few failure cases. <a class="reference internal" href="#torch.distributed.pipeline.sync.skip.skippable.verify_skippables" title="torch.distributed.pipeline.sync.skip.skippable.verify_skippables"><code>verify_skippables()</code></a> will report failure for these cases:</p> <pre data-language="python"># Layer1 stashes "1to3".
# Layer3 pops "1to3".

nn.Sequential(Layer1(), Layer2())
#               └──── ?

nn.Sequential(Layer2(), Layer3())
#                   ? ────┘

nn.Sequential(Layer1(), Layer2(), Layer3(), Layer3())
#               └───────────────────┘       ^^^^^^

nn.Sequential(Layer1(), Layer1(), Layer2(), Layer3())
#             ^^^^^^      └───────────────────┘
</pre> <p>To use the same name for multiple skip tensors, they must be isolated by different namespaces. See <code>isolate()</code>.</p> <dl class="field-list simple"> <dt class="field-odd">Raises</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><strong>TypeError</strong></a> – one or more pairs of <code>stash</code> and <code>pop</code> are not matched.</p> </dd> </dl> </dd>
</dl>    <h2 id="tutorials">Tutorials</h2> <p>The following tutorials give a good overview of how to use the <a class="reference internal" href="#torch.distributed.pipeline.sync.Pipe" title="torch.distributed.pipeline.sync.Pipe"><code>Pipe</code></a> API to train your models with the rest of the components that PyTorch provides:</p> <ul class="simple"> <li><a class="reference external" href="https://pytorch.org/tutorials/intermediate/pipeline_tutorial.html">Training Transformer models using Pipeline Parallelism</a></li> <li><a class="reference external" href="https://pytorch.org/tutorials/advanced/ddp_pipeline.html">Training Transformer models using Distributed Data Parallel and Pipeline Parallelism</a></li> </ul>   <h2 id="acknowledgements">Acknowledgements</h2> <p>The implementation for pipeline parallelism is based on <a class="reference external" href="https://github.com/facebookresearch/fairscale/tree/main/fairscale/nn/pipe">fairscale’s pipe implementation</a> and <a class="reference external" href="https://github.com/kakaobrain/torchgpipe">torchgpipe</a>. We would like to thank both teams for their contributions and guidance towards bringing pipeline parallelism into PyTorch.</p><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/pipeline.html" class="_attribution-link">https://pytorch.org/docs/2.1/pipeline.html</a>
  </p>
</div>
