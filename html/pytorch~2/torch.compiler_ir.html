<h1 id="torch-compiler-ir">IRs</h1> <p id="irs">PyTorch 2.0 offers two set of IRs for backends to interface with: Core Aten IR and Prims IR.</p>  <h2 id="core-aten-ir">Core Aten IR</h2> <p>Core aten ops is the core subset of aten operators that can be used to compose other operators. Core aten IR is fully functional, and there is no <code>inplace</code> or <code>_out</code> variants in this opset. In contrast to Prims IR, core aten ops reuses the existing aten ops in “native_functions.yaml”, and it doesn’t further decompose ops into explicit type promotion and broadcasting ops. This opset is designed to serve as the functional IR to interface with backends.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This opset is still under active development, more ops will be added in the future.</p> </div> <table class="colwidths-auto docutils align-default"> <thead> <tr>
<th class="head"><p>Operator</p></th> <th class="head"><p>Schema</p></th> </tr> </thead>  <tr>
<td><p><code>aten._adaptive_avg_pool2d</code></p></td> <td><p>_adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten._adaptive_avg_pool2d_backward</code></p></td> <td><p>_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten._adaptive_avg_pool3d</code></p></td> <td><p>_adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten._cdist_forward</code></p></td> <td><p>_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten._embedding_bag</code></p></td> <td><p>_embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -&gt; (Tensor, Tensor, Tensor, Tensor)</p></td> </tr> <tr>
<td><p><code>aten._local_scalar_dense</code></p></td> <td><p>_local_scalar_dense(Tensor self) -&gt; Scalar</p></td> </tr> <tr>
<td><p><code>aten._log_softmax</code></p></td> <td><p>_log_softmax(Tensor self, int dim, bool half_to_float) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten._native_batch_norm_legit</code></p></td> <td><p>_native_batch_norm_legit(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, bool training, float momentum, float eps) -&gt; (Tensor, Tensor, Tensor)</p></td> </tr> <tr>
<td><p><code>aten._native_batch_norm_legit.no_stats</code></p></td> <td><p>_native_batch_norm_legit.no_stats(Tensor input, Tensor? weight, Tensor? bias, bool training, float momentum, float eps) -&gt; (Tensor, Tensor, Tensor)</p></td> </tr> <tr>
<td><p><code>aten._native_batch_norm_legit_no_training</code></p></td> <td><p>_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -&gt; (Tensor, Tensor, Tensor)</p></td> </tr> <tr>
<td><p><code>aten._pdist_forward</code></p></td> <td><p>_pdist_forward(Tensor self, float p=2) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten._softmax</code></p></td> <td><p>_softmax(Tensor self, int dim, bool half_to_float) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten._to_copy</code></p></td> <td><p>_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.abs</code></p></td> <td><p>abs(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.acos</code></p></td> <td><p>acos(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.acosh</code></p></td> <td><p>acosh(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.adaptive_avg_pool1d</code></p></td> <td><p>adaptive_avg_pool1d(Tensor self, int[1] output_size) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.add.Scalar</code></p></td> <td><p>add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.add.Tensor</code></p></td> <td><p>add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.addmm</code></p></td> <td><p>addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.alias</code></p></td> <td><p>alias(Tensor(a) self) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>aten.amax</code></p></td> <td><p>amax(Tensor self, int[1] dim=[], bool keepdim=False) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.amin</code></p></td> <td><p>amin(Tensor self, int[1] dim=[], bool keepdim=False) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.any</code></p></td> <td><p>any(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.any.dim</code></p></td> <td><p>any.dim(Tensor self, int dim, bool keepdim=False) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.arange.start_step</code></p></td> <td><p>arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.argmax</code></p></td> <td><p>argmax(Tensor self, int? dim=None, bool keepdim=False) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.argmin</code></p></td> <td><p>argmin(Tensor self, int? dim=None, bool keepdim=False) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.as_strided</code></p></td> <td><p>as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>aten.asin</code></p></td> <td><p>asin(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.asinh</code></p></td> <td><p>asinh(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.atan</code></p></td> <td><p>atan(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.atanh</code></p></td> <td><p>atanh(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.avg_pool1d</code></p></td> <td><p>avg_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, bool ceil_mode=False, bool count_include_pad=True) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.avg_pool2d</code></p></td> <td><p>avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.avg_pool2d_backward</code></p></td> <td><p>avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.avg_pool3d</code></p></td> <td><p>avg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.bitwise_and.Scalar</code></p></td> <td><p>bitwise_and.Scalar(Tensor self, Scalar other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.bitwise_and.Tensor</code></p></td> <td><p>bitwise_and.Tensor(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.bitwise_not</code></p></td> <td><p>bitwise_not(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.bitwise_or.Scalar</code></p></td> <td><p>bitwise_or.Scalar(Tensor self, Scalar other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.bitwise_or.Tensor</code></p></td> <td><p>bitwise_or.Tensor(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.bitwise_xor.Scalar</code></p></td> <td><p>bitwise_xor.Scalar(Tensor self, Scalar other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.bitwise_xor.Tensor</code></p></td> <td><p>bitwise_xor.Tensor(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.bmm</code></p></td> <td><p>bmm(Tensor self, Tensor mat2) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.cat</code></p></td> <td><p>cat(Tensor[] tensors, int dim=0) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.ceil</code></p></td> <td><p>ceil(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.clamp</code></p></td> <td><p>clamp(Tensor self, Scalar? min=None, Scalar? max=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.clamp.Tensor</code></p></td> <td><p>clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.clone</code></p></td> <td><p>clone(Tensor self, *, MemoryFormat? memory_format=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.col2im</code></p></td> <td><p>col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.constant_pad_nd</code></p></td> <td><p>constant_pad_nd(Tensor self, SymInt[] pad, Scalar value=0) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.convolution</code></p></td> <td><p>convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.convolution_backward</code></p></td> <td><p>convolution_backward(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask) -&gt; (Tensor, Tensor, Tensor)</p></td> </tr> <tr>
<td><p><code>aten.cos</code></p></td> <td><p>cos(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.cosh</code></p></td> <td><p>cosh(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.cumsum</code></p></td> <td><p>cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.div.Scalar</code></p></td> <td><p>div.Scalar(Tensor self, Scalar other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.div.Tensor</code></p></td> <td><p>div.Tensor(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.embedding</code></p></td> <td><p>embedding(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.embedding_dense_backward</code></p></td> <td><p>embedding_dense_backward(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.empty.memory_format</code></p></td> <td><p>empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.empty_strided</code></p></td> <td><p>empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.eq.Scalar</code></p></td> <td><p>eq.Scalar(Tensor self, Scalar other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.eq.Tensor</code></p></td> <td><p>eq.Tensor(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.erf</code></p></td> <td><p>erf(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.exp</code></p></td> <td><p>exp(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.expand</code></p></td> <td><p>expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>aten.fill.Scalar</code></p></td> <td><p>fill.Scalar(Tensor self, Scalar value) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.flip</code></p></td> <td><p>flip(Tensor self, int[] dims) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.floor</code></p></td> <td><p>floor(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.fmod.Scalar</code></p></td> <td><p>fmod.Scalar(Tensor self, Scalar other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.fmod.Tensor</code></p></td> <td><p>fmod.Tensor(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.full</code></p></td> <td><p>full(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.gather</code></p></td> <td><p>gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.ge.Scalar</code></p></td> <td><p>ge.Scalar(Tensor self, Scalar other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.ge.Tensor</code></p></td> <td><p>ge.Tensor(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.gelu</code></p></td> <td><p>gelu(Tensor self, *, str approximate=’none’) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.grid_sampler_2d</code></p></td> <td><p>grid_sampler_2d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.gt.Scalar</code></p></td> <td><p>gt.Scalar(Tensor self, Scalar other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.gt.Tensor</code></p></td> <td><p>gt.Tensor(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.hardtanh</code></p></td> <td><p>hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.index.Tensor</code></p></td> <td><p>index.Tensor(Tensor self, Tensor?[] indices) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.index_put</code></p></td> <td><p>index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.index_select</code></p></td> <td><p>index_select(Tensor self, int dim, Tensor index) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.isinf</code></p></td> <td><p>isinf(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.isnan</code></p></td> <td><p>isnan(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.le.Scalar</code></p></td> <td><p>le.Scalar(Tensor self, Scalar other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.le.Tensor</code></p></td> <td><p>le.Tensor(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.leaky_relu</code></p></td> <td><p>leaky_relu(Tensor self, Scalar negative_slope=0.01) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.log</code></p></td> <td><p>log(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.logical_and</code></p></td> <td><p>logical_and(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.logical_not</code></p></td> <td><p>logical_not(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.logical_or</code></p></td> <td><p>logical_or(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.logical_xor</code></p></td> <td><p>logical_xor(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.lt.Scalar</code></p></td> <td><p>lt.Scalar(Tensor self, Scalar other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.lt.Tensor</code></p></td> <td><p>lt.Tensor(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.max.dim</code></p></td> <td><p>max.dim(Tensor self, int dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)</p></td> </tr> <tr>
<td><p><code>aten.max_pool2d_with_indices</code></p></td> <td><p>max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -&gt; (Tensor, Tensor)</p></td> </tr> <tr>
<td><p><code>aten.max_pool2d_with_indices_backward</code></p></td> <td><p>max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.max_pool3d_with_indices</code></p></td> <td><p>max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -&gt; (Tensor, Tensor)</p></td> </tr> <tr>
<td><p><code>aten.maximum</code></p></td> <td><p>maximum(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.mean</code></p></td> <td><p>mean(Tensor self, *, ScalarType? dtype=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.mean.dim</code></p></td> <td><p>mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.min.dim</code></p></td> <td><p>min.dim(Tensor self, int dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)</p></td> </tr> <tr>
<td><p><code>aten.minimum</code></p></td> <td><p>minimum(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.mm</code></p></td> <td><p>mm(Tensor self, Tensor mat2) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.mul.Scalar</code></p></td> <td><p>mul.Scalar(Tensor self, Scalar other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.mul.Tensor</code></p></td> <td><p>mul.Tensor(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.native_dropout</code></p></td> <td><p>native_dropout(Tensor input, float p, bool? train) -&gt; (Tensor, Tensor)</p></td> </tr> <tr>
<td><p><code>aten.native_group_norm</code></p></td> <td><p>native_group_norm(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps) -&gt; (Tensor, Tensor, Tensor)</p></td> </tr> <tr>
<td><p><code>aten.native_group_norm_backward</code></p></td> <td><p>native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask) -&gt; (Tensor, Tensor, Tensor)</p></td> </tr> <tr>
<td><p><code>aten.native_layer_norm</code></p></td> <td><p>native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -&gt; (Tensor, Tensor, Tensor)</p></td> </tr> <tr>
<td><p><code>aten.native_layer_norm_backward</code></p></td> <td><p>native_layer_norm_backward(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -&gt; (Tensor, Tensor, Tensor)</p></td> </tr> <tr>
<td><p><code>aten.ne.Scalar</code></p></td> <td><p>ne.Scalar(Tensor self, Scalar other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.ne.Tensor</code></p></td> <td><p>ne.Tensor(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.neg</code></p></td> <td><p>neg(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.nonzero</code></p></td> <td><p>nonzero(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.permute</code></p></td> <td><p>permute(Tensor(a) self, int[] dims) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>aten.pixel_shuffle</code></p></td> <td><p>pixel_shuffle(Tensor self, int upscale_factor) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.pow.Tensor_Scalar</code></p></td> <td><p>pow.Tensor_Scalar(Tensor self, Scalar exponent) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.pow.Tensor_Tensor</code></p></td> <td><p>pow.Tensor_Tensor(Tensor self, Tensor exponent) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.prod</code></p></td> <td><p>prod(Tensor self, *, ScalarType? dtype=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.prod.dim_int</code></p></td> <td><p>prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.rand</code></p></td> <td><p>rand(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.randn</code></p></td> <td><p>randn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.randperm</code></p></td> <td><p>randperm(SymInt n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.reciprocal</code></p></td> <td><p>reciprocal(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.reflection_pad1d</code></p></td> <td><p>reflection_pad1d(Tensor self, SymInt[2] padding) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.reflection_pad2d</code></p></td> <td><p>reflection_pad2d(Tensor self, SymInt[4] padding) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.reflection_pad3d</code></p></td> <td><p>reflection_pad3d(Tensor self, SymInt[6] padding) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.relu</code></p></td> <td><p>relu(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.remainder.Scalar</code></p></td> <td><p>remainder.Scalar(Tensor self, Scalar other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.remainder.Tensor</code></p></td> <td><p>remainder.Tensor(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.repeat</code></p></td> <td><p>repeat(Tensor self, SymInt[] repeats) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.replication_pad2d</code></p></td> <td><p>replication_pad2d(Tensor self, SymInt[4] padding) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.replication_pad3d</code></p></td> <td><p>replication_pad3d(Tensor self, SymInt[6] padding) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.roll</code></p></td> <td><p>roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.round</code></p></td> <td><p>round(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.rsqrt</code></p></td> <td><p>rsqrt(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.scalar_tensor</code></p></td> <td><p>scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.scatter.src</code></p></td> <td><p>scatter.src(Tensor self, int dim, Tensor index, Tensor src) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.scatter.value</code></p></td> <td><p>scatter.value(Tensor self, int dim, Tensor index, Scalar value) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.scatter_add</code></p></td> <td><p>scatter_add(Tensor self, int dim, Tensor index, Tensor src) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.scatter_reduce.two</code></p></td> <td><p>scatter_reduce.two(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.select.int</code></p></td> <td><p>select.int(Tensor(a) self, int dim, SymInt index) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>aten.select_scatter</code></p></td> <td><p>select_scatter(Tensor self, Tensor src, int dim, SymInt index) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.sigmoid</code></p></td> <td><p>sigmoid(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.sign</code></p></td> <td><p>sign(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.sin</code></p></td> <td><p>sin(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.sinh</code></p></td> <td><p>sinh(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.slice.Tensor</code></p></td> <td><p>slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>aten.slice_scatter</code></p></td> <td><p>slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.sort</code></p></td> <td><p>sort(Tensor self, int dim=-1, bool descending=False) -&gt; (Tensor values, Tensor indices)</p></td> </tr> <tr>
<td><p><code>aten.split_with_sizes</code></p></td> <td><p>split_with_sizes(Tensor(a -&gt; *) self, SymInt[] split_sizes, int dim=0) -&gt; Tensor(a)[]</p></td> </tr> <tr>
<td><p><code>aten.sqrt</code></p></td> <td><p>sqrt(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.squeeze.dim</code></p></td> <td><p>squeeze.dim(Tensor(a) self, int dim) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>aten.squeeze.dims</code></p></td> <td><p>squeeze.dims(Tensor(a) self, int[] dim) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>aten.sub.Scalar</code></p></td> <td><p>sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.sub.Tensor</code></p></td> <td><p>sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.sum.dim_IntList</code></p></td> <td><p>sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.sym_numel</code></p></td> <td><p>sym_numel(Tensor self) -&gt; SymInt</p></td> </tr> <tr>
<td><p><code>aten.sym_size.int</code></p></td> <td><p>sym_size.int(Tensor self, int dim) -&gt; SymInt</p></td> </tr> <tr>
<td><p><code>aten.sym_storage_offset</code></p></td> <td><p>sym_storage_offset(Tensor self) -&gt; SymInt</p></td> </tr> <tr>
<td><p><code>aten.sym_stride.int</code></p></td> <td><p>sym_stride.int(Tensor self, int dim) -&gt; SymInt</p></td> </tr> <tr>
<td><p><code>aten.tan</code></p></td> <td><p>tan(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.tanh</code></p></td> <td><p>tanh(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.topk</code></p></td> <td><p>topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -&gt; (Tensor values, Tensor indices)</p></td> </tr> <tr>
<td><p><code>aten.unsqueeze</code></p></td> <td><p>unsqueeze(Tensor(a) self, int dim) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>aten.upsample_bilinear2d.vec</code></p></td> <td><p>upsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.upsample_nearest2d.vec</code></p></td> <td><p>upsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.var.correction</code></p></td> <td><p>var.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.var.dim</code></p></td> <td><p>var.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>aten.view</code></p></td> <td><p>view(Tensor(a) self, SymInt[] size) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>aten.where.self</code></p></td> <td><p>where.self(Tensor condition, Tensor self, Tensor other) -&gt; Tensor</p></td> </tr>  </table>   <h2 id="prims-ir">Prims IR</h2> <p>Prims IR is a set of primitive operators that can be used to compose other operators. Prims IR is a lower level opset than core aten IR, and it further decomposes ops into explicit type promotion and broadcasting ops: prims.convert_element_type and prims.broadcast_in_dim. This opset is designed to interface with compiler backends.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This opset is still under active development, more ops will be added in the future.</p> </div> <table class="colwidths-auto docutils align-default"> <thead> <tr>
<th class="head"><p>Operator</p></th> <th class="head"><p>Schema</p></th> </tr> </thead>  <tr>
<td><p><code>prims.abs</code></p></td> <td><p>abs(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.acos</code></p></td> <td><p>acos(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.acosh</code></p></td> <td><p>acosh(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.asin</code></p></td> <td><p>asin(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.asinh</code></p></td> <td><p>asinh(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.atan</code></p></td> <td><p>atan(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.atanh</code></p></td> <td><p>atanh(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.cos</code></p></td> <td><p>cos(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.cosh</code></p></td> <td><p>cosh(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.bessel_i0</code></p></td> <td><p>bessel_i0(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.bessel_i0e</code></p></td> <td><p>bessel_i0e(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.bessel_i1</code></p></td> <td><p>bessel_i1(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.bessel_i1e</code></p></td> <td><p>bessel_i1e(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.bessel_j0</code></p></td> <td><p>bessel_j0(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.bessel_j1</code></p></td> <td><p>bessel_j1(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.bitwise_not</code></p></td> <td><p>bitwise_not(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.cbrt</code></p></td> <td><p>cbrt(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.ceil</code></p></td> <td><p>ceil(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.conj_physical</code></p></td> <td><p>conj_physical(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.digamma</code></p></td> <td><p>digamma(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.erf</code></p></td> <td><p>erf(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.erf_inv</code></p></td> <td><p>erf_inv(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.erfc</code></p></td> <td><p>erfc(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.erfcx</code></p></td> <td><p>erfcx(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.exp</code></p></td> <td><p>exp(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.expm1</code></p></td> <td><p>expm1(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.exp2</code></p></td> <td><p>exp2(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.fill</code></p></td> <td><p>fill(Tensor self, Scalar value) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.floor</code></p></td> <td><p>floor(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.imag</code></p></td> <td><p>imag(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.isfinite</code></p></td> <td><p>isfinite(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.lgamma</code></p></td> <td><p>lgamma(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.log</code></p></td> <td><p>log(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.log1p</code></p></td> <td><p>log1p(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.log2</code></p></td> <td><p>log2(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.log10</code></p></td> <td><p>log10(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.ndtri</code></p></td> <td><p>ndtri(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.neg</code></p></td> <td><p>neg(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.real</code></p></td> <td><p>real(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.reciprocal</code></p></td> <td><p>reciprocal(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.round</code></p></td> <td><p>round(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.sign</code></p></td> <td><p>sign(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.signbit</code></p></td> <td><p>signbit(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.sin</code></p></td> <td><p>sin(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.sinh</code></p></td> <td><p>sinh(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.spherical_bessel_j0</code></p></td> <td><p>spherical_bessel_j0(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.sqrt</code></p></td> <td><p>sqrt(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.tan</code></p></td> <td><p>tan(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.tanh</code></p></td> <td><p>tanh(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.trunc</code></p></td> <td><p>trunc(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.add</code></p></td> <td><p>add(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.atan2</code></p></td> <td><p>atan2(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.bitwise_and</code></p></td> <td><p>bitwise_and(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.bitwise_or</code></p></td> <td><p>bitwise_or(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.bitwise_xor</code></p></td> <td><p>bitwise_xor(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.div</code></p></td> <td><p>div(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.eq</code></p></td> <td><p>eq(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.fmax</code></p></td> <td><p>fmax(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.fmin</code></p></td> <td><p>fmin(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.fmod</code></p></td> <td><p>fmod(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.gcd</code></p></td> <td><p>gcd(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.ge</code></p></td> <td><p>ge(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.gt</code></p></td> <td><p>gt(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.hypot</code></p></td> <td><p>hypot(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.igamma</code></p></td> <td><p>igamma(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.igammac</code></p></td> <td><p>igammac(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.le</code></p></td> <td><p>le(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.lt</code></p></td> <td><p>lt(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.maximum</code></p></td> <td><p>maximum(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.minimum</code></p></td> <td><p>minimum(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.mul</code></p></td> <td><p>mul(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.ne</code></p></td> <td><p>ne(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.nextafter</code></p></td> <td><p>nextafter(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.pow</code></p></td> <td><p>pow(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.remainder</code></p></td> <td><p>remainder(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.rsqrt</code></p></td> <td><p>rsqrt(Tensor self) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.shift_left</code></p></td> <td><p>shift_left(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.shift_right_arithmetic</code></p></td> <td><p>shift_right_arithmetic(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.sub</code></p></td> <td><p>sub(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.zeta</code></p></td> <td><p>zeta(Tensor self, Tensor other) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.as_strided</code></p></td> <td><p>as_strided(Tensor(a!) a, SymInt[] size, SymInt[] stride, SymInt storage_offset) -&gt; Tensor(a!)</p></td> </tr> <tr>
<td><p><code>prims.broadcast_in_dim</code></p></td> <td><p>broadcast_in_dim(Tensor(a) a, SymInt[] shape, int[] broadcast_dimensions) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>prims.collapse_view</code></p></td> <td><p>collapse_view(Tensor(a) a, int start, int end) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>prims.conj</code></p></td> <td><p>conj(Tensor(a) a) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>prims.slice</code></p></td> <td><p>slice(Tensor(a) a, SymInt[] start_indices, SymInt[] limit_indices, SymInt[]? strides=None) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>prims.slice_in_dim</code></p></td> <td><p>slice_in_dim(Tensor(a) a, SymInt start_index, SymInt limit_index, int stride=1, int axis=0) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>prims.split_dim</code></p></td> <td><p>split_dim(Tensor(a) a, int dim, SymInt outer_length) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>prims.squeeze</code></p></td> <td><p>squeeze(Tensor(a) a, int[] dimensions) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>prims.transpose</code></p></td> <td><p>transpose(Tensor(a) a, int[] permutation) -&gt; Tensor(a)</p></td> </tr> <tr>
<td><p><code>prims.view_of</code></p></td> <td><p>view_of(Tensor(a) a) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.as_strided_scatter</code></p></td> <td><p>as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt storage_offset) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.collapse</code></p></td> <td><p>collapse(Tensor a, int start, int end) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.cat</code></p></td> <td><p>cat(Tensor[] tensors, int dim) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.reshape</code></p></td> <td><p>reshape(Tensor a, SymInt[] shape) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.rev</code></p></td> <td><p>rev(Tensor a, int[] dims) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.where</code></p></td> <td><p>where(Tensor pred, Tensor a, Tensor b) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.clone</code></p></td> <td><p>clone(Tensor self, *, MemoryFormat? memory_format=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.convert_element_type</code></p></td> <td><p>convert_element_type(Tensor a, ScalarType dtype) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.device_put</code></p></td> <td><p>device_put(Tensor a, Device device) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.item</code></p></td> <td><p>item(Tensor a) -&gt; Scalar</p></td> </tr> <tr>
<td><p><code>prims.maximum_value</code></p></td> <td><p>maximum_value(ScalarType dtype) -&gt; Scalar</p></td> </tr> <tr>
<td><p><code>prims.minimum_value</code></p></td> <td><p>minimum_value(ScalarType dtype) -&gt; Scalar</p></td> </tr> <tr>
<td><p><code>prims.copy_strided</code></p></td> <td><p>copy_strided(Tensor a, SymInt[] stride) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.copy_to</code></p></td> <td><p>copy_to(Tensor(a!) a, Tensor b) -&gt; Tensor(a!)</p></td> </tr> <tr>
<td><p><code>prims.resize</code></p></td> <td><p>resize(Tensor(a!) a, SymInt[] shape) -&gt; Tensor(a!)</p></td> </tr> <tr>
<td><p><code>prims.amax</code></p></td> <td><p>amax(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.amin</code></p></td> <td><p>amin(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.prod</code></p></td> <td><p>prod(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.sum</code></p></td> <td><p>sum(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.xor_sum</code></p></td> <td><p>xor_sum(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.var</code></p></td> <td><p>var(Tensor inp, int[]? dims, *, float correction, ScalarType? output_dtype=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.empty_strided</code></p></td> <td><p>empty_strided(SymInt[] shape, SymInt[] strides, *, ScalarType dtype, Device device, bool requires_grad) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.empty_permuted</code></p></td> <td><p>empty_permuted(SymInt[] shape, int[] physical_layout, *, ScalarType dtype, Device device, bool requires_grad) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.scalar_tensor</code></p></td> <td><p>scalar_tensor(Scalar s, *, ScalarType? dtype=None, Device? device=None) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.iota</code></p></td> <td><p>iota(SymInt length, *, SymInt start, SymInt step, ScalarType dtype, Device device, bool requires_grad) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.svd</code></p></td> <td><p>svd(Tensor A, *, bool full_matrices) -&gt; (Tensor U, Tensor S, Tensor Vh)</p></td> </tr> <tr>
<td><p><code>prims.normal</code></p></td> <td><p>normal(SymInt[] shape, *, Scalar mean, Scalar std, ScalarType dtype, Device device, bool requires_grad) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.uniform</code></p></td> <td><p>uniform(SymInt[] shape, *, Scalar low, Scalar high, ScalarType dtype, Device device) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.fft_r2c</code></p></td> <td><p>fft_r2c(Tensor self, *, int[] dim, bool onesided) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.fft_c2c</code></p></td> <td><p>fft_c2c(Tensor self, *, int[] dim, bool forward) -&gt; Tensor</p></td> </tr> <tr>
<td><p><code>prims.fft_c2r</code></p></td> <td><p>fft_c2r(Tensor self, *, int[] dim, SymInt last_dim_size) -&gt; Tensor</p></td> </tr>  </table><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/torch.compiler_ir.html" class="_attribution-link">https://pytorch.org/docs/2.1/torch.compiler_ir.html</a>
  </p>
</div>
