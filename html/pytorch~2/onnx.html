<h1 id="torch-onnx">torch.onnx</h1>  <h2 id="overview">Overview</h2> <p><a class="reference external" href="https://onnx.ai/">Open Neural Network eXchange (ONNX)</a> is an open standard format for representing machine learning models. The <code>torch.onnx</code> module captures the computation graph from a native PyTorch <a class="reference internal" href="generated/torch.nn.module.html#torch.nn.Module" title="torch.nn.Module"><code>torch.nn.Module</code></a> model and converts it into an <a class="reference external" href="https://github.com/onnx/onnx/blob/main/docs/IR.md">ONNX graph</a>.</p> <p>The exported model can be consumed by any of the many <a class="reference external" href="https://onnx.ai/supported-tools.html#deployModel">runtimes that support ONNX</a>, including Microsoft’s <a class="reference external" href="https://www.onnxruntime.ai">ONNX Runtime</a>.</p> <p><strong>There are two flavors of ONNX exporter API that you can use, as listed below:</strong></p>   <h2 id="torchdynamo-based-onnx-exporter">TorchDynamo-based ONNX Exporter</h2> <p><em>The TorchDynamo-based ONNX exporter is the newest (and Beta) exporter for PyTorch 2.0 and newer</em></p> <p>TorchDynamo engine is leveraged to hook into Python’s frame evaluation API and dynamically rewrite its bytecode into an FX Graph. The resulting FX Graph is then polished before it is finally translated into an ONNX graph.</p> <p>The main advantage of this approach is that the <a class="reference external" href="https://pytorch.org/docs/stable/fx.html">FX graph</a> is captured using bytecode analysis that preserves the dynamic nature of the model instead of using traditional static tracing techniques.</p> <p><a class="reference internal" href="onnx_dynamo.html"><span class="doc">Learn more about the TorchDynamo-based ONNX Exporter</span></a></p>   <h2 id="torchscript-based-onnx-exporter">TorchScript-based ONNX Exporter</h2> <p><em>The TorchScript-based ONNX exporter is available since PyTorch 1.2.0</em></p> <p><a class="reference external" href="https://pytorch.org/docs/stable/jit.html">TorchScript</a> is leveraged to trace (through <a class="reference internal" href="generated/torch.jit.trace.html#torch.jit.trace" title="torch.jit.trace"><code>torch.jit.trace()</code></a>) the model and capture a static computation graph.</p> <p>As a consequence, the resulting graph has a couple limitations:</p> <ul class="simple"> <li>It does not record any control-flow, like if-statements or loops;</li> <li>Does not handle nuances between <code>training</code> and <code>eval</code> mode;</li> <li>Does not truly handle dynamic inputs</li> </ul> <p>As an attempt to support the static tracing limitations, the exporter also supports TorchScript scripting (through <a class="reference internal" href="generated/torch.jit.script.html#torch.jit.script" title="torch.jit.script"><code>torch.jit.script()</code></a>), which adds support for data-dependent control-flow, for example. However, TorchScript itself is a subset of the Python language, so not all features in Python are supported, such as in-place operations.</p> <p><a class="reference internal" href="onnx_torchscript.html"><span class="doc">Learn more about the TorchScript-based ONNX Exporter</span></a></p>   <h2 id="contributing-developing">Contributing / Developing</h2> <p>The ONNX exporter is a community project and we welcome contributions. We follow the <a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md">PyTorch guidelines for contributions</a>, but you might also be interested in reading our <a class="reference external" href="https://github.com/pytorch/pytorch/wiki/PyTorch-ONNX-exporter">development wiki</a>.</p><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/onnx.html" class="_attribution-link">https://pytorch.org/docs/2.1/onnx.html</a>
  </p>
</div>
