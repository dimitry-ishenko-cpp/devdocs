<h1 id="unfold">Unfold</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.nn.Unfold">
<code>class torch.nn.Unfold(kernel_size, dilation=1, padding=0, stride=1)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/modules/fold.html#Unfold"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Extracts sliding local blocks from a batched input tensor.</p> <p>Consider a batched <code>input</code> tensor of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C, *)</annotation></semantics></math></span></span></span>, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span></span> is the batch dimension, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span></span></span> is the channel dimension, and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo></mrow><annotation encoding="application/x-tex">*</annotation></semantics></math></span></span></span> represent arbitrary spatial dimensions. This operation flattens each sliding <code>kernel_size</code>-sized block within the spatial dimensions of <code>input</code> into a column (i.e., last dimension) of a 3-D <code>output</code> tensor of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo>×</mo><mo>∏</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>L</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C \times \prod(\text{kernel\_size}), L)</annotation></semantics></math></span></span></span>, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>×</mo><mo>∏</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">C \times \prod(\text{kernel\_size})</annotation></semantics></math></span></span></span> is the total number of values within each block (a block has <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∏</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\prod(\text{kernel\_size})</annotation></semantics></math></span></span></span> spatial locations each containing a <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span></span></span>-channeled vector), and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span></span></span> is the total number of such blocks:</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo>=</mo><munder><mo>∏</mo><mi>d</mi></munder><mrow><mo fence="true">⌊</mo><mfrac><mrow><mtext>spatial_size</mtext><mo stretchy="false">[</mo><mi>d</mi><mo stretchy="false">]</mo><mo>+</mo><mn>2</mn><mo>×</mo><mtext>padding</mtext><mo stretchy="false">[</mo><mi>d</mi><mo stretchy="false">]</mo><mo>−</mo><mtext>dilation</mtext><mo stretchy="false">[</mo><mi>d</mi><mo stretchy="false">]</mo><mo>×</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">[</mo><mi>d</mi><mo stretchy="false">]</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn></mrow><mrow><mtext>stride</mtext><mo stretchy="false">[</mo><mi>d</mi><mo stretchy="false">]</mo></mrow></mfrac><mo>+</mo><mn>1</mn><mo fence="true">⌋</mo></mrow><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">L = \prod_d \left\lfloor\frac{\text{spatial\_size}[d] + 2 \times \text{padding}[d] % - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor, </annotation></semantics></math></span></span></span>
</div>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>spatial_size</mtext></mrow><annotation encoding="application/x-tex">\text{spatial\_size}</annotation></semantics></math></span></span></span> is formed by the spatial dimensions of <code>input</code> (<span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo></mrow><annotation encoding="application/x-tex">*</annotation></semantics></math></span></span></span> above), and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span></span></span> is over all spatial dimensions.</p> <p>Therefore, indexing <code>output</code> at the last dimension (column dimension) gives all values within a certain block.</p> <p>The <code>padding</code>, <code>stride</code> and <code>dilation</code> arguments specify how the sliding blocks are retrieved.</p> <ul class="simple"> <li>
<code>stride</code> controls the stride for the sliding blocks.</li> <li>
<code>padding</code> controls the amount of implicit zero-paddings on both sides for <code>padding</code> number of points for each dimension before reshaping.</li> <li>
<code>dilation</code> controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code>dilation</code> does.</li> </ul> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a>) – the size of the sliding blocks</li> <li>
<strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em>, </em><em>optional</em>) – a parameter that controls the stride of elements within the neighborhood. Default: 1</li> <li>
<strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em>, </em><em>optional</em>) – implicit zero padding to be added on both sides of input. Default: 0</li> <li>
<strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em>, </em><em>optional</em>) – the stride of the sliding blocks in the input spatial dimensions. Default: 1</li> </ul> </dd> </dl> <ul class="simple"> <li>If <code>kernel_size</code>, <code>dilation</code>, <code>padding</code> or <code>stride</code> is an int or a tuple of length 1, their values will be replicated across all spatial dimensions.</li> <li>For the case of two input spatial dimensions this operation is sometimes called <code>im2col</code>.</li> </ul> <div class="admonition note"> <p class="admonition-title">Note</p> <p><a class="reference internal" href="torch.nn.fold.html#torch.nn.Fold" title="torch.nn.Fold"><code>Fold</code></a> calculates each combined value in the resulting large tensor by summing all values from all containing blocks. <a class="reference internal" href="#torch.nn.Unfold" title="torch.nn.Unfold"><code>Unfold</code></a> extracts the values in the local blocks by copying from the large tensor. So, if the blocks overlap, they are not inverses of each other.</p> <p>In general, folding and unfolding operations are related as follows. Consider <a class="reference internal" href="torch.nn.fold.html#torch.nn.Fold" title="torch.nn.Fold"><code>Fold</code></a> and <a class="reference internal" href="#torch.nn.Unfold" title="torch.nn.Unfold"><code>Unfold</code></a> instances created with the same parameters:</p> <pre data-language="python">&gt;&gt;&gt; fold_params = dict(kernel_size=..., dilation=..., padding=..., stride=...)
&gt;&gt;&gt; fold = nn.Fold(output_size=..., **fold_params)
&gt;&gt;&gt; unfold = nn.Unfold(**fold_params)
</pre> <p>Then for any (supported) <code>input</code> tensor the following equality holds:</p> <pre data-language="python">fold(unfold(input)) == divisor * input
</pre> <p>where <code>divisor</code> is a tensor that depends only on the shape and dtype of the <code>input</code>:</p> <pre data-language="python">&gt;&gt;&gt; input_ones = torch.ones(input.shape, dtype=input.dtype)
&gt;&gt;&gt; divisor = fold(unfold(input_ones))
</pre> <p>When the <code>divisor</code> tensor contains no zero elements, then <code>fold</code> and <code>unfold</code> operations are inverses of each other (up to constant divisor).</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Currently, only 4-D input tensors (batched image-like tensors) are supported.</p> </div> <dl class="simple"> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C, *)</annotation></semantics></math></span></span></span>
</li> <li>Output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo>×</mo><mo>∏</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>L</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C \times \prod(\text{kernel\_size}), L)</annotation></semantics></math></span></span></span> as described above</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; unfold = nn.Unfold(kernel_size=(2, 3))
&gt;&gt;&gt; input = torch.randn(2, 5, 3, 4)
&gt;&gt;&gt; output = unfold(input)
&gt;&gt;&gt; # each patch contains 30 values (2x3=6 vectors, each of 5 channels)
&gt;&gt;&gt; # 4 blocks (2x3 kernels) in total in the 3x4 input
&gt;&gt;&gt; output.size()
torch.Size([2, 30, 4])

&gt;&gt;&gt; # Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)
&gt;&gt;&gt; inp = torch.randn(1, 3, 10, 12)
&gt;&gt;&gt; w = torch.randn(2, 3, 4, 5)
&gt;&gt;&gt; inp_unf = torch.nn.functional.unfold(inp, (4, 5))
&gt;&gt;&gt; out_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2)
&gt;&gt;&gt; out = torch.nn.functional.fold(out_unf, (7, 8), (1, 1))
&gt;&gt;&gt; # or equivalently (and avoiding a copy),
&gt;&gt;&gt; # out = out_unf.view(1, 2, 7, 8)
&gt;&gt;&gt; (torch.nn.functional.conv2d(inp, w) - out).abs().max()
tensor(1.9073e-06)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.Unfold.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.Unfold.html</a>
  </p>
</div>
