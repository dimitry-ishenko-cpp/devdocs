<h1 id="torch-sparse-mm">torch.sparse.mm</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.sparse.mm">
<code>torch.sparse.mm()</code> </dt> <dd> <p>Performs a matrix multiplication of the sparse matrix <code>mat1</code> and the (sparse or strided) matrix <code>mat2</code>. Similar to <a class="reference internal" href="torch.mm.html#torch.mm" title="torch.mm"><code>torch.mm()</code></a>, if <code>mat1</code> is a <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>×</mo><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(n \times m)</annotation></semantics></math></span></span></span> tensor, <code>mat2</code> is a <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>×</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(m \times p)</annotation></semantics></math></span></span></span> tensor, out will be a <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>×</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(n \times p)</annotation></semantics></math></span></span></span> tensor. When <code>mat1</code> is a COO tensor it must have <code>sparse_dim = 2</code>. When inputs are COO tensors, this function also supports backward for both inputs.</p> <p>Supports both CSR and COO storage formats.</p>  <div class="admonition note"> <p class="admonition-title">Note</p> <p>This function doesn’t support computing derivaties with respect to CSR matrices.</p> <p>This function also additionally accepts an optional <code>reduce</code> argument that allows specification of an optional reduction operation, mathematically performs the following operation:</p> </div> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><munderover><mo>⨁</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>K</mi><mo>−</mo><mn>1</mn></mrow></munderover><msub><mi>x</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><msub><mi>y</mi><mrow><mi>k</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">z_{ij} = \bigoplus_{k = 0}^{K - 1} x_{ik} y_{kj}</annotation></semantics></math></span></span></span>
</div>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⨁</mo></mrow><annotation encoding="application/x-tex">\bigoplus</annotation></semantics></math></span></span></span> defines the reduce operator. <code>reduce</code> is implemented only for CSR storage format on CPU device.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>mat1</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – the first sparse matrix to be multiplied</li> <li>
<strong>mat2</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – the second matrix to be multiplied, which could be sparse or dense</li> <li>
<strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><em>optional</em>) – the reduction operation to apply for non-unique indices (<code>"sum"</code>, <code>"mean"</code>, <code>"amax"</code>, <code>"amin"</code>). Default <code>"sum"</code>.</li> </ul> </dd> </dl> <dl class="simple"> <dt>Shape:</dt>
<dd>
<p>The format of the output tensor of this function follows: - sparse x sparse -&gt; sparse - sparse x dense -&gt; dense</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; a = torch.tensor([[1., 0, 2], [0, 3, 0]]).to_sparse().requires_grad_()
&gt;&gt;&gt; a
tensor(indices=tensor([[0, 0, 1],
                       [0, 2, 1]]),
       values=tensor([1., 2., 3.]),
       size=(2, 3), nnz=3, layout=torch.sparse_coo, requires_grad=True)
&gt;&gt;&gt; b = torch.tensor([[0, 1.], [2, 0], [0, 0]], requires_grad=True)
&gt;&gt;&gt; b
tensor([[0., 1.],
        [2., 0.],
        [0., 0.]], requires_grad=True)
&gt;&gt;&gt; y = torch.sparse.mm(a, b)
&gt;&gt;&gt; y
tensor([[0., 1.],
        [6., 0.]], grad_fn=&lt;SparseAddmmBackward0&gt;)
&gt;&gt;&gt; y.sum().backward()
&gt;&gt;&gt; a.grad
tensor(indices=tensor([[0, 0, 1],
                       [0, 2, 1]]),
       values=tensor([1., 0., 2.]),
       size=(2, 3), nnz=3, layout=torch.sparse_coo)
&gt;&gt;&gt; c = a.detach().to_sparse_csr()
&gt;&gt;&gt; c
tensor(crow_indices=tensor([0, 2, 3]),
       col_indices=tensor([0, 2, 1]),
       values=tensor([1., 2., 3.]), size=(2, 3), nnz=3,
       layout=torch.sparse_csr)
&gt;&gt;&gt; y1 = torch.sparse.mm(c, b, 'sum')
&gt;&gt;&gt; y1
tensor([[0., 1.],
        [6., 0.]], grad_fn=&lt;SparseMmReduceImplBackward0&gt;)
&gt;&gt;&gt; y2 = torch.sparse.mm(c, b, 'max')
&gt;&gt;&gt; y2
tensor([[0., 1.],
        [6., 0.]], grad_fn=&lt;SparseMmReduceImplBackward0&gt;)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.sparse.mm.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.sparse.mm.html</a>
  </p>
</div>
