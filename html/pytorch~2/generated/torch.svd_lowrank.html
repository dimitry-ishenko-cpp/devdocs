<h1 id="torch-svd-lowrank">torch.svd_lowrank</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.svd_lowrank">
<code>torch.svd_lowrank(A, q=6, niter=2, M=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/_lowrank.html#svd_lowrank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Return the singular value decomposition <code>(U, S, V)</code> of a matrix, batches of matrices, or a sparse matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span></span></span> such that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>≈</mo><mi>U</mi><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><msup><mi>V</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">A \approx U diag(S) V^T</annotation></semantics></math></span></span></span>. In case <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span></span> is given, then SVD is computed for the matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>−</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">A - M</annotation></semantics></math></span></span></span>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The implementation is based on the Algorithm 5.1 from Halko et al, 2009.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>To obtain repeatable results, reset the seed for the pseudorandom number generator</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The input is assumed to be a low-rank matrix.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>In general, use the full-rank SVD implementation <a class="reference internal" href="torch.linalg.svd.html#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a> for dense matrices due to its 10-fold higher performance characteristics. The low-rank SVD will be useful for huge sparse matrices that <a class="reference internal" href="torch.linalg.svd.html#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a> cannot handle.</p> </div> <dl> <dt>Args::</dt>
<dd>
<p>A (Tensor): the input tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, m, n)</annotation></semantics></math></span></span></span></p> <p>q (int, optional): a slightly overestimated rank of A.</p> <dl class="simple"> <dt>niter (int, optional): the number of subspace iterations to</dt>
<dd>
<p>conduct; niter must be a nonnegative integer, and defaults to 2</p> </dd> <dt>M (Tensor, optional): the input tensor’s mean of size</dt>
<dd>
<p><span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mn>1</mn><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, 1, n)</annotation></semantics></math></span></span></span>.</p> </dd> </dl> </dd> <dt>References::</dt>
<dd>
<ul class="simple"> <li>Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions, arXiv:0909.4061 [math.NA; math.PR], 2009 (available at <a class="reference external" href="https://arxiv.org/abs/0909.4061">arXiv</a>).</li> </ul> </dd> </dl> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.12)">Tuple</a>[<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>]</p> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.svd_lowrank.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.svd_lowrank.html</a>
  </p>
</div>
