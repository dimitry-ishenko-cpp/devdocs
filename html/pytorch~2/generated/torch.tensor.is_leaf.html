<h1 id="torch-tensor-is-leaf">torch.Tensor.is_leaf</h1> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.Tensor.is_leaf">
<code>Tensor.is_leaf</code> </dt> <dd>
<p>All Tensors that have <a class="reference internal" href="torch.tensor.requires_grad.html#torch.Tensor.requires_grad" title="torch.Tensor.requires_grad"><code>requires_grad</code></a> which is <code>False</code> will be leaf Tensors by convention.</p> <p>For Tensors that have <a class="reference internal" href="torch.tensor.requires_grad.html#torch.Tensor.requires_grad" title="torch.Tensor.requires_grad"><code>requires_grad</code></a> which is <code>True</code>, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so <code>grad_fn</code> is None.</p> <p>Only leaf Tensors will have their <a class="reference internal" href="torch.tensor.grad.html#torch.Tensor.grad" title="torch.Tensor.grad"><code>grad</code></a> populated during a call to <a class="reference internal" href="torch.tensor.backward.html#torch.Tensor.backward" title="torch.Tensor.backward"><code>backward()</code></a>. To get <a class="reference internal" href="torch.tensor.grad.html#torch.Tensor.grad" title="torch.Tensor.grad"><code>grad</code></a> populated for non-leaf Tensors, you can use <a class="reference internal" href="torch.tensor.retain_grad.html#torch.Tensor.retain_grad" title="torch.Tensor.retain_grad"><code>retain_grad()</code></a>.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; a = torch.rand(10, requires_grad=True)
&gt;&gt;&gt; a.is_leaf
True
&gt;&gt;&gt; b = torch.rand(10, requires_grad=True).cuda()
&gt;&gt;&gt; b.is_leaf
False
# b was created by the operation that cast a cpu Tensor into a cuda Tensor
&gt;&gt;&gt; c = torch.rand(10, requires_grad=True) + 2
&gt;&gt;&gt; c.is_leaf
False
# c was created by the addition operation
&gt;&gt;&gt; d = torch.rand(10).cuda()
&gt;&gt;&gt; d.is_leaf
True
# d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)
&gt;&gt;&gt; e = torch.rand(10).cuda().requires_grad_()
&gt;&gt;&gt; e.is_leaf
True
# e requires gradients and has no operations creating it
&gt;&gt;&gt; f = torch.rand(10, requires_grad=True, device="cuda")
&gt;&gt;&gt; f.is_leaf
True
# f requires grad, has no operation creating it
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.Tensor.is_leaf.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.Tensor.is_leaf.html</a>
  </p>
</div>
