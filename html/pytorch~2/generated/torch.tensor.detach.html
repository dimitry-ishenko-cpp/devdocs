<h1 id="torch-tensor-detach">torch.Tensor.detach</h1> <dl class="py method"> <dt class="sig sig-object py" id="torch.Tensor.detach">
<code>Tensor.detach()</code> </dt> <dd>
<p>Returns a new Tensor, detached from the current graph.</p> <p>The result will never require gradient.</p> <p>This method also affects forward mode AD gradients and the result will never have forward mode AD gradients.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as <code>resize_</code> / <code>resize_as_</code> / <code>set_</code> / <code>transpose_</code>) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as <code>zero_</code> / <code>copy_</code> / <code>add_</code>) to the returned tensor will not update the original tensor anymore, and will instead trigger an error.</p> </div> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.Tensor.detach.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.Tensor.detach.html</a>
  </p>
</div>
