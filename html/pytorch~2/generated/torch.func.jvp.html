<h1 id="torch-func-jvp">torch.func.jvp</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.func.jvp">
<code>torch.func.jvp(func, primals, tangents, *, strict=False, has_aux=False)</code> </dt> <dd>
<p>Standing for the Jacobian-vector product, returns a tuple containing the output of <code>func(*primals)</code> and the “Jacobian of <code>func</code> evaluated at <code>primals</code>” times <code>tangents</code>. This is also known as forward-mode autodiff.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>func</strong> (<em>function</em>) – A Python function that takes one or more arguments, one of which must be a Tensor, and returns one or more Tensors</li> <li>
<strong>primals</strong> (<em>Tensors</em>) – Positional arguments to <code>func</code> that must all be Tensors. The returned function will also be computing the derivative with respect to these arguments</li> <li>
<strong>tangents</strong> (<em>Tensors</em>) – The “vector” for which Jacobian-vector-product is computed. Must be the same structure and sizes as the inputs to <code>func</code>.</li> <li>
<strong>has_aux</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – Flag indicating that <code>func</code> returns a <code>(output, aux)</code> tuple where the first element is the output of the function to be differentiated and the second element is other auxiliary objects that will not be differentiated. Default: False.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>Returns a <code>(output, jvp_out)</code> tuple containing the output of <code>func</code> evaluated at <code>primals</code> and the Jacobian-vector product. If <code>has_aux is True</code>, then instead returns a <code>(output, jvp_out, aux)</code> tuple.</p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>You may see this API error out with “forward-mode AD not implemented for operator X”. If so, please file a bug report and we will prioritize it.</p> </div> <p>jvp is useful when you wish to compute gradients of a function R^1 -&gt; R^N</p> <pre data-language="python">&gt;&gt;&gt; from torch.func import jvp
&gt;&gt;&gt; x = torch.randn([])
&gt;&gt;&gt; f = lambda x: x * torch.tensor([1., 2., 3])
&gt;&gt;&gt; value, grad = jvp(f, (x,), (torch.tensor(1.),))
&gt;&gt;&gt; assert torch.allclose(value, f(x))
&gt;&gt;&gt; assert torch.allclose(grad, torch.tensor([1., 2, 3]))
</pre> <p><a class="reference internal" href="#torch.func.jvp" title="torch.func.jvp"><code>jvp()</code></a> can support functions with multiple inputs by passing in the tangents for each of the inputs</p> <pre data-language="python">&gt;&gt;&gt; from torch.func import jvp
&gt;&gt;&gt; x = torch.randn(5)
&gt;&gt;&gt; y = torch.randn(5)
&gt;&gt;&gt; f = lambda x, y: (x * y)
&gt;&gt;&gt; _, output = jvp(f, (x, y), (torch.ones(5), torch.ones(5)))
&gt;&gt;&gt; assert torch.allclose(output, x + y)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.func.jvp.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.func.jvp.html</a>
  </p>
</div>
