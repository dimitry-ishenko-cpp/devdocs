<h1 id="torch-linalg-eigh">torch.linalg.eigh</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.linalg.eigh">
<code>torch.linalg.eigh(A, UPLO='L', *, out=None)</code> </dt> <dd>
<p>Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.</p> <p>Letting <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">K</mi></mrow><annotation encoding="application/x-tex">\mathbb{K}</annotation></semantics></math></span></span></span> be <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\mathbb{R}</annotation></semantics></math></span></span></span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">C</mi></mrow><annotation encoding="application/x-tex">\mathbb{C}</annotation></semantics></math></span></span></span>, the <strong>eigenvalue decomposition</strong> of a complex Hermitian or real symmetric matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi mathvariant="double-struck">K</mi><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A \in \mathbb{K}^{n \times n}</annotation></semantics></math></span></span></span> is defined as</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>A</mi><mo>=</mo><mi>Q</mi><mi mathvariant="normal">diag</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="normal">Λ</mi><mo stretchy="false">)</mo><msup><mi>Q</mi><mtext>H</mtext></msup><mpadded width="0px"><mrow><mspace width="2em"></mspace><mi>Q</mi><mo>∈</mo><msup><mi mathvariant="double-struck">K</mi><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">Λ</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>n</mi></msup></mrow></mpadded></mrow><annotation encoding="application/x-tex">A = Q \operatorname{diag}(\Lambda) Q^{\text{H}}\mathrlap{\qquad Q \in \mathbb{K}^{n \times n}, \Lambda \in \mathbb{R}^n}</annotation></semantics></math></span></span></span>
</div>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mtext>H</mtext></msup></mrow><annotation encoding="application/x-tex">Q^{\text{H}}</annotation></semantics></math></span></span></span> is the conjugate transpose when <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span></span></span> is complex, and the transpose when <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span></span></span> is real-valued. <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span></span></span> is orthogonal in the real case and unitary in the complex case.</p> <p>Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if <code>A</code> is a batch of matrices then the output has the same batch dimensions.</p> <p><code>A</code> is assumed to be Hermitian (resp. symmetric), but this is not checked internally, instead:</p> <ul class="simple"> <li>If <code>UPLO</code><code>= ‘L’</code> (default), only the lower triangular part of the matrix is used in the computation.</li> <li>If <code>UPLO</code><code>= ‘U’</code>, only the upper triangular part of the matrix is used.</li> </ul> <p>The eigenvalues are returned in ascending order.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When inputs are on a CUDA device, this function synchronizes that device with the CPU.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The eigenvalues of real symmetric or complex Hermitian matrices are always real.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>The eigenvectors of a symmetric matrix are not unique, nor are they continuous with respect to <code>A</code>. Due to this lack of uniqueness, different hardware and software may compute different eigenvectors.</p> <p>This non-uniqueness is caused by the fact that multiplying an eigenvector by <code>-1</code> in the real case or by <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mrow><mi>i</mi><mi>ϕ</mi></mrow></msup><mo separator="true">,</mo><mi>ϕ</mi><mo>∈</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">e^{i \phi}, \phi \in \mathbb{R}</annotation></semantics></math></span></span></span> in the complex case produces another set of valid eigenvectors of the matrix. For this reason, the loss function shall not depend on the phase of the eigenvectors, as this quantity is not well-defined. This is checked for complex inputs when computing the gradients of this function. As such, when inputs are complex and are on a CUDA device, the computation of the gradients of this function synchronizes that device with the CPU.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Gradients computed using the <code>eigenvectors</code> tensor will only be finite when <code>A</code> has distinct eigenvalues. Furthermore, if the distance between any two eigenvalues is close to zero, the gradient will be numerically unstable, as it depends on the eigenvalues <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math></span></span></span> through the computation of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mrow><msub><mrow><mi>min</mi><mo>⁡</mo></mrow><mrow><mi>i</mi><mo mathvariant="normal">≠</mo><mi>j</mi></mrow></msub><msub><mi>λ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>λ</mi><mi>j</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{\min_{i \neq j} \lambda_i - \lambda_j}</annotation></semantics></math></span></span></span>.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>User may see pytorch crashes if running <code>eigh</code> on CUDA devices with CUDA versions before 12.1 update 1 with large ill-conditioned matrices as inputs. Refer to <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/numerical_accuracy.html#linear-algebra-stability"><span class="std std-ref">Linear Algebra Numerical Stability</span></a> for more details. If this is the case, user may (1) tune their matrix inputs to be less ill-conditioned, or (2) use <a class="reference internal" href="../backends.html#torch.backends.cuda.preferred_linalg_library" title="torch.backends.cuda.preferred_linalg_library"><code>torch.backends.cuda.preferred_linalg_library()</code></a> to try other supported backends.</p> </div> <div class="admonition seealso"> <p class="admonition-title">See also</p> <p><a class="reference internal" href="torch.linalg.eigvalsh.html#torch.linalg.eigvalsh" title="torch.linalg.eigvalsh"><code>torch.linalg.eigvalsh()</code></a> computes only the eigenvalues of a Hermitian matrix. Unlike <a class="reference internal" href="#torch.linalg.eigh" title="torch.linalg.eigh"><code>torch.linalg.eigh()</code></a>, the gradients of <a class="reference internal" href="torch.linalg.eigvalsh.html#torch.linalg.eigvalsh" title="torch.linalg.eigvalsh"><code>eigvalsh()</code></a> are always numerically stable.</p> <p><a class="reference internal" href="torch.linalg.cholesky.html#torch.linalg.cholesky" title="torch.linalg.cholesky"><code>torch.linalg.cholesky()</code></a> for a different decomposition of a Hermitian matrix. The Cholesky decomposition gives less information about the matrix but is much faster to compute than the eigenvalue decomposition.</p> <p><a class="reference internal" href="torch.linalg.eig.html#torch.linalg.eig" title="torch.linalg.eig"><code>torch.linalg.eig()</code></a> for a (slower) function that computes the eigenvalue decomposition of a not necessarily Hermitian square matrix.</p> <p><a class="reference internal" href="torch.linalg.svd.html#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a> for a (slower) function that computes the more general SVD decomposition of matrices of any shape.</p> <p><a class="reference internal" href="torch.linalg.qr.html#torch.linalg.qr" title="torch.linalg.qr"><code>torch.linalg.qr()</code></a> for another (much faster) decomposition that works on general matrices.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>A</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – tensor of shape <code>(*, n, n)</code> where <code>*</code> is zero or more batch dimensions consisting of symmetric or Hermitian matrices.</li> <li>
<strong>UPLO</strong> (<em>'L'</em><em>, </em><em>'U'</em><em>, </em><em>optional</em>) – controls whether to use the upper or lower triangular part of <code>A</code> in the computations. Default: <code>‘L’</code>.</li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em>, </em><em>optional</em>) – output tuple of two tensors. Ignored if <code>None</code>. Default: <code>None</code>.</p> </dd> <dt class="field-odd">Returns</dt> <dd class="field-odd">

<p>A named tuple <code>(eigenvalues, eigenvectors)</code> which corresponds to <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span></span></span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span></span></span> above.</p> <p><code>eigenvalues</code> will always be real-valued, even when <code>A</code> is complex. It will also be ordered in ascending order.</p> <p><code>eigenvectors</code> will have the same dtype as <code>A</code> and will contain the eigenvectors as its columns.</p> </dd> </dl> <dl> <dt>Examples::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; A = torch.randn(2, 2, dtype=torch.complex128)
&gt;&gt;&gt; A = A + A.T.conj()  # creates a Hermitian matrix
&gt;&gt;&gt; A
tensor([[2.9228+0.0000j, 0.2029-0.0862j],
        [0.2029+0.0862j, 0.3464+0.0000j]], dtype=torch.complex128)
&gt;&gt;&gt; L, Q = torch.linalg.eigh(A)
&gt;&gt;&gt; L
tensor([0.3277, 2.9415], dtype=torch.float64)
&gt;&gt;&gt; Q
tensor([[-0.0846+-0.0000j, -0.9964+0.0000j],
        [ 0.9170+0.3898j, -0.0779-0.0331j]], dtype=torch.complex128)
&gt;&gt;&gt; torch.dist(Q @ torch.diag(L.cdouble()) @ Q.T.conj(), A)
tensor(6.1062e-16, dtype=torch.float64)
</pre> <pre data-language="python">&gt;&gt;&gt; A = torch.randn(3, 2, 2, dtype=torch.float64)
&gt;&gt;&gt; A = A + A.mT  # creates a batch of symmetric matrices
&gt;&gt;&gt; L, Q = torch.linalg.eigh(A)
&gt;&gt;&gt; torch.dist(Q @ torch.diag_embed(L) @ Q.mH, A)
tensor(1.5423e-15, dtype=torch.float64)
</pre> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.linalg.eigh.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.linalg.eigh.html</a>
  </p>
</div>
