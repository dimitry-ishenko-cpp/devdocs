<h1 id="cudagraph">CUDAGraph</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.cuda.CUDAGraph">
<code>class torch.cuda.CUDAGraph</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/cuda/graphs.html#CUDAGraph"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Wrapper around a CUDA graph.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This API is in beta and may change in future releases.</p> </div> <dl class="py method"> <dt class="sig sig-object py" id="torch.cuda.CUDAGraph.capture_begin">
<code>capture_begin(pool=None, capture_error_mode='global')</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/cuda/graphs.html#CUDAGraph.capture_begin"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Begins capturing CUDA work on the current stream.</p> <p>Typically, you shouldn’t call <code>capture_begin</code> yourself. Use <a class="reference internal" href="torch.cuda.graph.html#torch.cuda.graph" title="torch.cuda.graph"><code>graph</code></a> or <a class="reference internal" href="torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables()</code></a>, which call <code>capture_begin</code> internally.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>pool</strong> (<em>optional</em>) – Token (returned by <a class="reference internal" href="torch.cuda.graph_pool_handle.html#torch.cuda.graph_pool_handle" title="torch.cuda.graph_pool_handle"><code>graph_pool_handle()</code></a> or <a class="reference internal" href="#torch.cuda.CUDAGraph.pool" title="torch.cuda.CUDAGraph.pool"><code>other_Graph_instance.pool()</code></a>) that hints this graph may share memory with the indicated pool. See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/cuda.html#graph-memory-management"><span class="std std-ref">Graph memory management</span></a>.</li> <li>
<strong>capture_error_mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><em>optional</em>) – specifies the cudaStreamCaptureMode for the graph capture stream. Can be “global”, “thread_local” or “relaxed”. During cuda graph capture, some actions, such as cudaMalloc, may be unsafe. “global” will error on actions in other threads, “thread_local” will only error for actions in the current thread, and “relaxed” will not error on these actions. Do NOT change this setting unless you’re familiar with <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html#group__CUDART__STREAM_1g9d0535d93a214cbf126835257b16ba85">cudaStreamCaptureMode</a>
</li> </ul> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.cuda.CUDAGraph.capture_end">
<code>capture_end()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/cuda/graphs.html#CUDAGraph.capture_end"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Ends CUDA graph capture on the current stream. After <code>capture_end</code>, <code>replay</code> may be called on this instance.</p> <p>Typically, you shouldn’t call <code>capture_end</code> yourself. Use <a class="reference internal" href="torch.cuda.graph.html#torch.cuda.graph" title="torch.cuda.graph"><code>graph</code></a> or <a class="reference internal" href="torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables()</code></a>, which call <code>capture_end</code> internally.</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.cuda.CUDAGraph.debug_dump">
<code>debug_dump(debug_path)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/cuda/graphs.html#CUDAGraph.debug_dump"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>debug_path</strong> (<em>required</em>) – Path to dump the graph to.</p> </dd> </dl> <p>Calls a debugging function to dump the graph if the debugging is enabled via CUDAGraph.enable_debug_mode()</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.cuda.CUDAGraph.enable_debug_mode">
<code>enable_debug_mode()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/cuda/graphs.html#CUDAGraph.enable_debug_mode"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Enables debugging mode for CUDAGraph.debug_dump.</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.cuda.CUDAGraph.pool">
<code>pool()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/cuda/graphs.html#CUDAGraph.pool"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns an opaque token representing the id of this graph’s memory pool. This id can optionally be passed to another graph’s <code>capture_begin</code>, which hints the other graph may share the same memory pool.</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.cuda.CUDAGraph.replay">
<code>replay()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/cuda/graphs.html#CUDAGraph.replay"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Replays the CUDA work captured by this graph.</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.cuda.CUDAGraph.reset">
<code>reset()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/cuda/graphs.html#CUDAGraph.reset"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Deletes the graph currently held by this instance.</p> </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.cuda.CUDAGraph.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.cuda.CUDAGraph.html</a>
  </p>
</div>
