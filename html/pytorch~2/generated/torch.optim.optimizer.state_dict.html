<h1 id="torch-optim-optimizer-state-dict">torch.optim.Optimizer.state_dict</h1> <dl class="py method"> <dt class="sig sig-object py" id="torch.optim.Optimizer.state_dict">
<code>Optimizer.state_dict()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/optim/optimizer.html#Optimizer.state_dict"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the state of the optimizer as a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)"><code>dict</code></a>.</p> <p>It contains two entries:</p> <ul class="simple"> <li>
<dl class="simple"> <dt>
<code>state: a Dict holding current optimization state. Its content</code> </dt>
<dd>
<p>differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. <code>state</code> is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.</p> </dd> </dl> </li> <li>
<dl class="simple"> <dt>
<code>param_groups: a List containing all parameter groups where each</code> </dt>
<dd>
<p>parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.</p> </dd> </dl> </li> </ul> <p>NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group <code>params</code> (int IDs) and the optimizer <code>param_groups</code> (actual <code>nn.Parameter</code> s) in order to match state WITHOUT additional verification.</p> <p>A returned state dict might look something like:</p> <pre data-language="text">{
    'state': {
        0: {'momentum_buffer': tensor(...), ...},
        1: {'momentum_buffer': tensor(...), ...},
        2: {'momentum_buffer': tensor(...), ...},
        3: {'momentum_buffer': tensor(...), ...}
    },
    'param_groups': [
        {
            'lr': 0.01,
            'weight_decay': 0,
            ...
            'params': [0]
        },
        {
            'lr': 0.001,
            'weight_decay': 0.5,
            ...
            'params': [1, 2, 3]
        }
    ]
}
</pre> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.12)">Dict</a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">Any</a>]</p> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.optim.Optimizer.state_dict.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.optim.Optimizer.state_dict.html</a>
  </p>
</div>
