<h1 id="torch-nn-utils-skip-init">torch.nn.utils.skip_init</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.utils.skip_init">
<code>torch.nn.utils.skip_init(module_cls, *args, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/utils/init.html#skip_init"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Given a module class object and args / kwargs, instantiates the module without initializing parameters / buffers. This can be useful if initialization is slow or if custom initialization will be performed, making the default initialization unnecessary. There are some caveats to this, due to the way this function is implemented:</p> <p>1. The module must accept a <code>device</code> arg in its constructor that is passed to any parameters or buffers created during construction.</p> <p>2. The module must not perform any computation on parameters in its constructor except initialization (i.e. functions from <code>torch.nn.init</code>).</p> <p>If these conditions are satisfied, the module can be instantiated with parameter / buffer values uninitialized, as if having been created using <a class="reference internal" href="torch.empty.html#torch.empty" title="torch.empty"><code>torch.empty()</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>module_cls</strong> – Class object; should be a subclass of <a class="reference internal" href="torch.nn.module.html#torch.nn.Module" title="torch.nn.Module"><code>torch.nn.Module</code></a>
</li> <li>
<strong>args</strong> – args to pass to the module’s constructor</li> <li>
<strong>kwargs</strong> – kwargs to pass to the module’s constructor</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>Instantiated module with uninitialized parameters / buffers</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; import torch
&gt;&gt;&gt; m = torch.nn.utils.skip_init(torch.nn.Linear, 5, 1)
&gt;&gt;&gt; m.weight
Parameter containing:
tensor([[0.0000e+00, 1.5846e+29, 7.8307e+00, 2.5250e-29, 1.1210e-44]],
       requires_grad=True)
&gt;&gt;&gt; m2 = torch.nn.utils.skip_init(torch.nn.Linear, in_features=6, out_features=1)
&gt;&gt;&gt; m2.weight
Parameter containing:
tensor([[-1.4677e+24,  4.5915e-41,  1.4013e-45,  0.0000e+00, -1.4677e+24,
          4.5915e-41]], requires_grad=True)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.utils.skip_init.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.utils.skip_init.html</a>
  </p>
</div>
