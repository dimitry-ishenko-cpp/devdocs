<h1 id="torch-autograd-graph-node-register-hook">torch.autograd.graph.Node.register_hook</h1> <dl class="py method"> <dt class="sig sig-object py" id="torch.autograd.graph.Node.register_hook">
<code>abstract Node.register_hook(fn)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/autograd/graph.html#Node.register_hook"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Registers a backward hook.</p> <p>The hook will be called every time a gradient with respect to the Node is computed. The hook should have the following signature:</p> <pre data-language="python">hook(grad_inputs: Tuple[Tensor], grad_outputs: Tuple[Tensor]) -&gt; Tuple[Tensor] or None
</pre> <p>The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of <code>grad_inputs</code>.</p> <p>This function returns a handle with a method <code>handle.remove()</code> that removes the hook from the module.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/autograd.html#backward-hooks-execution"><span class="std std-ref">Backward Hooks execution</span></a> for more information on how when this hook is executed, and how its execution is ordered relative to other hooks.</p> </div> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; import torch
&gt;&gt;&gt; a = torch.tensor([0., 0., 0.], requires_grad=True)
&gt;&gt;&gt; b = a.clone()
&gt;&gt;&gt; assert isinstance(b.grad_fn, torch.autograd.graph.Node)
&gt;&gt;&gt; handle = b.grad_fn.register_hook(lambda gI, gO: (gO[0] * 2,))
&gt;&gt;&gt; b.sum().backward(retain_graph=True)
&gt;&gt;&gt; print(a.grad)
tensor([2., 2., 2.])
&gt;&gt;&gt; handle.remove() # Removes the hook
&gt;&gt;&gt; a.grad = None
&gt;&gt;&gt; b.sum().backward(retain_graph=True)
&gt;&gt;&gt; print(a.grad)
tensor([1., 1., 1.])
</pre> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><em>RemovableHandle</em></p> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.autograd.graph.Node.register_hook.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.autograd.graph.Node.register_hook.html</a>
  </p>
</div>
