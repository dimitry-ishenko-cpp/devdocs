<h1 id="torch-nn-functional-scaled-dot-product-attention">torch.nn.functional.scaled_dot_product_attention</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.functional.scaled_dot_product_attention">
<code>torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None) → Tensor:</code> </dt> <dd>
<p>Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed, and applying dropout if a probability greater than 0.0 is specified.</p> <pre data-language="python"># Efficient implementation equivalent to the following:
def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None) -&gt; torch.Tensor:
    # Efficient implementation equivalent to the following:
    L, S = query.size(-2), key.size(-2)
    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale
    attn_bias = torch.zeros(L, S, dtype=query.dtype)
    if is_causal:
        assert attn_mask is None
        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)
        attn_bias.masked_fill_(temp_mask.logical_not(), float("-inf"))
        attn_bias.to(query.dtype)

    if attn_mask is not None:
        if attn_mask.dtype == torch.bool:
            attn_mask.masked_fill_(attn_mask.logical_not(), float("-inf"))
        else:
            attn_bias += attn_mask
    attn_weight = query @ key.transpose(-2, -1) * scale_factor
    attn_weight += attn_bias
    attn_weight = torch.softmax(attn_weight, dim=-1)
    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)
    return attn_weight @ value
</pre> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This function is beta and subject to change.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>There are currently three supported implementations of scaled dot product attention:</p>  <ul class="simple"> <li><a class="reference external" href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></li> <li><a class="reference external" href="https://github.com/facebookresearch/xformers">Memory-Efficient Attention</a></li> <li>A PyTorch implementation defined in C++ matching the above formulation</li> </ul>  <p>The function may call optimized kernels for improved performance when using the CUDA backend. For all other backends, the PyTorch implementation will be used.</p> <p>All implementations are enabled by default. Scaled dot product attention attempts to automatically select the most optimal implementation based on the inputs. In order to provide more fine-grained control over what implementation is used, the following functions are provided for enabling and disabling implementations. The context manager is the preferred mechanism:</p>  <ul class="simple"> <li>
<a class="reference internal" href="../backends.html#torch.backends.cuda.sdp_kernel" title="torch.backends.cuda.sdp_kernel"><code>torch.backends.cuda.sdp_kernel()</code></a>: A context manager used to enable/disable any of the implementations.</li> <li>
<a class="reference internal" href="../backends.html#torch.backends.cuda.enable_flash_sdp" title="torch.backends.cuda.enable_flash_sdp"><code>torch.backends.cuda.enable_flash_sdp()</code></a>: Enables or Disables FlashAttention.</li> <li>
<a class="reference internal" href="../backends.html#torch.backends.cuda.enable_mem_efficient_sdp" title="torch.backends.cuda.enable_mem_efficient_sdp"><code>torch.backends.cuda.enable_mem_efficient_sdp()</code></a>: Enables or Disables Memory-Efficient Attention.</li> <li>
<a class="reference internal" href="../backends.html#torch.backends.cuda.enable_math_sdp" title="torch.backends.cuda.enable_math_sdp"><code>torch.backends.cuda.enable_math_sdp()</code></a>: Enables or Disables the PyTorch C++ implementation.</li> </ul>  <p>Each of the fused kernels has specific input limitations. If the user requires the use of a specific fused implementation, disable the PyTorch C++ implementation using <a class="reference internal" href="../backends.html#torch.backends.cuda.sdp_kernel" title="torch.backends.cuda.sdp_kernel"><code>torch.backends.cuda.sdp_kernel()</code></a>. In the event that a fused implementation is not available, an error will be raised with the reasons why the fused implementation cannot run.</p> <p>Due to the nature of fusing floating point operations, the output of this function may be different depending on what backend kernel is chosen. The c++ implementation supports torch.float64 and can be used when higher precision is required. For more information please see <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/numerical_accuracy.html"><span class="doc">Numerical accuracy</span></a></p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/randomness.html"><span class="doc">Reproducibility</span></a> for more information.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>query</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – Query tensor; shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>L</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, ..., L, E)</annotation></semantics></math></span></span></span>.</li> <li>
<strong>key</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – Key tensor; shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>S</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, ..., S, E)</annotation></semantics></math></span></span></span>.</li> <li>
<strong>value</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – Value tensor; shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>S</mi><mo separator="true">,</mo><mi>E</mi><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, ..., S, Ev)</annotation></semantics></math></span></span></span>.</li> <li>
<strong>attn_mask</strong> (<em>optional Tensor</em>) – Attention mask; shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>L</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, ..., L, S)</annotation></semantics></math></span></span></span>. Two types of masks are supported. A boolean mask where a value of True indicates that the element <em>should</em> take part in attention. A float mask of the same type as query, key, value that is added to the attention score.</li> <li>
<strong>dropout_p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – Dropout probability; if greater than 0.0, dropout is applied</li> <li>
<strong>is_causal</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If true, assumes causal attention masking and errors if both attn_mask and is_causal are set.</li> <li>
<strong>scale</strong> (<em>optional python:float</em>) – Scaling factor applied prior to softmax. If None, the default value is set to <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><msqrt><mi>E</mi></msqrt></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{\sqrt{E}}</annotation></semantics></math></span></span></span>.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>Attention output; shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>L</mi><mo separator="true">,</mo><mi>E</mi><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, ..., L, Ev)</annotation></semantics></math></span></span></span>.</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p>output (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p> </dd> </dl> <dl class="simple"> <dt>Shape legend:</dt>
<dd>
<ul class="simple"> <li><span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>:</mo><mtext>Batch size</mtext><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo>:</mo><mtext>Any number of other batch dimensions (optional)</mtext></mrow><annotation encoding="application/x-tex">N: \text{Batch size} ... : \text{Any number of other batch dimensions (optional)}</annotation></semantics></math></span></span></span></li> <li><span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>:</mo><mtext>Source sequence length</mtext></mrow><annotation encoding="application/x-tex">S: \text{Source sequence length}</annotation></semantics></math></span></span></span></li> <li><span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>:</mo><mtext>Target sequence length</mtext></mrow><annotation encoding="application/x-tex">L: \text{Target sequence length}</annotation></semantics></math></span></span></span></li> <li><span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>:</mo><mtext>Embedding dimension of the query and key</mtext></mrow><annotation encoding="application/x-tex">E: \text{Embedding dimension of the query and key}</annotation></semantics></math></span></span></span></li> <li><span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mi>v</mi><mo>:</mo><mtext>Embedding dimension of the value</mtext></mrow><annotation encoding="application/x-tex">Ev: \text{Embedding dimension of the value}</annotation></semantics></math></span></span></span></li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; # Optionally use the context manager to ensure one of the fused kernels is run
&gt;&gt;&gt; query = torch.rand(32, 8, 128, 64, dtype=torch.float16, device="cuda")
&gt;&gt;&gt; key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device="cuda")
&gt;&gt;&gt; value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device="cuda")
&gt;&gt;&gt; with torch.backends.cuda.sdp_kernel(enable_math=False):
&gt;&gt;&gt;     F.scaled_dot_product_attention(query,key,value)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.functional.scaled_dot_product_attention.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.functional.scaled_dot_product_attention.html</a>
  </p>
</div>
