<h1 id="torch-tensor">torch.tensor</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.tensor">
<code>torch.tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) → Tensor</code> </dt> <dd>
<p>Constructs a tensor with no autograd history (also known as a “leaf tensor”, see <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/autograd.html"><span class="doc">Autograd mechanics</span></a>) by copying <code>data</code>.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>When working with tensors prefer using <a class="reference internal" href="torch.tensor.clone.html#torch.Tensor.clone" title="torch.Tensor.clone"><code>torch.Tensor.clone()</code></a>, <a class="reference internal" href="torch.tensor.detach.html#torch.Tensor.detach" title="torch.Tensor.detach"><code>torch.Tensor.detach()</code></a>, and <a class="reference internal" href="torch.tensor.requires_grad_.html#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code>torch.Tensor.requires_grad_()</code></a> for readability. Letting <code>t</code> be a tensor, <code>torch.tensor(t)</code> is equivalent to <code>t.clone().detach()</code>, and <code>torch.tensor(t, requires_grad=True)</code> is equivalent to <code>t.clone().detach().requires_grad_(True)</code>.</p> </div> <div class="admonition seealso"> <p class="admonition-title">See also</p> <p><a class="reference internal" href="torch.as_tensor.html#torch.as_tensor" title="torch.as_tensor"><code>torch.as_tensor()</code></a> preserves autograd history and avoids copies where possible. <a class="reference internal" href="torch.from_numpy.html#torch.from_numpy" title="torch.from_numpy"><code>torch.from_numpy()</code></a> creates a tensor that shares storage with a NumPy array.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>data</strong> (<em>array_like</em>) – Initial data for the tensor. Can be a list, tuple, NumPy <code>ndarray</code>, scalar, and other types.</p> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<ul class="simple"> <li>
<strong>dtype</strong> (<a class="reference internal" href="../tensor_attributes.html#torch.dtype" title="torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, infers data type from <code>data</code>.</li> <li>
<strong>device</strong> (<a class="reference internal" href="../tensor_attributes.html#torch.device" title="torch.device"><code>torch.device</code></a>, optional) – the device of the constructed tensor. If None and data is a tensor then the device of data is used. If None and data is not a tensor then the result tensor is constructed on the current device.</li> <li>
<strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</li> <li>
<strong>pin_memory</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: <code>False</code>.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])
tensor([[ 0.1000,  1.2000],
        [ 2.2000,  3.1000],
        [ 4.9000,  5.2000]])

&gt;&gt;&gt; torch.tensor([0, 1])  # Type inference on data
tensor([ 0,  1])

&gt;&gt;&gt; torch.tensor([[0.11111, 0.222222, 0.3333333]],
...              dtype=torch.float64,
...              device=torch.device('cuda:0'))  # creates a double tensor on a CUDA device
tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')

&gt;&gt;&gt; torch.tensor(3.14159)  # Create a zero-dimensional (scalar) tensor
tensor(3.1416)

&gt;&gt;&gt; torch.tensor([])  # Create an empty tensor (of size (0,))
tensor([])
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.tensor.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.tensor.html</a>
  </p>
</div>
