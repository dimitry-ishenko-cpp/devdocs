<h1 id="gaussiannllloss">GaussianNLLLoss</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.nn.GaussianNLLLoss">
<code>class torch.nn.GaussianNLLLoss(*, full=False, eps=1e-06, reduction='mean')</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/modules/loss.html#GaussianNLLLoss"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Gaussian negative log likelihood loss.</p> <p>The targets are treated as samples from Gaussian distributions with expectations and variances predicted by the neural network. For a <code>target</code> tensor modelled as having Gaussian distribution with a tensor of expectations <code>input</code> and a tensor of positive variances <code>var</code> the loss is:</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>loss</mtext><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo fence="true">(</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mtext>max</mtext><mrow><mo fence="true">(</mo><mtext>var</mtext><mo separator="true">,</mo><mtext> eps</mtext><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow><mo>+</mo><mfrac><msup><mrow><mo fence="true">(</mo><mtext>input</mtext><mo>−</mo><mtext>target</mtext><mo fence="true">)</mo></mrow><mn>2</mn></msup><mrow><mtext>max</mtext><mrow><mo fence="true">(</mo><mtext>var</mtext><mo separator="true">,</mo><mtext> eps</mtext><mo fence="true">)</mo></mrow></mrow></mfrac><mo fence="true">)</mo></mrow><mo>+</mo><mtext>const.</mtext></mrow><annotation encoding="application/x-tex">\text{loss} = \frac{1}{2}\left(\log\left(\text{max}\left(\text{var}, \ \text{eps}\right)\right) + \frac{\left(\text{input} - \text{target}\right)^2} {\text{max}\left(\text{var}, \ \text{eps}\right)}\right) + \text{const.} </annotation></semantics></math></span></span></span>
</div>
<p>where <code>eps</code> is used for stability. By default, the constant term of the loss function is omitted unless <code>full</code> is <code>True</code>. If <code>var</code> is not the same size as <code>input</code> (due to a homoscedastic assumption), it must either have a final dimension of 1 or have one fewer dimension (with all other sizes being the same) for correct broadcasting.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>full</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – include the constant term in the loss calculation. Default: <code>False</code>.</li> <li>
<strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em>, </em><em>optional</em>) – value used to clamp <code>var</code> (see note below), for stability. Default: 1e-6.</li> <li>
<strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><em>optional</em>) – specifies the reduction to apply to the output:<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the output is the average of all batch member losses, <code>'sum'</code>: the output is the sum of all batch member losses. Default: <code>'mean'</code>.</li> </ul> </dd> </dl> <dl> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, *)</annotation></semantics></math></span></span></span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*)</annotation></semantics></math></span></span></span> where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo></mrow><annotation encoding="application/x-tex">*</annotation></semantics></math></span></span></span> means any number of additional dimensions</li> <li>Target: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, *)</annotation></semantics></math></span></span></span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*)</annotation></semantics></math></span></span></span>, same shape as the input, or same shape as the input but with one dimension equal to 1 (to allow for broadcasting)</li> <li>Var: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, *)</annotation></semantics></math></span></span></span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*)</annotation></semantics></math></span></span></span>, same shape as the input, or same shape as the input but with one dimension equal to 1, or same shape as the input but with one fewer dimension (to allow for broadcasting)</li> <li>Output: scalar if <code>reduction</code> is <code>'mean'</code> (default) or <code>'sum'</code>. If <code>reduction</code> is <code>'none'</code>, then <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, *)</annotation></semantics></math></span></span></span>, same shape as the input</li> </ul> </dd> <dt>Examples::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; loss = nn.GaussianNLLLoss()
&gt;&gt;&gt; input = torch.randn(5, 2, requires_grad=True)
&gt;&gt;&gt; target = torch.randn(5, 2)
&gt;&gt;&gt; var = torch.ones(5, 2, requires_grad=True)  # heteroscedastic
&gt;&gt;&gt; output = loss(input, target, var)
&gt;&gt;&gt; output.backward()
</pre> <pre data-language="python">&gt;&gt;&gt; loss = nn.GaussianNLLLoss()
&gt;&gt;&gt; input = torch.randn(5, 2, requires_grad=True)
&gt;&gt;&gt; target = torch.randn(5, 2)
&gt;&gt;&gt; var = torch.ones(5, 1, requires_grad=True)  # homoscedastic
&gt;&gt;&gt; output = loss(input, target, var)
&gt;&gt;&gt; output.backward()
</pre> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The clamping of <code>var</code> is ignored with respect to autograd, and so the gradients are unaffected by it.</p> </div> <dl class="simple"> <dt>Reference:</dt>
<dd>
<p>Nix, D. A. and Weigend, A. S., “Estimating the mean and variance of the target probability distribution”, Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN’94), Orlando, FL, USA, 1994, pp. 55-60 vol.1, doi: 10.1109/ICNN.1994.374138.</p> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.GaussianNLLLoss.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.GaussianNLLLoss.html</a>
  </p>
</div>
