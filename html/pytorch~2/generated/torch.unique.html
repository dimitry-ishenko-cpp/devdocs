<h1 id="torch-unique">torch.unique</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.unique">
<code>torch.unique(input, sorted=True, return_inverse=False, return_counts=False, dim=None) → Tuple[Tensor, Tensor, Tensor]</code> </dt> <dd>
<p>Returns the unique elements of the input tensor.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This function is different from <a class="reference internal" href="torch.unique_consecutive.html#torch.unique_consecutive" title="torch.unique_consecutive"><code>torch.unique_consecutive()</code></a> in the sense that this function also eliminates non-consecutive duplicate values.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Currently in the CUDA implementation and the CPU implementation when dim is specified, <code>torch.unique</code> always sort the tensor at the beginning regardless of the <code>sort</code> argument. Sorting could be slow, so if your input tensor is already sorted, it is recommended to use <a class="reference internal" href="torch.unique_consecutive.html#torch.unique_consecutive" title="torch.unique_consecutive"><code>torch.unique_consecutive()</code></a> which avoids the sorting.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input tensor</li> <li>
<strong>sorted</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – Whether to sort the unique elements in ascending order before returning as output.</li> <li>
<strong>return_inverse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – Whether to also return the indices for where elements in the original input ended up in the returned unique list.</li> <li>
<strong>return_counts</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – Whether to also return the counts for each unique element.</li> <li>
<strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em>) – the dimension to operate upon. If <code>None</code>, the unique of the flattened input is returned. Otherwise, each of the tensors indexed by the given dimension is treated as one of the elements to apply the unique operation upon. See examples for more details. Default: <code>None</code>
</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">

<p>A tensor or a tuple of tensors containing</p>  <ul class="simple"> <li>
<strong>output</strong> (<em>Tensor</em>): the output list of unique scalar elements.</li> <li>
<strong>inverse_indices</strong> (<em>Tensor</em>): (optional) if <code>return_inverse</code> is True, there will be an additional returned tensor (same shape as input) representing the indices for where elements in the original input map to in the output; otherwise, this function will only return a single tensor.</li> <li>
<strong>counts</strong> (<em>Tensor</em>): (optional) if <code>return_counts</code> is True, there will be an additional returned tensor (same shape as output or output.size(dim), if dim was specified) representing the number of occurrences for each unique value or tensor.</li> </ul>  </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p>(<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a> (optional), <a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a> (optional))</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))
&gt;&gt;&gt; output
tensor([1, 2, 3])

&gt;&gt;&gt; output, inverse_indices = torch.unique(
...     torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)
&gt;&gt;&gt; output
tensor([1, 2, 3])
&gt;&gt;&gt; inverse_indices
tensor([0, 2, 1, 2])

&gt;&gt;&gt; output, inverse_indices = torch.unique(
...     torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)
&gt;&gt;&gt; output
tensor([1, 2, 3])
&gt;&gt;&gt; inverse_indices
tensor([[0, 2],
        [1, 2]])

&gt;&gt;&gt; a = torch.tensor([
...     [
...         [1, 1, 0, 0],
...         [1, 1, 0, 0],
...         [0, 0, 1, 1],
...     ],
...     [
...         [0, 0, 1, 1],
...         [0, 0, 1, 1],
...         [1, 1, 1, 1],
...     ],
...     [
...         [1, 1, 0, 0],
...         [1, 1, 0, 0],
...         [0, 0, 1, 1],
...     ],
... ])

&gt;&gt;&gt; # If we call `torch.unique(a, dim=0)`, each of the tensors `a[idx, :, :]`
&gt;&gt;&gt; # will be compared. We can see that `a[0, :, :]` and `a[2, :, :]` match
&gt;&gt;&gt; # each other, so one of them will be removed.
&gt;&gt;&gt; (a[0, :, :] == a[2, :, :]).all()
tensor(True)
&gt;&gt;&gt; a_unique_dim0 = torch.unique(a, dim=0)
&gt;&gt;&gt; a_unique_dim0
tensor([[[0, 0, 1, 1],
         [0, 0, 1, 1],
         [1, 1, 1, 1]],
        [[1, 1, 0, 0],
         [1, 1, 0, 0],
         [0, 0, 1, 1]]])

&gt;&gt;&gt; # Notice which sub-tensors from `a` match with the sub-tensors from
&gt;&gt;&gt; # `a_unique_dim0`:
&gt;&gt;&gt; (a_unique_dim0[0, :, :] == a[1, :, :]).all()
tensor(True)
&gt;&gt;&gt; (a_unique_dim0[1, :, :] == a[0, :, :]).all()
tensor(True)

&gt;&gt;&gt; # For `torch.unique(a, dim=1)`, each of the tensors `a[:, idx, :]` are
&gt;&gt;&gt; # compared. `a[:, 0, :]` and `a[:, 1, :]` match each other, so one of
&gt;&gt;&gt; # them will be removed.
&gt;&gt;&gt; (a[:, 0, :] == a[:, 1, :]).all()
tensor(True)
&gt;&gt;&gt; torch.unique(a, dim=1)
tensor([[[0, 0, 1, 1],
         [1, 1, 0, 0]],
        [[1, 1, 1, 1],
         [0, 0, 1, 1]],
        [[0, 0, 1, 1],
         [1, 1, 0, 0]]])

&gt;&gt;&gt; # For `torch.unique(a, dim=2)`, the tensors `a[:, :, idx]` are compared.
&gt;&gt;&gt; # `a[:, :, 0]` and `a[:, :, 1]` match each other. Also, `a[:, :, 2]` and
&gt;&gt;&gt; # `a[:, :, 3]` match each other as well. So in this case, two of the
&gt;&gt;&gt; # sub-tensors will be removed.
&gt;&gt;&gt; (a[:, :, 0] == a[:, :, 1]).all()
tensor(True)
&gt;&gt;&gt; (a[:, :, 2] == a[:, :, 3]).all()
tensor(True)
&gt;&gt;&gt; torch.unique(a, dim=2)
tensor([[[0, 1],
         [0, 1],
         [1, 0]],
        [[1, 0],
         [1, 0],
         [1, 1]],
        [[0, 1],
         [0, 1],
         [1, 0]]])
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.unique.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.unique.html</a>
  </p>
</div>
