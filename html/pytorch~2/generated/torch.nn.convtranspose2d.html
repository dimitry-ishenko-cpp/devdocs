<h1 id="convtranspose2d">ConvTranspose2d</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.nn.ConvTranspose2d">
<code>class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/modules/conv.html#ConvTranspose2d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies a 2D transposed convolution operator over an input image composed of several input planes.</p> <p>This module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation as it does not compute a true inverse of convolution). For more information, see the visualizations <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">here</a> and the <a class="reference external" href="https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf">Deconvolutional Networks</a> paper.</p> <p>This module supports <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/cuda.html#tf32-on-ampere"><span class="std std-ref">TensorFloat32</span></a>.</p> <p>On certain ROCm devices, when using float16 inputs this module will use <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/numerical_accuracy.html#fp16-on-mi200"><span class="std std-ref">different precision</span></a> for backward.</p> <ul> <li>
<code>stride</code> controls the stride for the cross-correlation.</li> <li>
<code>padding</code> controls the amount of implicit zero padding on both sides for <code>dilation * (kernel_size - 1) - padding</code> number of points. See note below for details.</li> <li>
<code>output_padding</code> controls the additional size added to one side of the output shape. See note below for details.</li> <li>
<code>dilation</code> controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but the link <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">here</a> has a nice visualization of what <code>dilation</code> does.</li> <li>
<p><code>groups</code> controls the connections between inputs and outputs. <code>in_channels</code> and <code>out_channels</code> must both be divisible by <code>groups</code>. For example,</p>  <ul class="simple"> <li>At groups=1, all inputs are convolved to all outputs.</li> <li>At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated.</li> <li>At groups= <code>in_channels</code>, each input channel is convolved with its own set of filters (of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mtext>out_channels</mtext><mtext>in_channels</mtext></mfrac></mrow><annotation encoding="application/x-tex">\frac{\text{out\_channels}}{\text{in\_channels}}</annotation></semantics></math></span></span></span>).</li> </ul>  </li> </ul> <p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>output_padding</code> can either be:</p>  <ul class="simple"> <li>a single <code>int</code> – in which case the same value is used for the height and width dimensions</li> <li>a <code>tuple</code> of two ints – in which case, the first <code>int</code> is used for the height dimension, and the second <code>int</code> for the width dimension</li> </ul>  <div class="admonition note"> <p class="admonition-title">Note</p> <p>The <code>padding</code> argument effectively adds <code>dilation * (kernel_size - 1) - padding</code> amount of zero padding to both sizes of the input. This is set so that when a <a class="reference internal" href="torch.nn.conv2d.html#torch.nn.Conv2d" title="torch.nn.Conv2d"><code>Conv2d</code></a> and a <a class="reference internal" href="#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d"><code>ConvTranspose2d</code></a> are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when <code>stride &gt; 1</code>, <a class="reference internal" href="torch.nn.conv2d.html#torch.nn.Conv2d" title="torch.nn.Conv2d"><code>Conv2d</code></a> maps multiple input shapes to the same output shape. <code>output_padding</code> is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that <code>output_padding</code> is only used to find output shape, but does not actually add zero-padding to output.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/randomness.html"><span class="doc">Reproducibility</span></a> for more information.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – Number of channels in the input image</li> <li>
<strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – Number of channels produced by the convolution</li> <li>
<strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a>) – Size of the convolving kernel</li> <li>
<strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</li> <li>
<strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em>, </em><em>optional</em>) – <code>dilation * (kernel_size - 1) - padding</code> zero-padding will be added to both sides of each dimension in the input. Default: 0</li> <li>
<strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em>, </em><em>optional</em>) – Additional size added to one side of each dimension in the output shape. Default: 0</li> <li>
<strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li> <li>
<strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code>
</li> <li>
<strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</li> </ul> </dd> </dl> <dl> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>W</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C_{in}, H_{in}, W_{in})</annotation></semantics></math></span></span></span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>W</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(C_{in}, H_{in}, W_{in})</annotation></semantics></math></span></span></span>
</li> <li>Output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>C</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo separator="true">,</mo><msub><mi>W</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C_{out}, H_{out}, W_{out})</annotation></semantics></math></span></span></span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>C</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo separator="true">,</mo><msub><mi>W</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(C_{out}, H_{out}, W_{out})</annotation></semantics></math></span></span></span>, where</li> </ul> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>=</mo><mo stretchy="false">(</mo><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>×</mo><mtext>stride</mtext><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo>−</mo><mn>2</mn><mo>×</mo><mtext>padding</mtext><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo>+</mo><mtext>dilation</mtext><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo>×</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mtext>output_padding</mtext><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1 </annotation></semantics></math></span></span></span>
</div>
<div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>W</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>=</mo><mo stretchy="false">(</mo><msub><mi>W</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>×</mo><mtext>stride</mtext><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo>−</mo><mn>2</mn><mo>×</mo><mtext>padding</mtext><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo>+</mo><mtext>dilation</mtext><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo>×</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mtext>output_padding</mtext><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1 </annotation></semantics></math></span></span></span>
</div>
</dd> </dl> <dl class="field-list simple"> <dt class="field-odd">Variables</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>weight</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – the learnable weights of the module of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>in_channels</mtext><mo separator="true">,</mo><mfrac><mtext>out_channels</mtext><mtext>groups</mtext></mfrac><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},</annotation></semantics></math></span></span></span> <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>kernel_size[0]</mtext><mo separator="true">,</mo><mtext>kernel_size[1]</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{kernel\_size[0]}, \text{kernel\_size[1]})</annotation></semantics></math></span></span></span>. The values of these weights are sampled from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">U</mi><mo stretchy="false">(</mo><mo>−</mo><msqrt><mi>k</mi></msqrt><mo separator="true">,</mo><msqrt><mi>k</mi></msqrt><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{U}(-\sqrt{k}, \sqrt{k})</annotation></semantics></math></span></span></span> where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mfrac><mrow><mi>g</mi><mi>r</mi><mi>o</mi><mi>u</mi><mi>p</mi><mi>s</mi></mrow><mrow><msub><mi>C</mi><mtext>out</mtext></msub><mo>∗</mo><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mn>1</mn></msubsup><mtext>kernel_size</mtext><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel\_size}[i]}</annotation></semantics></math></span></span></span>
</li> <li>
<strong>bias</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – the learnable bias of the module of shape (out_channels) If <code>bias</code> is <code>True</code>, then the values of these weights are sampled from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">U</mi><mo stretchy="false">(</mo><mo>−</mo><msqrt><mi>k</mi></msqrt><mo separator="true">,</mo><msqrt><mi>k</mi></msqrt><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{U}(-\sqrt{k}, \sqrt{k})</annotation></semantics></math></span></span></span> where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mfrac><mrow><mi>g</mi><mi>r</mi><mi>o</mi><mi>u</mi><mi>p</mi><mi>s</mi></mrow><mrow><msub><mi>C</mi><mtext>out</mtext></msub><mo>∗</mo><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mn>1</mn></msubsup><mtext>kernel_size</mtext><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel\_size}[i]}</annotation></semantics></math></span></span></span>
</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
&gt;&gt;&gt; input = torch.randn(20, 16, 50, 100)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; # exact output size can be also specified as an argument
&gt;&gt;&gt; input = torch.randn(1, 16, 12, 12)
&gt;&gt;&gt; downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)
&gt;&gt;&gt; upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)
&gt;&gt;&gt; h = downsample(input)
&gt;&gt;&gt; h.size()
torch.Size([1, 16, 6, 6])
&gt;&gt;&gt; output = upsample(h, output_size=input.size())
&gt;&gt;&gt; output.size()
torch.Size([1, 16, 12, 12])
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.ConvTranspose2d.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.ConvTranspose2d.html</a>
  </p>
</div>
