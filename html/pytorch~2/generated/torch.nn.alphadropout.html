<h1 id="alphadropout">AlphaDropout</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.nn.AlphaDropout">
<code>class torch.nn.AlphaDropout(p=0.5, inplace=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/modules/dropout.html#AlphaDropout"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies Alpha Dropout over the input.</p> <p>Alpha Dropout is a type of Dropout that maintains the self-normalizing property. For an input with zero mean and unit standard deviation, the output of Alpha Dropout maintains the original mean and standard deviation of the input. Alpha Dropout goes hand-in-hand with SELU activation function, which ensures that the outputs have zero mean and unit standard deviation.</p> <p>During training, it randomly masks some of the elements of the input tensor with probability <em>p</em> using samples from a bernoulli distribution. The elements to masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit standard deviation.</p> <p>During evaluation the module simply computes an identity function.</p> <p>More details can be found in the paper <a class="reference external" href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a> .</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – probability of an element to be dropped. Default: 0.5</li> <li>
<strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If set to <code>True</code>, will do this operation in-place</li> </ul> </dd> </dl> <dl class="simple"> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*)</annotation></semantics></math></span></span></span>. Input can be of any shape</li> <li>Output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*)</annotation></semantics></math></span></span></span>. Output is of the same shape as input</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; m = nn.AlphaDropout(p=0.2)
&gt;&gt;&gt; input = torch.randn(20, 16)
&gt;&gt;&gt; output = m(input)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.AlphaDropout.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.AlphaDropout.html</a>
  </p>
</div>
