<h1 id="torch-tensor-numpy">torch.Tensor.numpy</h1> <dl class="py method"> <dt class="sig sig-object py" id="torch.Tensor.numpy">
<code>Tensor.numpy(*, force=False) → numpy.ndarray</code> </dt> <dd>
<p>Returns the tensor as a NumPy <code>ndarray</code>.</p> <p>If <code>force</code> is <code>False</code> (the default), the conversion is performed only if the tensor is on the CPU, does not require grad, does not have its conjugate bit set, and is a dtype and layout that NumPy supports. The returned ndarray and the tensor will share their storage, so changes to the tensor will be reflected in the ndarray and vice versa.</p> <p>If <code>force</code> is <code>True</code> this is equivalent to calling <code>t.detach().cpu().resolve_conj().resolve_neg().numpy()</code>. If the tensor isn’t on the CPU or the conjugate or negative bit is set, the tensor won’t share its storage with the returned ndarray. Setting <code>force</code> to <code>True</code> can be a useful shorthand.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>force</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – if <code>True</code>, the ndarray may be a copy of the tensor instead of always sharing memory, defaults to <code>False</code>.</p> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.Tensor.numpy.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.Tensor.numpy.html</a>
  </p>
</div>
