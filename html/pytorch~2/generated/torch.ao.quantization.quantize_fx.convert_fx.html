<h1 id="convert-fx">convert_fx</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.ao.quantization.quantize_fx.convert_fx">
<code>class torch.ao.quantization.quantize_fx.convert_fx(graph_module, convert_custom_config=None, _remove_qconfig=True, qconfig_mapping=None, backend_config=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/quantize_fx.html#convert_fx"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Convert a calibrated or trained model to a quantized model</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>graph_module</strong> (<em>*</em>) – A prepared and calibrated/trained model (GraphModule)</li> <li>
<strong>convert_custom_config</strong> (<em>*</em>) – custom configurations for convert function. See <a class="reference internal" href="torch.ao.quantization.fx.custom_config.convertcustomconfig.html#torch.ao.quantization.fx.custom_config.ConvertCustomConfig" title="torch.ao.quantization.fx.custom_config.ConvertCustomConfig"><code>ConvertCustomConfig</code></a> for more details</li> <li>
<strong>_remove_qconfig</strong> (<em>*</em>) – Option to remove the qconfig attributes in the model after convert.</li> <li>
<p><strong>qconfig_mapping</strong> (<em>*</em>) – </p>
<p>config for specifying how to convert a model for quantization.</p>   <p>The keys must include the ones in the qconfig_mapping passed to <code>prepare_fx</code> or <code>prepare_qat_fx</code>, with the same values or <code>None</code>. Additional keys can be specified with values set to <code>None</code>.</p>  <p>For each entry whose value is set to None, we skip quantizing that entry in the model:</p> <pre data-language="python">qconfig_mapping = QConfigMapping
    .set_global(qconfig_from_prepare)
    .set_object_type(torch.nn.functional.add, None)  # skip quantizing torch.nn.functional.add
    .set_object_type(torch.nn.functional.linear, qconfig_from_prepare)
    .set_module_name("foo.bar", None)  # skip quantizing module "foo.bar"
</pre>  <ul> <li>
<dl class="simple"> <dt>
<code>backend_config (BackendConfig): A configuration for the backend which describes how</code> </dt>
<dd>
<p>operators should be quantized in the backend, this includes quantization mode support (static/dynamic/weight_only), dtype support (quint8/qint8 etc.), observer placement for each operators and fused operators. See <a class="reference internal" href="torch.ao.quantization.backend_config.backendconfig.html#torch.ao.quantization.backend_config.BackendConfig" title="torch.ao.quantization.backend_config.BackendConfig"><code>BackendConfig</code></a> for more details</p> </dd> </dl> </li> </ul> </li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>A quantized model (torch.nn.Module)</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="../fx.html#torch.fx.GraphModule" title="torch.fx.graph_module.GraphModule">GraphModule</a></p> </dd> </dl> <p>Example:</p> <pre data-language="python"># prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training
# convert_fx converts a calibrated/trained model to a quantized model for the
# target hardware, this includes converting the model first to a reference
# quantized model, and then lower the reference quantized model to a backend
# Currently, the supported backends are fbgemm (onednn), qnnpack (xnnpack) and
# they share the same set of quantized operators, so we are using the same
# lowering procedure
#
# backend_config defines the corresponding reference quantized module for
# the weighted modules in the model, e.g. nn.Linear
# TODO: add backend_config after we split the backend_config for fbgemm and qnnpack
# e.g. backend_config = get_default_backend_config("fbgemm")
quantized_model = convert_fx(prepared_model)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.ao.quantization.quantize_fx.convert_fx.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.ao.quantization.quantize_fx.convert_fx.html</a>
  </p>
</div>
