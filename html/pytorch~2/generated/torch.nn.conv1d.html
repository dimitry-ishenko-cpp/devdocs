<h1 id="conv1d">Conv1d</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.nn.Conv1d">
<code>class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/modules/conv.html#Conv1d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies a 1D convolution over an input signal composed of several input planes.</p> <p>In the simplest case, the output value of the layer with input size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>C</mi><mtext>in</mtext></msub><mo separator="true">,</mo><mi>L</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C_{\text{in}}, L)</annotation></semantics></math></span></span></span> and output <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>C</mi><mtext>out</mtext></msub><mo separator="true">,</mo><msub><mi>L</mi><mtext>out</mtext></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C_{\text{out}}, L_{\text{out}})</annotation></semantics></math></span></span></span> can be precisely described as:</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>out</mtext><mo stretchy="false">(</mo><msub><mi>N</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>C</mi><msub><mtext>out</mtext><mi>j</mi></msub></msub><mo stretchy="false">)</mo><mo>=</mo><mtext>bias</mtext><mo stretchy="false">(</mo><msub><mi>C</mi><msub><mtext>out</mtext><mi>j</mi></msub></msub><mo stretchy="false">)</mo><mo>+</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>−</mo><mn>1</mn></mrow></munderover><mtext>weight</mtext><mo stretchy="false">(</mo><msub><mi>C</mi><msub><mtext>out</mtext><mi>j</mi></msub></msub><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo><mo>⋆</mo><mtext>input</mtext><mo stretchy="false">(</mo><msub><mi>N</mi><mi>i</mi></msub><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) + \sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k) </annotation></semantics></math></span></span></span>
</div>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⋆</mo></mrow><annotation encoding="application/x-tex">\star</annotation></semantics></math></span></span></span> is the valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span></span> is a batch size, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span></span></span> denotes a number of channels, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span></span></span> is a length of signal sequence.</p> <p>This module supports <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/cuda.html#tf32-on-ampere"><span class="std std-ref">TensorFloat32</span></a>.</p> <p>On certain ROCm devices, when using float16 inputs this module will use <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/numerical_accuracy.html#fp16-on-mi200"><span class="std std-ref">different precision</span></a> for backward.</p> <ul> <li>
<code>stride</code> controls the stride for the cross-correlation, a single number or a one-element tuple.</li> <li>
<code>padding</code> controls the amount of padding applied to the input. It can be either a string {‘valid’, ‘same’} or a tuple of ints giving the amount of implicit padding applied on both sides.</li> <li>
<code>dilation</code> controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code>dilation</code> does.</li> <li>
<p><code>groups</code> controls the connections between inputs and outputs. <code>in_channels</code> and <code>out_channels</code> must both be divisible by <code>groups</code>. For example,</p>  <ul class="simple"> <li>At groups=1, all inputs are convolved to all outputs.</li> <li>At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated.</li> <li>At groups= <code>in_channels</code>, each input channel is convolved with its own set of filters (of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mtext>out_channels</mtext><mtext>in_channels</mtext></mfrac></mrow><annotation encoding="application/x-tex">\frac{\text{out\_channels}}{\text{in\_channels}}</annotation></semantics></math></span></span></span>).</li> </ul>  </li> </ul> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When <code>groups == in_channels</code> and <code>out_channels == K * in_channels</code>, where <code>K</code> is a positive integer, this operation is also known as a “depthwise convolution”.</p> <p>In other words, for an input of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>L</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C_{in}, L_{in})</annotation></semantics></math></span></span></span>, a depthwise convolution with a depthwise multiplier <code>K</code> can be performed with the arguments <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>C</mi><mtext>in</mtext></msub><mo>=</mo><msub><mi>C</mi><mtext>in</mtext></msub><mo separator="true">,</mo><msub><mi>C</mi><mtext>out</mtext></msub><mo>=</mo><msub><mi>C</mi><mtext>in</mtext></msub><mo>×</mo><mtext>K</mtext><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mtext>groups</mtext><mo>=</mo><msub><mi>C</mi><mtext>in</mtext></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(C_\text{in}=C_\text{in}, C_\text{out}=C_\text{in} \times \text{K}, ..., \text{groups}=C_\text{in})</annotation></semantics></math></span></span></span>.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/randomness.html"><span class="doc">Reproducibility</span></a> for more information.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p><code>padding='valid'</code> is the same as no padding. <code>padding='same'</code> pads the input so the output has the shape as the input. However, this mode doesn’t support any stride values other than 1.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This module supports complex data types i.e. <code>complex32, complex64, complex128</code>.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – Number of channels in the input image</li> <li>
<strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – Number of channels produced by the convolution</li> <li>
<strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a>) – Size of the convolving kernel</li> <li>
<strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</li> <li>
<strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><em>optional</em>) – Padding added to both sides of the input. Default: 0</li> <li>
<strong>padding_mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><em>optional</em>) – <code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default: <code>'zeros'</code>
</li> <li>
<strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</li> <li>
<strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li> <li>
<strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code>
</li> </ul> </dd> </dl> <dl> <dt>Shape:</dt>
<dd>
<ul> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>L</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C_{in}, L_{in})</annotation></semantics></math></span></span></span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>L</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(C_{in}, L_{in})</annotation></semantics></math></span></span></span>
</li> <li>
<p>Output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>C</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo separator="true">,</mo><msub><mi>L</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C_{out}, L_{out})</annotation></semantics></math></span></span></span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>C</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo separator="true">,</mo><msub><mi>L</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(C_{out}, L_{out})</annotation></semantics></math></span></span></span>, where</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>L</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>=</mo><mrow><mo fence="true">⌊</mo><mfrac><mrow><msub><mi>L</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>+</mo><mn>2</mn><mo>×</mo><mtext>padding</mtext><mo>−</mo><mtext>dilation</mtext><mo>×</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn></mrow><mtext>stride</mtext></mfrac><mo>+</mo><mn>1</mn><mo fence="true">⌋</mo></mrow></mrow><annotation encoding="application/x-tex">L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation} \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor </annotation></semantics></math></span></span></span>
</div>
</li> </ul> </dd> </dl> <dl class="field-list simple"> <dt class="field-odd">Variables</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>weight</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – the learnable weights of the module of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>out_channels</mtext><mo separator="true">,</mo><mfrac><mtext>in_channels</mtext><mtext>groups</mtext></mfrac><mo separator="true">,</mo><mtext>kernel_size</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}}, \text{kernel\_size})</annotation></semantics></math></span></span></span>. The values of these weights are sampled from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">U</mi><mo stretchy="false">(</mo><mo>−</mo><msqrt><mi>k</mi></msqrt><mo separator="true">,</mo><msqrt><mi>k</mi></msqrt><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{U}(-\sqrt{k}, \sqrt{k})</annotation></semantics></math></span></span></span> where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mfrac><mrow><mi>g</mi><mi>r</mi><mi>o</mi><mi>u</mi><mi>p</mi><mi>s</mi></mrow><mrow><msub><mi>C</mi><mtext>in</mtext></msub><mo>∗</mo><mtext>kernel_size</mtext></mrow></mfrac></mrow><annotation encoding="application/x-tex">k = \frac{groups}{C_\text{in} * \text{kernel\_size}}</annotation></semantics></math></span></span></span>
</li> <li>
<strong>bias</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – the learnable bias of the module of shape (out_channels). If <code>bias</code> is <code>True</code>, then the values of these weights are sampled from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">U</mi><mo stretchy="false">(</mo><mo>−</mo><msqrt><mi>k</mi></msqrt><mo separator="true">,</mo><msqrt><mi>k</mi></msqrt><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{U}(-\sqrt{k}, \sqrt{k})</annotation></semantics></math></span></span></span> where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mfrac><mrow><mi>g</mi><mi>r</mi><mi>o</mi><mi>u</mi><mi>p</mi><mi>s</mi></mrow><mrow><msub><mi>C</mi><mtext>in</mtext></msub><mo>∗</mo><mtext>kernel_size</mtext></mrow></mfrac></mrow><annotation encoding="application/x-tex">k = \frac{groups}{C_\text{in} * \text{kernel\_size}}</annotation></semantics></math></span></span></span>
</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; m = nn.Conv1d(16, 33, 3, stride=2)
&gt;&gt;&gt; input = torch.randn(20, 16, 50)
&gt;&gt;&gt; output = m(input)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.Conv1d.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.Conv1d.html</a>
  </p>
</div>
