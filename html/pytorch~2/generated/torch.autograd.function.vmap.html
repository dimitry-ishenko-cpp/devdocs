<h1 id="torch-autograd-function-vmap">torch.autograd.Function.vmap</h1> <dl class="py method"> <dt class="sig sig-object py" id="torch.autograd.Function.vmap">
<code>static Function.vmap(info, in_dims, *args)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/autograd/function.html#Function.vmap"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Defines a rule for the behavior of this autograd.Function underneath <a class="reference internal" href="torch.vmap.html#torch.vmap" title="torch.vmap"><code>torch.vmap()</code></a>. For a <a class="reference internal" href="../autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code>torch.autograd.Function()</code></a> to support <a class="reference internal" href="torch.vmap.html#torch.vmap" title="torch.vmap"><code>torch.vmap()</code></a>, you must either override this staticmethod, or set <code>generate_vmap_rule</code> to <code>True</code> (you may not do both).</p> <p>If you choose to override this staticmethod: it must accept</p> <ul class="simple"> <li>an <code>info</code> object as the first argument. <code>info.batch_size</code> specifies the size of the dimension being vmapped over, while <code>info.randomness</code> is the randomness option passed to <a class="reference internal" href="torch.vmap.html#torch.vmap" title="torch.vmap"><code>torch.vmap()</code></a>.</li> <li>an <code>in_dims</code> tuple as the second argument. For each arg in <code>args</code>, <code>in_dims</code> has a corresponding <code>Optional[int]</code>. It is <code>None</code> if the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer specifying what dimension of the Tensor is being vmapped over.</li> <li>
<code>*args</code>, which is the same as the args to <a class="reference internal" href="torch.autograd.function.forward.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code>forward()</code></a>.</li> </ul> <p>The return of the vmap staticmethod is a tuple of <code>(output, out_dims)</code>. Similar to <code>in_dims</code>, <code>out_dims</code> should be of the same structure as <code>output</code> and contain one <code>out_dim</code> per output that specifies if the output has the vmapped dimension and what index it is in.</p> <p>Please see <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/extending.func.html#func-autograd-function"><span class="std std-ref">Extending torch.func with autograd.Function</span></a> for more details.</p> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.autograd.Function.vmap.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.autograd.Function.vmap.html</a>
  </p>
</div>
