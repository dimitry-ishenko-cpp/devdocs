<h1 id="torch-func-linearize">torch.func.linearize</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.func.linearize">
<code>torch.func.linearize(func, *primals)</code> </dt> <dd>
<p>Returns the value of <code>func</code> at <code>primals</code> and linear approximation at <code>primals</code>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>func</strong> (<em>Callable</em>) – A Python function that takes one or more arguments.</li> <li>
<strong>primals</strong> (<em>Tensors</em>) – Positional arguments to <code>func</code> that must all be Tensors. These are the values at which the function is linearly approximated.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>Returns a <code>(output, jvp_fn)</code> tuple containing the output of <code>func</code> applied to <code>primals</code> and a function that computes the jvp of <code>func</code> evaluated at <code>primals</code>.</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.12)">Tuple</a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">Any</a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.12)">Callable</a>]</p> </dd> </dl> <p>linearize is useful if jvp is to be computed multiple times at <code>primals</code>. However, to achieve this, linearize saves intermediate computation and has higher memory requrements than directly applying <code>jvp</code>. So, if all the <code>tangents</code> are known, it maybe more efficient to compute vmap(jvp) instead of using linearize.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>linearize evaluates <code>func</code> twice. Please file an issue for an implementation with a single evaluation.</p> </div> <dl> <dt>Example::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; import torch
&gt;&gt;&gt; from torch.func import linearize
&gt;&gt;&gt; def fn(x):
...     return x.sin()
...
&gt;&gt;&gt; output, jvp_fn = linearize(fn, torch.zeros(3, 3))
&gt;&gt;&gt; jvp_fn(torch.ones(3, 3))
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
&gt;&gt;&gt;
</pre> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.func.linearize.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.func.linearize.html</a>
  </p>
</div>
