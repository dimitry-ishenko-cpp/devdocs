<h1 id="torch-cuda-jiterator-create-jit-fn">torch.cuda.jiterator._create_jit_fn</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.cuda.jiterator._create_jit_fn">
<code>torch.cuda.jiterator._create_jit_fn(code_string, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/cuda/jiterator.html#_create_jit_fn"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Create a jiterator-generated cuda kernel for an elementwise op.</p> <p>The code string has to be a valid CUDA function that describes the computation for a single element. The code string has to follow the c++ template pattern, as shown in the example below. This function will be inlined into elementwise kernel template, and compiled on the fly. Compiled kernel will be cached in memory, as well as local temp dir.</p> <p>Jiterator-generated kernels accepts noncontiguous tensors, and supports broadcasting and type promotion.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>code_string</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – CUDA code string to be compiled by jiterator. The entry functor must return by value.</li> <li>
<strong>kwargs</strong> (<em>Dict</em><em>, </em><em>optional</em>) – Keyword arguments for generated function</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.12)">Callable</a></p> </dd> </dl> <p>Example:</p> <pre data-language="python">code_string = "template &lt;typename T&gt; T my_kernel(T x, T y, T alpha) { return -x + alpha * y; }"
jitted_fn = create_jit_fn(code_string, alpha=1.0)
a = torch.rand(3, device='cuda')
b = torch.rand(3, device='cuda')
# invoke jitted function like a regular python function
result = jitted_fn(a, b, alpha=3.14)
</pre> <p>code_string also allows multiple function definitions, and the last function will be treated as the entry function.</p> <p>Example:</p> <pre data-language="python">code_string = "template &lt;typename T&gt; T util_fn(T x, T y) { return ::sin(x) + ::cos(y); }"
code_string += "template &lt;typename T&gt; T my_kernel(T x, T y, T val) { return ::min(val, util_fn(x, y)); }"
jitted_fn = create_jit_fn(code_string, val=0.0)
a = torch.rand(3, device='cuda')
b = torch.rand(3, device='cuda')
# invoke jitted function like a regular python function
result = jitted_fn(a, b)  # using default val=0.0
</pre> <p>Jiterator can be used together with python registration to override an operator’s cuda kernel. Following example is overriding gelu’s cuda kernel with relu.</p> <p>Example:</p> <pre data-language="python">code_string = "template &lt;typename T&gt; T my_gelu(T a) { return a &gt; 0 ? a : 0; }"
my_gelu = create_jit_fn(code_string)
my_lib = torch.library.Library("aten", "IMPL")
my_lib.impl('aten::gelu', my_gelu, "CUDA")
# torch.nn.GELU and torch.nn.function.gelu are now overridden
a = torch.rand(3, device='cuda')
torch.allclose(torch.nn.functional.gelu(a), torch.nn.functional.relu(a))
</pre> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This API is in beta and may change in future releases.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This API only supports up to 8 inputs and 1 output</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>All input tensors must live in CUDA device</p> </div> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.cuda.jiterator._create_jit_fn.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.cuda.jiterator._create_jit_fn.html</a>
  </p>
</div>
