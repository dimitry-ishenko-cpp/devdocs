<h1 id="dtypeconfig">DTypeConfig</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.ao.quantization.backend_config.DTypeConfig">
<code>class torch.ao.quantization.backend_config.DTypeConfig(input_dtype=None, output_dtype=None, weight_dtype=None, bias_dtype=None, is_dynamic=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/backend_config/backend_config.html#DTypeConfig"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Config object that specifies the supported data types passed as arguments to quantize ops in the reference model spec, for input and output activations, weights, and biases.</p> <p>For example, consider the following reference model:</p>  <p>quant1 - [dequant1 - fp32_linear - quant2] - dequant2</p>  <p>The pattern in the square brackets refers to the reference pattern of statically quantized linear. Setting the input dtype as <code>torch.quint8</code> in the DTypeConfig means we pass in <code>torch.quint8</code> as the dtype argument to the first quantize op (quant1). Similarly, setting the output dtype as <code>torch.quint8</code> means we pass in <code>torch.quint8</code> as the dtype argument to the second quantize op (quant2).</p> <p>Note that the dtype here does not refer to the interface dtypes of the op. For example, the “input dtype” here is not the dtype of the input tensor passed to the quantized linear op. Though it can still be the same as the interface dtype, this is not always the case, e.g. the interface dtype is fp32 in dynamic quantization but the “input dtype” specified in the DTypeConfig would still be quint8. The semantics of dtypes here are the same as the semantics of the dtypes specified in the observers.</p> <p>These dtypes are matched against the ones specified in the user’s QConfig. If there is a match, and the QConfig satisfies the constraints specified in the DTypeConfig (if any), then we will quantize the given pattern using this DTypeConfig. Otherwise, the QConfig is ignored and the pattern will not be quantized.</p> <p>Example usage:</p> <pre data-language="python">&gt;&gt;&gt; dtype_config1 = DTypeConfig(
...     input_dtype=torch.quint8,
...     output_dtype=torch.quint8,
...     weight_dtype=torch.qint8,
...     bias_dtype=torch.float)

&gt;&gt;&gt; dtype_config2 = DTypeConfig(
...     input_dtype=DTypeWithConstraints(
...         dtype=torch.quint8,
...         quant_min_lower_bound=0,
...         quant_max_upper_bound=255,
...     ),
...     output_dtype=DTypeWithConstraints(
...         dtype=torch.quint8,
...         quant_min_lower_bound=0,
...         quant_max_upper_bound=255,
...     ),
...     weight_dtype=DTypeWithConstraints(
...         dtype=torch.qint8,
...         quant_min_lower_bound=-128,
...         quant_max_upper_bound=127,
...     ),
...     bias_dtype=torch.float)

&gt;&gt;&gt; dtype_config1.input_dtype
torch.quint8

&gt;&gt;&gt; dtype_config2.input_dtype
torch.quint8

&gt;&gt;&gt; dtype_config2.input_dtype_with_constraints
DTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None)
</pre>  <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.backend_config.DTypeConfig.from_dict">
<code>classmethod from_dict(dtype_config_dict)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/backend_config/backend_config.html#DTypeConfig.from_dict"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<dl class="simple"> <dt>
<code>Create a DTypeConfig from a dictionary with the following items (all optional):</code> </dt>
<dd>
<p>“input_dtype”: torch.dtype or <code>DTypeWithConstraints</code> “output_dtype”: torch.dtype or <code>DTypeWithConstraints</code> “weight_dtype”: torch.dtype or <code>DTypeWithConstraints</code> “bias_type”: torch.dtype “is_dynamic”: bool</p> </dd> </dl> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.backend_config.DTypeConfig" title="torch.ao.quantization.backend_config.backend_config.DTypeConfig">DTypeConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.backend_config.DTypeConfig.to_dict">
<code>to_dict()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/backend_config/backend_config.html#DTypeConfig.to_dict"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Convert this <code>DTypeConfig</code> to a dictionary with the items described in <a class="reference internal" href="#torch.ao.quantization.backend_config.DTypeConfig.from_dict" title="torch.ao.quantization.backend_config.DTypeConfig.from_dict"><code>from_dict()</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.12)">Dict</a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">Any</a>]</p> </dd> </dl> </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.ao.quantization.backend_config.DTypeConfig.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.ao.quantization.backend_config.DTypeConfig.html</a>
  </p>
</div>
