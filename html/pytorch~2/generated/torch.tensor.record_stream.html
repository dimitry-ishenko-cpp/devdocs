<h1 id="torch-tensor-record-stream">torch.Tensor.record_stream</h1> <dl class="py method"> <dt class="sig sig-object py" id="torch.Tensor.record_stream">
<code>Tensor.record_stream(stream)</code> </dt> <dd>
<p>Ensures that the tensor memory is not reused for another tensor until all current work queued on <code>stream</code> are complete.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The caching allocator is aware of only the stream where a tensor was allocated. Due to the awareness, it already correctly manages the life cycle of tensors on only one stream. But if a tensor is used on a stream different from the stream of origin, the allocator might reuse the memory unexpectedly. Calling this method lets the allocator know which streams have used the tensor.</p> </div> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.Tensor.record_stream.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.Tensor.record_stream.html</a>
  </p>
</div>
