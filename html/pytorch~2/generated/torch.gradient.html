<h1 id="torch-gradient">torch.gradient</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.gradient">
<code>torch.gradient(input, *, spacing=1, dim=None, edge_order=1) → List of Tensors</code> </dt> <dd>
<p>Estimates the gradient of a function <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>:</mo><msup><mi mathvariant="double-struck">R</mi><mi>n</mi></msup><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">g : \mathbb{R}^n \rightarrow \mathbb{R}</annotation></semantics></math></span></span></span> in one or more dimensions using the <a class="reference external" href="https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf">second-order accurate central differences method</a> and either first or second order estimates at the boundaries.</p> <p>The gradient of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span></span></span> is estimated using samples. By default, when <code>spacing</code> is not specified, the samples are entirely described by <code>input</code>, and the mapping of input coordinates to an output is the same as the tensor’s mapping of indices to values. For example, for a three-dimensional <code>input</code> the function described is <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>:</mo><msup><mi mathvariant="double-struck">R</mi><mn>3</mn></msup><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">g : \mathbb{R}^3 \rightarrow \mathbb{R}</annotation></semantics></math></span></span></span>, and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mn>3</mn><mo stretchy="false">)</mo><mo>=</mo><mo>=</mo><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mn>3</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">g(1, 2, 3)\ == input[1, 2, 3]</annotation></semantics></math></span></span></span>.</p> <p>When <code>spacing</code> is specified, it modifies the relationship between <code>input</code> and input coordinates. This is detailed in the “Keyword Arguments” section below.</p> <p>The gradient is estimated by estimating each partial derivative of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span></span></span> independently. This estimation is accurate if <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span></span></span> is in <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>C</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">C^3</annotation></semantics></math></span></span></span> (it has at least 3 continuous derivatives), and the estimation can be improved by providing closer samples. Mathematically, the value at each interior point of a partial derivative is estimated using <a class="reference external" href="https://en.wikipedia.org/wiki/Taylor%27s_theorem">Taylor’s theorem with remainder</a>. Letting <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span></span></span> be an interior point with <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>−</mo><msub><mi>h</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">x-h_l</annotation></semantics></math></span></span></span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>+</mo><msub><mi>h</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">x+h_r</annotation></semantics></math></span></span></span> be points neighboring it to the left and right respectively, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><msub><mi>h</mi><mi>r</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x+h_r)</annotation></semantics></math></span></span></span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><msub><mi>h</mi><mi>l</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x-h_l)</annotation></semantics></math></span></span></span> can be estimated using:</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><msub><mi>h</mi><mi>r</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><msub><mi>h</mi><mi>r</mi></msub><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><msup><msub><mi>h</mi><mi>r</mi></msub><mn>2</mn></msup><mfrac><mrow><msup><mi>f</mi><mrow><mo mathvariant="normal">′</mo><mo mathvariant="normal">′</mo></mrow></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mn>2</mn></mfrac><mo>+</mo><msup><msub><mi>h</mi><mi>r</mi></msub><mn>3</mn></msup><mfrac><mrow><msup><mi>f</mi><mrow><mo mathvariant="normal">′</mo><mo mathvariant="normal">′</mo><mo mathvariant="normal">′</mo></mrow></msup><mo stretchy="false">(</mo><msub><mi>ξ</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mn>6</mn></mfrac><mo separator="true">,</mo><msub><mi>ξ</mi><mn>1</mn></msub><mo>∈</mo><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>x</mi><mo>+</mo><msub><mi>h</mi><mi>r</mi></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><msub><mi>h</mi><mi>l</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><msub><mi>h</mi><mi>l</mi></msub><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><msup><msub><mi>h</mi><mi>l</mi></msub><mn>2</mn></msup><mfrac><mrow><msup><mi>f</mi><mrow><mo mathvariant="normal">′</mo><mo mathvariant="normal">′</mo></mrow></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mn>2</mn></mfrac><mo>−</mo><msup><msub><mi>h</mi><mi>l</mi></msub><mn>3</mn></msup><mfrac><mrow><msup><mi>f</mi><mrow><mo mathvariant="normal">′</mo><mo mathvariant="normal">′</mo><mo mathvariant="normal">′</mo></mrow></msup><mo stretchy="false">(</mo><msub><mi>ξ</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><mn>6</mn></mfrac><mo separator="true">,</mo><msub><mi>ξ</mi><mn>2</mn></msub><mo>∈</mo><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>x</mi><mo>−</mo><msub><mi>h</mi><mi>l</mi></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} f(x+h_r) = f(x) + h_r f'(x) + {h_r}^2 \frac{f''(x)}{2} + {h_r}^3 \frac{f'''(\xi_1)}{6}, \xi_1 \in (x, x+h_r) \\ f(x-h_l) = f(x) - h_l f'(x) + {h_l}^2 \frac{f''(x)}{2} - {h_l}^3 \frac{f'''(\xi_2)}{6}, \xi_2 \in (x, x-h_l) \\ \end{aligned} </annotation></semantics></math></span></span></span>
</div>
<p>Using the fact that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>∈</mo><msup><mi>C</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">f \in C^3</annotation></semantics></math></span></span></span> and solving the linear system, we derive:</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>≈</mo><mfrac><mrow><msup><msub><mi>h</mi><mi>l</mi></msub><mn>2</mn></msup><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><msub><mi>h</mi><mi>r</mi></msub><mo stretchy="false">)</mo><mo>−</mo><msup><msub><mi>h</mi><mi>r</mi></msub><mn>2</mn></msup><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><msub><mi>h</mi><mi>l</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><msup><msub><mi>h</mi><mi>r</mi></msub><mn>2</mn></msup><mo>−</mo><msup><msub><mi>h</mi><mi>l</mi></msub><mn>2</mn></msup><mo stretchy="false">)</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>h</mi><mi>r</mi></msub><msup><msub><mi>h</mi><mi>l</mi></msub><mn>2</mn></msup><mo>+</mo><msup><msub><mi>h</mi><mi>r</mi></msub><mn>2</mn></msup><msub><mi>h</mi><mi>l</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">f'(x) \approx \frac{ {h_l}^2 f(x+h_r) - {h_r}^2 f(x-h_l) + ({h_r}^2-{h_l}^2 ) f(x) }{ {h_r} {h_l}^2 + {h_r}^2 {h_l} } </annotation></semantics></math></span></span></span>
</div>
<div class="admonition note"> <p class="admonition-title">Note</p> <p>We estimate the gradient of functions in complex domain <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>:</mo><msup><mi mathvariant="double-struck">C</mi><mi>n</mi></msup><mo>→</mo><mi mathvariant="double-struck">C</mi></mrow><annotation encoding="application/x-tex">g : \mathbb{C}^n \rightarrow \mathbb{C}</annotation></semantics></math></span></span></span> in the same way.</p> </div> <p>The value of each partial derivative at the boundary points is computed differently. See edge_order below.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>input</strong> (<code>Tensor</code>) – the tensor that represents the values of the function</p> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<ul class="simple"> <li>
<strong>spacing</strong> (<code>scalar</code>, <code>list of scalar</code>, <code>list of Tensor</code>, optional) – <code>spacing</code> can be used to modify how the <code>input</code> tensor’s indices relate to sample coordinates. If <code>spacing</code> is a scalar then the indices are multiplied by the scalar to produce the coordinates. For example, if <code>spacing=2</code> the indices (1, 2, 3) become coordinates (2, 4, 6). If <code>spacing</code> is a list of scalars then the corresponding indices are multiplied. For example, if <code>spacing=(2, -1, 3)</code> the indices (1, 2, 3) become coordinates (2, -2, 9). Finally, if <code>spacing</code> is a list of one-dimensional tensors then each tensor specifies the coordinates for the corresponding dimension. For example, if the indices are (1, 2, 3) and the tensors are (t0, t1, t2), then the coordinates are (t0[1], t1[2], t2[3])</li> <li>
<strong>dim</strong> (<code>int</code>, <code>list of int</code>, optional) – the dimension or dimensions to approximate the gradient over. By default the partial gradient in every dimension is computed. Note that when <code>dim</code> is specified the elements of the <code>spacing</code> argument must correspond with the specified dims.”</li> <li>
<strong>edge_order</strong> (<code>int</code>, optional) – 1 or 2, for <a class="reference external" href="https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf">first-order</a> or <a class="reference external" href="https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf">second-order</a> estimation of the boundary (“edge”) values, respectively.</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; # Estimates the gradient of f(x)=x^2 at points [-2, -1, 2, 4]
&gt;&gt;&gt; coordinates = (torch.tensor([-2., -1., 1., 4.]),)
&gt;&gt;&gt; values = torch.tensor([4., 1., 1., 16.], )
&gt;&gt;&gt; torch.gradient(values, spacing = coordinates)
(tensor([-3., -2., 2., 5.]),)

&gt;&gt;&gt; # Estimates the gradient of the R^2 -&gt; R function whose samples are
&gt;&gt;&gt; # described by the tensor t. Implicit coordinates are [0, 1] for the outermost
&gt;&gt;&gt; # dimension and [0, 1, 2, 3] for the innermost dimension, and function estimates
&gt;&gt;&gt; # partial derivative for both dimensions.
&gt;&gt;&gt; t = torch.tensor([[1, 2, 4, 8], [10, 20, 40, 80]])
&gt;&gt;&gt; torch.gradient(t)
(tensor([[ 9., 18., 36., 72.],
         [ 9., 18., 36., 72.]]),
 tensor([[ 1.0000, 1.5000, 3.0000, 4.0000],
         [10.0000, 15.0000, 30.0000, 40.0000]]))

&gt;&gt;&gt; # A scalar value for spacing modifies the relationship between tensor indices
&gt;&gt;&gt; # and input coordinates by multiplying the indices to find the
&gt;&gt;&gt; # coordinates. For example, below the indices of the innermost
&gt;&gt;&gt; # 0, 1, 2, 3 translate to coordinates of [0, 2, 4, 6], and the indices of
&gt;&gt;&gt; # the outermost dimension 0, 1 translate to coordinates of [0, 2].
&gt;&gt;&gt; torch.gradient(t, spacing = 2.0) # dim = None (implicitly [0, 1])
(tensor([[ 4.5000, 9.0000, 18.0000, 36.0000],
          [ 4.5000, 9.0000, 18.0000, 36.0000]]),
 tensor([[ 0.5000, 0.7500, 1.5000, 2.0000],
          [ 5.0000, 7.5000, 15.0000, 20.0000]]))
&gt;&gt;&gt; # doubling the spacing between samples halves the estimated partial gradients.

&gt;&gt;&gt;
&gt;&gt;&gt; # Estimates only the partial derivative for dimension 1
&gt;&gt;&gt; torch.gradient(t, dim = 1) # spacing = None (implicitly 1.)
(tensor([[ 1.0000, 1.5000, 3.0000, 4.0000],
         [10.0000, 15.0000, 30.0000, 40.0000]]),)

&gt;&gt;&gt; # When spacing is a list of scalars, the relationship between the tensor
&gt;&gt;&gt; # indices and input coordinates changes based on dimension.
&gt;&gt;&gt; # For example, below, the indices of the innermost dimension 0, 1, 2, 3 translate
&gt;&gt;&gt; # to coordinates of [0, 3, 6, 9], and the indices of the outermost dimension
&gt;&gt;&gt; # 0, 1 translate to coordinates of [0, 2].
&gt;&gt;&gt; torch.gradient(t, spacing = [3., 2.])
(tensor([[ 4.5000, 9.0000, 18.0000, 36.0000],
         [ 4.5000, 9.0000, 18.0000, 36.0000]]),
 tensor([[ 0.3333, 0.5000, 1.0000, 1.3333],
         [ 3.3333, 5.0000, 10.0000, 13.3333]]))

&gt;&gt;&gt; # The following example is a replication of the previous one with explicit
&gt;&gt;&gt; # coordinates.
&gt;&gt;&gt; coords = (torch.tensor([0, 2]), torch.tensor([0, 3, 6, 9]))
&gt;&gt;&gt; torch.gradient(t, spacing = coords)
(tensor([[ 4.5000, 9.0000, 18.0000, 36.0000],
         [ 4.5000, 9.0000, 18.0000, 36.0000]]),
 tensor([[ 0.3333, 0.5000, 1.0000, 1.3333],
         [ 3.3333, 5.0000, 10.0000, 13.3333]]))
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.gradient.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.gradient.html</a>
  </p>
</div>
