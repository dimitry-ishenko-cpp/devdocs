<h1 id="torch-nn-functional-cosine-similarity">torch.nn.functional.cosine_similarity</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.functional.cosine_similarity">
<code>torch.nn.functional.cosine_similarity(x1, x2, dim=1, eps=1e-8) → Tensor</code> </dt> <dd>
<p>Returns cosine similarity between <code>x1</code> and <code>x2</code>, computed along dim. <code>x1</code> and <code>x2</code> must be broadcastable to a common shape. <code>dim</code> refers to the dimension in this common shape. Dimension <code>dim</code> of the output is squeezed (see <a class="reference internal" href="torch.squeeze.html#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting in the output tensor having 1 fewer dimension.</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>similarity</mtext><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>⋅</mo><msub><mi>x</mi><mn>2</mn></msub></mrow><mrow><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="normal">∥</mi><msub><mi>x</mi><mn>1</mn></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo separator="true">,</mo><mi>ϵ</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="normal">∥</mi><msub><mi>x</mi><mn>2</mn></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo separator="true">,</mo><mi>ϵ</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2, \epsilon) \cdot \max(\Vert x_2 \Vert _2, \epsilon)} </annotation></semantics></math></span></span></span>
</div>
<p>Supports <a class="reference internal" href="../tensor_attributes.html#type-promotion-doc"><span class="std std-ref">type promotion</span></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>x1</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – First input.</li> <li>
<strong>x2</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – Second input.</li> <li>
<strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em>) – Dimension along which cosine similarity is computed. Default: 1</li> <li>
<strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em>, </em><em>optional</em>) – Small value to avoid division by zero. Default: 1e-8</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; input1 = torch.randn(100, 128)
&gt;&gt;&gt; input2 = torch.randn(100, 128)
&gt;&gt;&gt; output = F.cosine_similarity(input1, input2)
&gt;&gt;&gt; print(output)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.functional.cosine_similarity.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.functional.cosine_similarity.html</a>
  </p>
</div>
