<h1 id="torch-utils-rename-privateuse1-backend">torch.utils.rename_privateuse1_backend</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.utils.rename_privateuse1_backend">
<code>torch.utils.rename_privateuse1_backend(backend_name) → None</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/backend_registration.html#rename_privateuse1_backend"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>This API should be use to rename the privateuse1 backend device to make it more convenient to use as a device name within PyTorch APIs.</p> <p>The steps are:</p> <ol class="arabic simple"> <li>(In C++) implement kernels for various torch operations, and register them to the PrivateUse1 dispatch key.</li> <li>(In python) call torch.utils.rename_privateuse1_backend(“foo”)</li> </ol> <p>You can now use “foo” as an ordinary device string in python.</p> <p>Note: this API can only be called once per process. Attempting to change the external backend after it’s already been set will result in an error.</p> <p>Note(AMP): If you want to support AMP on your device, you can register a custom backend module. The backend must register a custom backend module with <code>torch._register_device_module("foo", BackendModule)</code>. BackendModule needs to have the following API’s:</p> <ol class="arabic simple"> <li>
<code>get_amp_supported_dtype() -&gt; List[torch.dtype]</code> get the supported dtypes on your “foo” device in AMP, maybe the “foo” device supports one more dtype.</li> <li>
<code>is_autocast_enabled() -&gt; bool</code> check the AMP is enabled or not on your “foo” device.</li> <li>
<code>get_autocast_dtype() -&gt; torch.dtype</code> get the supported dtype on your “foo” device in AMP, which is set by <code>set_autocast_dtype</code> or the default dtype, and the default dtype is <code>torch.float16</code>.</li> <li>
<code>set_autocast_enabled(bool) -&gt; None</code> enable the AMP or not on your “foo” device.</li> <li>
<code>set_autocast_dtype(dtype) -&gt; None</code> set the supported dtype on your “foo” device in AMP, and the dtype be contained in the dtypes got from <code>get_amp_supported_dtype</code>.</li> </ol> <p>Note(random): If you want to support to set seed for your device, BackendModule needs to have the following API’s:</p> <ol class="arabic simple"> <li>
<code>_is_in_bad_fork() -&gt; bool</code> Return <code>True</code> if now it is in bad_fork, else return <code>False</code>.</li> <li>
<code>manual_seed_all(seed int) -&gt; None</code> Sets the seed for generating random numbers for your devices.</li> <li>
<code>device_count() -&gt; int</code> Returns the number of “foo”s available.</li> <li>
<code>get_rng_state(device: Union[int, str, torch.device] = 'foo') -&gt; Tensor</code> Returns a list of ByteTensor representing the random number states of all devices.</li> <li>
<code>set_rng_state(new_state: Tensor, device: Union[int, str, torch.device] = 'foo') -&gt; None</code> Sets the random number generator state of the specified “foo” device.</li> </ol> <p>And there are some common funcs:</p> <ol class="arabic simple"> <li>
<code>is_available() -&gt; bool</code> Returns a bool indicating if “foo” is currently available.</li> <li>
<code>current_device() -&gt; int</code> Returns the index of a currently selected device.</li> </ol> <p>For more details, see <a class="reference external" href="https://pytorch.org/tutorials/advanced/extend_dispatcher.html#get-a-dispatch-key-for-your-backend">https://pytorch.org/tutorials/advanced/extend_dispatcher.html#get-a-dispatch-key-for-your-backend</a> For an existing example, see <a class="reference external" href="https://github.com/bdhirsh/pytorch_open_registration_example">https://github.com/bdhirsh/pytorch_open_registration_example</a></p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; torch.utils.rename_privateuse1_backend("foo")
# This will work, assuming that you've implemented the right C++ kernels
# to implement torch.ones.
&gt;&gt;&gt; a = torch.ones(2, device="foo")
</pre>  </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.utils.rename_privateuse1_backend.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.utils.rename_privateuse1_backend.html</a>
  </p>
</div>
