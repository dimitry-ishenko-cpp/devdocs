<h1 id="torch-func-grad">torch.func.grad</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.func.grad">
<code>torch.func.grad(func, argnums=0, has_aux=False)</code> </dt> <dd>
<p><code>grad</code> operator helps computing gradients of <code>func</code> with respect to the input(s) specified by <code>argnums</code>. This operator can be nested to compute higher-order gradients.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>func</strong> (<em>Callable</em>) – A Python function that takes one or more arguments. Must return a single-element Tensor. If specified <code>has_aux</code> equals <code>True</code>, function can return a tuple of single-element Tensor and other auxiliary objects: <code>(output, aux)</code>.</li> <li>
<strong>argnums</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>]</em>) – Specifies arguments to compute gradients with respect to. <code>argnums</code> can be single integer or tuple of integers. Default: 0.</li> <li>
<strong>has_aux</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – Flag indicating that <code>func</code> returns a tensor and other auxiliary objects: <code>(output, aux)</code>. Default: False.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>Function to compute gradients with respect to its inputs. By default, the output of the function is the gradient tensor(s) with respect to the first argument. If specified <code>has_aux</code> equals <code>True</code>, tuple of gradients and output auxiliary objects is returned. If <code>argnums</code> is a tuple of integers, a tuple of output gradients with respect to each <code>argnums</code> value is returned.</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.12)">Callable</a></p> </dd> </dl> <p>Example of using <code>grad</code>:</p> <pre data-language="python">&gt;&gt;&gt; from torch.func import grad
&gt;&gt;&gt; x = torch.randn([])
&gt;&gt;&gt; cos_x = grad(lambda x: torch.sin(x))(x)
&gt;&gt;&gt; assert torch.allclose(cos_x, x.cos())
&gt;&gt;&gt;
&gt;&gt;&gt; # Second-order gradients
&gt;&gt;&gt; neg_sin_x = grad(grad(lambda x: torch.sin(x)))(x)
&gt;&gt;&gt; assert torch.allclose(neg_sin_x, -x.sin())
</pre> <p>When composed with <code>vmap</code>, <code>grad</code> can be used to compute per-sample-gradients:</p> <pre data-language="python">&gt;&gt;&gt; from torch.func import grad, vmap
&gt;&gt;&gt; batch_size, feature_size = 3, 5
&gt;&gt;&gt;
&gt;&gt;&gt; def model(weights, feature_vec):
&gt;&gt;&gt;     # Very simple linear model with activation
&gt;&gt;&gt;     assert feature_vec.dim() == 1
&gt;&gt;&gt;     return feature_vec.dot(weights).relu()
&gt;&gt;&gt;
&gt;&gt;&gt; def compute_loss(weights, example, target):
&gt;&gt;&gt;     y = model(weights, example)
&gt;&gt;&gt;     return ((y - target) ** 2).mean()  # MSELoss
&gt;&gt;&gt;
&gt;&gt;&gt; weights = torch.randn(feature_size, requires_grad=True)
&gt;&gt;&gt; examples = torch.randn(batch_size, feature_size)
&gt;&gt;&gt; targets = torch.randn(batch_size)
&gt;&gt;&gt; inputs = (weights, examples, targets)
&gt;&gt;&gt; grad_weight_per_example = vmap(grad(compute_loss), in_dims=(None, 0, 0))(*inputs)
</pre> <p>Example of using <code>grad</code> with <code>has_aux</code> and <code>argnums</code>:</p> <pre data-language="python">&gt;&gt;&gt; from torch.func import grad
&gt;&gt;&gt; def my_loss_func(y, y_pred):
&gt;&gt;&gt;    loss_per_sample = (0.5 * y_pred - y) ** 2
&gt;&gt;&gt;    loss = loss_per_sample.mean()
&gt;&gt;&gt;    return loss, (y_pred, loss_per_sample)
&gt;&gt;&gt;
&gt;&gt;&gt; fn = grad(my_loss_func, argnums=(0, 1), has_aux=True)
&gt;&gt;&gt; y_true = torch.rand(4)
&gt;&gt;&gt; y_preds = torch.rand(4, requires_grad=True)
&gt;&gt;&gt; out = fn(y_true, y_preds)
&gt;&gt;&gt; # &gt; output is ((grads w.r.t y_true, grads w.r.t y_preds), (y_pred, loss_per_sample))
</pre> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Using PyTorch <code>torch.no_grad</code> together with <code>grad</code>.</p> <p>Case 1: Using <code>torch.no_grad</code> inside a function:</p> <pre data-language="python">&gt;&gt;&gt; def f(x):
&gt;&gt;&gt;     with torch.no_grad():
&gt;&gt;&gt;         c = x ** 2
&gt;&gt;&gt;     return x - c
</pre> <p>In this case, <code>grad(f)(x)</code> will respect the inner <code>torch.no_grad</code>.</p> <p>Case 2: Using <code>grad</code> inside <code>torch.no_grad</code> context manager:</p> <pre data-language="python">&gt;&gt;&gt; with torch.no_grad():
&gt;&gt;&gt;     grad(f)(x)
</pre> <p>In this case, <code>grad</code> will respect the inner <code>torch.no_grad</code>, but not the outer one. This is because <code>grad</code> is a “function transform”: its result should not depend on the result of a context manager outside of <code>f</code>.</p> </div> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.func.grad.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.func.grad.html</a>
  </p>
</div>
