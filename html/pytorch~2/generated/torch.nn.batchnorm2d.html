<h1 id="batchnorm2d">BatchNorm2d</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.nn.BatchNorm2d">
<code>class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/modules/batchnorm.html#BatchNorm2d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>y</mi><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi mathvariant="normal">E</mi><mo stretchy="false">[</mo><mi>x</mi><mo stretchy="false">]</mo></mrow><msqrt><mrow><mrow><mi mathvariant="normal">V</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi></mrow><mo stretchy="false">[</mo><mi>x</mi><mo stretchy="false">]</mo><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac><mo>∗</mo><mi>γ</mi><mo>+</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta</annotation></semantics></math></span></span></span>
</div>
<p>The mean and standard-deviation are calculated per-dimension over the mini-batches and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span></span></span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span></span></span> are learnable parameter vectors of size <code>C</code> (where <code>C</code> is the input size). By default, the elements of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span></span></span> are set to 1 and the elements of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span></span></span> are set to 0. At train time in the forward pass, the standard-deviation is calculated via the biased estimator, equivalent to <code>torch.var(input, unbiased=False)</code>. However, the value stored in the moving average of the standard-deviation is calculated via the unbiased estimator, equivalent to <code>torch.var(input, unbiased=True)</code>.</p> <p>Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p> <p>If <code>track_running_stats</code> is set to <code>False</code>, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>x</mi><mo>^</mo></mover><mtext>new</mtext></msub><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mtext>momentum</mtext><mo stretchy="false">)</mo><mo>×</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo>+</mo><mtext>momentum</mtext><mo>×</mo><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t</annotation></semantics></math></span></span></span>, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{x}</annotation></semantics></math></span></span></span> is the estimated statistic and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span></span></span> is the new observed value.</p> </div> <p>Because the Batch Normalization is done over the <code>C</code> dimension, computing statistics on <code>(N, H, W)</code> slices, it’s common terminology to call this Spatial Batch Normalization.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>num_features</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span></span></span> from an expected input of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C, H, W)</annotation></semantics></math></span></span></span>
</li> <li>
<strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – a value added to the denominator for numerical stability. Default: 1e-5</li> <li>
<strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – the value used for the running_mean and running_var computation. Can be set to <code>None</code> for cumulative moving average (i.e. simple average). Default: 0.1</li> <li>
<strong>affine</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – a boolean value that when set to <code>True</code>, this module has learnable affine parameters. Default: <code>True</code>
</li> <li>
<strong>track_running_stats</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics, and initializes statistics buffers <code>running_mean</code> and <code>running_var</code> as <code>None</code>. When these buffers are <code>None</code>, this module always uses batch statistics. in both training and eval modes. Default: <code>True</code>
</li> </ul> </dd> </dl> <dl class="simple"> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C, H, W)</annotation></semantics></math></span></span></span>
</li> <li>Output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C, H, W)</annotation></semantics></math></span></span></span> (same shape as input)</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm2d(100)
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm2d(100, affine=False)
&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45)
&gt;&gt;&gt; output = m(input)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.BatchNorm2d.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.BatchNorm2d.html</a>
  </p>
</div>
