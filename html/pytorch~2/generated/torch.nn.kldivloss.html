<h1 id="kldivloss">KLDivLoss</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.nn.KLDivLoss">
<code>class torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean', log_target=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/modules/loss.html#KLDivLoss"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>The Kullback-Leibler divergence loss.</p> <p>For tensors of the same shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mtext>pred</mtext></msub><mo separator="true">,</mo><msub><mi>y</mi><mtext>true</mtext></msub></mrow><annotation encoding="application/x-tex">y_{\text{pred}},\ y_{\text{true}}</annotation></semantics></math></span></span></span>, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mtext>pred</mtext></msub></mrow><annotation encoding="application/x-tex">y_{\text{pred}}</annotation></semantics></math></span></span></span> is the <code>input</code> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mtext>true</mtext></msub></mrow><annotation encoding="application/x-tex">y_{\text{true}}</annotation></semantics></math></span></span></span> is the <code>target</code>, we define the <strong>pointwise KL-divergence</strong> as</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mtext>pred</mtext></msub><mo separator="true">,</mo><msub><mi>y</mi><mtext>true</mtext></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>y</mi><mtext>true</mtext></msub><mo>⋅</mo><mi>log</mi><mo>⁡</mo><mfrac><msub><mi>y</mi><mtext>true</mtext></msub><msub><mi>y</mi><mtext>pred</mtext></msub></mfrac><mo>=</mo><msub><mi>y</mi><mtext>true</mtext></msub><mo>⋅</mo><mo stretchy="false">(</mo><mi>log</mi><mo>⁡</mo><msub><mi>y</mi><mtext>true</mtext></msub><mo>−</mo><mi>log</mi><mo>⁡</mo><msub><mi>y</mi><mtext>pred</mtext></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(y_{\text{pred}},\ y_{\text{true}}) = y_{\text{true}} \cdot \log \frac{y_{\text{true}}}{y_{\text{pred}}} = y_{\text{true}} \cdot (\log y_{\text{true}} - \log y_{\text{pred}})</annotation></semantics></math></span></span></span>
</div>
<p>To avoid underflow issues when computing this quantity, this loss expects the argument <code>input</code> in the log-space. The argument <code>target</code> may also be provided in the log-space if <code>log_target</code><code>= True</code>.</p> <p>To summarise, this function is roughly equivalent to computing</p> <pre data-language="python">if not log_target: # default
    loss_pointwise = target * (target.log() - input)
else:
    loss_pointwise = target.exp() * (target - input)
</pre> <p>and then reducing this result depending on the argument <code>reduction</code> as</p> <pre data-language="python">if reduction == "mean":  # default
    loss = loss_pointwise.mean()
elif reduction == "batchmean":  # mathematically correct
    loss = loss_pointwise.sum() / input.size(0)
elif reduction == "sum":
    loss = loss_pointwise.sum()
else:  # reduction == "none"
    loss = loss_pointwise
</pre> <div class="admonition note"> <p class="admonition-title">Note</p> <p>As all the other losses in PyTorch, this function expects the first argument, <code>input</code>, to be the output of the model (e.g. the neural network) and the second, <code>target</code>, to be the observations in the dataset. This differs from the standard mathematical notation <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mi>L</mi><mo stretchy="false">(</mo><mi>P</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>Q</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">KL(P\ ||\ Q)</annotation></semantics></math></span></span></span> where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span></span></span> denotes the distribution of the observations and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span></span></span> denotes the model.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p><code>reduction</code><code>= “mean”</code> doesn’t return the true KL divergence value, please use <code>reduction</code><code>= “batchmean”</code> which aligns with the mathematical definition.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when <code>reduce</code> is <code>False</code>. Default: <code>True</code>
</li> <li>
<strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code>
</li> <li>
<strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output. Default: <code>“mean”</code>
</li> <li>
<strong>log_target</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – Specifies whether <code>target</code> is the log space. Default: <code>False</code>
</li> </ul> </dd> </dl> <dl class="simple"> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*)</annotation></semantics></math></span></span></span>, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo></mrow><annotation encoding="application/x-tex">*</annotation></semantics></math></span></span></span> means any number of dimensions.</li> <li>Target: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*)</annotation></semantics></math></span></span></span>, same shape as the input.</li> <li>Output: scalar by default. If <code>reduction</code> is <code>‘none’</code>, then <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*)</annotation></semantics></math></span></span></span>, same shape as the input.</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; import torch.nn.functional as F
&gt;&gt;&gt; kl_loss = nn.KLDivLoss(reduction="batchmean")
&gt;&gt;&gt; # input should be a distribution in the log space
&gt;&gt;&gt; input = F.log_softmax(torch.randn(3, 5, requires_grad=True), dim=1)
&gt;&gt;&gt; # Sample a batch of distributions. Usually this would come from the dataset
&gt;&gt;&gt; target = F.softmax(torch.rand(3, 5), dim=1)
&gt;&gt;&gt; output = kl_loss(input, target)

&gt;&gt;&gt; kl_loss = nn.KLDivLoss(reduction="batchmean", log_target=True)
&gt;&gt;&gt; log_target = F.log_softmax(torch.rand(3, 5), dim=1)
&gt;&gt;&gt; output = kl_loss(input, log_target)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.KLDivLoss.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.KLDivLoss.html</a>
  </p>
</div>
