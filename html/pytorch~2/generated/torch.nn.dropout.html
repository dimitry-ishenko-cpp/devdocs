<h1 id="dropout">Dropout</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.nn.Dropout">
<code>class torch.nn.Dropout(p=0.5, inplace=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/modules/dropout.html#Dropout"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>During training, randomly zeroes some of the elements of the input tensor with probability <code>p</code> using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.</p> <p>This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper <a class="reference external" href="https://arxiv.org/abs/1207.0580">Improving neural networks by preventing co-adaptation of feature detectors</a> .</p> <p>Furthermore, the outputs are scaled by a factor of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{1-p}</annotation></semantics></math></span></span></span> during training. This means that during evaluation the module simply computes an identity function.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – probability of an element to be zeroed. Default: 0.5</li> <li>
<strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If set to <code>True</code>, will do this operation in-place. Default: <code>False</code>
</li> </ul> </dd> </dl> <dl class="simple"> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*)</annotation></semantics></math></span></span></span>. Input can be of any shape</li> <li>Output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*)</annotation></semantics></math></span></span></span>. Output is of the same shape as input</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; m = nn.Dropout(p=0.2)
&gt;&gt;&gt; input = torch.randn(20, 16)
&gt;&gt;&gt; output = m(input)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.Dropout.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.Dropout.html</a>
  </p>
</div>
