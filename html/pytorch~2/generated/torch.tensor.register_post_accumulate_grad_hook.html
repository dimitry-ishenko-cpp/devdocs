<h1 id="torch-tensor-register-post-accumulate-grad-hook">torch.Tensor.register_post_accumulate_grad_hook</h1> <dl class="py method"> <dt class="sig sig-object py" id="torch.Tensor.register_post_accumulate_grad_hook">
<code>Tensor.register_post_accumulate_grad_hook(hook)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/_tensor.html#Tensor.register_post_accumulate_grad_hook"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Registers a backward hook that runs after grad accumulation.</p> <p>The hook will be called after all gradients for a tensor have been accumulated, meaning that the .grad field has been updated on that tensor. The post accumulate grad hook is ONLY applicable for leaf tensors (tensors without a .grad_fn field). Registering this hook on a non-leaf tensor will error!</p> <p>The hook should have the following signature:</p> <pre data-language="python">hook(param: Tensor) -&gt; None
</pre> <p>Note that, unlike other autograd hooks, this hook operates on the tensor that requires grad and not the grad itself. The hook can in-place modify and access its Tensor argument, including its .grad field.</p> <p>This function returns a handle with a method <code>handle.remove()</code> that removes the hook from the module.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/autograd.html#backward-hooks-execution"><span class="std std-ref">Backward Hooks execution</span></a> for more information on how when this hook is executed, and how its execution is ordered relative to other hooks. Since this hook runs during the backward pass, it will run in no_grad mode (unless create_graph is True). You can use torch.enable_grad() to re-enable autograd within the hook if you need it.</p> </div> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad=True)
&gt;&gt;&gt; lr = 0.01
&gt;&gt;&gt; # simulate a simple SGD update
&gt;&gt;&gt; h = v.register_post_accumulate_grad_hook(lambda p: p.add_(p.grad, alpha=-lr))
&gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.]))
&gt;&gt;&gt; v
tensor([-0.0100, -0.0200, -0.0300], requires_grad=True)

&gt;&gt;&gt; h.remove()  # removes the hook
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.Tensor.register_post_accumulate_grad_hook.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.Tensor.register_post_accumulate_grad_hook.html</a>
  </p>
</div>
