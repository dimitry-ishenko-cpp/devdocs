<h1 id="torch-nn-utils-weight-norm">torch.nn.utils.weight_norm</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.utils.weight_norm">
<code>torch.nn.utils.weight_norm(module, name='weight', dim=0)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/utils/weight_norm.html#weight_norm"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies weight normalization to a parameter in the given module.</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">w</mi><mo>=</mo><mi>g</mi><mfrac><mi mathvariant="bold">v</mi><mrow><mi mathvariant="normal">∥</mi><mi mathvariant="bold">v</mi><mi mathvariant="normal">∥</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|} </annotation></semantics></math></span></span></span>
</div>
<p>Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by <code>name</code> (e.g. <code>'weight'</code>) with two parameters: one specifying the magnitude (e.g. <code>'weight_g'</code>) and one specifying the direction (e.g. <code>'weight_v'</code>). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every <code>forward()</code> call.</p> <p>By default, with <code>dim=0</code>, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use <code>dim=None</code>.</p> <p>See <a class="reference external" href="https://arxiv.org/abs/1602.07868">https://arxiv.org/abs/1602.07868</a></p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This function is deprecated. Use <code>torch.nn.utils.parametrizations.weight_norm()</code> which uses the modern parametrization API. The new <code>weight_norm</code> is compatible with <code>state_dict</code> generated from old <code>weight_norm</code>.</p> <p>Migration guide:</p> <ul class="simple"> <li>The magnitude (<code>weight_g</code>) and direction (<code>weight_v</code>) are now expressed as <code>parametrizations.weight.original0</code> and <code>parametrizations.weight.original1</code> respectively. If this is bothering you, please comment on <a class="reference external" href="https://github.com/pytorch/pytorch/issues/102999">https://github.com/pytorch/pytorch/issues/102999</a>
</li> <li>To remove the weight normalization reparametrization, use <a class="reference internal" href="torch.nn.utils.parametrize.remove_parametrizations.html#torch.nn.utils.parametrize.remove_parametrizations" title="torch.nn.utils.parametrize.remove_parametrizations"><code>torch.nn.utils.parametrize.remove_parametrizations()</code></a>.</li> <li>The weight is no longer recomputed once at module forward; instead, it will be recomputed on every access. To restore the old behavior, use <a class="reference internal" href="torch.nn.utils.parametrize.cached.html#torch.nn.utils.parametrize.cached" title="torch.nn.utils.parametrize.cached"><code>torch.nn.utils.parametrize.cached()</code></a> before invoking the module in question.</li> </ul> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>module</strong> (<a class="reference internal" href="torch.nn.module.html#torch.nn.Module" title="torch.nn.Module">Module</a>) – containing module</li> <li>
<strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><em>optional</em>) – name of weight parameter</li> <li>
<strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em>) – dimension over which to compute the norm</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>The original module with the weight norm hook</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><em>T_module</em></p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = weight_norm(nn.Linear(20, 40), name='weight')
&gt;&gt;&gt; m
Linear(in_features=20, out_features=40, bias=True)
&gt;&gt;&gt; m.weight_g.size()
torch.Size([40, 1])
&gt;&gt;&gt; m.weight_v.size()
torch.Size([40, 20])
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.utils.weight_norm.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.utils.weight_norm.html</a>
  </p>
</div>
