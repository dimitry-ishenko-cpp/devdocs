<h1 id="torch-sparse-sum">torch.sparse.sum</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.sparse.sum">
<code>torch.sparse.sum(input, dim=None, dtype=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/sparse.html#sum"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the sum of each row of the sparse tensor <code>input</code> in the given dimensions <code>dim</code>. If <code>dim</code> is a list of dimensions, reduce over all of them. When sum over all <code>sparse_dim</code>, this method returns a dense tensor instead of a sparse tensor.</p> <p>All summed <code>dim</code> are squeezed (see <a class="reference internal" href="torch.squeeze.html#torch.squeeze" title="torch.squeeze"><code>torch.squeeze()</code></a>), resulting an output tensor having <code>dim</code> fewer dimensions than <code>input</code>.</p> <p>During backward, only gradients at <code>nnz</code> locations of <code>input</code> will propagate back. Note that the gradients of <code>input</code> is coalesced.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input sparse tensor</li> <li>
<strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em> of </em><em>ints</em>) – a dimension or a list of dimensions to reduce. Default: reduce over all dims.</li> <li>
<strong>dtype</strong> (<a class="reference internal" href="../tensor_attributes.html#torch.dtype" title="torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned Tensor. Default: dtype of <code>input</code>.</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; nnz = 3
&gt;&gt;&gt; dims = [5, 5, 2, 3]
&gt;&gt;&gt; I = torch.cat([torch.randint(0, dims[0], size=(nnz,)),
                   torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)
&gt;&gt;&gt; V = torch.randn(nnz, dims[2], dims[3])
&gt;&gt;&gt; size = torch.Size(dims)
&gt;&gt;&gt; S = torch.sparse_coo_tensor(I, V, size)
&gt;&gt;&gt; S
tensor(indices=tensor([[2, 0, 3],
                       [2, 4, 1]]),
       values=tensor([[[-0.6438, -1.6467,  1.4004],
                       [ 0.3411,  0.0918, -0.2312]],

                      [[ 0.5348,  0.0634, -2.0494],
                       [-0.7125, -1.0646,  2.1844]],

                      [[ 0.1276,  0.1874, -0.6334],
                       [-1.9682, -0.5340,  0.7483]]]),
       size=(5, 5, 2, 3), nnz=3, layout=torch.sparse_coo)

# when sum over only part of sparse_dims, return a sparse tensor
&gt;&gt;&gt; torch.sparse.sum(S, [1, 3])
tensor(indices=tensor([[0, 2, 3]]),
       values=tensor([[-1.4512,  0.4073],
                      [-0.8901,  0.2017],
                      [-0.3183, -1.7539]]),
       size=(5, 2), nnz=3, layout=torch.sparse_coo)

# when sum over all sparse dim, return a dense tensor
# with summed dims squeezed
&gt;&gt;&gt; torch.sparse.sum(S, [0, 1, 3])
tensor([-2.6596, -1.1450])
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.sparse.sum.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.sparse.sum.html</a>
  </p>
</div>
