<h1 id="preparecustomconfig">PrepareCustomConfig</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.ao.quantization.fx.custom_config.PrepareCustomConfig">
<code>class torch.ao.quantization.fx.custom_config.PrepareCustomConfig</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/fx/custom_config.html#PrepareCustomConfig"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Custom configuration for <a class="reference internal" href="torch.ao.quantization.quantize_fx.prepare_fx.html#torch.ao.quantization.quantize_fx.prepare_fx" title="torch.ao.quantization.quantize_fx.prepare_fx"><code>prepare_fx()</code></a> and <a class="reference internal" href="torch.ao.quantization.quantize_fx.prepare_qat_fx.html#torch.ao.quantization.quantize_fx.prepare_qat_fx" title="torch.ao.quantization.quantize_fx.prepare_qat_fx"><code>prepare_qat_fx()</code></a>.</p> <p>Example usage:</p> <pre data-language="python">prepare_custom_config = PrepareCustomConfig()             .set_standalone_module_name("module1", qconfig_mapping, example_inputs,                 child_prepare_custom_config, backend_config)             .set_standalone_module_class(MyStandaloneModule, qconfig_mapping, example_inputs,                 child_prepare_custom_config, backend_config)             .set_float_to_observed_mapping(FloatCustomModule, ObservedCustomModule)             .set_non_traceable_module_names(["module2", "module3"])             .set_non_traceable_module_classes([NonTraceableModule1, NonTraceableModule2])             .set_input_quantized_indexes([0])             .set_output_quantized_indexes([0])             .set_preserved_attributes(["attr1", "attr2"])
</pre> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.fx.custom_config.PrepareCustomConfig.from_dict">
<code>classmethod from_dict(prepare_custom_config_dict)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/fx/custom_config.html#PrepareCustomConfig.from_dict"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Create a <code>PrepareCustomConfig</code> from a dictionary with the following items:</p>  <p>“standalone_module_name”: a list of (module_name, qconfig_mapping, example_inputs, child_prepare_custom_config, backend_config) tuples</p> <p>“standalone_module_class” a list of (module_class, qconfig_mapping, example_inputs, child_prepare_custom_config, backend_config) tuples</p> <p>“float_to_observed_custom_module_class”: a nested dictionary mapping from quantization mode to an inner mapping from float module classes to observed module classes, e.g. {“static”: {FloatCustomModule: ObservedCustomModule}}</p> <p>“non_traceable_module_name”: a list of modules names that are not symbolically traceable “non_traceable_module_class”: a list of module classes that are not symbolically traceable “input_quantized_idxs”: a list of indexes of graph inputs that should be quantized “output_quantized_idxs”: a list of indexes of graph outputs that should be quantized “preserved_attributes”: a list of attributes that persist even if they are not used in <code>forward</code></p>  <p>This function is primarily for backward compatibility and may be removed in the future.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.fx.custom_config.PrepareCustomConfig" title="torch.ao.quantization.fx.custom_config.PrepareCustomConfig">PrepareCustomConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_float_to_observed_mapping">
<code>set_float_to_observed_mapping(float_class, observed_class, quant_type=QuantType.STATIC)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/fx/custom_config.html#PrepareCustomConfig.set_float_to_observed_mapping"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the mapping from a custom float module class to a custom observed module class.</p> <p>The observed module class must have a <code>from_float</code> class method that converts the float module class to the observed module class. This is currently only supported for static quantization.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.fx.custom_config.PrepareCustomConfig" title="torch.ao.quantization.fx.custom_config.PrepareCustomConfig">PrepareCustomConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_input_quantized_indexes">
<code>set_input_quantized_indexes(indexes)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/fx/custom_config.html#PrepareCustomConfig.set_input_quantized_indexes"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the indexes of the inputs of the graph that should be quantized. Inputs are otherwise assumed to be in fp32 by default instead.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.fx.custom_config.PrepareCustomConfig" title="torch.ao.quantization.fx.custom_config.PrepareCustomConfig">PrepareCustomConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_classes">
<code>set_non_traceable_module_classes(module_classes)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/fx/custom_config.html#PrepareCustomConfig.set_non_traceable_module_classes"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the modules that are not symbolically traceable, identified by class.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.fx.custom_config.PrepareCustomConfig" title="torch.ao.quantization.fx.custom_config.PrepareCustomConfig">PrepareCustomConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_names">
<code>set_non_traceable_module_names(module_names)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/fx/custom_config.html#PrepareCustomConfig.set_non_traceable_module_names"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the modules that are not symbolically traceable, identified by name.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.fx.custom_config.PrepareCustomConfig" title="torch.ao.quantization.fx.custom_config.PrepareCustomConfig">PrepareCustomConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_output_quantized_indexes">
<code>set_output_quantized_indexes(indexes)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/fx/custom_config.html#PrepareCustomConfig.set_output_quantized_indexes"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the indexes of the outputs of the graph that should be quantized. Outputs are otherwise assumed to be in fp32 by default instead.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.fx.custom_config.PrepareCustomConfig" title="torch.ao.quantization.fx.custom_config.PrepareCustomConfig">PrepareCustomConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_preserved_attributes">
<code>set_preserved_attributes(attributes)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/fx/custom_config.html#PrepareCustomConfig.set_preserved_attributes"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the names of the attributes that will persist in the graph module even if they are not used in the model’s <code>forward</code> method.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.fx.custom_config.PrepareCustomConfig" title="torch.ao.quantization.fx.custom_config.PrepareCustomConfig">PrepareCustomConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_class">
<code>set_standalone_module_class(module_class, qconfig_mapping, example_inputs, prepare_custom_config, backend_config)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/fx/custom_config.html#PrepareCustomConfig.set_standalone_module_class"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the configuration for running a standalone module identified by <code>module_class</code>.</p> <p>If <code>qconfig_mapping</code> is None, the parent <code>qconfig_mapping</code> will be used instead. If <code>prepare_custom_config</code> is None, an empty <code>PrepareCustomConfig</code> will be used. If <code>backend_config</code> is None, the parent <code>backend_config</code> will be used instead.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.fx.custom_config.PrepareCustomConfig" title="torch.ao.quantization.fx.custom_config.PrepareCustomConfig">PrepareCustomConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_name">
<code>set_standalone_module_name(module_name, qconfig_mapping, example_inputs, prepare_custom_config, backend_config)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/fx/custom_config.html#PrepareCustomConfig.set_standalone_module_name"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the configuration for running a standalone module identified by <code>module_name</code>.</p> <p>If <code>qconfig_mapping</code> is None, the parent <code>qconfig_mapping</code> will be used instead. If <code>prepare_custom_config</code> is None, an empty <code>PrepareCustomConfig</code> will be used. If <code>backend_config</code> is None, the parent <code>backend_config</code> will be used instead.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.fx.custom_config.PrepareCustomConfig" title="torch.ao.quantization.fx.custom_config.PrepareCustomConfig">PrepareCustomConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.fx.custom_config.PrepareCustomConfig.to_dict">
<code>to_dict()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/fx/custom_config.html#PrepareCustomConfig.to_dict"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Convert this <code>PrepareCustomConfig</code> to a dictionary with the items described in <a class="reference internal" href="#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.from_dict" title="torch.ao.quantization.fx.custom_config.PrepareCustomConfig.from_dict"><code>from_dict()</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.12)">Dict</a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">Any</a>]</p> </dd> </dl> </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html</a>
  </p>
</div>
