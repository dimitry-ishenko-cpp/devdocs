<h1 id="rnncell">RNNCell</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.nn.RNNCell">
<code>class torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh', device=None, dtype=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/modules/rnn.html#RNNCell"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>An Elman RNN cell with tanh or ReLU non-linearity.</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>h</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>W</mi><mrow><mi>i</mi><mi>h</mi></mrow></msub><mi>x</mi><mo>+</mo><msub><mi>b</mi><mrow><mi>i</mi><mi>h</mi></mrow></msub><mo>+</mo><msub><mi>W</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><mi>h</mi><mo>+</mo><msub><mi>b</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h' = \tanh(W_{ih} x + b_{ih} + W_{hh} h + b_{hh})</annotation></semantics></math></span></span></span>
</div>
<p>If <code>nonlinearity</code> is <code>‘relu’</code>, then ReLU is used in place of tanh.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – The number of expected features in the input <code>x</code>
</li> <li>
<strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – The number of features in the hidden state <code>h</code>
</li> <li>
<strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If <code>False</code>, then the layer does not use bias weights <code>b_ih</code> and <code>b_hh</code>. Default: <code>True</code>
</li> <li>
<strong>nonlinearity</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – The non-linearity to use. Can be either <code>'tanh'</code> or <code>'relu'</code>. Default: <code>'tanh'</code>
</li> </ul> </dd> </dl> <dl class="simple"> <dt>Inputs: input, hidden</dt>
<dd>
<ul class="simple"> <li>
<strong>input</strong>: tensor containing input features</li> <li>
<strong>hidden</strong>: tensor containing the initial hidden state Defaults to zero if not provided.</li> </ul> </dd> <dt>Outputs: h’</dt>
<dd>
<ul class="simple"> <li>
<strong>h’</strong> of shape <code>(batch, hidden_size)</code>: tensor containing the next hidden state for each element in the batch</li> </ul> </dd> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, H_{in})</annotation></semantics></math></span></span></span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(H_{in})</annotation></semantics></math></span></span></span> tensor containing input features where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">H_{in}</annotation></semantics></math></span></span></span> = <code>input_size</code>.</li> <li>hidden: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, H_{out})</annotation></semantics></math></span></span></span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(H_{out})</annotation></semantics></math></span></span></span> tensor containing the initial hidden state where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">H_{out}</annotation></semantics></math></span></span></span> = <code>hidden_size</code>. Defaults to zero if not provided.</li> <li>output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, H_{out})</annotation></semantics></math></span></span></span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(H_{out})</annotation></semantics></math></span></span></span> tensor containing the next hidden state.</li> </ul> </dd> </dl> <dl class="field-list simple"> <dt class="field-odd">Variables</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>weight_ih</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>) – the learnable input-hidden weights, of shape <code>(hidden_size, input_size)</code>
</li> <li>
<strong>weight_hh</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">torch.Tensor</a>) – the learnable hidden-hidden weights, of shape <code>(hidden_size, hidden_size)</code>
</li> <li>
<strong>bias_ih</strong> – the learnable input-hidden bias, of shape <code>(hidden_size)</code>
</li> <li>
<strong>bias_hh</strong> – the learnable hidden-hidden bias, of shape <code>(hidden_size)</code>
</li> </ul> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>All the weights and biases are initialized from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">U</mi><mo stretchy="false">(</mo><mo>−</mo><msqrt><mi>k</mi></msqrt><mo separator="true">,</mo><msqrt><mi>k</mi></msqrt><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{U}(-\sqrt{k}, \sqrt{k})</annotation></semantics></math></span></span></span> where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mfrac><mn>1</mn><mtext>hidden_size</mtext></mfrac></mrow><annotation encoding="application/x-tex">k = \frac{1}{\text{hidden\_size}}</annotation></semantics></math></span></span></span></p> </div> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; rnn = nn.RNNCell(10, 20)
&gt;&gt;&gt; input = torch.randn(6, 3, 10)
&gt;&gt;&gt; hx = torch.randn(3, 20)
&gt;&gt;&gt; output = []
&gt;&gt;&gt; for i in range(6):
...     hx = rnn(input[i], hx)
...     output.append(hx)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.RNNCell.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.RNNCell.html</a>
  </p>
</div>
