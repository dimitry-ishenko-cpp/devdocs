<h1 id="rnnbase">RNNBase</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.nn.RNNBase">
<code>class torch.nn.RNNBase(mode, input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0.0, bidirectional=False, proj_size=0, device=None, dtype=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/modules/rnn.html#RNNBase"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Base class for RNN modules (RNN, LSTM, GRU).</p> <p>Implements aspects of RNNs shared by the RNN, LSTM, and GRU classes, such as module initialization and utility methods for parameter storage management.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The forward method is not implemented by the RNNBase class.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>LSTM and GRU classes override some methods implemented by RNNBase.</p> </div>  <dl class="py method"> <dt class="sig sig-object py" id="torch.nn.RNNBase.flatten_parameters">
<code>flatten_parameters()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/modules/rnn.html#RNNBase.flatten_parameters"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Resets parameter data pointer so that they can use faster code paths.</p> <p>Right now, this works only if the module is on the GPU and cuDNN is enabled. Otherwise, itâ€™s a no-op.</p>  </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.RNNBase.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.RNNBase.html</a>
  </p>
</div>
