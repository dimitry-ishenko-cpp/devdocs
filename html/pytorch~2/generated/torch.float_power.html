<h1 id="torch-float-power">torch.float_power</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.float_power">
<code>torch.float_power(input, exponent, *, out=None) → Tensor</code> </dt> <dd>
<p>Raises <code>input</code> to the power of <code>exponent</code>, elementwise, in double precision. If neither input is complex returns a <code>torch.float64</code> tensor, and if one or more inputs is complex returns a <code>torch.complex128</code> tensor.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This function always computes in double precision, unlike <a class="reference internal" href="torch.pow.html#torch.pow" title="torch.pow"><code>torch.pow()</code></a>, which implements more typical <a class="reference internal" href="../tensor_attributes.html#type-promotion-doc"><span class="std std-ref">type promotion</span></a>. This is useful when the computation needs to be performed in a wider or more precise dtype, or the results of the computation may contain fractional values not representable in the input dtypes, like when an integer base is raised to a negative integer exponent.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a><em> or </em><em>Number</em>) – the base value(s)</li> <li>
<strong>exponent</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a><em> or </em><em>Number</em>) – the exponent value(s)</li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference internal" href="../tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – the output tensor.</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; a = torch.randint(10, (4,))
&gt;&gt;&gt; a
tensor([6, 4, 7, 1])
&gt;&gt;&gt; torch.float_power(a, 2)
tensor([36., 16., 49.,  1.], dtype=torch.float64)

&gt;&gt;&gt; a = torch.arange(1, 5)
&gt;&gt;&gt; a
tensor([ 1,  2,  3,  4])
&gt;&gt;&gt; exp = torch.tensor([2, -3, 4, -5])
&gt;&gt;&gt; exp
tensor([ 2, -3,  4, -5])
&gt;&gt;&gt; torch.float_power(a, exp)
tensor([1.0000e+00, 1.2500e-01, 8.1000e+01, 9.7656e-04], dtype=torch.float64)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.float_power.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.float_power.html</a>
  </p>
</div>
