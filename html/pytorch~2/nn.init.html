<h1 id="nn-init-doc">torch.nn.init</h1> <div class="admonition warning" id="torch-nn-init"> <p class="admonition-title">Warning</p> <p>All the functions in this module are intended to be used to initialize neural network parameters, so they all run in <a class="reference internal" href="generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad"><code>torch.no_grad()</code></a> mode and will not be taken into account by autograd.</p> </div> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.init.calculate_gain">
<code>torch.nn.init.calculate_gain(nonlinearity, param=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/init.html#calculate_gain"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Return the recommended gain value for the given nonlinearity function. The values are as follows:</p> <table class="docutils colwidths-auto align-default"> <thead> <tr>
<th class="head"><p>nonlinearity</p></th> <th class="head"><p>gain</p></th> </tr> </thead>  <tr>
<td><p>Linear / Identity</p></td> <td><p><span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span></span></span></p></td> </tr> <tr>
<td><p>Conv{1,2,3}D</p></td> <td><p><span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span></span></span></p></td> </tr> <tr>
<td><p>Sigmoid</p></td> <td><p><span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span></span></span></p></td> </tr> <tr>
<td><p>Tanh</p></td> <td><p><span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>5</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{5}{3}</annotation></semantics></math></span></span></span></p></td> </tr> <tr>
<td><p>ReLU</p></td> <td><p><span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mn>2</mn></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{2}</annotation></semantics></math></span></span></span></p></td> </tr> <tr>
<td><p>Leaky Relu</p></td> <td><p><span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mfrac><mn>2</mn><mrow><mn>1</mn><mo>+</mo><msup><mtext>negative_slope</mtext><mn>2</mn></msup></mrow></mfrac></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{\frac{2}{1 + \text{negative\_slope}^2}}</annotation></semantics></math></span></span></span></p></td> </tr> <tr>
<td><p>SELU</p></td> <td><p><span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>3</mn><mn>4</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{3}{4}</annotation></semantics></math></span></span></span></p></td> </tr>  </table> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>In order to implement <a class="reference external" href="https://papers.nips.cc/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html">Self-Normalizing Neural Networks</a> , you should use <code>nonlinearity='linear'</code> instead of <code>nonlinearity='selu'</code>. This gives the initial weights a variance of <code>1 / N</code>, which is necessary to induce a stable fixed point in the forward pass. In contrast, the default gain for <code>SELU</code> sacrifices the normalization effect for more stable gradient flow in rectangular layers.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>nonlinearity</strong> – the non-linear function (<code>nn.functional</code> name)</li> <li>
<strong>param</strong> – optional parameter for the non-linear function</li> </ul> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2
</pre> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.init.uniform_">
<code>torch.nn.init.uniform_(tensor, a=0.0, b=1.0)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/init.html#uniform_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fills the input Tensor with values drawn from the uniform distribution <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">U</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{U}(a, b)</annotation></semantics></math></span></span></span>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – an n-dimensional <code>torch.Tensor</code>
</li> <li>
<strong>a</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – the lower bound of the uniform distribution</li> <li>
<strong>b</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – the upper bound of the uniform distribution</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; w = torch.empty(3, 5)
&gt;&gt;&gt; nn.init.uniform_(w)
</pre> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.init.normal_">
<code>torch.nn.init.normal_(tensor, mean=0.0, std=1.0)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/init.html#normal_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fills the input Tensor with values drawn from the normal distribution <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mtext>mean</mtext><mo separator="true">,</mo><msup><mtext>std</mtext><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(\text{mean}, \text{std}^2)</annotation></semantics></math></span></span></span>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – an n-dimensional <code>torch.Tensor</code>
</li> <li>
<strong>mean</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – the mean of the normal distribution</li> <li>
<strong>std</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – the standard deviation of the normal distribution</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; w = torch.empty(3, 5)
&gt;&gt;&gt; nn.init.normal_(w)
</pre> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.init.constant_">
<code>torch.nn.init.constant_(tensor, val)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/init.html#constant_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fills the input Tensor with the value <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>val</mtext></mrow><annotation encoding="application/x-tex">\text{val}</annotation></semantics></math></span></span></span>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – an n-dimensional <code>torch.Tensor</code>
</li> <li>
<strong>val</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – the value to fill the tensor with</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; w = torch.empty(3, 5)
&gt;&gt;&gt; nn.init.constant_(w, 0.3)
</pre> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.init.ones_">
<code>torch.nn.init.ones_(tensor)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/init.html#ones_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fills the input Tensor with the scalar value <code>1</code>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – an n-dimensional <code>torch.Tensor</code></p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; w = torch.empty(3, 5)
&gt;&gt;&gt; nn.init.ones_(w)
</pre> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.init.zeros_">
<code>torch.nn.init.zeros_(tensor)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/init.html#zeros_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fills the input Tensor with the scalar value <code>0</code>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – an n-dimensional <code>torch.Tensor</code></p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; w = torch.empty(3, 5)
&gt;&gt;&gt; nn.init.zeros_(w)
</pre> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.init.eye_">
<code>torch.nn.init.eye_(tensor)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/init.html#eye_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fills the 2-dimensional input <code>Tensor</code> with the identity matrix. Preserves the identity of the inputs in <code>Linear</code> layers, where as many inputs are preserved as possible.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>tensor</strong> – a 2-dimensional <code>torch.Tensor</code></p> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; w = torch.empty(3, 5)
&gt;&gt;&gt; nn.init.eye_(w)
</pre> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.init.dirac_">
<code>torch.nn.init.dirac_(tensor, groups=1)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/init.html#dirac_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fills the {3, 4, 5}-dimensional input <code>Tensor</code> with the Dirac delta function. Preserves the identity of the inputs in <code>Convolutional</code> layers, where as many input channels are preserved as possible. In case of groups&gt;1, each group of channels preserves identity</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>tensor</strong> – a {3, 4, 5}-dimensional <code>torch.Tensor</code>
</li> <li>
<strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em>) – number of groups in the conv layer (default: 1)</li> </ul> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; w = torch.empty(3, 16, 5, 5)
&gt;&gt;&gt; nn.init.dirac_(w)
&gt;&gt;&gt; w = torch.empty(3, 24, 5, 5)
&gt;&gt;&gt; nn.init.dirac_(w, 3)
</pre> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.init.xavier_uniform_">
<code>torch.nn.init.xavier_uniform_(tensor, gain=1.0)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/init.html#xavier_uniform_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fills the input <code>Tensor</code> with values according to the method described in <code>Understanding the difficulty of training deep feedforward neural networks</code> - Glorot, X. &amp; Bengio, Y. (2010), using a uniform distribution. The resulting tensor will have values sampled from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">U</mi><mo stretchy="false">(</mo><mo>−</mo><mi>a</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{U}(-a, a)</annotation></semantics></math></span></span></span> where</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>a</mi><mo>=</mo><mtext>gain</mtext><mo>×</mo><msqrt><mfrac><mn>6</mn><mrow><mtext>fan_in</mtext><mo>+</mo><mtext>fan_out</mtext></mrow></mfrac></msqrt></mrow><annotation encoding="application/x-tex">a = \text{gain} \times \sqrt{\frac{6}{\text{fan\_in} + \text{fan\_out}}} </annotation></semantics></math></span></span></span>
</div>
<p>Also known as Glorot initialization.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – an n-dimensional <code>torch.Tensor</code>
</li> <li>
<strong>gain</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – an optional scaling factor</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; w = torch.empty(3, 5)
&gt;&gt;&gt; nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))
</pre> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.init.xavier_normal_">
<code>torch.nn.init.xavier_normal_(tensor, gain=1.0)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/init.html#xavier_normal_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fills the input <code>Tensor</code> with values according to the method described in <code>Understanding the difficulty of training deep feedforward neural networks</code> - Glorot, X. &amp; Bengio, Y. (2010), using a normal distribution. The resulting tensor will have values sampled from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><msup><mtext>std</mtext><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(0, \text{std}^2)</annotation></semantics></math></span></span></span> where</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>std</mtext><mo>=</mo><mtext>gain</mtext><mo>×</mo><msqrt><mfrac><mn>2</mn><mrow><mtext>fan_in</mtext><mo>+</mo><mtext>fan_out</mtext></mrow></mfrac></msqrt></mrow><annotation encoding="application/x-tex">\text{std} = \text{gain} \times \sqrt{\frac{2}{\text{fan\_in} + \text{fan\_out}}} </annotation></semantics></math></span></span></span>
</div>
<p>Also known as Glorot initialization.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – an n-dimensional <code>torch.Tensor</code>
</li> <li>
<strong>gain</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – an optional scaling factor</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; w = torch.empty(3, 5)
&gt;&gt;&gt; nn.init.xavier_normal_(w)
</pre> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.init.kaiming_uniform_">
<code>torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/init.html#kaiming_uniform_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fills the input <code>Tensor</code> with values according to the method described in <code>Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</code> - He, K. et al. (2015), using a uniform distribution. The resulting tensor will have values sampled from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">U</mi><mo stretchy="false">(</mo><mo>−</mo><mtext>bound</mtext><mo separator="true">,</mo><mtext>bound</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{U}(-\text{bound}, \text{bound})</annotation></semantics></math></span></span></span> where</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>bound</mtext><mo>=</mo><mtext>gain</mtext><mo>×</mo><msqrt><mfrac><mn>3</mn><mtext>fan_mode</mtext></mfrac></msqrt></mrow><annotation encoding="application/x-tex">\text{bound} = \text{gain} \times \sqrt{\frac{3}{\text{fan\_mode}}} </annotation></semantics></math></span></span></span>
</div>
<p>Also known as He initialization.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – an n-dimensional <code>torch.Tensor</code>
</li> <li>
<strong>a</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – the negative slope of the rectifier used after this layer (only used with <code>'leaky_relu'</code>)</li> <li>
<strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – either <code>'fan_in'</code> (default) or <code>'fan_out'</code>. Choosing <code>'fan_in'</code> preserves the magnitude of the variance of the weights in the forward pass. Choosing <code>'fan_out'</code> preserves the magnitudes in the backwards pass.</li> <li>
<strong>nonlinearity</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – the non-linear function (<code>nn.functional</code> name), recommended to use only with <code>'relu'</code> or <code>'leaky_relu'</code> (default).</li> </ul> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; w = torch.empty(3, 5)
&gt;&gt;&gt; nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')
</pre> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.init.kaiming_normal_">
<code>torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/init.html#kaiming_normal_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fills the input <code>Tensor</code> with values according to the method described in <code>Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</code> - He, K. et al. (2015), using a normal distribution. The resulting tensor will have values sampled from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><msup><mtext>std</mtext><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(0, \text{std}^2)</annotation></semantics></math></span></span></span> where</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>std</mtext><mo>=</mo><mfrac><mtext>gain</mtext><msqrt><mtext>fan_mode</mtext></msqrt></mfrac></mrow><annotation encoding="application/x-tex">\text{std} = \frac{\text{gain}}{\sqrt{\text{fan\_mode}}} </annotation></semantics></math></span></span></span>
</div>
<p>Also known as He initialization.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – an n-dimensional <code>torch.Tensor</code>
</li> <li>
<strong>a</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – the negative slope of the rectifier used after this layer (only used with <code>'leaky_relu'</code>)</li> <li>
<strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – either <code>'fan_in'</code> (default) or <code>'fan_out'</code>. Choosing <code>'fan_in'</code> preserves the magnitude of the variance of the weights in the forward pass. Choosing <code>'fan_out'</code> preserves the magnitudes in the backwards pass.</li> <li>
<strong>nonlinearity</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – the non-linear function (<code>nn.functional</code> name), recommended to use only with <code>'relu'</code> or <code>'leaky_relu'</code> (default).</li> </ul> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; w = torch.empty(3, 5)
&gt;&gt;&gt; nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')
</pre> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.init.trunc_normal_">
<code>torch.nn.init.trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0, generator=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/init.html#trunc_normal_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fills the input Tensor with values drawn from a truncated normal distribution. The values are effectively drawn from the normal distribution <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mtext>mean</mtext><mo separator="true">,</mo><msup><mtext>std</mtext><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(\text{mean}, \text{std}^2)</annotation></semantics></math></span></span></span> with values outside <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[a, b]</annotation></semantics></math></span></span></span> redrawn until they are within the bounds. The method used for generating the random values works best when <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>≤</mo><mtext>mean</mtext><mo>≤</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a \leq \text{mean} \leq b</annotation></semantics></math></span></span></span>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>) – an n-dimensional <code>torch.Tensor</code>
</li> <li>
<strong>mean</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – the mean of the normal distribution</li> <li>
<strong>std</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – the standard deviation of the normal distribution</li> <li>
<strong>a</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – the minimum cutoff value</li> <li>
<strong>b</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – the maximum cutoff value</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; w = torch.empty(3, 5)
&gt;&gt;&gt; nn.init.trunc_normal_(w)
</pre> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.init.orthogonal_">
<code>torch.nn.init.orthogonal_(tensor, gain=1)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/init.html#orthogonal_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fills the input <code>Tensor</code> with a (semi) orthogonal matrix, as described in <code>Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</code> - Saxe, A. et al. (2013). The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>tensor</strong> – an n-dimensional <code>torch.Tensor</code>, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>≥</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">n \geq 2</annotation></semantics></math></span></span></span>
</li> <li>
<strong>gain</strong> – optional scaling factor</li> </ul> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; w = torch.empty(3, 5)
&gt;&gt;&gt; nn.init.orthogonal_(w)
</pre> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.init.sparse_">
<code>torch.nn.init.sparse_(tensor, sparsity, std=0.01)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/init.html#sparse_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fills the 2D input <code>Tensor</code> as a sparse matrix, where the non-zero elements will be drawn from the normal distribution <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>0.01</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(0, 0.01)</annotation></semantics></math></span></span></span>, as described in <code>Deep learning via Hessian-free optimization</code> - Martens, J. (2010).</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>tensor</strong> – an n-dimensional <code>torch.Tensor</code>
</li> <li>
<strong>sparsity</strong> – The fraction of elements in each column to be set to zero</li> <li>
<strong>std</strong> – the standard deviation of the normal distribution used to generate the non-zero values</li> </ul> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; w = torch.empty(3, 5)
&gt;&gt;&gt; nn.init.sparse_(w, sparsity=0.1)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/nn.init.html" class="_attribution-link">https://pytorch.org/docs/2.1/nn.init.html</a>
  </p>
</div>
