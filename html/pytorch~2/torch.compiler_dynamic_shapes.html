<h1 id="dynamic-shapes">Dynamic shapes</h1> <p>Code: <a class="reference external" href="https://github.com/pytorch/pytorch/blob/db4572dbf18f1cf50cf662547e272d3117063747/torch/fx/experimental/symbolic_shapes.py">symbolic_shapes.py</a></p> <p>See also: <a class="reference external" href="https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit#heading=h.fh8zzonyw8ng">The dynamic shapes manual</a></p>  <h2 id="motivation">Motivation</h2> <p>Deep learning compilers commonly only work for static shapes, that is to say, they produced compiled programs which only work for a single specific configuration of input shapes, and must recompile if any input shape changes. This assumption works great for the majority of commonly run deep learning models today, but there are a few situations where it is insufficient:</p> <ul class="simple"> <li>Some dimensions, such as batch size or sequence length, may vary. For example, an inference service performing adaptive batching will execute inference requests with varying batch sizes depending on how many requests it received within its batching window. We may also want to consider padding out variable size sequences only to the maximum sequence length within a batch, which may vary from batch-to-batch.</li> <li>Some models exhibit data-dependent output shapes, that is to say, the size of their outputs and intermediates may depend on the actual input data which may vary across runs. For example, detection models may first generate a variable number of potential bounding boxes before running a more expensive image recognition model to identify if the subject is in a bounding box. The number of bounding boxes is data dependent.</li> <li>One particularly important case of data-dependent shapes occurs when dealing with sparse representations, such as sparse tensors, jagged tensors, and graph neural networks. In all of these cases, the amount of data to be processed depends on the sparse structure of the problem, which will typically vary in a data-dependent way.</li> </ul> <p>In supporting dynamic shapes, we chose not to support dynamic rank programs, e.g., programs whose inputs tensors change in dimensionality, as this pattern rarely occurs in real-world deep learning programs, and it avoids the need to reason inductively over symbolic lists of shapes.</p>   <h2 id="abridged-public-api">Abridged public API</h2> <p>The default dynamic behavior in PyTorch 2.1 is:</p> <ul class="simple"> <li>PT2 assumes everything is static by default</li> <li>If we recompile because a size changed, we will instead attempt to recompile that size as being dynamic (sizes that have changed are likely to change in the future). This generalization may fail (e.g., because user code does a conditional branch on the size in question or missing dynamic shapes support in PT2). If you are trying to understand why PT2 has overspecialized some code, run with <code>TORCH_LOGS=dynamic</code> and look for “eval” entries that say when guards are added and why.</li> <li>If you know ahead of time something will be dynamic, you can skip the first recompile with <code>torch._dynamo.mark_dynamic(tensor, dim)</code>.</li> <li>If you say <code>torch.compile(dynamic=False)</code>, we will turn off automatic dynamic shapes on recompiles and always recompile for each distinct size. Conversely, if you say <code>torch.compile(dynamic=True)</code>, we will try to make everything as dynamic as possible. This is mostly useful for small operators; if you try it on a big model it will (1) probably crash PT2 and (2) run slow for no good reason.</li> </ul>   <h2 id="the-guard-model">The Guard Model</h2> <p>When considering how to add support for dynamic shapes to TorchDynamo and TorchInductor, we made a major design decision: in order to reuse decompositions and other preexisting code written in Python/C++ targeting the PyTorch API, we must be able to trace through dynamic shapes. Unlike a fully symbolic system which might capture both branches of a conditional, we always pick one branch and specialize our trace under the assumption that we only use this trace when we would have made the same choice for that branch in the future. To do this, we maintain a “hint” for every symbolic size saying what its concrete value is at compile time (as TorchDynamo is a just-in-time compiler, it always knows what the actual input sizes are.) When we perform a condition on a tensor, we simply consult the hint to find out which branch to take.</p> <p>This greatly simplifies the symbolic shape formulas we produce, but means we have a much more involved system for managing guards. Consider, for example, the following program:</p> <pre data-language="python">def f(x, y):
    z = torch.cat([x, y])
    if z.size(0) &gt; 2:
        return z.mul(2)
    else:
        return z.add(2)
</pre> <p>The final IR we will compile with TorchInductor will either be <code>torch.cat([x, y]).add(2)</code> or <code>torch.cat([x, y]).mul(2)</code> (with the condition flattened away), but to determine which branch we are in, we would need to know the size of <code>z</code>, an intermediate. Because TorchDynamo must know upfront if a compiled trace is valid (we do not support bailouts, like some JIT compilers), we must be able to reduce <code>z.size(0)</code> as an expression in terms of the inputs, <code>x.size(0) + y.size(0)</code>. This is done by writing meta functions for all operators in PyTorch which can propagate size information to the output of a tensor without actually performing computation on the node.</p>   <h2 id="overall-architecture">Overall architecture</h2> <p>Symbolic shapes workflow:</p> <ol class="arabic simple"> <li>When we start compiling a frame in Dynamo, we allocate a ShapeEnv (attached to FakeTensorMode) which keeps track of symbolic shapes state.</li> <li>We allocate symbolic sizes for tensors on entry (what is static or dynamic is a policy decision, with some knobs).</li> <li>We propagate the symbolic sizes through operators, maintaining both (1) FX IR so that we can faithfully export symbolic compute, and (2) Sympy expressions representing the size vars, so we can reason about them.</li> <li>When we condition on symbolic sizes, either in Dynamo tracing or in Inductor optimization, we add guards based on the conditional. These can be induced from both Python and C++.</li> <li>These guards can induce further simplifications on symbolic variables. For example, if you assert <code>s0 == 4</code>, we can now replace all occurrences of <code>s0</code> with <code>4</code>.</li> <li>When we’re done tracing and optimizing, we install all of these guards with the compiled code; the compiled code is only reusable if all the guards evaluate true.</li> </ol> <p>Important files:</p> <ul class="simple"> <li>C++ SymInt API: <code>c10/core/SymInt.h</code>, <code>SymFloat.h</code>, <code>SymBool.h</code>
</li> <li>Python SymInt API: <code>torch/__init__.py</code> (look for <code>SymInt/SymFloat/SymBool</code>)</li> <li>C++ plumbing: <code>c10/core/SymNodeImpl.h</code>, <code>torch/csrc/utils/python_symnode.h</code>, <code>torch/csrc/jit/python/init.cpp</code>
</li> <li>Python infrastructure: <code>torch/fx/experimental/symbolic_shapes.py</code>
</li> <li>Other important files: <code>torch/_subclasses/fake_tensor.py</code>, <code>torch/_meta_registrations.py</code>, decomps, PrimTorch refs</li> </ul>   <h2 id="abridged-internal-api">Abridged internal API</h2> <p>Understanding the Python class hierarchy:</p> <ul class="simple"> <li>SymInt/SymFloat/SymBool: these are user-visible classes that simulate their int/float/bool counterparts. If you add two SymInts, we give you a new SymInt that symbolically tracks that the integer addition had occurred.</li> <li>SymNode: this is the internal structure (accessible via e.g., <code>symint.node</code>) which holds the actual symbolic tracking info. SymNode is type erased; this makes it more convenient to represent mixed-type operations. Note that technically you don’t have to call into Python SymNode from SymInt; for example, XLA’s C++ <code>SymNodeImpl</code> would take the place of SymNode.</li> <li>ShapeEnv: per-compile context state which keeps track of all the free symbols and guards we have accumulated so far. Every SymNode records its ShapeEnv (but not vice versa; SymNodes only get used if they participate in a guard).</li> </ul> <p>C++ is fairly similar:</p> <ul class="simple"> <li>c10::SymInt/SymFloat/SymBool: user-visible classes that simulate int/float/bool.</li> <li>c10::SymNode/SymNodeImpl: analogous to SymNode</li> <li>There is no ShapeEnv in C++; for ease of debugging, the entire symbolic reasoning apparatus is in Python.</li> </ul> <p>When you write code that is traceable with <code>make_fx</code>, it must be able to deal with SymInt/SymFloat/SymBool flowing through it. <a class="reference external" href="https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit#heading=h.fh8zzonyw8ng">The dynamic shapes manual</a> gives some guidance for how to do this.</p>   <h2 id="dimdynamic-policy">DimDynamic policy</h2> <p>Symbolic reasoning:</p> <ul class="simple"> <li>Value ranges</li> <li>Sympy usage notes</li> <li>Constraints</li> <li>DimDynamic/Constraint</li> </ul>   <h2 id="unbacked-symints">Unbacked SymInts</h2> <p>To resolve control flow, we check the hint, aka actual value, of a symbolic integer to determine which branch to go. However, in some cases, we may not have a hint: so-called unbacked symbolic integers arise when a size variable emerges from a data-dependent operation like <code>.nonzero()</code> or <code>.item()</code>. It is illegal to perform control flow on these symbolic integers, so we must graph break on these operations.</p> <p>Naively implemented, this is too restrictive: most PyTorch programs will immediately fail if you try to do anything with unbacked symbolic integers. Here are the most important enhancements to make this actually work:</p> <ul class="simple"> <li>On tensor creation, PyTorch precomputes a lot of data about a tensor; for example, if you use <code>empty_strided</code> to create a tensor, we will eagerly sort the strides and determine if the tensor is non-overlapping and dense. Sorts produce a lot of guards. However, it is more common to produce a tensor directly with a higher-level API like <code>empty</code>, which is guaranteed to produce a non-overlapping and dense tensor. We modified PyTorch to avoid needlessly recomputing these properties.</li> <li>Even if nontrivial compute is needed, sometimes a property is never actually queried at all. Making these precomputed properties lazy allows us to avoid guarding on an unbacked symbolic integer unless it is actually needed.</li> <li>The data in an integer tensor is generally not known to be non-negative. However, we provide an API <code>constrain_range</code> whereby a user can specify that a size is bounded above and below by known limits.</li> </ul> <p>In future versions of PT2 (beyond PT2.1), we will extend our reasoning system to infer that an unbacked symbolic integer is size-like based on usage. For example, if you pass the result of an <code>.item()</code> call to a factory function like <code>torch.empty</code>, we will automatically infer that the result is a size (because if it was not, it would fail.) This assumption would get validated at runtime, raising an error if it was not fulfilled.</p><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/torch.compiler_dynamic_shapes.html" class="_attribution-link">https://pytorch.org/docs/2.1/torch.compiler_dynamic_shapes.html</a>
  </p>
</div>
