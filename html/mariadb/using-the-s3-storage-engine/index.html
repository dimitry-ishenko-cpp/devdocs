<h1>Using the S3 Storage Engine</h1> <div> <div class="node creole"> <div class="answer formatted"> <div class="mariadb_from_10_5 mariadb from_10_5 product">
<strong class="product_title">MariaDB starting with <a href="../what-is-mariadb-105/index.html">10.5</a></strong><p>The <a href="../s3-storage-engine/index.html">S3 storage engine</a> has been available since <a href="https://mariadb.com/kb/en/mariadb-1054-release-notes/">MariaDB 10.5.4</a>.</p> </div>  <p>The <a href="../s3-storage-engine/index.html">S3 storage engine</a> is read only and allows one to archive MariaDB tables in Amazon S3, or any third-party public or private cloud that implements S3 API (of which there are many), but still have them accessible for reading in MariaDB.</p> <h2 class="anchored_heading" id="installing-the-plugin">Installing the Plugin</h2> <p>As of <a href="https://mariadb.com/kb/en/mariadb-1057-release-notes/">MariaDB 10.5.7</a>, the S3 storage engine is currently <a href="../release-criteria/index.html">gamma maturity</a>, so the following step can be omitted. </p> <p>On earlier releases, when it was <a href="../release-criteria/index.html">alpha maturity</a>, it will not load by default on a stable release of the server due to the default value of the <a href="../server-system-variables/index.html#plugin_maturity">plugin_maturity</a> variable. Set to <code>alpha</code> (or below) in your config file to permit installation of the plugin:</p> <pre class="fixed" data-language="sql">[mysqld]
plugin-maturity = alpha
</pre>
<p>and restart the server.</p> <p>Now <a href="../plugin-overview/index.html#installing-a-plugin">install the plugin library</a>, for example:</p> <pre class="fixed" data-language="sql">INSTALL SONAME 'ha_s3';
</pre>
<p>If the library is not available, for example: </p> <pre class="fixed" data-language="sql">INSTALL SONAME 'ha_s3';
ERROR 1126 (HY000): Can't open shared library '/var/lib/mysql/lib64/mysql/plugin/ha_s3.so' 
  (errno: 13, cannot open shared object file: No such file or directory)
</pre>
<p>you may need to install a separate package for the S3 storage engine, for example: </p> <pre class="fixed" data-language="sql">shell&gt; yum install MariaDB-s3-engine
</pre>
<h2 class="anchored_heading" id="moving-data-to-s3">Moving Data to S3</h2> <p>To move data from an existing table to S3, one can run:</p> <pre class="fixed" data-language="sql">ALTER TABLE old_table ENGINE=S3 COMPRESSION_ALGORITHM=zlib
</pre>
<p>To get data back to a 'normal' table one can do:</p> <pre class="fixed" data-language="sql">ALTER TABLE s3_table ENGINE=INNODB
</pre>
<h2 class="anchored_heading" id="new-options-for-alter-tablealter-table">New Options for <a href="../alter-table/index.html">ALTER TABLE</a>
</h2> <ul start="1">
<li>
<strong><code>S3_BLOCK_SIZE</code> :</strong> Set to 4M as default. This is the block size for all index and data pages stored in S3. </li>
<li>
<strong><code>COMPRESSION_ALGORITHM</code> :</strong> Set to 'none' as default. Which compression algorithm to use for block stored in S3. Options are: <code>none</code> or <code>zlib</code>. </li>
</ul> <p><a href="../alter-table/index.html">ALTER TABLE</a> can be used on S3 tables as normal to add columns or change column definitions.</p> <h2 class="anchored_heading" id="mysqld-startup-options-for-s3">mysqld Startup Options for S3</h2> <p>To be able to use S3 for storage one <strong>*must</strong>* define how to access S3 and where data are stored in S3:</p> <ul start="1">
<li>
<strong><a href="../s3-storage-engine-system-variables/index.html#s3_access_key">s3_access_key</a>:</strong> The AWS access key to access your data </li>
<li>
<strong><a href="../s3-storage-engine-system-variables/index.html#s3_secret_key">s3_secret_key</a>:</strong> The AWS secret key to access your data </li>
<li>
<strong><a href="../s3-storage-engine-system-variables/index.html#s3_bucket">s3_bucket</a>: </strong> The AWS bucket where your data should be stored. All MariaDB table data is stored in this bucket. </li>
<li>
<strong><a href="../s3-storage-engine-system-variables/index.html#s3_region">s3_region</a>: </strong> The AWS region where your data should be stored. </li>
</ul> <p>If you are using an S3 service that is using HTTP to connect (like <a href="https://min.io/">https://min.io/</a>) you also need the set the following variables:</p> <ul start="1">
<li>
<strong><a href="../s3-storage-engine-system-variables/index.html#s3_port">s3_port</a>:</strong> Port number to connect to (0 means use default) </li>
<li>
<strong><a href="../s3-storage-engine-system-variables/index.html#s3_use_http">s3_use_http</a>:</strong> If true, force use of HTTP protocol </li>
</ul> <p>If you are going to use a primary-replica setup, you should look at the following variables:</p> <ul start="1">
<li>
<strong><a href="../s3-storage-engine-system-variables/index.html#s3-replicate-alter-as-create-select">s3_replicate_alter_as_create_select</a>: </strong> When converting an S3 table to local table, log all rows in binary log. Defaults to <code class="fixed" style="white-space:pre-wrap">TRUE</code>. This allows the replica to replicate <code class="fixed" style="white-space:pre-wrap">CREATE TABLE .. SELECT FROM s3_table</code> even it the replica doesn't have access to the original <code class="fixed" style="white-space:pre-wrap">s3_table</code>. </li>
<li>
<strong><a href="../s3-storage-engine-system-variables/index.html#s3-slave-ignore-updates">s3_slave_ignore_updates</a>: </strong> Should be set if primary and replica share the same S3 instance. This tells the replica that it can ignore any updates to the S3 tables as they are already applied on the primary. Defaults to <code class="fixed" style="white-space:pre-wrap">FALSE</code>. </li>
</ul> <p>The above defaults assume that the primary and replica don't share the same S3 instance.</p> <p>Other, less critical options, are:</p> <ul start="1">
<li>
<strong><a href="../s3-storage-engine-system-variables/index.html#s3_host_name">s3_host_name</a>:</strong> Hostname for the S3 service. "s3.amazonaws.com", Amazon S3 service, by default. </li>
<li>
<strong><a href="../s3-storage-engine-system-variables/index.html#s3_protocol_version">s3_protocol_version</a>:</strong> Protocol used to communication with S3. One of "Auto", "Amazon" or "Original" where "Auto" is the default. If you get errors like "8 Access Denied" when you are connecting to another service provider, then try to change this option. The reason for this variable is that Amazon has changed some parts of the S3 protocol since they originally introduced it but other service providers are still using the original protocol. </li>
<li>
<strong><a href="../s3-storage-engine-system-variables/index.html#s3_block_size">s3_block_size</a>:</strong> Set to 4M as default. This is the default block size for a table, if not specified in <a href="../create-table/index.html">CREATE TABLE</a>. </li>
<li>
<strong><a href="../s3-storage-engine-system-variables/index.html#s3_pagecache_buffer_size">s3_pagecache_buffer_size</a>:</strong> Default 128M. The size of the buffer used for data and index blocks for S3 tables. Increase this to get better index handling (for all reads and multiple writes) to as much as you can afford. </li>
</ul> <p>Last some options you probably don't have to ever touch:</p> <ul start="1">
<li>
<strong><a href="../s3-storage-engine-system-variables/index.html#s3_pagecache_age_threshold">s3_pagecache_age_threshold</a> : </strong> Default 300: This characterizes the number of hits a hot block has to be untouched until it is considered aged enough to be downgraded to a warm block. This specifies the percentage ratio of that number of hits to the total number of blocks in the page cache. </li>
<li>
<strong><a href="../s3-storage-engine-system-variables/index.html#s3_pagecache_division_limit">s3_pagecache_division_limit</a>: </strong> Default 100. The minimum percentage of warm blocks in key cache. </li>
<li>
<strong><a href="../s3-storage-engine-system-variables/index.html#s3_pagecache_file_hash_size">s3_pagecache_file_hash_size</a>: </strong> Default 512. Number of hash buckets for open files. If you have a lot of S3 files open you should increase this for faster flush of changes. A good value is probably 1/10 of number of possible open S3 files. </li>
<li>
<strong><a href="../s3-storage-engine-system-variables/index.html#s3_debug">s3_debug</a>: </strong> Default 0. Generates a trace file from libmarias3 on stderr (mysqld.err) for debugging the S3 protocol. </li>
</ul> <h2 class="anchored_heading" id="typical-mycnf-entry-for-connecting-to-amazon-s3-service">Typical my.cnf Entry for connecting to Amazon S3 service</h2> <pre class="fixed" data-language="sql">[mariadb]
s3=ON
s3-bucket=mariadb
s3-access-key=xxxx
s3-secret-key=xxx
s3-region=eu-north-1
s3-host-name=s3.amazonaws.com
# The following is useful if you want to use minio as a S3 server. (https://min.io/)
#s3-port=9000
#s3-use-http=ON

# Primary and replica share same S3 tables.
s3-slave-ignore-updates=1

[aria_s3_copy]
s3-bucket=mariadb
s3-access-key=xxxx
s3-secret-key=xxx
s3-region=eu-north-1
s3-host-name=s3.amazonaws.com
# The following is useful if you want to use minio as a S3 server. (https://min.io/)
#s3-port=9000
#s3-use-http=ON
</pre>
<h2 class="anchored_heading" id="typical-mycnf-entry-for-connecting-to-a-httpsminiominio-s3-server">Typical my.cnf entry for connecting to a <a href="https://min.io">minio</a> S3 server</h2> <pre class="fixed" data-language="sql">[mariadb]
s3=ON
s3-host-name="127.0.0.1"
s3-bucket=storage-engine
s3-access-key=minio
s3-secret-key=minioadmin
s3-port=9000
s3-use-http=ON

[aria_s3_copy]
s3=ON
s3-host-name="127.0.0.1"
s3-bucket=storage-engine
s3-access-key=minio
s3-secret-key=minioadmin
s3-port=9000
s3-use-http=ON
</pre>
<h2 class="anchored_heading" id="typical-usage-case-for-s3-tables">Typical Usage Case for S3 Tables</h2> <p>The typical use case would be that there exists tables that after some time would become fairly inactive, but are still important so that they can not be removed. In that case, an option is to move such a table to an archiving service, which is accessible through an S3 API.</p> <p>Notice that S3 means the Cloud Object Storage API defined by Amazon AWS. Often the whole of Amazon’s Cloud Object Storage is referred to as S3. In the context of the S3 archive storage engine, it refers to the API itself that defines how to store objects in a cloud service, being it Amazon’s or someone else’s. OpenStack for example provides an S3 API for storing objects.</p> <p>The main benefit of storing things in an S3 compatible storage is that the cost of storage is much cheaper than many other alternatives. Many S3 implementations also provide reliable long-term storage.</p> <h2 class="anchored_heading" id="operations-allowed-on-s3-tables">Operations Allowed on S3 Tables</h2> <ul start="1">
<li>
<a href="../alter-table/index.html">ALTER TABLE</a> S3 supports all types, keys and other options that are supported by the <a href="../aria-storage-engine/index.html">Aria</a> engine. One can also perform <a href="../alter-table/index.html">ALTER TABLE</a> on an S3 table to add or modify columns etc. </li>
<li>
<a href="../drop-table/index.html">DROP TABLE</a> </li>
<li>
<a href="../select/index.html">SELECT</a> Any SELECT operations you can perform on a normal table should work with an S3 table. </li>
<li>
<a href="../show-tables/index.html">SHOW TABLES</a> will show all tables that exist in the current defined S3 location. </li>
<li>S3 tables can be part of <a href="../partitions-files/index.html">partitions</a>. See Discovery below. </li>
</ul> <h2 class="anchored_heading" id="discovery">Discovery</h2> <p>The S3 storage engine supports full <a href="../table-discovery/index.html">MariaDB discovery</a>. This means that if you have the S3 storage engine enabled and properly configured, the table stored in S3 will automatically be discovered when it's accessed with <a href="../show-tables/index.html">SHOW TABLES</a>, <a href="../select/index.html">SELECT</a> or any other operation that tries to access it. In the case of SELECT, the .frm file from S3 will be copied to the local storage to speed up future accesses.</p> <p>When an S3 table is opened for the first time (it's not in the table cache) and there is a local .frm file, the S3 engine will check if it's still relevant, and if not, update or delete the .frm file.</p> <p>This means that if the table definition changes on S3 and it's in the local cache, one has to execute <code><a href="../flush/index.html">FLUSH TABLES</a></code> to get MariaDB to notice the change and update the .frm file.</p> <p>If partitioning S3 tables are used, the partition definitions will also be stored on S3 storage and will be discovered by other servers.</p> <p>Discovery of S3 tables is not done for tables in the <a href="../the-mysql-database-tables/index.html">mysql databases</a> to make mysqld boot faster and more securely.</p> <h2 class="anchored_heading" id="replication">Replication</h2> <p>S3 works with <a href="../replication-overview/index.html">replication</a>. One can use replication in two different scenarios:</p> <ul start="1">
<li>The primary and replica share the same S3 storage. In this case the primary will make all changes to the S3 data and the replica will ignore any changes in the replication stream to S3 data . This scenario is achieved by setting <a href="../s3-storage-engine-system-variables/index.html#s3-slave-ignore-updates">s3_slave_ignore_updates</a> to 1. </li>
<li>The primary and replica don't share the same S3 storage or the replica uses another storage engine for the S3 tables. This scenario is achieved by setting <a href="../s3-storage-engine-system-variables/index.html#s3-slave-ignore-updates">s3_slave_ignore_updates</a> to 0. </li>
</ul> <h2 class="anchored_heading" id="aria_s3_copy">aria_s3_copy</h2> <p><a href="../aria_s3_copy/index.html">aria_s3_copy</a> is an external tool that one can use to copy <a href="../aria-storage-engine/index.html">Aria</a> tables to and from S3. Use <code>aria_s3_copy --help</code> to get the options of how to use it.</p> <h2 class="anchored_heading" id="mariadb-dump">mariadb-dump</h2> <ul start="1"><li>
<a href="../mariadb-dump/index.html">mariadb-dump</a> will by default ignore S3 tables. If <code>mariadb-dump</code> is run with the <code>--copy-s3-tables</code> option, the resulting file will contain a CREATE statement for a similar <a href="../aria-storage-engine/index.html">Aria</a> table, followed by the table data and ending with an <code>ALTER TABLE xxx ENGINE=S3</code>. </li></ul> <h2 class="anchored_heading" id="analyze-table">ANALYZE TABLE</h2> <p>As of <a href="https://mariadb.com/kb/en/mariadb-10514-release-notes/">MariaDB 10.5.14</a>, <a href="../analyze-table/index.html">ANALYZE TABLE</a> is supported for S3 tables. As the S3 tables are read-only, a normal <code>ANALYZE TABLE</code> will not do anything. However using <code>ANALYZE TABLE table_name PERSISTENT FOR...</code> will now work.</p> <h2 class="anchored_heading" id="check-table">CHECK TABLE</h2> <p>As of <a href="https://mariadb.com/kb/en/mariadb-10514-release-notes/">MariaDB 10.5.14</a>, <a href="../check-table/index.html">CHECK TABLE</a> will work. As S3 tables are read only it is very unlikely that they can become corrupted. The only known way an S3 table could be corrupted if either the original table copied to S3 was corrupted or the process of copying the original table to S3 was somehow interrupted.</p> <h2 class="anchored_heading" id="current-limitations">Current Limitations</h2> <ul start="1">
<li>
<a href="../mysql-test-run/index.html">mysql-test-run</a> doesn't by default test the S3 engine as we can't embed AWS keys into mysql-test-run. </li>
<li>Replicas should not access S3 tables while they are ALTERed! This is because there is no locking implemented to S3 between servers. However, after a table (either the original S3 table or the partitioned S3 table) is changed on the primary, the replica will notice this on the next access and update its local definition. </li>
</ul> <h3 class="anchored_heading" id="limitations-in-alter-tablealter-partition">Limitations in <a href="../alter-table/index.html">ALTER .. PARTITION</a>
</h3> <p>All <a href="../alter-table/index.html">ALTER PARTITION</a> operations are supported on S3 partitioning tables except:</p> <ul start="1">
<li>REBUILD PARTITION </li>
<li>TRUNCATE PARTITION </li>
<li>REORGANIZE PARTITION </li>
</ul> <h2 class="anchored_heading" id="performance-considerations">Performance Considerations</h2> <p>Depending on your connection speed to your S3 provider, there can be some notable slowdowns in some operations.</p> <h3 class="anchored_heading" id="discovery">Discovery</h3> <p>As S3 is supporting discovery (automatically making tables available that are in S3) this can cause some small performance problems if the S3 engine is enabled. Partitioning S3 tables also support discovery.</p> <ul start="1">
<li>CREATE TABLE is a bit slower as the S3 engine has to check if the to-be-created table is already S3. </li>
<li>Queries on information_schema tables are slower as S3 has to check if there is new tables in S3. </li>
<li>DROP of non existing tables are slower as S3 has to check if the table is in S3. </li>
</ul> <p>There are no performance degradation's when accessing existing tables on the server. Accessing the S3 table the first time will copy the .frm file from S3 to the local disk, speeding up future accesses to the table.</p> <h3 class="anchored_heading" id="caching">Caching</h3> <ul start="1"><li>Accessing a table on S3 can take some time , especially if you are using big packets (<a href="../s3-storage-engine-system-variables/index.html#s3_block_size">s3_block_size</a>). However the second access to the same data should be fast as it's then cached in the S3 page cache. </li></ul> <h3 class="anchored_heading" id="things-to-try-to-increase-performance">Things to Try to Increase Performance</h3> <p>If you have performance problems with the S3 engine, here are some things you can try:</p> <ul start="1">
<li>Decreasing <a href="../s3-storage-engine-system-variables/index.html#s3_block_size">s3_block_size</a>. This can be done both globally and per table. </li>
<li>Use COMPRESSION_ALGORITHM=zlib when creating the table. This will decrease the amount of data transferred from S3 to the local cache. </li>
<li>Increasing the size of the s3 page cache: <a href="../s3-storage-engine-system-variables/index.html#s3_pagecache_buffer_size">s3_pagecache_buffer_size</a> </li>
</ul> <p>Try also to execute the query twice to check if the problem is that the data was not properly cached. When data is cached locally the performance should be excellent.</p> <h2 class="anchored_heading" id="future-development-ideas">Future Development Ideas</h2> <ul start="1">
<li>Store aws keys and region in the mysql.servers table (as <a href="../spider/index.html">Spider</a> and <a href="../federatedx/index.html">FederatedX</a>). This will allow one to have different tables on different S3 servers. </li>
<li>Store s3 bucket, access_key and secret key in a cache to better be able to better to reuse connections. This would save some memory and make some S3 accesses a bit faster as we could reuse old connections. </li>
</ul> <h2 class="anchored_heading" id="troubleshooting-s3-on-selinux">Troubleshooting S3 on SELinux</h2> <p>If you get errors such as:</p> <pre class="fixed" data-language="sql">ERROR 3 (HY000): Got error from put_object(bubu/produkt/frm): 5 Couldn't connect to server
</pre>
<p>one reason could be that your system doesn't allow MariaDB to connect to ports other than 3306. To procedure to enable other ports is the following:</p> <p>Search for the ports allowed for MariaDB:</p> <pre class="fixed" data-language="sql">$ sudo semanage port -l | grep mysqd_port_t
mysqld_port_t                tcp   1186, 3306, 63132-63164
</pre>
<p>Say you want to allow MariaDB to connect to port 32768:</p> <pre class="fixed" data-language="sql">$ sudo semanage port -a -t mysqld_port_t -p tcp 32768
</pre>
<p>You can verify that the new port, 32768, is now allowed for MariaDB:</p> <pre class="fixed" data-language="sql">$ sudo semanage port -l | grep mysqd_port_t
mysqld_port_t                tcp   32768,1186, 3306, 63132-63164
</pre>
<h2 class="anchored_heading" id="see-also">See Also</h2> <ul start="1"><li><a href="../s3-storage-engine-internals/index.html">S3 storage engine internals</a></li></ul> </div>     </div> <div id="content_disclaimer" class="graybox"> Content reproduced on this site is the property of its respective owners, and this content is not reviewed in advance by MariaDB. The views, information and opinions expressed by this content do not necessarily represent those of MariaDB or any other party. </div> </div><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2023 MariaDB<br>Licensed under the Creative Commons Attribution 3.0 Unported License and the GNU Free Documentation License.<br>
    <a href="https://mariadb.com/kb/en/using-the-s3-storage-engine/" class="_attribution-link">https://mariadb.com/kb/en/using-the-s3-storage-engine/</a>
  </p>
</div>
