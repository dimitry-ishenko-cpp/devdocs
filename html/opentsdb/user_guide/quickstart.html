<h1>Quick Start</h1> <p>Once you have a TSD up and running (after following the <a class="reference internal" href="../installation.html"><em>Installation</em></a> guide) you can follow the steps below to get some data into OpenTSDB. After you have some data stored, pull up the GUI and try generating some graphs.</p>  <h2>Create Your First Metrics</h2> <p>Metrics need to be registered before you can start storing data points for them. This helps to avoid ingesting unwanted data and catch typos. You can enable auto-metric creation via configuration. To register one or more metrics, call the <code class="docutils literal"><span class="pre">mkmetric</span></code> CLI:</p> <pre data-language="python">./tsdb mkmetric mysql.bytes_received mysql.bytes_sent
</pre>
 <p>This will create 2 metrics: <code class="docutils literal"><span class="pre">mysql.bytes_received</span></code> and <code class="docutils literal"><span class="pre">mysql.bytes_sent</span></code></p> <p>New tags, on the other hand, are automatically registered whenever they're used for the first time. Right now OpenTSDB only allows you to have up to 2^24 = 16,777,216 different metrics, 16,777,216 different tag names and 16,777,216 different tag values. This is because each one of those is assigned a UID on 3 bytes. Metric names, tag names and tag values have their own UID spaces, which is why you can have 16,777,216 of each kind. The size of each space is configurable but there is no knob that exposes this configuration parameter right now. So bear in mind that using user ID or event ID as a tag value will not work right now if you have a large site.</p>   <h2>Start Collecting Data</h2> <p>So now that we have our 2 metrics, we can start sending data to the TSD. Let's write a little shell script to collect some data off of MySQL and send it to the TSD:</p> <pre data-language="python">cat &gt;mysql-collector.sh &lt;&lt;\EOF
#!/bin/bash
set -e
while true; do
  mysql -u USER -pPASS --batch -N --execute "SHOW STATUS LIKE 'bytes%'" \
  | awk -F"\t" -v now=`date +%s` -v host=`hostname` \
  '{ print "put mysql." tolower($1) " " now " " $2 " host=" host }'
  sleep 15
done | nc -w 30 host.name.of.tsd PORT
EOF
chmod +x mysql-collector.sh
nohup ./mysql-collector.sh &amp;
</pre>
 <p>Every 15 seconds, the script will collect 2 data points from MySQL and send them to the TSD. You can use a smaller sleep interval for greater granularity.</p> <p>What does the script do? If you're not a big fan of shell and awk scripting, it may not be obvious how this works. But it's simple. The <code class="docutils literal"><span class="pre">set</span> <span class="pre">-e</span></code> command simply instructs bash to exit with an error if any of the commands fail. This simplifies error handling. The script then enters an infinite loop. In this loop, we query MySQL to retrieve 2 of its status variables:</p> <pre data-language="python">$ mysql -u USER -pPASS --execute "SHOW STATUS LIKE 'bytes%'"
+----------------+-------+
| Variable_name  | Value |
+----------------+-------+
| Bytes_received | 133   |
| Bytes_sent   | 190   |
+----------------+-------+
</pre>
 <p>The <code class="docutils literal"><span class="pre">--batch</span> <span class="pre">-N</span></code> flags ask the mysql command to remove the human friendly fluff so we don't have to filter it out ourselves. Then the output is piped to awk, which is told to split fields on tabs <code class="docutils literal"><span class="pre">-F"\t"</span></code> because with the <code class="docutils literal"><span class="pre">--batch</span></code> flag that's what mysql will use. We also create a couple of variables, one named <code class="docutils literal"><span class="pre">now`</span> <span class="pre">and</span> <span class="pre">initialize</span> <span class="pre">it</span> <span class="pre">to</span> <span class="pre">the</span> <span class="pre">current</span> <span class="pre">timestamp,</span> <span class="pre">the</span> <span class="pre">other</span> <span class="pre">named</span> <span class="pre">``host`</span> <span class="pre">and</span> <span class="pre">set</span> <span class="pre">to</span> <span class="pre">the</span> <span class="pre">hostname</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">local</span> <span class="pre">machine.</span> <span class="pre">Then,</span> <span class="pre">for</span> <span class="pre">every</span> <span class="pre">line,</span> <span class="pre">we</span> <span class="pre">print</span> <span class="pre">put</span> <span class="pre">``mysql.</span></code>, followed by the lower-case form of the first word, then by a space, then by the current timestamp, then by the second word (the value), another space, and finally <code class="docutils literal"><span class="pre">host=</span></code> and the current hostname. Rinse and repeat every 15 seconds. The <code class="docutils literal"><span class="pre">-w</span> <span class="pre">30</span></code> parameter given to <code class="docutils literal"><span class="pre">nc</span></code> simply sets a timeout on the connection to the TSD. Bear in mind, this is just an example, in practice you can use tcollector's MySQL collector.</p> <p>If you don't have a MySQL server to monitor, you can try this instead to collect basic load metrics from your Linux servers:</p> <pre data-language="python">cat &gt;loadavg-collector.sh &lt;&lt;\EOF
#!/bin/bash
set -e
while true; do
  awk -v now=`date +%s` -v host=`hostname` \
  '{ print "put proc.loadavg.1m " now " " $1 " host=" host;
  print "put proc.loadavg.5m " now " " $2 " host=" host }' /proc/loadavg
  sleep 15
done | nc -w 30 host.name.of.tsd PORT
EOF
chmod +x loadavg-collector.sh
nohup ./loadavg-collector.sh &amp;
</pre>
 <p>This will store a reading of the 1-minute and 5-minute load average of your server in OpenTSDB by sending simple "telnet-style commands" to the TSD:</p> <pre data-language="python">put proc.loadavg.1m 1288946927 0.36 host=foo
put proc.loadavg.5m 1288946927 0.62 host=foo
put proc.loadavg.1m 1288946942 0.43 host=foo
put proc.loadavg.5m 1288946942 0.62 host=foo
</pre>
   <h2>Batch Imports</h2> <p>Let's imagine that you have a cron job that crunches gigabytes of application logs every day or every hour to extract profiling data. For instance, you could be logging the time taken to process a request and your cron job would compute an average for every 30 second window. Maybe you're particularly interested in 2 types of requests handled by your application, so you'll compute separate averages for those requests, and an another average for every other request type. So your cron job may produce an output file that looks like this:</p> <pre data-language="python">1288900000 42 foo
1288900000 51 bar
1288900000 69 other
1288900030 40 foo
1288900030 59 bar
1288900030 80 other
</pre>
 <p>The first column is a timestamp, the second is the average latency for that 30 second window, and the third is the type of request we're talking about. If you run your cron job on a day worth of logs, you'll end up with 8640 such lines. In order to import those into OpenTSDB, you need to adjust your cron job slightly to produce its output in the following format:</p> <pre data-language="python">myservice.latency.avg 1288900000 42 reqtype=foo
myservice.latency.avg 1288900000 51 reqtype=bar
myservice.latency.avg 1288900000 69 reqtype=other
myservice.latency.avg 1288900030 40 reqtype=foo
myservice.latency.avg 1288900030 59 reqtype=bar
myservice.latency.avg 1288900030 80 reqtype=other
</pre>
 <p>Notice we're simply associating each data point with the name of a metric (myservice.latency.avg) and naming the tag that represents the request type. If each server has its own logs and you process them separately, you may want to add another tag to each line like the <code class="docutils literal"><span class="pre">host=foo</span></code> tag we saw in the previous section. This way you'll be able to plot the latency of each server individually, in addition to your average latency across the board and/or per request type. In order to import a data file in the format above (metric timestamp value tags) simply run the following command:</p> <pre data-language="python">./tsdb import your-file
</pre>
 <p>If your data file is large, consider gzip'ing it first. This can be as simple as piping the output of your cron job to <code class="docutils literal"><span class="pre">gzip</span> <span class="pre">-9</span> <span class="pre">&gt;output.gz</span></code> instead of writing directly to a file. The import command is able to read gzip'ed files and it greatly helps performance for large batch imports.</p>   <h2>Self Monitoring</h2> <p>Each TSD exports some stats about itself through the simple stats command. You can collect those stats and feed them back to the TSD every few seconds. First, create the necessary metrics:</p> <pre data-language="python">echo stats | nc -w 1 localhost 4242 \
| awk '{ print $1 }' | sort -u \
| xargs ./tsdb mkmetric
</pre>
 <p>This requests the stats from the TSD (assuming it's running on the local host and listening to port 4242), extract the names of the metrics from the stats and assigns them UIDs. Then you can use this simple script to collect stats and store them in OpenTSDB:</p> <pre data-language="python">#!/bin/bash
INTERVAL=15
while :; do
  echo stats || exit
  sleep $INTERVAL
done | nc -w 30 localhost $1 \
  | sed 's/^/put /' \
  | nc -w 30 localhost $1
</pre>
 <p>This way you will collect and store stats from the TSD every 15 seconds.</p>   <h2>Create a Graph</h2> <p>Once you've written some data using any of the methods above, you can now try to create a graph using that data. Pull up the GUI in your favorite browser. If you're running your TSD on the localhost, simply visit <a class="reference external" href="http://127.0.0.1:4242">http://127.0.0.1:4242</a>.</p> <p>First, pick one of the metrics and put that in the <code class="docutils literal"><span class="pre">Metric</span></code> box. For example, <code class="docutils literal"><span class="pre">proc.loadavg.1m</span></code>. As you type, you should see auto-complete lines pop up and you can click on any of them.</p> <p>Then click the <code class="docutils literal"><span class="pre">From</span></code> box at the top and a date-picker pop-up should appear. Select any time from yesterday and click on another box. At this point you should see "Loading graph.." very briefly followed by the actual graph. If the graph is empty, it may not have found the most recent data points so click the <code class="docutils literal"><span class="pre">(now)</span></code> link and the page should refresh.</p> <p>This initial graph will aggregate all of the time series for the metric you selected. Try limiting your query to a specific host by adding <code class="docutils literal"><span class="pre">host</span></code> as a value in the left-hand box next to the <code class="docutils literal"><span class="pre">Tags</span></code> label (if it isn't already there) and add the specific host name (e.g. <code class="docutils literal"><span class="pre">foo</span></code>) in the right-hand box. After clicking in another box you should see the graph re-draw with different information.</p><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2010&ndash;2016 The OpenTSDB Authors<br>Licensed under the GNU LGPLv2.1+ and GPLv3+ licenses.<br>
    <a href="http://opentsdb.net/docs/build/html/user_guide/quickstart.html" class="_attribution-link">http://opentsdb.net/docs/build/html/user_guide/quickstart.html</a>
  </p>
</div>
