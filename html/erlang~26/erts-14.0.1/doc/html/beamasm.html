<div class="innertube"> <h1 id="BeamAsm, the Erlang JIT">1 BeamAsm, the Erlang JIT</h1> <p> BeamAsm provides load-time conversion of Erlang BEAM instructions into native code on x86-64 and aarch64. This allows the loader to eliminate any instruction dispatching overhead and also specialize each instruction on their argument types. </p> <p> BeamAsm does hardly any cross instruction optimizations and the <code>x</code> and <code>y</code> register arrays work the same as when interpreting BEAM instructions. This allows the Erlang run-time system to be largely unchanged except for places that need to work with loaded BEAM instructions like code loading, tracing, and a few others. </p> <p> BeamAsm uses <code><a href="https://github.com/asmjit/asmjit">asmjit</a></code> to generate native code in run-time. Only small parts of the <code><a href="https://asmjit.com/doc/group__asmjit__assembler.html">Assembler API</a></code> of <code><a href="https://github.com/asmjit/asmjit">asmjit</a></code> is used. At the moment <code><a href="https://github.com/asmjit/asmjit">asmjit</a></code> only supports x86 32/64 bit and aarch64 assembler. </p> <h2 id="loading-code" class="title-link"> <div class="title-name">1.1 Loading Code</div>  </h2> <p> The code is loaded very similarly to how it is loaded for the interpreter. Each beam file is parsed and then optimized through the transformations described in <code><a href="beam_makeops.html#defining-transformation-rules">beam_makeops</a></code>. The transformations used in BeamAsm are much simpler than the interpreter's, as most of the transformations for the interpreter are done only to eliminate the instruction dispatch overhead. </p> <p> Then each instruction is encoded using the C++ functions in the <code>jit/$ARCH/instr_*.cpp</code> files. For example: </p> <pre data-language="erlang">void BeamModuleAssembler::emit_is_nonempty_list(const ArgVal &amp;Fail, const ArgVal &amp;Src) {
  a.test(getArgRef(Src), imm(_TAG_PRIMARY_MASK - TAG_PRIMARY_LIST));
  a.jne(labels[Fail.getLabel()]);
}</pre> <p> <code><a href="https://github.com/asmjit/asmjit">asmjit</a></code> provides a fairly straightforward mapping from a C++ function call to the x86 assembly instruction. The above instruction tests if the value in the <code>Src</code> register is a non-empty list and if it is not then it jumps to the fail label. </p> <p> For comparison, the interpreter has 8 combinations and specializations of this implementation to minimize the instruction dispatch overhead for common patterns. </p> <p> The original register allocation done by the Erlang compiler is used to manage the liveness of values and the physical registers are statically allocated to keep the necessary process state. At the moment this is the static register allocation on x86-64: </p> <pre data-language="erlang">rbx: ErtsSchedulerRegisters struct (contains x/float registers and some metadata)
rbp: Current frame pointer when `perf` support is enabled, otherwise this
     is an optional save slot for the Erlang stack pointer when executing C
     code.
r12: Active code index
r13: Current running process
r14: Remaining reductions
r15: Erlang heap pointer</pre> <p> Note that all of these are callee save registers under the System V and Windows ABIs which means that BeamAsm never has to spill any of these when making C function calls. </p> <p> The caller save registers are used as scratch registers within instructions but usually do not carry information between them. For some frequent instruction sequences such as tuple matching cross instruction optimization <strong>are</strong> done to avoid fetching the base address of the tuple in every <code>get_tuple_element</code> instruction. </p> <h3 id="reducing-code-size-and-load-time" class="title-link"> <div class="title-name">Reducing code size and load time</div>  </h3> <p> One of the strengths of the interpreter is that it uses relatively little memory for loaded code. This is because the implementation of each loaded instruction is shared and only the arguments to the instructions vary. Using as little memory as possible has many advantages; less memory is used, loading time decreases, higher cache hit-rate. </p> <p> In BeamAsm we need to achieve something similar since the load-time of a module scales almost linearly with the amount of memory it uses. Early BeamAsm prototypes used about double the amount of memory for code as the interpreter, while current versions use about 10% more. How was this achieved? </p> <p> In BeamAsm we heavily use shared code fragments to try to emit as much code as possible as global shared fragments instead of duplicating the code unnecessarily. For instance, the return instruction looks something like this: </p> <pre data-language="erlang">Label yield = a.newLabel();

/* Decrement reduction counter */
a.dec(FCALLS);
/* If FCALLS &lt; 0, jump to the yield-on-return fragment */
a.jl(resolve_fragment(ga-&gt;get_dispatch_return()));
a.ret();</pre> <p> The code above is not exactly what is emitted, but close enough. The thing to note is that the code for doing the context switch is never emitted. Instead, we jump to a global fragment that all return instructions share. This greatly reduces the amount of code that has to be emitted for each module. </p> <h2 id="running-erlang-code" class="title-link"> <div class="title-name">1.2 Running Erlang code</div>  </h2> <p> Running BeamAsm code is very similar to running the interpreter, except that native code is executed instead of interpreted code. </p> <p> We had to tweak the way the Erlang stack works in order to execute native instructions on it. While the interpreter uses a stack slot for the current frame's return address (setting it to <code>[]</code> when unused), the native code merely reserves enough space for it as the x86 <code>call</code> and <code>ret</code> instructions bump the stack pointer when executed. </p> <p> This only affects the <strong>current stack frame</strong>, and is functionally identical aside from two caveats: </p> <ol> <li> <p>Exceptions must not be thrown when the return address is reserved. </p> <p>It's hard to tell where the stack will end up after an exception; the return address won't be on the stack if we crash in the <strong>current stack frame</strong>, but will be if we crash in a function we call. Telling these apart turned out to rather complicated, so we decided to require the return address to be used when an exception is thrown. </p> <p><code>emit_handle_error</code> handles this for you, and shared fragments that have been called (rather than jumped to) satisfy this requirement by default. </p> </li> <li> <p>Garbage collection needs to take return addresses into account. </p> <p>If we're about to create a term we have to make sure that there's enough space for this term <strong>and</strong> a potential return address, or else the next <code>call</code> will clobber said term. This is taken care of in <code>emit_gc_test</code> and you generally don't need to think about it. </p> </li> </ol> <p> In addition to the above, we ensure that there's always at least <code>S_REDZONE</code> free words on the stack so we can make calls to shared fragments or trace handlers even when we lack a stack frame. This is merely a reservation and has no effect on how the stack works, and all values stored there must be valid Erlang terms in case of a garbage collection. </p> <h2 id="frame-pointers" class="title-link"> <div class="title-name">1.3 Frame pointers</div>  </h2> <p> To aid debuggers and sampling profilers, we support running Erlang code with native frame pointers. At the time of writing, this is only enabled together with <code>perf</code> support (<code>+JPperf true</code>) to save stack space, but we may add a flag to explicitly enable it in the future. </p> <p> When enabled, continuation pointers (CP) have both a return address <strong>and</strong> a frame pointer that points at the previous CP. CPs must form a valid chain at all times, and it's illegal to have "half" a CP when the stack is inspected. </p> <p> Frame pointers are pushed when entering an Erlang function and popped before leaving it, including on tail calls as the callee will immediately push the frame pointer on entry. This has a slight overhead but saves us the headache of having multiple entry points for each function depending on whether it's tail- or body-called, which would get very tricky once breakpoints enter the picture. </p> <h2 id="running-c-code" class="title-link"> <div class="title-name">1.4 Running C code</div>  </h2> <p> As Erlang stacks can be very small, we have to switch over to a different stack when we need to execute C code (which may expect a much larger stack). This is done through <code>emit_enter_runtime</code> and <code>emit_leave_runtime</code>, for example: </p> <pre data-language="erlang">mov_arg(ARG4, NumFree);

/* Move to the C stack and swap out our current reductions, stack-, and
 * heap pointer to the process structure. */
emit_enter_runtime&lt;Update::eReductions | Update::eStack | Update::eHeap&gt;();

a.mov(ARG1, c_p);
load_x_reg_array(ARG2);
make_move_patch(ARG3, lambdas[Fun.getValue()].patches);

/* Call `new_fun`, asserting that we're on the C stack. */
runtime_call&lt;4&gt;(new_fun);

/* Move back to the C stack, and read the updated values from the process
 * structure */
emit_leave_runtime&lt;Update::eReductions | Update::eStack | Update::eHeap&gt;();

a.mov(getXRef(0), RET);</pre> <p> All combinations of the <code>Update</code> constants are legal, but the ones given to <code>emit_leave_runtime</code> <strong>must</strong> be the same as those given to <code>emit_enter_runtime</code>. </p> <h2 id="tracing-and-nif-loading" class="title-link"> <div class="title-name">1.5 Tracing and NIF Loading</div>  </h2> <p> To make tracing and NIF loading work there needs to be a way to intercept any function call. In the interpreter, this is done by rewriting the loaded BEAM code, but this is more complicated in BeamAsm as we want to have a fast and compact way to do this. This is solved by emitting the code below at the start of each function (x86 variant below): </p> <pre data-language="erlang">0x0: short jmp 6 (address 0x8)
0x2: nop
0x3: relative near call to shared breakpoint fragment
0x8: actual code for function</pre> <p> When code starts to execute it will simply see the <code>short jmp 6</code> instruction which skips the prologue and starts to execute the code directly. </p> <p> When we want to enable a certain breakpoint we set the jmp target to be 1, which means it will land on the call to the shared breakpoint fragment. This fragment checks the current <code>breakpoint_flag</code> stored in the ErtsCodeInfo of this function, and then calls <code>erts_call_nif_early</code> and <code>erts_generic_breakpoint</code> accordingly. </p> <p> Note that the update of the branch and <code>breakpoint_flag</code> does not need to be atomic: it's fine if a process only sees one of these being updated, as the code that sets breakpoints/loads NIFs doesn't rely on the trampoline being active until thread progress has been made. </p> <p> The solution for AArch64 is similar. </p> <h3 id="updating-code" class="title-link"> <div class="title-name">Updating code</div>  </h3> <p> Because many environments enforce [W^X] it's not always possible to write directly to the code pages. Because of this we map code twice: once with an executable page and once with a writable page. Since they're backed by the same memory, writes to the writable page appear magically in the executable one. </p> <p> The <code>erts_writable_code_ptr</code> function can be used to get writable pointers given a module instance, provided that it has been unsealed first: </p> <pre data-language="erlang">for (i = 0; i &lt; n; i++) {
    const ErtsCodeInfo* ci_exec;
    ErtsCodeInfo* ci_rw;
    void *w_ptr;

    erts_unseal_module(&amp;modp-&gt;curr);

    ci_exec = code_hdr-&gt;functions[i];
    w_ptr = erts_writable_code_ptr(&amp;modp-&gt;curr, ci_exec);
    ci_rw = (ErtsCodeInfo*)w_ptr;

    uninstall_breakpoint(ci_rw, ci_exec);
    consolidate_bp_data(modp, ci_rw, 1);
    ASSERT(ci_rw-&gt;gen_bp == NULL);

    erts_seal_module(&amp;modp-&gt;curr);
}</pre> <p> Without the module instance there's no reliable way to figure out the writable address of a code page, and we rely on <strong>address space layout randomization</strong> (ASLR) to make it difficult to guess. On some platforms, security is further enhanced by protecting the writable area from writes until the module has been unsealed by <code>erts_unseal_module</code>. </p> <h3 id="export-tracing" class="title-link"> <div class="title-name">Export tracing</div>  </h3> <p> Unlike the interpreter, we don't execute code inside export entries as that's very annoying to do in the face of [W^X]. When tracing is enabled, we instead point to a fragment that looks at the current export entry and decides what to do. </p> <p> This fragment is shared between all export entries, and the export entry to operate on is assumed to be in a certain register (<code>RET</code> as of writing). This means that all remote calls <strong>must</strong> place the export entry in said register, even when we don't know beforehand that the call is remote, such as when calling a fun. </p> <p> This is pretty easy to do in assembler and the <code>emit_setup_dispatchable_call</code> helper handles it nicely for us, but we can't set registers when trapping out from C code. When trapping to an export entry from C code one must set <code>c_p-&gt;current</code> to the <code>ErtsCodeMFA</code> inside the export entry in question, and then set <code>c_p-&gt;i</code> to <code>beam_bif_export_trap</code>. </p> <p> The <code>BIF_TRAP</code> macros handle this for you, so you generally don't need to think about it. </p> <h2 id="description-of-each-file" class="title-link"> <div class="title-name">1.6 Description of each file</div>  </h2> <p> The BeamAsm implementation resides in the <code>$ERL_TOP/erts/emulator/beam/jit</code> folder. The files are: </p> <ul> <li> <code>asm_load.c</code> <ul><li> BeamAsm specific functions for loading code </li></ul> </li> <li> <code>beam_asm.h</code> <ul><li> Header file describing the C -&gt; C++ api </li></ul> </li> <li> <code>beam_jit_metadata.cpp</code> <ul><li> <code>gdb</code> and Linux <code>perf</code> support for BeamAsm </li></ul> </li> <li> <code>load.h</code> <ul><li> BeamAsm specific header for loading code </li></ul> </li> <li> <code>$ARCH/beam_asm.hpp</code> <ul><li> Header file describing the structs and classes used by BeamAsm. </li></ul> </li> <li> <code>$ARCH/beam_asm.cpp</code> <ul> <li> The BeamAsm initialization code </li> <li> The C -&gt; C++ interface functions. </li> </ul> </li> <li> <code>$ARCH/generators.tab</code>, <code>$ARCH/predicates.tab</code>, <code>$ARCH/ops.tab</code> <ul><li> BeamAsm specific transformations for instructions. See <code><a href="beam_makeops.html">beam_makeops</a></code> for more details. </li></ul> </li> <li> <code>$ARCH/beam_asm_module.cpp</code> <ul><li> The code for the BeamAsm module code generator logic </li></ul> </li> <li> <code>$ARCH/beam_asm_global.cpp</code> <ul><li> Global code fragments that are used by multiple instructions, e.g. error handling code. </li></ul> </li> <li> <code>$ARCH/instr_*.cpp</code> <ul><li> Implementation of individual instructions grouped into files by area </li></ul> </li> <li> <code>$ARCH/process_main.cpp</code> <ul><li> Implementation of the main process loop </li></ul> </li> </ul> <h2 id="linux-perf-support" class="title-link"> <div class="title-name">1.7 Linux perf support</div>  </h2> <p> perf can also be instrumented using BeamAsm symbols to provide more information. As with gdb, only the currently executing function will show up in the stack trace, which means that perf provides functionality similar to that of <code><a href="https://erlang.org/doc/man/eprof.html">eprof</a></code>. </p> <p> You can run perf on BeamAsm like this: </p> <pre data-language="erlang"># Start Erlang under perf
perf record -- erl +JPperf true
# Record a running instance started with `+JPperf true` for 10s
perf record --pid $BEAM_PID -- sleep 10
# Record a running instance started with `+JPperf true` until interrupted
perf record --pid $BEAM_PID</pre> <p> and then look at the results using <code>perf report</code> as you normally would with perf. </p> <p> Frame pointers are enabled when the <code>+JPperf true</code> option is passed, so you can use <code>perf record --call-graph=fp</code> to get more context. This will give you accurate call graphs for pure Erlang code, but in rare cases it fails to track transitions from Erlang to C code and back. <code><a href="https://lwn.net/Articles/680985/">`perf record --call-graph=lbr`</a></code> may work better in those cases, but it's worse at tracking in general. </p> <p> For example, you can run perf to analyze dialyzer building a PLT like this: </p> <pre data-language="erlang">ERL_FLAGS="+JPperf true +S 1" perf record --call-graph=fp \
  dialyzer --build_plt -Wunknown --apps compiler crypto erts kernel stdlib \
  syntax_tools asn1 edoc et ftp inets mnesia observer public_key \
  sasl runtime_tools snmp ssl tftp wx xmerl tools</pre> <p> The above code is run using <code>+S 1</code> to make the perf output easier to understand. If you then run <code>perf report -f --no-children</code> you may get something similar to this: </p> <div class="doc-image-wrapper"> <p class="doc-image-caption">Figure 1.1: Linux Perf report: dialyzer PLT build</p> </div> <p> Any Erlang function in the report is prefixed with a <code>$</code> and all C functions have their normal names. Any Erlang function that has the prefix <code>$global::</code> refers to a global shared fragment. </p> <p> So in the above, we can see that we spend the most time doing <code>eq</code>, i.e. comparing two terms. By expanding it and looking at its parents we can see that it is the function <code>erl_types:t_is_equal/2</code> that contributes the most to this value. Go and have a look at it in the source code to see if you can figure out why so much time is spent there. </p> <p> After <code>eq</code> we see the function <code>erl_types:t_has_var/1</code> where we spend almost 5% of the entire execution in. A while further down you can see <code>copy_struct_x</code> which is the function used to copy terms. If we expand it to view the parents we find that it is mostly <code>ets:lookup_element/3</code> that contributes to this time via the Erlang function <code>dialyzer_plt:ets_table_lookup/2</code>. </p> <h3 id="flame-graph" class="title-link"> <div class="title-name">Flame Graph</div>  </h3> <p> You can also create a Flame Graph from the perf output. Flame Graphs are basically just another way to look at the same data as the <code>perf report</code> output, but can be more easily shared with others and manipulated to give a graph tailor-made for your needs. For instance, if we run dialyzer with all schedulers: </p> <pre data-language="erlang">## Run dialyzer with multiple schedulers
ERL_FLAGS="+JPperf true" perf record --call-graph=fp \
  dialyzer --build_plt -Wunknown --apps compiler crypto erts kernel stdlib \
  syntax_tools asn1 edoc et ftp inets mnesia observer public_key \
  sasl runtime_tools snmp ssl tftp wx xmerl tools --statistics</pre> <p> And then use the scripts found at Brendan Gregg's <code><a href="https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html">CPU Flame Graphs</a></code> web page as follows: </p> <pre data-language="erlang">## Collect the results
perf script &gt; out.perf
## run stackcollapse
stackcollapse-perf.pl out.perf &gt; out.folded
## Create the svg
flamegraph.pl out.folded &gt; out.svg</pre> <p> We get a graph that would look something like this: </p> <div class="doc-image-wrapper"> <p class="doc-image-caption">Figure 1.2: Linux Perf FlameGraph: dialyzer PLT build</p> </div> <p> You can view a larger version <code><a href="figures/perf-beamasm.svg.html">here</a></code>. It contains the same information, but it is easier to share with others as it does not need the symbols in the executable. </p> <p> Using the same data we can also produce a graph where the scheduler profile data has been merged by using <code>sed</code>: </p> <pre data-language="erlang">## Strip [0-9]+_ and/or _[0-9]+ from all scheduler names
## scheduler names changed in OTP26, hence two expressions
sed -e 's/^[0-9]\+_//' -e 's/^erts_\([^_]\+\)_[0-9]\+/erts_\1/' out.folded &gt; out.folded_sched
## Create the svg
flamegraph.pl out.folded_sched &gt; out_sched.svg</pre> <div class="doc-image-wrapper"> <p class="doc-image-caption">Figure 1.3: Linux Perf FlameGraph: dialyzer PLT build</p> </div> <p> You can view a larger version <code><a href="figures/perf-beamasm-merged.svg.html">here</a></code>. There are many different transformations that you can do to make the graph show you what you want. </p> <h3 id="annotate-perf-functions" class="title-link"> <div class="title-name">Annotate perf functions</div>  </h3> <p> If you want to be able to use the <code>perf annotate</code> functionality (and in extension the annotate functionality in the <code>perf report</code> gui) you need to use a monotonic clock when calling <code>perf record</code>, i.e. <code>perf record -k mono</code>. So for a dialyzer run you would do this: </p> <pre data-language="erlang">ERL_FLAGS="+JPperf true +S 1" perf record -k mono --call-graph=fp \
  dialyzer --build_plt -Wunknown --apps compiler crypto erts kernel stdlib \
  syntax_tools asn1 edoc et ftp inets mnesia observer public_key \
  sasl runtime_tools snmp ssl tftp wx xmerl tools</pre> <p> In order to use the <code>perf.data</code> produced by this record you need to first call <code>perf inject --jit</code> like this: </p> <pre data-language="erlang">perf inject --jit -i perf.data -o perf.jitted.data</pre> <p> and then you can view an annotated function like this: </p> <pre data-language="erlang">perf annotate -M intel -i perf.jitted.data erl_types:t_has_var/1</pre> <p> or by pressing <code>a</code> in the <code>perf report</code> ui. Then you get something like this: </p> <div class="doc-image-wrapper"> <p class="doc-image-caption">Figure 1.4: Linux Perf FlameGraph: dialyzer PLT build</p> </div> <p> <code>perf annotate</code> will interleave the listing with the original source code whenever possible. You can use the <code>+{source,Filename}</code> or <code>+absolute_paths</code> compiler options to tell <code>perf</code> where to find the source code. </p> <div class="warning"> <div class="label">Warning</div> <div class="content">

<p> Calling <code>perf inject --jit</code> will create a lot of files in <code>/tmp/</code> and in <code>~/.debug/tmp/</code>. So make sure to cleanup in those directories from time to time or you may run out of inodes. </p> </div> </div> <h3 id="perf-tips-and-tricks" class="title-link"> <div class="title-name">perf tips and tricks</div>  </h3> <p> You can do a lot of neat things with <code>perf</code>. Below is a list of some of the options we have found useful: </p> <ul> <li> <code>perf report --no-children</code> Do not include the accumulation of all children in a call. </li> <li> <code>perf report --call-graph callee</code> Show the callee rather than the caller when expanding a function call. </li> <li> <code>perf archive</code> Create an archive with all the artifacts needed to inspect the data on another host. In early version of perf this command does not work, instead you can use <code><a href="https://github.com/torvalds/linux/blob/master/tools/perf/perf-archive.sh">this bash script</a></code>. </li> <li> <code>perf report</code> gives "failed to process sample" and/or "failed to process type: 68" This probably means that you are running a bugged version of perf. We have seen this when running Ubuntu 18.04 with kernel version 4. If you update to Ubuntu 20.04 or use Ubuntu 18.04 with kernel version 5 the problem should go away. </li> </ul> <h2 id="faq" class="title-link"> 1.8 faq  </h2> <h3 id="how-do-i-know-that-i'm-running-a-jit-enabled-erlang-" class="title-link"> <div class="title-name">How do I know that I'm running a JIT enabled Erlang?</div>  </h3> <p> You will see a banner containing <code>[jit]</code> shell when you start. You can also use <code>erlang:system_info(emu_flavor)</code> to check the flavor and it should be <code>jit</code>. </p> <p> There are two major reasons why when building Erlang/OTP you would not get the JIT. </p> <ul> <li> You are not building a 64-bit emulator for x86 or ARM </li> <li> You do not have a C++ compiler that supports C++-17 </li> </ul> <p> If you run <code>./configure --enable-jit</code> configure will abort when it discovers that your system cannot build the JIT. </p> <h3 id="is-the-interpreter-still-available-" class="title-link"> <div class="title-name">Is the interpreter still available?</div>  </h3> <p> Yes, you can still build the interpreter if you want to. In fact, it is what is used on platforms where BeamAsm does not yet work. You can either completely disable BeamAsm by passing <code>--disable-jit</code> to configure. Or you can build the interpreter using <code>make FLAVOR=emu</code> and then run it using <code>erl -emu_flavor emu</code>. </p> <p> It is possible to have both the JIT and interpreter available at the same time. </p> <h3 id="how-much-of-a-speedup-should-i-expect-from-beamasm-compared-to-the-interpreter-" class="title-link"> <div class="title-name">How much of a speedup should I expect from BeamAsm compared to the interpreter?</div>  </h3> <p> It depends a lot on what your application does. Anything from no difference to up to four times as fast is possible. </p> <p> BeamAsm tries very hard to not be slower than the interpreter, but there can be cases when that happens. One such could be very short-lived small scripts. If you come across any scenarios when this happens, please open a bug report at <code><a href="https://github.com/erlang/otp/issues">the Erlang/OTP bug tracker</a></code>. </p> <h3 id="would-it-be-possible-to-add-support-for-beamasm-on-other-cpu-architectures-" class="title-link"> <div class="title-name">Would it be possible to add support for BeamAsm on other CPU architectures?</div>  </h3> <p> Any new architecture needs support in the assembler as well. Since we use <code><a href="https://github.com/asmjit/asmjit">asmjit</a></code> for this, that means we need support in <code><a href="https://github.com/asmjit/asmjit">asmjit</a></code>. BeamAsm uses relatively few instructions (mostly, <code>mov</code>, <code>jmp</code>, <code>cmp</code>, <code>sub</code>, <code>add</code>), so it would not need to have full support of all instructions. </p> <p> Another approach would be to not use <code><a href="https://github.com/asmjit/asmjit">asmjit</a></code> for the new architecture, but instead use something different to assemble code during load-time. </p> <h3 id="would-it-be-possible-to-add-support-for-beamasm-on-another-os-" class="title-link"> <div class="title-name">Would it be possible to add support for BeamAsm on another OS?</div>  </h3> <p> Adding a new OS that runs x86-64 or aarch64 should not need any large changes if the OS supports mapping of memory as executable. If the ABI used by the OS is not supported changes related to calling C-functions also have to be made. </p> <p> As a reference, it took us about 2-3 weeks to implement support for Windows, and about three months to finish the aarch64 port. </p> </div><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2010&ndash;2023 Ericsson AB<br>Licensed under the Apache License, Version 2.0.<br>
    
  </p>
</div>
