<h1 id="formula-examples">Fitting models using R-style formulas</h1> <p id="fitting-models-using-r-style-formulas">Since version 0.5.0, <code>statsmodels</code> allows users to fit statistical models using R-style formulas. Internally, <code>statsmodels</code> uses the <a class="reference external" href="http://patsy.readthedocs.io/en/latest/">patsy</a> package to convert formulas and data to the matrices that are used in model fitting. The formula framework is quite powerful; this tutorial only scratches the surface. A full description of the formula language can be found in the <code>patsy</code> docs:</p> <ul class="simple"> <li><a class="reference external" href="http://patsy.readthedocs.io/en/latest/">Patsy formula language description</a></li> </ul>  <h2 id="loading-modules-and-functions">Loading modules and functions</h2> <pre data-language="python">In [1]: import statsmodels.formula.api as smf

In [2]: import numpy as np

In [3]: import pandas
</pre> <p>Notice that we called <code>statsmodels.formula.api</code> instead of the usual <code>statsmodels.api</code>. The <code>formula.api</code> hosts many of the same functions found in <code>api</code> (e.g. OLS, GLM), but it also holds lower case counterparts for most of these models. In general, lower case models accept <code>formula</code> and <code>df</code> arguments, whereas upper case ones take <code>endog</code> and <code>exog</code> design matrices. <code>formula</code> accepts a string which describes the model in terms of a <code>patsy</code> formula. <code>df</code> takes a <a class="reference external" href="http://pandas.pydata.org/">pandas</a> data frame.</p> <p><code>dir(smf)</code> will print a list of available models.</p> <p>Formula-compatible models have the following generic call signature: <code>(formula, data, subset=None, *args, **kwargs)</code></p>   <h2 id="ols-regression-using-formulas">OLS regression using formulas</h2> <p>To begin, we fit the linear model described on the <a class="reference external" href="gettingstarted.html">Getting Started</a> page. Download the data, subset columns, and list-wise delete to remove missing observations:</p> <pre data-language="python">In [4]: df = sm.datasets.get_rdataset("Guerry", "HistData").data

In [5]: df = df[['Lottery', 'Literacy', 'Wealth', 'Region']].dropna()

In [6]: df.head()
Out[6]: 
   Lottery  Literacy  Wealth Region
0       41        37      73      E
1       38        51      22      N
2       66        13      61      C
3       80        46      76      E
4       79        69      83      E
</pre> <p>Fit the model:</p> <pre data-language="python">In [7]: mod = smf.ols(formula='Lottery ~ Literacy + Wealth + Region', data=df)

In [8]: res = mod.fit()

In [9]: print(res.summary())
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                Lottery   R-squared:                       0.338
Model:                            OLS   Adj. R-squared:                  0.287
Method:                 Least Squares   F-statistic:                     6.636
Date:                Mon, 14 May 2018   Prob (F-statistic):           1.07e-05
Time:                        21:46:27   Log-Likelihood:                -375.30
No. Observations:                  85   AIC:                             764.6
Df Residuals:                      78   BIC:                             781.7
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
===============================================================================
                  coef    std err          t      P&gt;|t|      [0.025      0.975]
-------------------------------------------------------------------------------
Intercept      38.6517      9.456      4.087      0.000      19.826      57.478
Region[T.E]   -15.4278      9.727     -1.586      0.117     -34.793       3.938
Region[T.N]   -10.0170      9.260     -1.082      0.283     -28.453       8.419
Region[T.S]    -4.5483      7.279     -0.625      0.534     -19.039       9.943
Region[T.W]   -10.0913      7.196     -1.402      0.165     -24.418       4.235
Literacy       -0.1858      0.210     -0.886      0.378      -0.603       0.232
Wealth          0.4515      0.103      4.390      0.000       0.247       0.656
==============================================================================
Omnibus:                        3.049   Durbin-Watson:                   1.785
Prob(Omnibus):                  0.218   Jarque-Bera (JB):                2.694
Skew:                          -0.340   Prob(JB):                        0.260
Kurtosis:                       2.454   Cond. No.                         371.
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>   <h2 id="categorical-variables">Categorical variables</h2> <p>Looking at the summary printed above, notice that <code>patsy</code> determined that elements of <em>Region</em> were text strings, so it treated <em>Region</em> as a categorical variable. <code>patsy</code>’s default is also to include an intercept, so we automatically dropped one of the <em>Region</em> categories.</p> <p>If <em>Region</em> had been an integer variable that we wanted to treat explicitly as categorical, we could have done so by using the <code>C()</code> operator:</p> <pre data-language="python">In [10]: res = smf.ols(formula='Lottery ~ Literacy + Wealth + C(Region)', data=df).fit()

In [11]: print(res.params)
Intercept         38.651655
C(Region)[T.E]   -15.427785
C(Region)[T.N]   -10.016961
C(Region)[T.S]    -4.548257
C(Region)[T.W]   -10.091276
Literacy          -0.185819
Wealth             0.451475
dtype: float64
</pre> <p>Examples more advanced features <code>patsy</code>’s categorical variables function can be found here: <a class="reference external" href="contrasts.html">Patsy: Contrast Coding Systems for categorical variables</a></p>   <h2 id="operators">Operators</h2> <p>We have already seen that “~” separates the left-hand side of the model from the right-hand side, and that “+” adds new columns to the design matrix.</p>  <h3 id="removing-variables">Removing variables</h3> <p>The “-” sign can be used to remove columns/variables. For instance, we can remove the intercept from a model by:</p> <pre data-language="python">In [12]: res = smf.ols(formula='Lottery ~ Literacy + Wealth + C(Region) -1 ', data=df).fit()

In [13]: print(res.params)
C(Region)[C]    38.651655
C(Region)[E]    23.223870
C(Region)[N]    28.634694
C(Region)[S]    34.103399
C(Region)[W]    28.560379
Literacy        -0.185819
Wealth           0.451475
dtype: float64
</pre>   <h3 id="multiplicative-interactions">Multiplicative interactions</h3> <p>“:” adds a new column to the design matrix with the product of the other two columns. “*” will also include the individual columns that were multiplied together:</p> <pre data-language="python">In [14]: res1 = smf.ols(formula='Lottery ~ Literacy : Wealth - 1', data=df).fit()

In [15]: res2 = smf.ols(formula='Lottery ~ Literacy * Wealth - 1', data=df).fit()

In [16]: print(res1.params)
Literacy:Wealth    0.018176
dtype: float64

In [17]: print(res2.params)
</pre><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2009&ndash;2012 Statsmodels Developers<br>&copy; 2006&ndash;2008 Scipy Developers<br>&copy; 2006 Jonathan E. Taylor<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://www.statsmodels.org/stable/example_formulas.html" class="_attribution-link">http://www.statsmodels.org/stable/example_formulas.html</a>
  </p>
</div>
