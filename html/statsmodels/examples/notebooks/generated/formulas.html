   <h1 id="Formulas:-Fitting-models-using-R-style-formulas">Formulas: Fitting models using R-style formulas</h1>       <p>Since version 0.5.0, <code>statsmodels</code> allows users to fit statistical models using R-style formulas. Internally, <code>statsmodels</code> uses the <a href="http://patsy.readthedocs.org/">patsy</a> package to convert formulas and data to the matrices that are used in model fitting. The formula framework is quite powerful; this tutorial only scratches the surface. A full description of the formula language can be found in the <code>patsy</code> docs:</p> <ul> <li><a href="http://patsy.readthedocs.org/">Patsy formula language description</a></li> </ul> <h2 id="Loading-modules-and-functions">Loading modules and functions</h2>     <div class="input"> <div class="prompt input_prompt">In [1]:</div>   <pre data-language="python">from __future__ import print_function
import numpy as np
import statsmodels.api as sm
</pre>   </div>     <h4 id="Import-convention">Import convention</h4>       <p>You can import explicitly from statsmodels.formula.api</p>     <div class="input"> <div class="prompt input_prompt">In [2]:</div>   <pre data-language="python">from statsmodels.formula.api import ols
</pre>   </div>     <p>Alternatively, you can just use the <code>formula</code> namespace of the main <code>statsmodels.api</code>.</p>     <div class="input"> <div class="prompt input_prompt">In [3]:</div>   <pre data-language="python">sm.formula.ols
</pre>   </div> <div class="output_wrapper"> <div class="output"> <div class="output_area"> <div class="prompt output_prompt">Out[3]:</div> <div class="output_text output_subarea output_execute_result"> <pre>&lt;bound method Model.from_formula of &lt;class 'statsmodels.regression.linear_model.OLS'&gt;&gt;</pre> </div> </div> </div> </div>     <p>Or you can use the following conventioin</p>     <div class="input"> <div class="prompt input_prompt">In [4]:</div>   <pre data-language="python">import statsmodels.formula.api as smf
</pre>   </div>     <p>These names are just a convenient way to get access to each model's <code>from_formula</code> classmethod. See, for instance</p>     <div class="input"> <div class="prompt input_prompt">In [5]:</div>   <pre data-language="python">sm.OLS.from_formula
</pre>   </div> <div class="output_wrapper"> <div class="output"> <div class="output_area"> <div class="prompt output_prompt">Out[5]:</div> <div class="output_text output_subarea output_execute_result"> <pre>&lt;bound method Model.from_formula of &lt;class 'statsmodels.regression.linear_model.OLS'&gt;&gt;</pre> </div> </div> </div> </div>     <p>All of the lower case models accept <code>formula</code> and <code>data</code> arguments, whereas upper case ones take <code>endog</code> and <code>exog</code> design matrices. <code>formula</code> accepts a string which describes the model in terms of a <code>patsy</code> formula. <code>data</code> takes a <a href="http://pandas.pydata.org/">pandas</a> data frame or any other data structure that defines a <code>__getitem__</code> for variable names like a structured array or a dictionary of variables.</p> <p><code>dir(sm.formula)</code> will print a list of available models.</p> <p>Formula-compatible models have the following generic call signature: <code>(formula, data, subset=None, *args, **kwargs)</code></p>       <h2 id="OLS-regression-using-formulas">OLS regression using formulas</h2>
<p>To begin, we fit the linear model described on the <a href="gettingstarted.html">Getting Started</a> page. Download the data, subset columns, and list-wise delete to remove missing observations:</p>     <div class="input"> <div class="prompt input_prompt">In [6]:</div>   <pre data-language="python">dta = sm.datasets.get_rdataset("Guerry", "HistData", cache=True)
</pre>   </div>   <div class="input"> <div class="prompt input_prompt">In [7]:</div>   <pre data-language="python">df = dta.data[['Lottery', 'Literacy', 'Wealth', 'Region']].dropna()
df.head()
</pre>   </div> <div class="output_wrapper"> <div class="output"> <div class="output_area"> <div class="prompt output_prompt">Out[7]:</div> <div class="output_html rendered_html output_subarea output_execute_result"> <div>  <table class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>Lottery</th> <th>Literacy</th> <th>Wealth</th> <th>Region</th> </tr> </thead>  <tr> <th>0</th> <td>41</td> <td>37</td> <td>73</td> <td>E</td> </tr> <tr> <th>1</th> <td>38</td> <td>51</td> <td>22</td> <td>N</td> </tr> <tr> <th>2</th> <td>66</td> <td>13</td> <td>61</td> <td>C</td> </tr> <tr> <th>3</th> <td>80</td> <td>46</td> <td>76</td> <td>E</td> </tr> <tr> <th>4</th> <td>79</td> <td>69</td> <td>83</td> <td>E</td> </tr>  </table> </div> </div> </div> </div> </div>     <p>Fit the model:</p>     <div class="input"> <div class="prompt input_prompt">In [8]:</div>   <pre data-language="python">mod = ols(formula='Lottery ~ Literacy + Wealth + Region', data=df)
res = mod.fit()
print(res.summary())
</pre>   </div> <div class="output_wrapper"> <div class="output"> <div class="output_area">  <div class="output_subarea output_stream output_stdout output_text"> <pre>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                Lottery   R-squared:                       0.338
Model:                            OLS   Adj. R-squared:                  0.287
Method:                 Least Squares   F-statistic:                     6.636
Date:                Mon, 14 May 2018   Prob (F-statistic):           1.07e-05
Time:                        21:44:51   Log-Likelihood:                -375.30
No. Observations:                  85   AIC:                             764.6
Df Residuals:                      78   BIC:                             781.7
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
===============================================================================
                  coef    std err          t      P&gt;|t|      [0.025      0.975]
-------------------------------------------------------------------------------
Intercept      38.6517      9.456      4.087      0.000      19.826      57.478
Region[T.E]   -15.4278      9.727     -1.586      0.117     -34.793       3.938
Region[T.N]   -10.0170      9.260     -1.082      0.283     -28.453       8.419
Region[T.S]    -4.5483      7.279     -0.625      0.534     -19.039       9.943
Region[T.W]   -10.0913      7.196     -1.402      0.165     -24.418       4.235
Literacy       -0.1858      0.210     -0.886      0.378      -0.603       0.232
Wealth          0.4515      0.103      4.390      0.000       0.247       0.656
==============================================================================
Omnibus:                        3.049   Durbin-Watson:                   1.785
Prob(Omnibus):                  0.218   Jarque-Bera (JB):                2.694
Skew:                          -0.340   Prob(JB):                        0.260
Kurtosis:                       2.454   Cond. No.                         371.
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre> </div> </div> </div> </div>     <h2 id="Categorical-variables">Categorical variables</h2>
<p>Looking at the summary printed above, notice that <code>patsy</code> determined that elements of <em>Region</em> were text strings, so it treated <em>Region</em> as a categorical variable. <code>patsy</code>'s default is also to include an intercept, so we automatically dropped one of the <em>Region</em> categories.</p> <p>If <em>Region</em> had been an integer variable that we wanted to treat explicitly as categorical, we could have done so by using the <code>C()</code> operator:</p>     <div class="input"> <div class="prompt input_prompt">In [9]:</div>   <pre data-language="python">res = ols(formula='Lottery ~ Literacy + Wealth + C(Region)', data=df).fit()
print(res.params)
</pre>   </div> <div class="output_wrapper"> <div class="output"> <div class="output_area">  <div class="output_subarea output_stream output_stdout output_text"> <pre>Intercept         38.651655
C(Region)[T.E]   -15.427785
C(Region)[T.N]   -10.016961
C(Region)[T.S]    -4.548257
C(Region)[T.W]   -10.091276
Literacy          -0.185819
Wealth             0.451475
dtype: float64
</pre> </div> </div> </div> </div>     <p>Patsy's mode advanced features for categorical variables are discussed in: <a href="contrasts.html">Patsy: Contrast Coding Systems for categorical variables</a></p>       <h2 id="Operators">Operators</h2>
<p>We have already seen that "~" separates the left-hand side of the model from the right-hand side, and that "+" adds new columns to the design matrix.</p> <h3 id="Removing-variables">Removing variables</h3>
<p>The "-" sign can be used to remove columns/variables. For instance, we can remove the intercept from a model by:</p>     <div class="input"> <div class="prompt input_prompt">In [10]:</div>   <pre data-language="python">res = ols(formula='Lottery ~ Literacy + Wealth + C(Region) -1 ', data=df).fit()
print(res.params)
</pre>   </div> <div class="output_wrapper"> <div class="output"> <div class="output_area">  <div class="output_subarea output_stream output_stdout output_text"> <pre>C(Region)[C]    38.651655
C(Region)[E]    23.223870
C(Region)[N]    28.634694
C(Region)[S]    34.103399
C(Region)[W]    28.560379
Literacy        -0.185819
Wealth           0.451475
dtype: float64
</pre> </div> </div> </div> </div>     <h3 id="Multiplicative-interactions">Multiplicative interactions</h3>
<p>":" adds a new column to the design matrix with the interaction of the other two columns. "*" will also include the individual columns that were multiplied together:</p>     <div class="input"> <div class="prompt input_prompt">In [11]:</div>   <pre data-language="python">res1 = ols(formula='Lottery ~ Literacy : Wealth - 1', data=df).fit()
res2 = ols(formula='Lottery ~ Literacy * Wealth - 1', data=df).fit()
print(res1.params, '\n')
print(res2.params)
</pre>   </div> <div class="output_wrapper"> <div class="output"> <div class="output_area">  <div class="output_subarea output_stream output_stdout output_text"> <pre>Literacy:Wealth    0.018176
dtype: float64 

Literacy           0.427386
Wealth             1.080987
Literacy:Wealth   -0.013609
dtype: float64
</pre> </div> </div> </div> </div>     <p>Many other things are possible with operators. Please consult the <a href="https://patsy.readthedocs.org/en/latest/formulas.html">patsy docs</a> to learn more.</p>       <h2 id="Functions">Functions</h2>
<p>You can apply vectorized functions to the variables in your model:</p>     <div class="input"> <div class="prompt input_prompt">In [12]:</div>   <pre data-language="python">res = smf.ols(formula='Lottery ~ np.log(Literacy)', data=df).fit()
print(res.params)
</pre>   </div> <div class="output_wrapper"> <div class="output"> <div class="output_area">  <div class="output_subarea output_stream output_stdout output_text"> <pre>Intercept           115.609119
np.log(Literacy)    -20.393959
dtype: float64
</pre> </div> </div> </div> </div>     <p>Define a custom function:</p>     <div class="input"> <div class="prompt input_prompt">In [13]:</div>   <pre data-language="python">def log_plus_1(x):
    return np.log(x) + 1.
res = smf.ols(formula='Lottery ~ log_plus_1(Literacy)', data=df).fit()
print(res.params)
</pre>   </div> <div class="output_wrapper"> <div class="output"> <div class="output_area">  <div class="output_subarea output_stream output_stdout output_text"> <pre>Intercept               136.003079
log_plus_1(Literacy)    -20.393959
dtype: float64
</pre> </div> </div> </div> </div>     <p>Any function that is in the calling namespace is available to the formula.</p>       <h2 id="Using-formulas-with-models-that-do-not-(yet)-support-them">Using formulas with models that do not (yet) support them</h2>
<p>Even if a given <code>statsmodels</code> function does not support formulas, you can still use <code>patsy</code>'s formula language to produce design matrices. Those matrices can then be fed to the fitting function as <code>endog</code> and <code>exog</code> arguments.</p> <p>To generate <code>numpy</code> arrays:</p>     <div class="input"> <div class="prompt input_prompt">In [14]:</div>   <pre data-language="python">import patsy
f = 'Lottery ~ Literacy * Wealth'
y,X = patsy.dmatrices(f, df, return_type='matrix')
print(y[:5])
print(X[:5])
</pre>   </div> <div class="output_wrapper"> <div class="output"> <div class="output_area">  <div class="output_subarea output_stream output_stdout output_text"> <pre>[[41.]
 [38.]
 [66.]
 [80.]
 [79.]]
[[1.000e+00 3.700e+01 7.300e+01 2.701e+03]
 [1.000e+00 5.100e+01 2.200e+01 1.122e+03]
 [1.000e+00 1.300e+01 6.100e+01 7.930e+02]
 [1.000e+00 4.600e+01 7.600e+01 3.496e+03]
 [1.000e+00 6.900e+01 8.300e+01 5.727e+03]]
</pre> </div> </div> </div> </div>     <p>To generate pandas data frames:</p>     <div class="input"> <div class="prompt input_prompt">In [15]:</div>   <pre data-language="python">f = 'Lottery ~ Literacy * Wealth'
y,X = patsy.dmatrices(f, df, return_type='dataframe')
print(y[:5])
print(X[:5])
</pre>   </div> <div class="output_wrapper"> <div class="output"> <div class="output_area">  <div class="output_subarea output_stream output_stdout output_text"> <pre>   Lottery
0     41.0
1     38.0
2     66.0
3     80.0
4     79.0
   Intercept  Literacy  Wealth  Literacy:Wealth
0        1.0      37.0    73.0           2701.0
1        1.0      51.0    22.0           1122.0
2        1.0      13.0    61.0            793.0
3        1.0      46.0    76.0           3496.0
4        1.0      69.0    83.0           5727.0
</pre> </div> </div> </div> </div>   <div class="input"> <div class="prompt input_prompt">In [16]:</div>   <pre data-language="python">print(sm.OLS(y, X).fit().summary())
</pre>   </div> <div class="output_wrapper"> <div class="output"> <div class="output_area">  <div class="output_subarea output_stream output_stdout output_text"> <pre>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                Lottery   R-squared:                       0.309
Model:                            OLS   Adj. R-squared:                  0.283
Method:                 Least Squares   F-statistic:                     12.06
Date:                Mon, 14 May 2018   Prob (F-statistic):           1.32e-06
Time:                        21:44:51   Log-Likelihood:                -377.13
No. Observations:                  85   AIC:                             762.3
Df Residuals:                      81   BIC:                             772.0
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
===================================================================================
                      coef    std err          t      P&gt;|t|      [0.025      0.975]
-----------------------------------------------------------------------------------
Intercept          38.6348     15.825      2.441      0.017       7.149      70.121
Literacy           -0.3522      0.334     -1.056      0.294      -1.016       0.312
Wealth              0.4364      0.283      1.544      0.126      -0.126       0.999
Literacy:Wealth    -0.0005      0.006     -0.085      0.933      -0.013       0.012
==============================================================================
Omnibus:                        4.447   Durbin-Watson:                   1.953
Prob(Omnibus):                  0.108   Jarque-Bera (JB):                3.228
Skew:                          -0.332   Prob(JB):                        0.199
Kurtosis:                       2.314   Cond. No.                     1.40e+04
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.4e+04. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre> </div> </div> </div> </div><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2009&ndash;2012 Statsmodels Developers<br>&copy; 2006&ndash;2008 Scipy Developers<br>&copy; 2006 Jonathan E. Taylor<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://www.statsmodels.org/stable/examples/notebooks/generated/formulas.html" class="_attribution-link">http://www.statsmodels.org/stable/examples/notebooks/generated/formulas.html</a>
  </p>
</div>
