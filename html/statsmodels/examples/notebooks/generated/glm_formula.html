   <h1 id="Generalized-Linear-Models-(Formula)">Generalized Linear Models (Formula)</h1>       <p>This notebook illustrates how you can use R-style formulas to fit Generalized Linear Models.</p> <p>To begin, we load the <code>Star98</code> dataset and we construct a formula and pre-process the data:</p>     <div class="input"> <div class="prompt input_prompt">In [1]:</div>   <pre data-language="python">from __future__ import print_function
import statsmodels.api as sm
import statsmodels.formula.api as smf
star98 = sm.datasets.star98.load_pandas().data
formula = 'SUCCESS ~ LOWINC + PERASIAN + PERBLACK + PERHISP + PCTCHRT + \
           PCTYRRND + PERMINTE*AVYRSEXP*AVSALK + PERSPENK*PTRATIO*PCTAF'
dta = star98[['NABOVE', 'NBELOW', 'LOWINC', 'PERASIAN', 'PERBLACK', 'PERHISP',
              'PCTCHRT', 'PCTYRRND', 'PERMINTE', 'AVYRSEXP', 'AVSALK',
              'PERSPENK', 'PTRATIO', 'PCTAF']].copy()
endog = dta['NABOVE'] / (dta['NABOVE'] + dta.pop('NBELOW'))
del dta['NABOVE']
dta['SUCCESS'] = endog
</pre>   </div>     <p>Then, we fit the GLM model:</p>     <div class="input"> <div class="prompt input_prompt">In [2]:</div>   <pre data-language="python">mod1 = smf.glm(formula=formula, data=dta, family=sm.families.Binomial()).fit()
mod1.summary()
</pre>   </div> <div class="output_wrapper"> <div class="output"> <div class="output_area"> <div class="prompt output_prompt">Out[2]:</div> <div class="output_html rendered_html output_subarea output_execute_result"> <table class="simpletable"> <caption>Generalized Linear Model Regression Results</caption> <tr> <th>Dep. Variable:</th> <td>SUCCESS</td> <th> No. Observations: </th> <td> 303</td> </tr> <tr> <th>Model:</th> <td>GLM</td> <th> Df Residuals: </th> <td> 282</td> </tr> <tr> <th>Model Family:</th> <td>Binomial</td> <th> Df Model: </th> <td> 20</td> </tr> <tr> <th>Link Function:</th> <td>logit</td> <th> Scale: </th> <td> 1.0000</td> </tr> <tr> <th>Method:</th> <td>IRLS</td> <th> Log-Likelihood: </th> <td> -127.33</td> </tr> <tr> <th>Date:</th> <td>Mon, 14 May 2018</td> <th> Deviance: </th> <td> 8.5477</td> </tr> <tr> <th>Time:</th> <td>21:44:55</td> <th> Pearson chi2: </th> <td> 8.48</td> </tr> <tr> <th>No. Iterations:</th> <td>4</td> <th> Covariance Type: </th> <td>nonrobust</td> </tr> </table> <table class="simpletable"> <tr> <td></td> <th>coef</th> <th>std err</th> <th>z</th> <th>P&gt;|z|</th> <th>[0.025</th> <th>0.975]</th> </tr> <tr> <th>Intercept</th> <td> 0.4037</td> <td> 25.036</td> <td> 0.016</td> <td> 0.987</td> <td> -48.665</td> <td> 49.472</td> </tr> <tr> <th>LOWINC</th> <td> -0.0204</td> <td> 0.010</td> <td> -1.982</td> <td> 0.048</td> <td> -0.041</td> <td> -0.000</td> </tr> <tr> <th>PERASIAN</th> <td> 0.0159</td> <td> 0.017</td> <td> 0.910</td> <td> 0.363</td> <td> -0.018</td> <td> 0.050</td> </tr> <tr> <th>PERBLACK</th> <td> -0.0198</td> <td> 0.020</td> <td> -1.004</td> <td> 0.316</td> <td> -0.058</td> <td> 0.019</td> </tr> <tr> <th>PERHISP</th> <td> -0.0096</td> <td> 0.010</td> <td> -0.951</td> <td> 0.341</td> <td> -0.029</td> <td> 0.010</td> </tr> <tr> <th>PCTCHRT</th> <td> -0.0022</td> <td> 0.022</td> <td> -0.103</td> <td> 0.918</td> <td> -0.045</td> <td> 0.040</td> </tr> <tr> <th>PCTYRRND</th> <td> -0.0022</td> <td> 0.006</td> <td> -0.348</td> <td> 0.728</td> <td> -0.014</td> <td> 0.010</td> </tr> <tr> <th>PERMINTE</th> <td> 0.1068</td> <td> 0.787</td> <td> 0.136</td> <td> 0.892</td> <td> -1.436</td> <td> 1.650</td> </tr> <tr> <th>AVYRSEXP</th> <td> -0.0411</td> <td> 1.176</td> <td> -0.035</td> <td> 0.972</td> <td> -2.346</td> <td> 2.264</td> </tr> <tr> <th>PERMINTE:AVYRSEXP</th> <td> -0.0031</td> <td> 0.054</td> <td> -0.057</td> <td> 0.954</td> <td> -0.108</td> <td> 0.102</td> </tr> <tr> <th>AVSALK</th> <td> 0.0131</td> <td> 0.295</td> <td> 0.044</td> <td> 0.965</td> <td> -0.566</td> <td> 0.592</td> </tr> <tr> <th>PERMINTE:AVSALK</th> <td> -0.0019</td> <td> 0.013</td> <td> -0.145</td> <td> 0.885</td> <td> -0.028</td> <td> 0.024</td> </tr> <tr> <th>AVYRSEXP:AVSALK</th> <td> 0.0008</td> <td> 0.020</td> <td> 0.038</td> <td> 0.970</td> <td> -0.039</td> <td> 0.041</td> </tr> <tr> <th>PERMINTE:AVYRSEXP:AVSALK</th> <td> 5.978e-05</td> <td> 0.001</td> <td> 0.068</td> <td> 0.946</td> <td> -0.002</td> <td> 0.002</td> </tr> <tr> <th>PERSPENK</th> <td> -0.3097</td> <td> 4.233</td> <td> -0.073</td> <td> 0.942</td> <td> -8.606</td> <td> 7.987</td> </tr> <tr> <th>PTRATIO</th> <td> 0.0096</td> <td> 0.919</td> <td> 0.010</td> <td> 0.992</td> <td> -1.792</td> <td> 1.811</td> </tr> <tr> <th>PERSPENK:PTRATIO</th> <td> 0.0066</td> <td> 0.206</td> <td> 0.032</td> <td> 0.974</td> <td> -0.397</td> <td> 0.410</td> </tr> <tr> <th>PCTAF</th> <td> -0.0143</td> <td> 0.474</td> <td> -0.030</td> <td> 0.976</td> <td> -0.944</td> <td> 0.916</td> </tr> <tr> <th>PERSPENK:PCTAF</th> <td> 0.0105</td> <td> 0.098</td> <td> 0.107</td> <td> 0.915</td> <td> -0.182</td> <td> 0.203</td> </tr> <tr> <th>PTRATIO:PCTAF</th> <td> -0.0001</td> <td> 0.022</td> <td> -0.005</td> <td> 0.996</td> <td> -0.044</td> <td> 0.044</td> </tr> <tr> <th>PERSPENK:PTRATIO:PCTAF</th> <td> -0.0002</td> <td> 0.005</td> <td> -0.051</td> <td> 0.959</td> <td> -0.010</td> <td> 0.009</td> </tr> </table> </div> </div> </div> </div>     <p>Finally, we define a function to operate customized data transformation using the formula framework:</p>     <div class="input"> <div class="prompt input_prompt">In [3]:</div>   <pre data-language="python">def double_it(x):
    return 2 * x
formula = 'SUCCESS ~ double_it(LOWINC) + PERASIAN + PERBLACK + PERHISP + PCTCHRT + \
           PCTYRRND + PERMINTE*AVYRSEXP*AVSALK + PERSPENK*PTRATIO*PCTAF'
mod2 = smf.glm(formula=formula, data=dta, family=sm.families.Binomial()).fit()
mod2.summary()
</pre>   </div> <div class="output_wrapper"> <div class="output"> <div class="output_area"> <div class="prompt output_prompt">Out[3]:</div> <div class="output_html rendered_html output_subarea output_execute_result"> <table class="simpletable"> <caption>Generalized Linear Model Regression Results</caption> <tr> <th>Dep. Variable:</th> <td>SUCCESS</td> <th> No. Observations: </th> <td> 303</td> </tr> <tr> <th>Model:</th> <td>GLM</td> <th> Df Residuals: </th> <td> 282</td> </tr> <tr> <th>Model Family:</th> <td>Binomial</td> <th> Df Model: </th> <td> 20</td> </tr> <tr> <th>Link Function:</th> <td>logit</td> <th> Scale: </th> <td> 1.0000</td> </tr> <tr> <th>Method:</th> <td>IRLS</td> <th> Log-Likelihood: </th> <td> -127.33</td> </tr> <tr> <th>Date:</th> <td>Mon, 14 May 2018</td> <th> Deviance: </th> <td> 8.5477</td> </tr> <tr> <th>Time:</th> <td>21:44:55</td> <th> Pearson chi2: </th> <td> 8.48</td> </tr> <tr> <th>No. Iterations:</th> <td>4</td> <th> Covariance Type: </th> <td>nonrobust</td> </tr> </table> <table class="simpletable"> <tr> <td></td> <th>coef</th> <th>std err</th> <th>z</th> <th>P&gt;|z|</th> <th>[0.025</th> <th>0.975]</th> </tr> <tr> <th>Intercept</th> <td> 0.4037</td> <td> 25.036</td> <td> 0.016</td> <td> 0.987</td> <td> -48.665</td> <td> 49.472</td> </tr> <tr> <th>double_it(LOWINC)</th> <td> -0.0102</td> <td> 0.005</td> <td> -1.982</td> <td> 0.048</td> <td> -0.020</td> <td> -0.000</td> </tr> <tr> <th>PERASIAN</th> <td> 0.0159</td> <td> 0.017</td> <td> 0.910</td> <td> 0.363</td> <td> -0.018</td> <td> 0.050</td> </tr> <tr> <th>PERBLACK</th> <td> -0.0198</td> <td> 0.020</td> <td> -1.004</td> <td> 0.316</td> <td> -0.058</td> <td> 0.019</td> </tr> <tr> <th>PERHISP</th> <td> -0.0096</td> <td> 0.010</td> <td> -0.951</td> <td> 0.341</td> <td> -0.029</td> <td> 0.010</td> </tr> <tr> <th>PCTCHRT</th> <td> -0.0022</td> <td> 0.022</td> <td> -0.103</td> <td> 0.918</td> <td> -0.045</td> <td> 0.040</td> </tr> <tr> <th>PCTYRRND</th> <td> -0.0022</td> <td> 0.006</td> <td> -0.348</td> <td> 0.728</td> <td> -0.014</td> <td> 0.010</td> </tr> <tr> <th>PERMINTE</th> <td> 0.1068</td> <td> 0.787</td> <td> 0.136</td> <td> 0.892</td> <td> -1.436</td> <td> 1.650</td> </tr> <tr> <th>AVYRSEXP</th> <td> -0.0411</td> <td> 1.176</td> <td> -0.035</td> <td> 0.972</td> <td> -2.346</td> <td> 2.264</td> </tr> <tr> <th>PERMINTE:AVYRSEXP</th> <td> -0.0031</td> <td> 0.054</td> <td> -0.057</td> <td> 0.954</td> <td> -0.108</td> <td> 0.102</td> </tr> <tr> <th>AVSALK</th> <td> 0.0131</td> <td> 0.295</td> <td> 0.044</td> <td> 0.965</td> <td> -0.566</td> <td> 0.592</td> </tr> <tr> <th>PERMINTE:AVSALK</th> <td> -0.0019</td> <td> 0.013</td> <td> -0.145</td> <td> 0.885</td> <td> -0.028</td> <td> 0.024</td> </tr> <tr> <th>AVYRSEXP:AVSALK</th> <td> 0.0008</td> <td> 0.020</td> <td> 0.038</td> <td> 0.970</td> <td> -0.039</td> <td> 0.041</td> </tr> <tr> <th>PERMINTE:AVYRSEXP:AVSALK</th> <td> 5.978e-05</td> <td> 0.001</td> <td> 0.068</td> <td> 0.946</td> <td> -0.002</td> <td> 0.002</td> </tr> <tr> <th>PERSPENK</th> <td> -0.3097</td> <td> 4.233</td> <td> -0.073</td> <td> 0.942</td> <td> -8.606</td> <td> 7.987</td> </tr> <tr> <th>PTRATIO</th> <td> 0.0096</td> <td> 0.919</td> <td> 0.010</td> <td> 0.992</td> <td> -1.792</td> <td> 1.811</td> </tr> <tr> <th>PERSPENK:PTRATIO</th> <td> 0.0066</td> <td> 0.206</td> <td> 0.032</td> <td> 0.974</td> <td> -0.397</td> <td> 0.410</td> </tr> <tr> <th>PCTAF</th> <td> -0.0143</td> <td> 0.474</td> <td> -0.030</td> <td> 0.976</td> <td> -0.944</td> <td> 0.916</td> </tr> <tr> <th>PERSPENK:PCTAF</th> <td> 0.0105</td> <td> 0.098</td> <td> 0.107</td> <td> 0.915</td> <td> -0.182</td> <td> 0.203</td> </tr> <tr> <th>PTRATIO:PCTAF</th> <td> -0.0001</td> <td> 0.022</td> <td> -0.005</td> <td> 0.996</td> <td> -0.044</td> <td> 0.044</td> </tr> <tr> <th>PERSPENK:PTRATIO:PCTAF</th> <td> -0.0002</td> <td> 0.005</td> <td> -0.051</td> <td> 0.959</td> <td> -0.010</td> <td> 0.009</td> </tr> </table> </div> </div> </div> </div>     <p>As expected, the coefficient for <code>double_it(LOWINC)</code> in the second model is half the size of the <code>LOWINC</code> coefficient from the first model:</p>     <div class="input"> <div class="prompt input_prompt">In [4]:</div>   <pre data-language="python">print(mod1.params[1])
print(mod2.params[1] * 2)
</pre>   </div> <div class="output_wrapper"> <div class="output"> <div class="output_area">  <div class="output_subarea output_stream output_stdout output_text"> <pre>-0.020395987154757125
-0.020395987154757402
</pre> </div> </div> </div> </div><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2009&ndash;2012 Statsmodels Developers<br>&copy; 2006&ndash;2008 Scipy Developers<br>&copy; 2006 Jonathan E. Taylor<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://www.statsmodels.org/stable/examples/notebooks/generated/glm_formula.html" class="_attribution-link">http://www.statsmodels.org/stable/examples/notebooks/generated/glm_formula.html</a>
  </p>
</div>
