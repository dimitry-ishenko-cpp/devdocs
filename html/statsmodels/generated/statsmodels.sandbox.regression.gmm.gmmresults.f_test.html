<h1 id="statsmodels-sandbox-regression-gmm-gmmresults-f-test">statsmodels.sandbox.regression.gmm.GMMResults.f_test</h1> <dl class="method"> <dt id="statsmodels.sandbox.regression.gmm.GMMResults.f_test">
<code>GMMResults.f_test(r_matrix, cov_p=None, scale=1.0, invcov=None)</code> </dt> <dd>
<p>Compute the F-test for a joint linear hypothesis.</p> <p>This is a special case of <code>wald_test</code> that always uses the F distribution.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<ul class="first simple"> <li>
<strong>r_matrix</strong> (<em>array-like</em><em>, </em><em>str</em><em>, or </em><em>tuple</em>) – <ul> <li>array : An r x k array where r is the number of restrictions to test and k is the number of regressors. It is assumed that the linear combination is equal to zero.</li> <li>str : The full hypotheses to test can be given as a string. See the examples.</li> <li>tuple : A tuple of arrays in the form (R, q), <code>q</code> can be either a scalar or a length k row vector.</li> </ul> </li> <li>
<strong>cov_p</strong> (<em>array-like</em><em>, </em><em>optional</em>) – An alternative estimate for the parameter covariance matrix. If None is given, self.normalized_cov_params is used.</li> <li>
<strong>scale</strong> (<em>float</em><em>, </em><em>optional</em>) – Default is 1.0 for no scaling.</li> <li>
<strong>invcov</strong> (<em>array-like</em><em>, </em><em>optional</em>) – A q x q array to specify an inverse covariance matrix based on a restrictions matrix.</li> </ul> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>res</strong> – The results for the test are attributes of this results instance.</p> </td> </tr> <tr>
<th class="field-name">Return type:</th>
<td class="field-body">
<p class="first last">ContrastResults instance</p> </td> </tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import statsmodels.api as sm
&gt;&gt;&gt; data = sm.datasets.longley.load()
&gt;&gt;&gt; data.exog = sm.add_constant(data.exog)
&gt;&gt;&gt; results = sm.OLS(data.endog, data.exog).fit()
&gt;&gt;&gt; A = np.identity(len(results.params))
&gt;&gt;&gt; A = A[1:,:]
</pre> <p>This tests that each coefficient is jointly statistically significantly different from zero.</p> <pre data-language="python">&gt;&gt;&gt; print(results.f_test(A))
&lt;F test: F=array([[ 330.28533923]]), p=4.984030528700946e-10, df_denom=9, df_num=6&gt;
</pre> <p>Compare this to</p> <pre data-language="python">&gt;&gt;&gt; results.fvalue
330.2853392346658
&gt;&gt;&gt; results.f_pvalue
4.98403096572e-10
</pre> <pre data-language="python">&gt;&gt;&gt; B = np.array(([0,0,1,-1,0,0,0],[0,0,0,0,0,1,-1]))
</pre> <p>This tests that the coefficient on the 2nd and 3rd regressors are equal and jointly that the coefficient on the 5th and 6th regressors are equal.</p> <pre data-language="python">&gt;&gt;&gt; print(results.f_test(B))
&lt;F test: F=array([[ 9.74046187]]), p=0.005605288531708235, df_denom=9, df_num=2&gt;
</pre> <p>Alternatively, you can specify the hypothesis tests using a string</p> <pre data-language="python">&gt;&gt;&gt; from statsmodels.datasets import longley
&gt;&gt;&gt; from statsmodels.formula.api import ols
&gt;&gt;&gt; dta = longley.load_pandas().data
&gt;&gt;&gt; formula = 'TOTEMP ~ GNPDEFL + GNP + UNEMP + ARMED + POP + YEAR'
&gt;&gt;&gt; results = ols(formula, dta).fit()
&gt;&gt;&gt; hypotheses = '(GNPDEFL = GNP), (UNEMP = 2), (YEAR/1829 = 1)'
&gt;&gt;&gt; f_test = results.f_test(hypotheses)
&gt;&gt;&gt; print(f_test)
&lt;F test: F=array([[ 144.17976065]]), p=6.322026217355609e-08, df_denom=9, df_num=3&gt;
</pre> <div class="admonition seealso"> <p class="first admonition-title">See also</p> <p class="last"><a class="reference internal" href="http://www.statsmodels.org/stable/dev/generated/statsmodels.stats.contrast.ContrastResults.html#statsmodels.stats.contrast.ContrastResults" title="statsmodels.stats.contrast.ContrastResults"><code>statsmodels.stats.contrast.ContrastResults</code></a>, <a class="reference internal" href="statsmodels.sandbox.regression.gmm.gmmresults.wald_test.html#statsmodels.sandbox.regression.gmm.GMMResults.wald_test" title="statsmodels.sandbox.regression.gmm.GMMResults.wald_test"><code>wald_test</code></a>, <a class="reference internal" href="statsmodels.sandbox.regression.gmm.gmmresults.t_test.html#statsmodels.sandbox.regression.gmm.GMMResults.t_test" title="statsmodels.sandbox.regression.gmm.GMMResults.t_test"><code>t_test</code></a>, <a class="reference external" href="http://patsy.readthedocs.io/en/latest/API-reference.html#patsy.DesignInfo.linear_constraint" title="(in patsy v0.5.0+dev)"><code>patsy.DesignInfo.linear_constraint</code></a></p> </div> <h4 class="rubric">Notes</h4> <p>The matrix <code>r_matrix</code> is assumed to be non-singular. More precisely,</p> <p>r_matrix (pX pX.T) r_matrix.T</p> <p>is assumed invertible. Here, pX is the generalized inverse of the design matrix of the model. There can be problems in non-OLS models where the rank of the covariance of the noise is not full.</p> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2009&ndash;2012 Statsmodels Developers<br>&copy; 2006&ndash;2008 Scipy Developers<br>&copy; 2006 Jonathan E. Taylor<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://www.statsmodels.org/stable/generated/statsmodels.sandbox.regression.gmm.GMMResults.f_test.html" class="_attribution-link">http://www.statsmodels.org/stable/generated/statsmodels.sandbox.regression.gmm.GMMResults.f_test.html</a>
  </p>
</div>
