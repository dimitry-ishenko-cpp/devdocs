<h1 id="statsmodels-discrete-discrete-model-mnlogit-fit-regularized">statsmodels.discrete.discrete_model.MNLogit.fit_regularized</h1> <dl class="method"> <dt id="statsmodels.discrete.discrete_model.MNLogit.fit_regularized">
<code>MNLogit.fit_regularized(start_params=None, method='l1', maxiter='defined_by_method', full_output=1, disp=1, callback=None, alpha=0, trim_mode='auto', auto_trim_tol=0.01, size_trim_tol=0.0001, qc_tol=0.03, **kwargs)</code> </dt> <dd>
<p>Fit the model using a regularized maximum likelihood. The regularization method AND the solver used is determined by the argument method.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<ul class="first last simple"> <li>
<strong>start_params</strong> (<em>array-like</em><em>, </em><em>optional</em>) – Initial guess of the solution for the loglikelihood maximization. The default is an array of zeros.</li> <li>
<strong>method</strong> (<em>'l1'</em><em> or </em><em>'l1_cvxopt_cp'</em>) – See notes for details.</li> <li>
<strong>maxiter</strong> (<em>Integer</em><em> or </em><em>'defined_by_method'</em>) – Maximum number of iterations to perform. If ‘defined_by_method’, then use method defaults (see notes).</li> <li>
<strong>full_output</strong> (<em>bool</em>) – Set to True to have all available output in the Results object’s mle_retvals attribute. The output is dependent on the solver. See LikelihoodModelResults notes section for more information.</li> <li>
<strong>disp</strong> (<em>bool</em>) – Set to True to print convergence messages.</li> <li>
<strong>fargs</strong> (<em>tuple</em>) – Extra arguments passed to the likelihood function, i.e., loglike(x,*args)</li> <li>
<strong>callback</strong> (<em>callable callback</em><em>(</em><em>xk</em><em>)</em>) – Called after each iteration, as callback(xk), where xk is the current parameter vector.</li> <li>
<strong>retall</strong> (<em>bool</em>) – Set to True to return list of solutions at each iteration. Available in Results object’s mle_retvals attribute.</li> <li>
<strong>alpha</strong> (<em>non-negative scalar</em><em> or </em><em>numpy array</em><em> (</em><em>same size as parameters</em><em>)</em>) – The weight multiplying the l1 penalty term</li> <li>
<strong>trim_mode</strong> (<em>'auto</em><em>, </em><em>'size'</em><em>, or </em><em>'off'</em>) – If not ‘off’, trim (set to zero) parameters that would have been zero if the solver reached the theoretical minimum. If ‘auto’, trim params using the Theory above. If ‘size’, trim params if they have very small absolute value</li> <li>
<strong>size_trim_tol</strong> (<em>float</em><em> or </em><em>'auto'</em><em> (</em><em>default = 'auto'</em><em>)</em>) – For use when trim_mode == ‘size’</li> <li>
<strong>auto_trim_tol</strong> (<em>float</em>) – For sue when trim_mode == ‘auto’. Use</li> <li>
<strong>qc_tol</strong> (<em>float</em>) – Print warning and don’t allow auto trim when (ii) (above) is violated by this much.</li> <li>
<strong>qc_verbose</strong> (<em>Boolean</em>) – If true, print out a full QC report upon failure</li> </ul> </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>Extra parameters are not penalized if alpha is given as a scalar. An example is the shape parameter in NegativeBinomial <code>nb1</code> and <code>nb2</code>.</p> <p>Optional arguments for the solvers (available in Results.mle_settings):</p> <pre data-language="python">'l1'
    acc : float (default 1e-6)
        Requested accuracy as used by slsqp
'l1_cvxopt_cp'
    abstol : float
        absolute accuracy (default: 1e-7).
    reltol : float
        relative accuracy (default: 1e-6).
    feastol : float
        tolerance for feasibility conditions (default: 1e-7).
    refinement : int
        number of iterative refinement steps when solving KKT
        equations (default: 1).
</pre> <p>Optimization methodology</p> <p>With <span class="math notranslate nohighlight">\(L\)</span> the negative log likelihood, we solve the convex but non-smooth problem</p> <div class="math notranslate nohighlight"> \[\min_\beta L(\beta) + \sum_k\alpha_k |\beta_k|\]</div> <p>via the transformation to the smooth, convex, constrained problem in twice as many variables (adding the “added variables” <span class="math notranslate nohighlight">\(u_k\)</span>)</p> <div class="math notranslate nohighlight"> \[\min_{\beta,u} L(\beta) + \sum_k\alpha_k u_k,\]</div> <p>subject to</p> <div class="math notranslate nohighlight"> \[-u_k \leq \beta_k \leq u_k.\]</div> <p>With <span class="math notranslate nohighlight">\(\partial_k L\)</span> the derivative of <span class="math notranslate nohighlight">\(L\)</span> in the <span class="math notranslate nohighlight">\(k^{th}\)</span> parameter direction, theory dictates that, at the minimum, exactly one of two conditions holds:</p> <ol class="lowerroman simple"> <li>
<span class="math notranslate nohighlight">\(|\partial_k L| = \alpha_k\)</span> and <span class="math notranslate nohighlight">\(\beta_k \neq 0\)</span>
</li> <li>
<span class="math notranslate nohighlight">\(|\partial_k L| \leq \alpha_k\)</span> and <span class="math notranslate nohighlight">\(\beta_k = 0\)</span>
</li> </ol> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2009&ndash;2012 Statsmodels Developers<br>&copy; 2006&ndash;2008 Scipy Developers<br>&copy; 2006 Jonathan E. Taylor<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.MNLogit.fit_regularized.html" class="_attribution-link">http://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.MNLogit.fit_regularized.html</a>
  </p>
</div>
