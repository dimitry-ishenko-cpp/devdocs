<h1 id="statsmodels-stats-gof-powerdiscrepancy">statsmodels.stats.gof.powerdiscrepancy</h1> <dl class="function"> <dt id="statsmodels.stats.gof.powerdiscrepancy">
<code>statsmodels.stats.gof.powerdiscrepancy(observed, expected, lambd=0.0, axis=0, ddof=0)</code> <a class="reference internal" href="http://www.statsmodels.org/stable/_modules/statsmodels/stats/gof.html#powerdiscrepancy"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Calculates power discrepancy, a class of goodness-of-fit tests as a measure of discrepancy between observed and expected data.</p> <p>This contains several goodness-of-fit tests as special cases, see the describtion of lambd, the exponent of the power discrepancy. The pvalue is based on the asymptotic chi-square distribution of the test statistic.</p> <p>freeman_tukey: D(x| heta) = sum_j (sqrt{x_j} - sqrt{e_j})^2</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<ul class="first simple"> <li>
<strong>o</strong> (<em>Iterable</em>) – Observed values</li> <li>
<strong>e</strong> (<em>Iterable</em>) – Expected values</li> <li>
<strong>lambd</strong> (<em>float</em><em> or </em><em>string</em>) – <ul> <li>float : exponent <code>a</code> for power discrepancy</li> <li>’loglikeratio’: a = 0</li> <li>’freeman_tukey’: a = -0.5</li> <li>’pearson’: a = 1 (standard chisquare test statistic)</li> <li>’modified_loglikeratio’: a = -1</li> <li>’cressie_read’: a = 2/3</li> <li>’neyman’ : a = -2 (Neyman-modified chisquare, reference from a book?)</li> </ul> </li> <li>
<strong>axis</strong> (<em>int</em>) – axis for observations of one series</li> <li>
<strong>ddof</strong> (<em>int</em>) – degrees of freedom correction,</li> </ul> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">

<ul class="simple"> <li>
<strong>D_obs</strong> (<em>Discrepancy of observed values</em>)</li> <li>
<strong>pvalue</strong> (<em>pvalue</em>)</li> </ul> </td> </tr>  </table> <h4 class="rubric">References</h4> <dl class="docutils"> <dt>Cressie, Noel and Timothy R. C. Read, Multinomial Goodness-of-Fit Tests,</dt> <dd>Journal of the Royal Statistical Society. Series B (Methodological), Vol. 46, No. 3 (1984), pp. 440-464</dd> <dt>Campbell B. Read: Freeman-Tukey chi-squared goodness-of-fit statistics,</dt> <dd>Statistics &amp; Probability Letters 18 (1993) 271-278</dd> <dt>Nobuhiro Taneichi, Yuri Sekiya, Akio Suzukawa, Asymptotic Approximations</dt> <dd>for the Distributions of the Multinomial Goodness-of-Fit Statistics under Local Alternatives, Journal of Multivariate Analysis 81, 335?359 (2002)</dd> <dt>Steele, M. 1,2, C. Hurst 3 and J. Chaseling, Simulated Power of Discrete</dt> <dd>Goodness-of-Fit Tests for Likert Type Data</dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; observed = np.array([ 2.,  4.,  2.,  1.,  1.])
&gt;&gt;&gt; expected = np.array([ 0.2,  0.2,  0.2,  0.2,  0.2])
</pre> <p>for checking correct dimension with multiple series</p> <pre data-language="python">&gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,observed)).T, 10*expected, lambd='freeman_tukey',axis=1)
(array([[ 2.745166,  2.745166]]), array([[ 0.6013346,  0.6013346]]))
&gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,observed)).T, 10*expected,axis=1)
(array([[ 2.77258872,  2.77258872]]), array([[ 0.59657359,  0.59657359]]))
&gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,observed)).T, 10*expected, lambd=0,axis=1)
(array([[ 2.77258872,  2.77258872]]), array([[ 0.59657359,  0.59657359]]))
&gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,observed)).T, 10*expected, lambd=1,axis=1)
(array([[ 3.,  3.]]), array([[ 0.5578254,  0.5578254]]))
&gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,observed)).T, 10*expected, lambd=2/3.0,axis=1)
(array([[ 2.89714546,  2.89714546]]), array([[ 0.57518277,  0.57518277]]))
&gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,observed)).T, expected, lambd=2/3.0,axis=1)
(array([[ 2.89714546,  2.89714546]]), array([[ 0.57518277,  0.57518277]]))
&gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,observed)), expected, lambd=2/3.0, axis=0)
(array([[ 2.89714546,  2.89714546]]), array([[ 0.57518277,  0.57518277]]))
</pre> <p>each random variable can have different total count/sum</p> <pre data-language="python">&gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,2*observed)), expected, lambd=2/3.0, axis=0)
(array([[ 2.89714546,  5.79429093]]), array([[ 0.57518277,  0.21504648]]))
&gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,2*observed)), expected, lambd=2/3.0, axis=0)
(array([[ 2.89714546,  5.79429093]]), array([[ 0.57518277,  0.21504648]]))
&gt;&gt;&gt; powerdiscrepancy(np.column_stack((2*observed,2*observed)), expected, lambd=2/3.0, axis=0)
(array([[ 5.79429093,  5.79429093]]), array([[ 0.21504648,  0.21504648]]))
&gt;&gt;&gt; powerdiscrepancy(np.column_stack((2*observed,2*observed)), 20*expected, lambd=2/3.0, axis=0)
(array([[ 5.79429093,  5.79429093]]), array([[ 0.21504648,  0.21504648]]))
&gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,2*observed)), np.column_stack((10*expected,20*expected)), lambd=2/3.0, axis=0)
(array([[ 2.89714546,  5.79429093]]), array([[ 0.57518277,  0.21504648]]))
&gt;&gt;&gt; powerdiscrepancy(np.column_stack((observed,2*observed)), np.column_stack((10*expected,20*expected)), lambd=-1, axis=0)
(array([[ 2.77258872,  5.54517744]]), array([[ 0.59657359,  0.2357868 ]]))
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2009&ndash;2012 Statsmodels Developers<br>&copy; 2006&ndash;2008 Scipy Developers<br>&copy; 2006 Jonathan E. Taylor<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://www.statsmodels.org/stable/generated/statsmodels.stats.gof.powerdiscrepancy.html" class="_attribution-link">http://www.statsmodels.org/stable/generated/statsmodels.stats.gof.powerdiscrepancy.html</a>
  </p>
</div>
