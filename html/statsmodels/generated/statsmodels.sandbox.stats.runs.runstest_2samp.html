<h1 id="statsmodels-sandbox-stats-runs-runstest-2samp">statsmodels.sandbox.stats.runs.runstest_2samp</h1> <dl class="function"> <dt id="statsmodels.sandbox.stats.runs.runstest_2samp">
<code>statsmodels.sandbox.stats.runs.runstest_2samp(x, y=None, groups=None, correction=True)</code> <a class="reference internal" href="http://www.statsmodels.org/stable/_modules/statsmodels/sandbox/stats/runs.html#runstest_2samp"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Wald-Wolfowitz runstest for two samples</p> <p>This tests whether two samples come from the same distribution.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<ul class="first simple"> <li>
<strong>x</strong> (<em>array_like</em>) – data, numeric, contains either one group, if y is also given, or both groups, if additionally a group indicator is provided</li> <li>
<strong>y</strong> (<em>array_like</em><em> (</em><em>optional</em><em>)</em>) – data, numeric</li> <li>
<strong>groups</strong> (<em>array_like</em>) – group labels or indicator the data for both groups is given in a single 1-dimensional array, x. If group labels are not [0,1], then</li> <li>
<strong>correction</strong> (<em>bool</em>) – Following the SAS manual, for samplesize below 50, the test statistic is corrected by 0.5. This can be turned off with correction=False, and was included to match R, tseries, which does not use any correction.</li> </ul> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">

<ul class="simple"> <li>
<strong>z_stat</strong> (<em>float</em>) – test statistic, asymptotically normally distributed</li> <li>
<strong>p-value</strong> (<em>float</em>) – p-value, reject the null hypothesis if it is below an type 1 error level, alpha .</li> </ul> </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>Wald-Wolfowitz runs test.</p> <p>If there are ties, then then the test statistic and p-value that is reported, is based on the higher p-value between sorting all tied observations of the same group</p> <p>This test is intended for continuous distributions SAS has treatment for ties, but not clear, and sounds more complicated (minimum and maximum possible runs prevent use of argsort) (maybe it’s not so difficult, idea: add small positive noise to first one, run test, then to the other, run test, take max(?) p-value - DONE This gives not the minimum and maximum of the number of runs, but should be close. Not true, this is close to minimum but far away from maximum. maximum number of runs would use alternating groups in the ties.) Maybe adding random noise would be the better approach.</p> <p>SAS has exact distribution for sample size &lt;=30, doesn’t look standard but should be easy to add.</p> <p>currently two-sided test only</p> <p>This has not been verified against a reference implementation. In a short Monte Carlo simulation where both samples are normally distribute, the test seems to be correctly sized for larger number of observations (30 or larger), but conservative (i.e. reject less often than nominal) with a sample size of 10 in each group.</p> <div class="admonition seealso"> <p class="first admonition-title">See also</p> <p class="last"><code>runs_test_1samp</code>, <a class="reference internal" href="statsmodels.sandbox.stats.runs.runs.html#statsmodels.sandbox.stats.runs.Runs" title="statsmodels.sandbox.stats.runs.Runs"><code>Runs</code></a>, <code>RunsProb</code></p> </div> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2009&ndash;2012 Statsmodels Developers<br>&copy; 2006&ndash;2008 Scipy Developers<br>&copy; 2006 Jonathan E. Taylor<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://www.statsmodels.org/stable/generated/statsmodels.sandbox.stats.runs.runstest_2samp.html" class="_attribution-link">http://www.statsmodels.org/stable/generated/statsmodels.sandbox.stats.runs.runstest_2samp.html</a>
  </p>
</div>
