<h1 id="statsmodels-stats-inter-rater-fleiss-kappa">statsmodels.stats.inter_rater.fleiss_kappa</h1> <dl class="function"> <dt id="statsmodels.stats.inter_rater.fleiss_kappa">
<code>statsmodels.stats.inter_rater.fleiss_kappa(table, method='fleiss')</code> <a class="reference internal" href="http://www.statsmodels.org/stable/_modules/statsmodels/stats/inter_rater.html#fleiss_kappa"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fleiss’ and Randolph’s kappa multi-rater agreement measure</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr>
<th class="field-name">Parameters:</th>
<td class="field-body">
<ul class="first simple"> <li>
<strong>table</strong> (<em>array_like</em><em>, </em><em>2-D</em>) – assumes subjects in rows, and categories in columns</li> <li>
<strong>method</strong> (<em>string</em>) – Method ‘fleiss’ returns Fleiss’ kappa which uses the sample margin to define the chance outcome. Method ‘randolph’ or ‘uniform’ (only first 4 letters are needed) returns Randolph’s (2005) multirater kappa which assumes a uniform distribution of the categories to define the chance outcome.</li> </ul> </td> </tr> <tr>
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>kappa</strong> – Fleiss’s or Randolph’s kappa statistic for inter rater agreement</p> </td> </tr> <tr>
<th class="field-name">Return type:</th>
<td class="field-body">
<p class="first last">float</p> </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>no variance or hypothesis tests yet</p> <p>Interrater agreement measures like Fleiss’s kappa measure agreement relative to chance agreement. Different authors have proposed ways of defining these chance agreements. Fleiss’ is based on the marginal sample distribution of categories, while Randolph uses a uniform distribution of categories as benchmark. Warrens (2010) showed that Randolph’s kappa is always larger or equal to Fleiss’ kappa. Under some commonly observed condition, Fleiss’ and Randolph’s kappa provide lower and upper bounds for two similar kappa_like measures by Light (1971) and Hubert (1977).</p> <h4 class="rubric">References</h4> <p>Wikipedia <a class="reference external" href="http://en.wikipedia.org/wiki/Fleiss%27_kappa">http://en.wikipedia.org/wiki/Fleiss%27_kappa</a></p> <p>Fleiss, Joseph L. 1971. “Measuring Nominal Scale Agreement among Many Raters.” Psychological Bulletin 76 (5): 378-82. <a class="reference external" href="https://doi.org/10.1037/h0031619">https://doi.org/10.1037/h0031619</a>.</p> <p>Randolph, Justus J. 2005 “Free-Marginal Multirater Kappa (multirater K [free]): An Alternative to Fleiss’ Fixed-Marginal Multirater Kappa.” Presented at the Joensuu Learning and Instruction Symposium, vol. 2005 <a class="reference external" href="https://eric.ed.gov/?id=ED490661">https://eric.ed.gov/?id=ED490661</a></p> <p>Warrens, Matthijs J. 2010. “Inequalities between Multi-Rater Kappas.” Advances in Data Analysis and Classification 4 (4): 271-86. <a class="reference external" href="https://doi.org/10.1007/s11634-010-0073-4">https://doi.org/10.1007/s11634-010-0073-4</a>.</p> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2009&ndash;2012 Statsmodels Developers<br>&copy; 2006&ndash;2008 Scipy Developers<br>&copy; 2006 Jonathan E. Taylor<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://www.statsmodels.org/stable/generated/statsmodels.stats.inter_rater.fleiss_kappa.html" class="_attribution-link">http://www.statsmodels.org/stable/generated/statsmodels.stats.inter_rater.fleiss_kappa.html</a>
  </p>
</div>
