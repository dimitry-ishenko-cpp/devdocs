<h1 id="getting-started-with-kubernetes-provider">  Getting Started with Kubernetes provider </h1> <h2 id="kubernetes">  Kubernetes </h2> <p><a href="https://kubernetes.io/">Kubernetes</a> (K8S) is an open-source workload scheduler with focus on containerized applications.</p> <p>There are at least 2 steps involved in scheduling your first container on a Kubernetes cluster. You need the Kubernetes cluster with all its components running <em>somewhere</em> and then schedule the Kubernetes resources, like Pods, Replication Controllers, Services etc.</p> <p>This guide focuses mainly on the latter part and expects you to have a properly configured &amp; running Kubernetes cluster.</p> <p>The guide also expects you to run the cluster on a cloud provider where Kubernetes can automatically provision a load balancer.</p> <h2 id="why-terraform-">  Why Terraform? </h2> <p>While you could use <code>kubectl</code> or similar CLI-based tools mapped to API calls to manage all Kubernetes resources described in YAML files, orchestration with Terraform presents a few benefits.</p> <ul> <li>Use the same <a href="../../../configuration/syntax.html">configuration language</a> to provision the Kubernetes infrastructure and to deploy applications into it. </li> <li>drift detection - <code>terraform plan</code> will always present you the difference between reality at a given time and config you intend to apply. </li> <li>full lifecycle management - Terraform doesn't just initially create resources, but offers a single command for creation, update, and deletion of tracked resources without needing to inspect the API to identify those resources. </li> <li>synchronous feedback - While asynchronous behaviour is often useful, sometimes it's counter-productive as the job of identifying operation result (failures or details of created resource) is left to the user. e.g. you don't have IP/hostname of load balancer until it has finished provisioning, hence you can't create any DNS record pointing to it. </li> <li>
<a href="../../../internals/graph.html">graph of relationships</a> - Terraform understands relationships between resources which may help in scheduling - e.g. if a Persistent Volume Claim claims space from a particular Persistent Volume Terraform won't even attempt to create the PVC if creation of the PV has failed. </li> </ul> <h2 id="provider-setup">  Provider Setup </h2> <p>The easiest way to configure the provider is by creating/generating a config in a default location (<code>~/.kube/config</code>). That allows you to leave the provider block completely empty.</p> <pre data-language="ruby">provider "kubernetes" {}
</pre>
<p>If you wish to configure the provider statically you can do so</p> <pre data-language="ruby">provider "kubernetes" {
  host     = "https://104.196.242.174"
  username = "ClusterMaster"
  password = "MindTheGap"

  client_certificate     = "${file("~/.kube/client-cert.pem")}"
  client_key             = "${file("~/.kube/client-key.pem")}"
  cluster_ca_certificate = "${file("~/.kube/cluster-ca-cert.pem")}"
}
</pre>
<p>After specifying the provider we may now run the following command to download the latest version of the Kubernetes provider.</p> <pre>$ terraform init


Initializing provider plugins...
- Downloading plugin for provider "kubernetes"...

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.
</pre>
<h2 id="scheduling-a-simple-application">  Scheduling a Simple Application </h2> <p>The main object in any Kubernetes application is <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#what-is-a-pod">a Pod</a>. Pod consists of one or more containers that are placed on cluster nodes based on CPU or memory availability.</p> <p>Here we create a pod with a single container running the nginx web server, exposing port 80 (HTTP) which can be then exposed through the load balancer to the real user.</p> <p>Unlike in this simple example you'd commonly run more than a single instance of your application in production to reach high availability and adding labels will allow Kubernetes to find all pods (instances) for the purpose of forwarding the traffic to the exposed port.</p> <pre data-language="ruby">resource "kubernetes_pod" "nginx" {
  metadata {
    name = "nginx-example"
    labels {
      App = "nginx"
    }
  }

  spec {
    container {
      image = "nginx:1.7.8"
      name  = "example"

      port {
        container_port = 80
      }
    }
  }
}
</pre>
<p>The simplest way to expose your application to users is via <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a>. Service is capable of provisioning a load-balancer in some cloud providers and managing the relationship between pods and that load balancer as new pods are launched and others die for any reason.</p> <pre data-language="ruby">resource "kubernetes_service" "nginx" {
  metadata {
    name = "nginx-example"
  }
  spec {
    selector {
      App = "${kubernetes_pod.nginx.metadata.0.labels.App}"
    }
    port {
      port = 80
      target_port = 80
    }

    type = "LoadBalancer"
  }
}
</pre>
<p>We may also add an output which will expose the IP address to the user</p> <pre data-language="ruby">output "lb_ip" {
  value = "${kubernetes_service.nginx.load_balancer_ingress.0.ip}"
}
</pre>
<p>Please note that this assumes a cloud provider provisioning IP-based load balancer (like in Google Cloud Platform). If you run on a provider with hostname-based load balancer (like in Amazon Web Services) you should use the following snippet instead.</p> <pre data-language="ruby">output "lb_ip" {
  value = "${kubernetes_service.nginx.load_balancer_ingress.0.hostname}"
}
</pre>
<p>The plan will provide you an overview of planned changes, in this case we should see 2 resources (Pod + Service) being added. This commands gets more useful as your infrastructure grows and becomes more complex with more components depending on each other and it's especially helpful during updates.</p> <pre>$ terraform plan

Refreshing Terraform state in-memory prior to plan...
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.

The Terraform execution plan has been generated and is shown below.
Resources are shown in alphabetical order for quick scanning. Green resources
will be created (or destroyed and then created if an existing resource
exists), yellow resources are being changed in-place, and red resources
will be destroyed. Cyan entries are data sources to be read.

Note: You didn't specify an "-out" parameter to save this plan, so when
"apply" is called, Terraform can't guarantee this is what will execute.

  + kubernetes_pod.nginx
      metadata.#:                                  "1"
      metadata.0.generation:                       "&lt;computed&gt;"
      metadata.0.labels.%:                         "1"
      metadata.0.labels.App:                       "nginx"
      metadata.0.name:                             "nginx-example"
      metadata.0.namespace:                        "default"
      metadata.0.resource_version:                 "&lt;computed&gt;"
      metadata.0.self_link:                        "&lt;computed&gt;"
      metadata.0.uid:                              "&lt;computed&gt;"
      spec.#:                                      "1"
      spec.0.automount_service_account_token:      "&lt;computed&gt;"
      spec.0.container.#:                          "1"
      spec.0.container.0.image:                    "nginx:1.7.8"
      spec.0.container.0.image_pull_policy:        "&lt;computed&gt;"
      spec.0.container.0.name:                     "example"
      spec.0.container.0.port.#:                   "1"
      spec.0.container.0.port.0.container_port:    "80"
      spec.0.container.0.port.0.protocol:          "TCP"
      spec.0.container.0.resources.#:              "&lt;computed&gt;"
      spec.0.container.0.stdin:                    "false"
      spec.0.container.0.stdin_once:               "false"
      spec.0.container.0.termination_message_path: "/dev/termination-log"
      spec.0.container.0.tty:                      "false"
      spec.0.dns_policy:                           "ClusterFirst"
      spec.0.host_ipc:                             "false"
      spec.0.host_network:                         "false"
      spec.0.host_pid:                             "false"
      spec.0.hostname:                             "&lt;computed&gt;"
      spec.0.image_pull_secrets.#:                 "&lt;computed&gt;"
      spec.0.node_name:                            "&lt;computed&gt;"
      spec.0.restart_policy:                       "Always"
      spec.0.service_account_name:                 "&lt;computed&gt;"
      spec.0.termination_grace_period_seconds:     "30"

  + kubernetes_service.nginx
      load_balancer_ingress.#:     "&lt;computed&gt;"
      metadata.#:                  "1"
      metadata.0.generation:       "&lt;computed&gt;"
      metadata.0.name:             "nginx-example"
      metadata.0.namespace:        "default"
      metadata.0.resource_version: "&lt;computed&gt;"
      metadata.0.self_link:        "&lt;computed&gt;"
      metadata.0.uid:              "&lt;computed&gt;"
      spec.#:                      "1"
      spec.0.cluster_ip:           "&lt;computed&gt;"
      spec.0.port.#:               "1"
      spec.0.port.0.node_port:     "&lt;computed&gt;"
      spec.0.port.0.port:          "80"
      spec.0.port.0.protocol:      "TCP"
      spec.0.port.0.target_port:   "80"
      spec.0.selector.%:           "1"
      spec.0.selector.App:         "nginx"
      spec.0.session_affinity:     "None"
      spec.0.type:                 "LoadBalancer"


Plan: 2 to add, 0 to change, 0 to destroy.
</pre>
<p>As we're happy with the plan output we may carry on applying proposed changes. <code>terraform apply</code> will take of all the hard work which includes creating resources via API in the right order, supplying any defaults as necessary and waiting for resources to finish provisioning to the point when it can either present useful attributes or a failure (with reason) to the user.</p> <pre>$ terraform apply

kubernetes_pod.nginx: Creating...
  metadata.#:                                  "" =&gt; "1"
  metadata.0.generation:                       "" =&gt; "&lt;computed&gt;"
  metadata.0.labels.%:                         "" =&gt; "1"
  metadata.0.labels.App:                       "" =&gt; "nginx"
  metadata.0.name:                             "" =&gt; "nginx-example"
  metadata.0.namespace:                        "" =&gt; "default"
  metadata.0.resource_version:                 "" =&gt; "&lt;computed&gt;"
  metadata.0.self_link:                        "" =&gt; "&lt;computed&gt;"
  metadata.0.uid:                              "" =&gt; "&lt;computed&gt;"
  spec.#:                                      "" =&gt; "1"
  spec.0.automount_service_account_token:      "" =&gt; "&lt;computed&gt;"
  spec.0.container.#:                          "" =&gt; "1"
  spec.0.container.0.image:                    "" =&gt; "nginx:1.7.8"
  spec.0.container.0.image_pull_policy:        "" =&gt; "&lt;computed&gt;"
  spec.0.container.0.name:                     "" =&gt; "example"
  spec.0.container.0.port.#:                   "" =&gt; "1"
  spec.0.container.0.port.0.container_port:    "" =&gt; "80"
  spec.0.container.0.port.0.protocol:          "" =&gt; "TCP"
  spec.0.container.0.resources.#:              "" =&gt; "&lt;computed&gt;"
  spec.0.container.0.stdin:                    "" =&gt; "false"
  spec.0.container.0.stdin_once:               "" =&gt; "false"
  spec.0.container.0.termination_message_path: "" =&gt; "/dev/termination-log"
  spec.0.container.0.tty:                      "" =&gt; "false"
  spec.0.dns_policy:                           "" =&gt; "ClusterFirst"
  spec.0.host_ipc:                             "" =&gt; "false"
  spec.0.host_network:                         "" =&gt; "false"
  spec.0.host_pid:                             "" =&gt; "false"
  spec.0.hostname:                             "" =&gt; "&lt;computed&gt;"
  spec.0.image_pull_secrets.#:                 "" =&gt; "&lt;computed&gt;"
  spec.0.node_name:                            "" =&gt; "&lt;computed&gt;"
  spec.0.restart_policy:                       "" =&gt; "Always"
  spec.0.service_account_name:                 "" =&gt; "&lt;computed&gt;"
  spec.0.termination_grace_period_seconds:     "" =&gt; "30"
kubernetes_pod.nginx: Creation complete (ID: default/nginx-example)
kubernetes_service.nginx: Creating...
  load_balancer_ingress.#:     "" =&gt; "&lt;computed&gt;"
  metadata.#:                  "" =&gt; "1"
  metadata.0.generation:       "" =&gt; "&lt;computed&gt;"
  metadata.0.name:             "" =&gt; "nginx-example"
  metadata.0.namespace:        "" =&gt; "default"
  metadata.0.resource_version: "" =&gt; "&lt;computed&gt;"
  metadata.0.self_link:        "" =&gt; "&lt;computed&gt;"
  metadata.0.uid:              "" =&gt; "&lt;computed&gt;"
  spec.#:                      "" =&gt; "1"
  spec.0.cluster_ip:           "" =&gt; "&lt;computed&gt;"
  spec.0.port.#:               "" =&gt; "1"
  spec.0.port.0.node_port:     "" =&gt; "&lt;computed&gt;"
  spec.0.port.0.port:          "" =&gt; "80"
  spec.0.port.0.protocol:      "" =&gt; "TCP"
  spec.0.port.0.target_port:   "" =&gt; "80"
  spec.0.selector.%:           "" =&gt; "1"
  spec.0.selector.App:         "" =&gt; "nginx"
  spec.0.session_affinity:     "" =&gt; "None"
  spec.0.type:                 "" =&gt; "LoadBalancer"
kubernetes_service.nginx: Still creating... (10s elapsed)
kubernetes_service.nginx: Still creating... (20s elapsed)
kubernetes_service.nginx: Still creating... (30s elapsed)
kubernetes_service.nginx: Still creating... (40s elapsed)
kubernetes_service.nginx: Creation complete (ID: default/nginx-example)

Apply complete! Resources: 2 added, 0 changed, 0 destroyed.

The state of your infrastructure has been saved to the path
below. This state is required to modify and destroy your
infrastructure, so keep it safe. To inspect the complete state
use the `terraform show` command.

State path:

Outputs:

lb_ip = 35.197.9.247
</pre>
<p>You may now enter that IP address to your favourite browser and you should see the nginx welcome page.</p> <p>The <a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/">Kubernetes UI</a> provides another way to check both the pod and the service there once they're scheduled.</p> <h2 id="reaching-scalability-and-availability">  Reaching Scalability and Availability </h2> <p>The Replication Controller allows you to replicate pods. This is useful for maintaining overall availability and scalability of your application exposed to the user.</p> <p>We can just replace our Pod with RC from the previous config and keep the Service there.</p> <pre data-language="ruby">resource "kubernetes_replication_controller" "nginx" {
  metadata {
    name = "scalable-nginx-example"
    labels {
      App = "ScalableNginxExample"
    }
  }

  spec {
    replicas = 2
    selector {
      App = "ScalableNginxExample"
    }
    template {
      container {
        image = "nginx:1.7.8"
        name  = "example"

        port {
          container_port = 80
        }

        resources {
          limits {
            cpu    = "0.5"
            memory = "512Mi"
          }
          requests {
            cpu    = "250m"
            memory = "50Mi"
          }
        }
      }
    }
  }
}

resource "kubernetes_service" "nginx" {
  metadata {
    name = "nginx-example"
  }
  spec {
    selector {
      App = "${kubernetes_replication_controller.nginx.metadata.0.labels.App}"
    }
    port {
      port = 80
      target_port = 80
    }

    type = "LoadBalancer"
  }
}
</pre>
<p>You may notice we also specified how much CPU and memory do we expect single instance of that application to consume. This is incredibly helpful for Kubernetes as it helps avoiding under-provisioning or over-provisioning that would result in either unused resources (costing money) or lack of resources (causing the app to crash or slow down).</p> <pre>$ terraform plan

# ...

Plan: 2 to add, 0 to change, 0 to destroy.
</pre>
<pre>$ terraform apply

kubernetes_replication_controller.nginx: Creating...
# ...
kubernetes_replication_controller.nginx: Creation complete (ID: default/scalable-nginx-example)
kubernetes_service.nginx: Creating...
# ...
kubernetes_service.nginx: Still creating... (10s elapsed)
kubernetes_service.nginx: Still creating... (20s elapsed)
kubernetes_service.nginx: Still creating... (30s elapsed)
kubernetes_service.nginx: Still creating... (40s elapsed)
kubernetes_service.nginx: Still creating... (50s elapsed)
kubernetes_service.nginx: Creation complete (ID: default/nginx-example)

Apply complete! Resources: 2 added, 0 changed, 0 destroyed.

The state of your infrastructure has been saved to the path
below. This state is required to modify and destroy your
infrastructure, so keep it safe. To inspect the complete state
use the `terraform show` command.

State path:

Outputs:

lb_ip = 35.197.9.247
</pre>
<p>Unlike in previous example, the IP address here will direct traffic to one of the 2 pods scheduled in the cluster.</p> <h3 id="updating-configuration">  Updating Configuration </h3> <p>As our application user-base grows we might need more instances to be scheduled. The easiest way to achieve this is to increase <code>replicas</code> field in the config accordingly.</p> <pre data-language="ruby">resource "kubernetes_replication_controller" "example" {
# ...

  spec {
    replicas = 5

# ...

}
</pre>
<p>You can verify before hitting the API that you're only changing what you intended to change and that someone else didn't modify the resource you created earlier.</p> <pre>$ terraform plan

Refreshing Terraform state in-memory prior to plan...
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.

kubernetes_replication_controller.nginx: Refreshing state... (ID: default/scalable-nginx-example)
kubernetes_service.nginx: Refreshing state... (ID: default/nginx-example)

The Terraform execution plan has been generated and is shown below.
Resources are shown in alphabetical order for quick scanning. Green resources
will be created (or destroyed and then created if an existing resource
exists), yellow resources are being changed in-place, and red resources
will be destroyed. Cyan entries are data sources to be read.

Note: You didn't specify an "-out" parameter to save this plan, so when
"apply" is called, Terraform can't guarantee this is what will execute.

  ~ kubernetes_replication_controller.nginx
      spec.0.replicas: "2" =&gt; "5"


Plan: 0 to add, 1 to change, 0 to destroy.
</pre>
<p>As we're happy with the proposed plan, we can just apply that change.</p> <pre>$ terraform apply
</pre>
<p>and 3 more replicas will be scheduled &amp; attached to the load balancer.</p> <h2 id="bonus-managing-quotas-and-limits">  Bonus: Managing Quotas and Limits </h2> <p>As an operator managing cluster you're likely also responsible for using the cluster responsibly and fairly within teams.</p> <p>Resource Quotas and Limit Ranges both offer ways to put constraints in place around CPU, memory, disk space and other resources that will be consumed by cluster users.</p> <p>Resource Quota can constrain the whole namespace</p> <pre data-language="ruby">resource "kubernetes_resource_quota" "example" {
  metadata {
    name = "terraform-example"
  }
  spec {
    hard {
      pods = 10
    }
    scopes = ["BestEffort"]
  }
}
</pre>
<p>whereas Limit Range can impose limits on a specific resource type (e.g. Pod or Persistent Volume Claim).</p> <pre data-language="ruby">resource "kubernetes_limit_range" "example" {
    metadata {
        name = "terraform-example"
    }
    spec {
        limit {
            type = "Pod"
            max {
                cpu = "200m"
                memory = "1024M"
            }
        }
        limit {
            type = "PersistentVolumeClaim"
            min {
                storage = "24M"
            }
        }
        limit {
            type = "Container"
            default {
                cpu = "50m"
                memory = "24M"
            }
        }
    }
}
</pre>
<pre>$ terraform plan
</pre>
<pre>$ terraform apply
</pre>
<h2 id="conclusion">  Conclusion </h2> <p>Terraform offers you an effective way to manage both compute for your Kubernetes cluster and Kubernetes resources. Check out the extensive documentation of the Kubernetes provider linked from the menu.</p><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2018 HashiCorp</br>Licensed under the MPL 2.0 License.<br>
    <a href="https://www.terraform.io/docs/providers/kubernetes/guides/getting-started.html" class="_attribution-link">https://www.terraform.io/docs/providers/kubernetes/guides/getting-started.html</a>
  </p>
</div>
