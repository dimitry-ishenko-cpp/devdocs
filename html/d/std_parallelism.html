<h1>std.parallelism</h1>  <p><code>std.parallelism</code> implements high-level primitives for SMP parallelism. These include parallel foreach, parallel reduce, parallel eager map, pipelining and future/promise parallelism. <code>std.parallelism</code> is recommended when the same operation is to be executed in parallel on different data, or when a function is to be executed in a background thread and its result returned to a well-defined main thread. For communication between arbitrary threads, see <code>std.concurrency</code>. </p>
<p><code>std.parallelism</code> is based on the concept of a <code>Task</code>. A <code>Task</code> is an object that represents the fundamental unit of work in this library and may be executed in parallel with any other <code>Task</code>. Using <code>Task</code> directly allows programming with a future/promise paradigm. All other supported parallelism paradigms (parallel foreach, map, reduce, pipelining) represent an additional level of abstraction over <code>Task</code>. They automatically create one or more <code>Task</code> objects, or closely related types that are conceptually identical but not part of the public API. <br><br> After creation, a <code>Task</code> may be executed in a new thread, or submitted to a <code>TaskPool</code> for execution. A <code>TaskPool</code> encapsulates a task queue and its worker threads. Its purpose is to efficiently map a large number of <code>Task</code>s onto a smaller number of threads. A task queue is a FIFO queue of <code>Task</code> objects that have been submitted to the <code>TaskPool</code> and are awaiting execution. A worker thread is a thread that is associated with exactly one task queue. It executes the <code>Task</code> at the front of its queue when the queue has work available, or sleeps when no work is available. Each task queue is associated with zero or more worker threads. If the result of a <code>Task</code> is needed before execution by a worker thread has begun, the <code>Task</code> can be removed from the task queue and executed immediately in the thread where the result is needed. </p> <dl>
<dt>Warning</dt>
<dd> Unless marked as <code>@trusted</code> or <code>@safe</code>, artifacts in this module allow implicit data sharing between threads and cannot guarantee that client code is free from low level data races. </dd>
</dl> <dl>
<dt>Source</dt>
<dd> <span class="phobos_src"><a class="https" href="https://github.com/dlang/phobos/blob/master/std/parallelism.d">std/parallelism.d</a></span> </dd>
</dl> <dl>
<dt>Author</dt>
<dd> David Simcha </dd>
</dl> <dl>
<dt>License:</dt>
<dd><a href="http://boost.org/LICENSE_1_0.txt">Boost License 1.0</a></dd>
</dl> <dl>
<dt class="d_decl" id="Task">struct <strong id="Task">Task</strong>(alias fun, Args...); </dt> <dd>
<p><code>Task</code> represents the fundamental unit of work. A <code>Task</code> may be executed in parallel with any other <code>Task</code>. Using this struct directly allows future/promise parallelism. In this paradigm, a function (or delegate or other callable) is executed in a thread other than the one it was called from. The calling thread does not block while the function is being executed. A call to <code>workForce</code>, <code>yieldForce</code>, or <code>spinForce</code> is used to ensure that the <code>Task</code> has finished executing and to obtain the return value, if any. These functions and <code>done</code> also act as full memory barriers, meaning that any memory writes made in the thread that executed the <code>Task</code> are guaranteed to be visible in the calling thread after one of these functions returns. </p>
<p>The <a href="std_parallelism.html#task"><code>std.parallelism.task</code></a> and <a href="std_parallelism.html#scopedTask"><code>std.parallelism.scopedTask</code></a> functions can be used to create an instance of this struct. See <code>task</code> for usage examples. <br><br> Function results are returned from <code>yieldForce</code>, <code>spinForce</code> and <code>workForce</code> by ref. If <code>fun</code> returns by ref, the reference will point to the returned reference of <code>fun</code>. Otherwise it will point to a field in this struct. <br><br> Copying of this struct is disabled, since it would provide no useful semantics. If you want to pass this struct around, you should do so by reference or pointer. </p> <dl>
<dt>Bugs:</dt>
<dd>Changes to <code>ref</code> and <code>out</code> arguments are not propagated to the call site, only to <code>args</code> in this struct.</dd>
</dl> <dl>
<dt class="d_decl" id="Task.args">alias <strong id="args">args</strong> = _args[1 .. __dollar]; </dt> <dd>
<p>The arguments the function was called with. Changes to <code>out</code> and <code>ref</code> arguments will be visible here.</p> </dd> <dt class="d_decl" id="Task.ReturnType">alias <strong id="ReturnType">ReturnType</strong> = typeof(fun(_args)); </dt> <dd>
<p>The return type of the function called by this <code>Task</code>. This can be <code>void</code>.</p> </dd> <dt class="d_decl" id="Task.spinForce">@property ref @trusted ReturnType <strong id="spinForce">spinForce</strong>(); </dt> <dd>
<p>If the <code>Task</code> isn't started yet, execute it in the current thread. If it's done, return its return value, if any. If it's in progress, busy spin until it's done, then return the return value. If it threw an exception, rethrow that exception. </p>
<p>This function should be used when you expect the result of the <code>Task</code> to be available on a timescale shorter than that of an OS context switch.</p> </dd> <dt class="d_decl" id="Task.yieldForce">@property ref @trusted ReturnType <strong id="yieldForce">yieldForce</strong>(); </dt> <dd>
<p>If the <code>Task</code> isn't started yet, execute it in the current thread. If it's done, return its return value, if any. If it's in progress, wait on a condition variable. If it threw an exception, rethrow that exception. </p>
<p>This function should be used for expensive functions, as waiting on a condition variable introduces latency, but avoids wasted CPU cycles.</p> </dd> <dt class="d_decl" id="Task.workForce">@property ref @trusted ReturnType <strong id="workForce">workForce</strong>(); </dt> <dd>
<p>If this <code>Task</code> was not started yet, execute it in the current thread. If it is finished, return its result. If it is in progress, execute any other <code>Task</code> from the <code>TaskPool</code> instance that this <code>Task</code> was submitted to until this one is finished. If it threw an exception, rethrow that exception. If no other tasks are available or this <code>Task</code> was executed using <code>executeInNewThread</code>, wait on a condition variable.</p> </dd> <dt class="d_decl" id="Task.done">@property @trusted bool <strong id="done">done</strong>(); </dt> <dd>
<p>Returns <code>true</code> if the <code>Task</code> is finished executing. </p>
<dl>
<dt>Throws:</dt>
<dd>Rethrows any exception thrown during the execution of the <code>Task</code>.</dd>
</dl> </dd> <dt class="d_decl" id="Task.executeInNewThread">@trusted void <strong id="executeInNewThread">executeInNewThread</strong>(); <br><br>@trusted void <strong id="executeInNewThread">executeInNewThread</strong>(int priority); </dt> <dd>
<p>Create a new thread for executing this <code>Task</code>, execute it in the newly created thread, then terminate the thread. This can be used for future/promise parallelism. An explicit priority may be given to the <code>Task</code>. If one is provided, its value is forwarded to <code>core.thread.Thread.priority</code>. See <a href="std_parallelism.html#task"><code>std.parallelism.task</code></a> for usage example.</p> </dd> </dl> </dd> <dt class="d_decl" id="task">auto <strong id="task">task</strong>(alias fun, Args...)(Args args); </dt> <dd>
<p>Creates a <code>Task</code> on the GC heap that calls an alias. This may be executed via <code>Task.executeInNewThread</code> or by submitting to a <a href="std_parallelism.html#TaskPool"><code>std.parallelism.TaskPool</code></a>. A globally accessible instance of <code>TaskPool</code> is provided by <a href="std_parallelism.html#taskPool"><code>std.parallelism.taskPool</code></a>. </p>
<dl>
<dt>Returns:</dt>
<dd>A pointer to the <code>Task</code>. </dd>
</dl> <dl>
<dt>Example</dt>

</dl>
<pre data-language="d">// Read two files into memory at the same time.
import std.file;

void main()
{
    // Create and execute a Task for reading
    // foo.txt.
    auto file1Task = task!read("foo.txt");
    file1Task.executeInNewThread();

    // Read bar.txt in parallel.
    auto file2Data = read("bar.txt");

    // Get the results of reading foo.txt.
    auto file1Data = file1Task.yieldForce;
}
</pre>  <pre data-language="d">// Sorts an array using a parallel quick sort algorithm.
// The first partition is done serially.  Both recursion
// branches are then executed in parallel.
//
// Timings for sorting an array of 1,000,000 doubles on
// an Athlon 64 X2 dual core machine:
//
// This implementation:               176 milliseconds.
// Equivalent serial implementation:  280 milliseconds
void parallelSort(T)(T[] data)
{
    // Sort small subarrays serially.
    if (data.length &lt; 100)
    {
         std.algorithm.sort(data);
         return;
    }

    // Partition the array.
    swap(data[$ / 2], data[$ - 1]);
    auto pivot = data[$ - 1];
    bool lessThanPivot(T elem) { return elem &lt; pivot; }

    auto greaterEqual = partition!lessThanPivot(data[0..$ - 1]);
    swap(data[$ - greaterEqual.length - 1], data[$ - 1]);

    auto less = data[0..$ - greaterEqual.length - 1];
    greaterEqual = data[$ - greaterEqual.length..$];

    // Execute both recursion branches in parallel.
    auto recurseTask = task!parallelSort(greaterEqual);
    taskPool.put(recurseTask);
    parallelSort(less);
    recurseTask.yieldForce;
}
</pre>  </dd> <dt class="d_decl" id="task.2">auto <strong id="task">task</strong>(F, Args...)(F delegateOrFp, Args args)<br><small>  Constraints: if (is(typeof(delegateOrFp(args))) &amp;&amp; !isSafeTask!F); </small>
</dt> <dd>
<p>Creates a <code>Task</code> on the GC heap that calls a function pointer, delegate, or class/struct with overloaded opCall. </p>
<dl>
<dt>Example</dt>

</dl>
<pre data-language="d">// Read two files in at the same time again,
// but this time use a function pointer instead
// of an alias to represent std.file.read.
import std.file;

void main()
{
    // Create and execute a Task for reading
    // foo.txt.
    auto file1Task = task(&amp;read!string, "foo.txt", size_t.max);
    file1Task.executeInNewThread();

    // Read bar.txt in parallel.
    auto file2Data = read("bar.txt");

    // Get the results of reading foo.txt.
    auto file1Data = file1Task.yieldForce;
}
</pre>  <dl>
<dt>Notes</dt>
<dd> This function takes a non-scope delegate, meaning it can be used with closures. If you can't allocate a closure due to objects on the stack that have scoped destruction, see <code>scopedTask</code>, which takes a scope delegate.</dd>
</dl> </dd> <dt class="d_decl" id="task.3">@trusted auto <strong id="task">task</strong>(F, Args...)(F fun, Args args)<br><small>  Constraints: if (is(typeof(fun(args))) &amp;&amp; isSafeTask!F); </small>
</dt> <dd>
<p>Version of <code>task</code> usable from <code>@safe</code> code. Usage mechanics are identical to the non-@safe case, but safety introduces some restrictions: </p>
<p><ol>
<li>
<code>fun</code> must be @safe or @trusted. </li> <li>
<code>F</code> must not have any unshared aliasing as defined by <a href="std_traits.html#hasUnsharedAliasing"><code>std.traits.hasUnsharedAliasing</code></a>. This means it may not be an unshared delegate or a non-shared class or struct with overloaded <code>opCall</code>. This also precludes accepting template alias parameters. </li> <li>
<code>Args</code> must not have unshared aliasing. </li> <li>
<code>fun</code> must not return by reference. </li> <li>The return type must not have unshared aliasing unless <code>fun</code> is <code>pure</code> or the <code>Task</code> is executed via <code>executeInNewThread</code> instead of using a <code>TaskPool</code>.</li> </ol></p> </dd> <dt class="d_decl" id="scopedTask">auto <strong id="scopedTask">scopedTask</strong>(alias fun, Args...)(Args args); <br><br>auto <strong id="scopedTask">scopedTask</strong>(F, Args...)(scope F delegateOrFp, Args args)<br><small>  Constraints: if (is(typeof(delegateOrFp(args))) &amp;&amp; !isSafeTask!F); </small><br><br>@trusted auto <strong id="scopedTask">scopedTask</strong>(F, Args...)(F fun, Args args)<br><small>  Constraints: if (is(typeof(fun(args))) &amp;&amp; isSafeTask!F); </small>
</dt> <dd>
<p>These functions allow the creation of <code>Task</code> objects on the stack rather than the GC heap. The lifetime of a <code>Task</code> created by <code>scopedTask</code> cannot exceed the lifetime of the scope it was created in. </p>
<p><code>scopedTask</code> might be preferred over <code>task</code>: <ol>
<li>When a <code>Task</code> that calls a delegate is being created and a closure cannot be allocated due to objects on the stack that have scoped destruction. The delegate overload of <code>scopedTask</code> takes a <code>scope</code> delegate. </li> <li>As a micro-optimization, to avoid the heap allocation associated with <code>task</code> or with the creation of a closure. </li> </ol> Usage is otherwise identical to <code>task</code>. </p> <dl>
<dt>Notes</dt>
<dd> <code>Task</code> objects created using <code>scopedTask</code> will automatically call <code>Task.yieldForce</code> in their destructor if necessary to ensure the <code>Task</code> is complete before the stack frame they reside on is destroyed.</dd>
</dl> </dd> <dt class="d_decl" id="totalCPUs">alias <strong id="totalCPUs">totalCPUs</strong> = __lazilyInitializedConstant!(immutable(uint), 4294967295u, totalCPUsImpl).__lazilyInitializedConstant; </dt> <dd>
<p>The total number of CPU cores available on the current machine, as reported by the operating system.</p> </dd> <dt class="d_decl" id="TaskPool">class <strong id="TaskPool">TaskPool</strong>; </dt> <dd>
<p>This class encapsulates a task queue and a set of worker threads. Its purpose is to efficiently map a large number of <code>Task</code>s onto a smaller number of threads. A task queue is a FIFO queue of <code>Task</code> objects that have been submitted to the <code>TaskPool</code> and are awaiting execution. A worker thread is a thread that executes the <code>Task</code> at the front of the queue when one is available and sleeps when the queue is empty. </p>
<p>This class should usually be used via the global instantiation available via the <a href="std_parallelism.html#taskPool"><code>std.parallelism.taskPool</code></a> property. Occasionally it is useful to explicitly instantiate a <code>TaskPool</code>: <ol>
<li>When you want <code>TaskPool</code> instances with multiple priorities, for example a low priority pool and a high priority pool. </li> <li>When the threads in the global task pool are waiting on a synchronization primitive (for example a mutex), and you want to parallelize the code that needs to run before these threads can be resumed. </li> </ol> </p> <dl>
<dt>Note</dt>
<dd> The worker threads in this pool will not stop until <code>stop</code> or <code>finish</code> is called, even if the main thread has finished already. This may lead to programs that never end. If you do not want this behaviour, you can set <code>isDaemon</code> to true.</dd>
</dl> <dl>
<dt class="d_decl" id="TaskPool.this">@trusted this(); </dt> <dd>
<p>Default constructor that initializes a <code>TaskPool</code> with <code>totalCPUs</code> - 1 worker threads. The minus 1 is included because the main thread will also be available to do work. </p>
<dl>
<dt>Note</dt>
<dd> On single-core machines, the primitives provided by <code>TaskPool</code> operate transparently in single-threaded mode.</dd>
</dl> </dd> <dt class="d_decl" id="TaskPool.this.2">@trusted this(size_t nWorkers); </dt> <dd>
<p>Allows for custom number of worker threads.</p> </dd> <dt class="d_decl" id="TaskPool.parallel">ParallelForeach!R <strong id="parallel">parallel</strong>(R)(R range, size_t workUnitSize); <br><br>ParallelForeach!R <strong id="parallel">parallel</strong>(R)(R range); </dt> <dd>
<p>Implements a parallel foreach loop over a range. This works by implicitly creating and submitting one <code>Task</code> to the <code>TaskPool</code> for each worker thread. A work unit is a set of consecutive elements of <code>range</code> to be processed by a worker thread between communication with any other thread. The number of elements processed per work unit is controlled by the <code>workUnitSize</code> parameter. Smaller work units provide better load balancing, but larger work units avoid the overhead of communicating with other threads frequently to fetch the next work unit. Large work units also avoid false sharing in cases where the range is being modified. The less time a single iteration of the loop takes, the larger <code>workUnitSize</code> should be. For very expensive loop bodies, <code>workUnitSize</code> should be 1. An overload that chooses a default work unit size is also available. </p>
<dl>
<dt>Example</dt>

</dl>
<pre data-language="d">// Find the logarithm of every number from 1 to
// 10_000_000 in parallel.
auto logs = new double[10_000_000];

// Parallel foreach works with or without an index
// variable.  It can be iterate by ref if range.front
// returns by ref.

// Iterate over logs using work units of size 100.
foreach (i, ref elem; taskPool.parallel(logs, 100))
{
    elem = log(i + 1.0);
}

// Same thing, but use the default work unit size.
//
// Timings on an Athlon 64 X2 dual core machine:
//
// Parallel foreach:  388 milliseconds
// Regular foreach:   619 milliseconds
foreach (i, ref elem; taskPool.parallel(logs))
{
    elem = log(i + 1.0);
}
</pre>  <dl>
<dt>Notes</dt>
<dd> The memory usage of this implementation is guaranteed to be constant in <code>range.length</code>. </dd>
</dl> Breaking from a parallel foreach loop via a break, labeled break, labeled continue, return or goto statement throws a <code>ParallelForeachError</code>.  In the case of non-random access ranges, parallel foreach buffers lazily to an array of size <code>workUnitSize</code> before executing the parallel portion of the loop. The exception is that, if a parallel foreach is executed over a range returned by <code>asyncBuf</code> or <code>map</code>, the copying is elided and the buffers are simply swapped. In this case <code>workUnitSize</code> is ignored and the work unit size is set to the buffer size of <code>range</code>.  A memory barrier is guaranteed to be executed on exit from the loop, so that results produced by all threads are visible in the calling thread.  <b>Exception Handling</b>:  When at least one exception is thrown from inside a parallel foreach loop, the submission of additional <code>Task</code> objects is terminated as soon as possible, in a non-deterministic manner. All executing or enqueued work units are allowed to complete. Then, all exceptions that were thrown by any work unit are chained using <code>Throwable.next</code> and rethrown. The order of the exception chaining is non-deterministic. </dd> <dt class="d_decl" id="TaskPool.amap">template <strong id="amap">amap</strong>(functions...)</dt> <dd> <dl>
<dt class="d_decl" id="TaskPool.amap.amap">auto <strong id="amap">amap</strong>(Args...)(Args args)<br><small>  Constraints: if (isRandomAccessRange!(Args[0])); </small>
</dt> <dd>
<p>Eager parallel map. The eagerness of this function means it has less overhead than the lazily evaluated <code>TaskPool.map</code> and should be preferred where the memory requirements of eagerness are acceptable. <code>functions</code> are the functions to be evaluated, passed as template alias parameters in a style similar to <a href="std_algorithm_iteration.html#map"><code>std.algorithm.iteration.map</code></a>. The first argument must be a random access range. For performance reasons, amap will assume the range elements have not yet been initialized. Elements will be overwritten without calling a destructor nor doing an assignment. As such, the range must not contain meaningful data: either un-initialized objects, or objects in their <code>.init</code> state. </p>
<p><pre data-language="d">auto numbers = iota(100_000_000.0);

// Find the square roots of numbers.
//
// Timings on an Athlon 64 X2 dual core machine:
//
// Parallel eager map:                   0.802 s
// Equivalent serial implementation:     1.768 s
auto squareRoots = taskPool.amap!sqrt(numbers);
</pre> <br><br> Immediately after the range argument, an optional work unit size argument may be provided. Work units as used by <code>amap</code> are identical to those defined for parallel foreach. If no work unit size is provided, the default work unit size is used. <br><br> <pre data-language="d">// Same thing, but make work unit size 100.
auto squareRoots = taskPool.amap!sqrt(numbers, 100);
</pre> <br><br> An output range for returning the results may be provided as the last argument. If one is not provided, an array of the proper type will be allocated on the garbage collected heap. If one is provided, it must be a random access range with assignable elements, must have reference semantics with respect to assignment to its elements, and must have the same length as the input range. Writing to adjacent elements from different threads must be safe. <br><br> <pre data-language="d">// Same thing, but explicitly allocate an array
// to return the results in.  The element type
// of the array may be either the exact type
// returned by functions or an implicit conversion
// target.
auto squareRoots = new float[numbers.length];
taskPool.amap!sqrt(numbers, squareRoots);

// Multiple functions, explicit output range, and
// explicit work unit size.
auto results = new Tuple!(float, real)[numbers.length];
taskPool.amap!(sqrt, log)(numbers, 100, results);
</pre> </p> <dl>
<dt>Note</dt>
<dd> A memory barrier is guaranteed to be executed after all results are written but before returning so that results produced by all threads are visible in the calling thread. </dd>
</dl> <dl>
<dt>Tips</dt>
<dd> To perform the mapping operation in place, provide the same range for the input and output range. </dd>
</dl> To parallelize the copying of a range with expensive to evaluate elements to an array, pass an identity function (a function that just returns whatever argument is provided to it) to <code>amap</code>.  <b>Exception Handling</b>:  When at least one exception is thrown from inside the map functions, the submission of additional <code>Task</code> objects is terminated as soon as possible, in a non-deterministic manner. All currently executing or enqueued work units are allowed to complete. Then, all exceptions that were thrown from any work unit are chained using <code>Throwable.next</code> and rethrown. The order of the exception chaining is non-deterministic. </dd> </dl> </dd> <dt class="d_decl" id="TaskPool.map">template <strong id="map">map</strong>(functions...)</dt> <dd> <dl>
<dt class="d_decl" id="TaskPool.map.map">auto <strong id="map">map</strong>(S)(S source, size_t bufSize = 100, size_t workUnitSize = size_t.max)<br><small>  Constraints: if (isInputRange!S); </small>
</dt> <dd>
<p>A semi-lazy parallel map that can be used for pipelining. The map functions are evaluated for the first <code>bufSize</code> elements and stored in a buffer and made available to <code>popFront</code>. Meanwhile, in the background a second buffer of the same size is filled. When the first buffer is exhausted, it is swapped with the second buffer and filled while the values from what was originally the second buffer are read. This implementation allows for elements to be written to the buffer without the need for atomic operations or synchronization for each write, and enables the mapping function to be evaluated efficiently in parallel. </p>
<p><code>map</code> has more overhead than the simpler procedure used by <code>amap</code> but avoids the need to keep all results in memory simultaneously and works with non-random access ranges. </p> <dl>
<dt>Parameters:</dt>
<dd><table>
<tr>
<td>S <code>source</code>
</td> <td>The <a href="std_range_primitives.html#isInputRange">input range</a> to be mapped. If <code>source</code> is not random access it will be lazily buffered to an array of size <code>bufSize</code> before the map function is evaluated. (For an exception to this rule, see Notes.)</td>
</tr> <tr>
<td>size_t <code>bufSize</code>
</td> <td>The size of the buffer to store the evaluated elements.</td>
</tr> <tr>
<td>size_t <code>workUnitSize</code>
</td> <td>The number of elements to evaluate in a single <code>Task</code>. Must be less than or equal to <code>bufSize</code>, and should be a fraction of <code>bufSize</code> such that all worker threads can be used. If the default of size_t.max is used, workUnitSize will be set to the pool-wide default.</td>
</tr> </table></dd>
</dl> <dl>
<dt>Returns:</dt>
<dd>An input range representing the results of the map. This range has a length iff <code>source</code> has a length. </dd>
</dl> <dl>
<dt>Notes</dt>
<dd> If a range returned by <code>map</code> or <code>asyncBuf</code> is used as an input to <code>map</code>, then as an optimization the copying from the output buffer of the first range to the input buffer of the second range is elided, even though the ranges returned by <code>map</code> and <code>asyncBuf</code> are non-random access ranges. This means that the <code>bufSize</code> parameter passed to the current call to <code>map</code> will be ignored and the size of the buffer will be the buffer size of <code>source</code>. </dd>
</dl> <dl>
<dt>Example</dt>

</dl>
<pre data-language="d">// Pipeline reading a file, converting each line
// to a number, taking the logarithms of the numbers,
// and performing the additions necessary to find
// the sum of the logarithms.

auto lineRange = File("numberList.txt").byLine();
auto dupedLines = std.algorithm.map!"a.idup"(lineRange);
auto nums = taskPool.map!(to!double)(dupedLines);
auto logs = taskPool.map!log10(nums);

double sum = 0;
foreach (elem; logs)
{
    sum += elem;
}
</pre>  <b>Exception Handling</b>:  Any exceptions thrown while iterating over <code>source</code> or computing the map function are re-thrown on a call to <code>popFront</code> or, if thrown during construction, are simply allowed to propagate to the caller. In the case of exceptions thrown while computing the map function, the exceptions are chained as in <code>TaskPool.amap</code>. </dd> </dl> </dd> <dt class="d_decl" id="TaskPool.asyncBuf">auto <strong id="asyncBuf">asyncBuf</strong>(S)(S source, size_t bufSize = 100)<br><small>  Constraints: if (isInputRange!S); </small>
</dt> <dd>
<p>Given a <code>source</code> range that is expensive to iterate over, returns an <a href="std_range_primitives.html#isInputRange">input range</a> that asynchronously buffers the contents of <code>source</code> into a buffer of <code>bufSize</code> elements in a worker thread, while making previously buffered elements from a second buffer, also of size <code>bufSize</code>, available via the range interface of the returned object. The returned range has a length iff <code>hasLength!S</code>. <code>asyncBuf</code> is useful, for example, when performing expensive operations on the elements of ranges that represent data on a disk or network. </p>
<dl>
<dt>Example</dt>

</dl>
<pre data-language="d">import std.conv, std.stdio;

void main()
{
    // Fetch lines of a file in a background thread
    // while processing previously fetched lines,
    // dealing with byLine's buffer recycling by
    // eagerly duplicating every line.
    auto lines = File("foo.txt").byLine();
    auto duped = std.algorithm.map!"a.idup"(lines);

    // Fetch more lines in the background while we
    // process the lines already read into memory
    // into a matrix of doubles.
    double[][] matrix;
    auto asyncReader = taskPool.asyncBuf(duped);

    foreach (line; asyncReader)
    {
        auto ls = line.split("\t");
        matrix ~= to!(double[])(ls);
    }
}
</pre>  <b>Exception Handling</b>:  Any exceptions thrown while iterating over <code>source</code> are re-thrown on a call to <code>popFront</code> or, if thrown during construction, simply allowed to propagate to the caller. </dd> <dt class="d_decl" id="TaskPool.asyncBuf.2">auto <strong id="asyncBuf">asyncBuf</strong>(C1, C2)(C1 next, C2 empty, size_t initialBufSize = 0, size_t nBuffers = 100)<br><small>  Constraints: if (is(typeof(C2.init()) : bool) &amp;&amp; (Parameters!C1.length == 1) &amp;&amp; (Parameters!C2.length == 0) &amp;&amp; isArray!(Parameters!C1[0])); </small>
</dt> <dd>
<p>Given a callable object <code>next</code> that writes to a user-provided buffer and a second callable object <code>empty</code> that determines whether more data is available to write via <code>next</code>, returns an input range that asynchronously calls <code>next</code> with a set of size <code>nBuffers</code> of buffers and makes the results available in the order they were obtained via the input range interface of the returned object. Similarly to the input range overload of <code>asyncBuf</code>, the first half of the buffers are made available via the range interface while the second half are filled and vice-versa. </p>
<dl>
<dt>Parameters:</dt>
<dd><table>
<tr>
<td>C1 <code>next</code>
</td> <td>A callable object that takes a single argument that must be an array with mutable elements. When called, <code>next</code> writes data to the array provided by the caller.</td>
</tr> <tr>
<td>C2 <code>empty</code>
</td> <td>A callable object that takes no arguments and returns a type implicitly convertible to <code>bool</code>. This is used to signify that no more data is available to be obtained by calling <code>next</code>.</td>
</tr> <tr>
<td>size_t <code>initialBufSize</code>
</td> <td>The initial size of each buffer. If <code>next</code> takes its array by reference, it may resize the buffers.</td>
</tr> <tr>
<td>size_t <code>nBuffers</code>
</td> <td>The number of buffers to cycle through when calling <code>next</code>.</td>
</tr> </table></dd>
</dl> <dl>
<dt>Example</dt>

</dl>
<pre data-language="d">// Fetch lines of a file in a background
// thread while processing previously fetched
// lines, without duplicating any lines.
auto file = File("foo.txt");

void next(ref char[] buf)
{
    file.readln(buf);
}

// Fetch more lines in the background while we
// process the lines already read into memory
// into a matrix of doubles.
double[][] matrix;
auto asyncReader = taskPool.asyncBuf(&amp;next, &amp;file.eof);

foreach (line; asyncReader)
{
    auto ls = line.split("\t");
    matrix ~= to!(double[])(ls);
}
</pre>  <b>Exception Handling</b>:  Any exceptions thrown while iterating over <code>range</code> are re-thrown on a call to <code>popFront</code>.  <dl>
<dt>Warning</dt>
<dd> Using the range returned by this function in a parallel foreach loop will not work because buffers may be overwritten while the task that processes them is in queue. This is checked for at compile time and will result in a static assertion failure.</dd>
</dl> </dd> <dt class="d_decl" id="TaskPool.reduce">template <strong id="reduce">reduce</strong>(functions...)</dt> <dd> <dl>
<dt class="d_decl" id="TaskPool.reduce.reduce">auto <strong id="reduce">reduce</strong>(Args...)(Args args); </dt> <dd>
<p>Parallel reduce on a random access range. Except as otherwise noted, usage is similar to <a href="std_algorithm_iteration.html#reduce"><code>std.algorithm.iteration.reduce</code></a>. There is also <a href="#fold"><code>fold</code></a> which does the same thing with a different parameter order. </p>
<p> This function works by splitting the range to be reduced into work units, which are slices to be reduced in parallel. Once the results from all work units are computed, a final serial reduction is performed on these results to compute the final answer. Therefore, care must be taken to choose the seed value appropriately. <br><br> Because the reduction is being performed in parallel, <code>functions</code> must be associative. For notational simplicity, let # be an infix operator representing <code>functions</code>. Then, (a # b) # c must equal a # (b # c). Floating point addition is not associative even though addition in exact arithmetic is. Summing floating point numbers using this function may give different results than summing serially. However, for many practical purposes floating point addition can be treated as associative. <br><br> Note that, since <code>functions</code> are assumed to be associative, additional optimizations are made to the serial portion of the reduction algorithm. These take advantage of the instruction level parallelism of modern CPUs, in addition to the thread-level parallelism that the rest of this module exploits. This can lead to better than linear speedups relative to <a href="std_algorithm_iteration.html#reduce"><code>std.algorithm.iteration.reduce</code></a>, especially for fine-grained benchmarks like dot products. <br><br> An explicit seed may be provided as the first argument. If provided, it is used as the seed for all work units and for the final reduction of results from all work units. Therefore, if it is not the identity value for the operation being performed, results may differ from those generated by <a href="std_algorithm_iteration.html#reduce"><code>std.algorithm.iteration.reduce</code></a> or depending on how many work units are used. The next argument must be the range to be reduced. <pre data-language="d">// Find the sum of squares of a range in parallel, using
// an explicit seed.
//
// Timings on an Athlon 64 X2 dual core machine:
//
// Parallel reduce:                     72 milliseconds
// Using std.algorithm.reduce instead:  181 milliseconds
auto nums = iota(10_000_000.0f);
auto sumSquares = taskPool.reduce!"a + b"(
    0.0, std.algorithm.map!"a * a"(nums)
);
</pre> <br><br> If no explicit seed is provided, the first element of each work unit is used as a seed. For the final reduction, the result from the first work unit is used as the seed. <pre data-language="d">// Find the sum of a range in parallel, using the first
// element of each work unit as the seed.
auto sum = taskPool.reduce!"a + b"(nums);
</pre> <br><br> An explicit work unit size may be specified as the last argument. Specifying too small a work unit size will effectively serialize the reduction, as the final reduction of the result of each work unit will dominate computation time. If <code>TaskPool.size</code> for this instance is zero, this parameter is ignored and one work unit is used. <pre data-language="d">// Use a work unit size of 100.
auto sum2 = taskPool.reduce!"a + b"(nums, 100);

// Work unit size of 100 and explicit seed.
auto sum3 = taskPool.reduce!"a + b"(0.0, nums, 100);
</pre> <br><br> Parallel reduce supports multiple functions, like <code>std.algorithm.reduce</code>. <pre data-language="d">// Find both the min and max of nums.
auto minMax = taskPool.reduce!(min, max)(nums);
assert(minMax[0] == reduce!min(nums));
assert(minMax[1] == reduce!max(nums));
</pre> <br><br> <b>Exception Handling</b>: <br><br> After this function is finished executing, any exceptions thrown are chained together via <code>Throwable.next</code> and rethrown. The chaining order is non-deterministic. </p> <dl>
<dt>See Also:</dt>
<dd>
<a href="#fold"><code>fold</code></a> is functionally equivalent to <a href="#reduce"><code>reduce</code></a> except the range parameter comes first and there is no need to use <a href="std_typecons.html#tuple"><code>tuple</code></a> for multiple seeds.</dd>
</dl> </dd> </dl> </dd> <dt class="d_decl" id="TaskPool.fold">template <strong id="fold">fold</strong>(functions...)</dt> <dd> <dl>
<dt class="d_decl" id="TaskPool.fold.fold">auto <strong id="fold">fold</strong>(Args...)(Args args); </dt> <dd>
<p>Implements the homonym function (also known as <code>accumulate</code>, <code>compress</code>, <code>inject</code>, or <code>foldl</code>) present in various programming languages of functional flavor. </p>
<p><code>fold</code> is functionally equivalent to <a href="#reduce"><code>reduce</code></a> except the range parameter comes first and there is no need to use <a href="std_typecons.html#tuple"> <code>tuple</code></a> for multiple seeds. <br><br> There may be one or more callable entities (<code>functions</code> argument) to apply. </p> <dl>
<dt>Parameters:</dt>
<dd><table>
<tr>
<td>Args <code>args</code>
</td> <td>Just the range to fold over; or the range and one seed per function; or the range, one seed per function, and the work unit size</td>
</tr> </table></dd>
</dl> <dl>
<dt>Returns:</dt>
<dd>The accumulated result as a single value for single function and as a tuple of values for multiple functions </dd>
</dl> <dl>
<dt>See Also:</dt>
<dd>Similar to <a href="std_algorithm_iteration.html#fold"><code>std.algorithm.iteration.fold</code></a>, <code>fold</code> is a wrapper around <a href="#reduce"><code>reduce</code></a>. </dd>
</dl> <dl>
<dt>Example</dt>

</dl>
<pre data-language="d">static int adder(int a, int b)
{
    return a + b;
}
static int multiplier(int a, int b)
{
    return a * b;
}

// Just the range
auto x = taskPool.fold!adder([1, 2, 3, 4]);
assert(x == 10);

// The range and the seeds (0 and 1 below; also note multiple
// functions in this example)
auto y = taskPool.fold!(adder, multiplier)([1, 2, 3, 4], 0, 1);
assert(y[0] == 10);
assert(y[1] == 24);

// The range, the seed (0), and the work unit size (20)
auto z = taskPool.fold!adder([1, 2, 3, 4], 0, 20);
assert(z == 10);
</pre>  </dd> </dl> </dd> <dt class="d_decl" id="TaskPool.workerIndex">const nothrow @property @safe size_t <strong id="workerIndex">workerIndex</strong>(); </dt> <dd>
<p>Gets the index of the current thread relative to this <code>TaskPool</code>. Any thread not in this pool will receive an index of 0. The worker threads in this pool receive unique indices of 1 through <code>this.size</code>. </p>
<p>This function is useful for maintaining worker-local resources. </p> <dl>
<dt>Example</dt>

</dl>
<pre data-language="d">// Execute a loop that computes the greatest common
// divisor of every number from 0 through 999 with
// 42 in parallel.  Write the results out to
// a set of files, one for each thread.  This allows
// results to be written out without any synchronization.

import std.conv, std.range, std.numeric, std.stdio;

void main()
{
    auto filesHandles = new File[taskPool.size + 1];
    scope(exit) {
        foreach (ref handle; fileHandles)
        {
            handle.close();
        }
    }

    foreach (i, ref handle; fileHandles)
    {
        handle = File("workerResults" ~ to!string(i) ~ ".txt");
    }

    foreach (num; parallel(iota(1_000)))
    {
        auto outHandle = fileHandles[taskPool.workerIndex];
        outHandle.writeln(num, '\t', gcd(num, 42));
    }
}
</pre>  </dd> <dt class="d_decl" id="TaskPool.WorkerLocalStorage">struct <strong id="WorkerLocalStorage">WorkerLocalStorage</strong>(T); </dt> <dd>
<p>Struct for creating worker-local storage. Worker-local storage is thread-local storage that exists only for worker threads in a given <code>TaskPool</code> plus a single thread outside the pool. It is allocated on the garbage collected heap in a way that avoids false sharing, and doesn't necessarily have global scope within any thread. It can be accessed from any worker thread in the <code>TaskPool</code> that created it, and one thread outside this <code>TaskPool</code>. All threads outside the pool that created a given instance of worker-local storage share a single slot. </p>
<p>Since the underlying data for this struct is heap-allocated, this struct has reference semantics when passed between functions. <br><br> The main uses cases for <code>WorkerLocalStorageStorage</code> are: <ol>
<li>Performing parallel reductions with an imperative, as opposed to functional, programming style. In this case, it's useful to treat <code>WorkerLocalStorageStorage</code> as local to each thread for only the parallel portion of an algorithm. </li> <li>Recycling temporary buffers across iterations of a parallel foreach loop. </li> </ol> </p> <dl>
<dt>Example</dt>

</dl>
<pre data-language="d">// Calculate pi as in our synopsis example, but
// use an imperative instead of a functional style.
immutable n = 1_000_000_000;
immutable delta = 1.0L / n;

auto sums = taskPool.workerLocalStorage(0.0L);
foreach (i; parallel(iota(n)))
{
    immutable x = ( i - 0.5L ) * delta;
    immutable toAdd = delta / ( 1.0 + x * x );
    sums.get += toAdd;
}

// Add up the results from each worker thread.
real pi = 0;
foreach (threadResult; sums.toRange)
{
    pi += 4.0L * threadResult;
}
</pre>  <dl>
<dt class="d_decl" id="TaskPool.WorkerLocalStorage.get">@property ref auto <strong id="get">get</strong>(this Qualified)(); </dt> <dd>
<p>Get the current thread's instance. Returns by ref. Note that calling <code>get</code> from any thread outside the <code>TaskPool</code> that created this instance will return the same reference, so an instance of worker-local storage should only be accessed from one thread outside the pool that created it. If this rule is violated, undefined behavior will result. </p>
<p>If assertions are enabled and <code>toRange</code> has been called, then this WorkerLocalStorage instance is no longer worker-local and an assertion failure will result when calling this method. This is not checked when assertions are disabled for performance reasons.</p> </dd> <dt class="d_decl" id="TaskPool.WorkerLocalStorage.get.2">@property void <strong id="get">get</strong>(T val); </dt> <dd>
<p>Assign a value to the current thread's instance. This function has the same caveats as its overload.</p> </dd> <dt class="d_decl" id="TaskPool.WorkerLocalStorage.toRange">@property WorkerLocalStorageRange!T <strong id="toRange">toRange</strong>(); </dt> <dd>
<p>Returns a range view of the values for all threads, which can be used to further process the results of each thread after running the parallel part of your algorithm. Do not use this method in the parallel portion of your algorithm. </p>
<p>Calling this function sets a flag indicating that this struct is no longer worker-local, and attempting to use the <code>get</code> method again will result in an assertion failure if assertions are enabled.</p> </dd> </dl> </dd> <dt class="d_decl" id="TaskPool.WorkerLocalStorageRange">struct <strong id="WorkerLocalStorageRange">WorkerLocalStorageRange</strong>(T); </dt> <dd>
<p>Range primitives for worker-local storage. The purpose of this is to access results produced by each worker thread from a single thread once you are no longer using the worker-local storage from multiple threads. Do not use this struct in the parallel portion of your algorithm. </p>
<p>The proper way to instantiate this object is to call <code>WorkerLocalStorage.toRange</code>. Once instantiated, this object behaves as a finite random-access range with assignable, lvalue elements and a length equal to the number of worker threads in the <code>TaskPool</code> that created it plus 1.</p> </dd> <dt class="d_decl" id="TaskPool.workerLocalStorage">WorkerLocalStorage!T <strong id="workerLocalStorage">workerLocalStorage</strong>(T)(lazy T initialVal = T.init); </dt> <dd>
<p>Creates an instance of worker-local storage, initialized with a given value. The value is <code>lazy</code> so that you can, for example, easily create one instance of a class for each worker. For usage example, see the <code>WorkerLocalStorage</code> struct.</p> </dd> <dt class="d_decl" id="TaskPool.stop">@trusted void <strong id="stop">stop</strong>(); </dt> <dd>
<p>Signals to all worker threads to terminate as soon as they are finished with their current <code>Task</code>, or immediately if they are not executing a <code>Task</code>. <code>Task</code>s that were in queue will not be executed unless a call to <code>Task.workForce</code>, <code>Task.yieldForce</code> or <code>Task.spinForce</code> causes them to be executed. </p>
<p>Use only if you have waited on every <code>Task</code> and therefore know the queue is empty, or if you speculatively executed some tasks and no longer need the results.</p> </dd> <dt class="d_decl" id="TaskPool.finish">@trusted void <strong id="finish">finish</strong>(bool blocking = false); </dt> <dd>
<p>Signals worker threads to terminate when the queue becomes empty. </p>
<p>If blocking argument is true, wait for all worker threads to terminate before returning. This option might be used in applications where task results are never consumed-- e.g. when <code>TaskPool</code> is employed as a rudimentary scheduler for tasks which communicate by means other than return values. </p> <dl>
<dt>Warning</dt>
<dd> Calling this function with <code>blocking = true</code> from a worker thread that is a member of the same <code>TaskPool</code> that <code>finish</code> is being called on will result in a deadlock.</dd>
</dl> </dd> <dt class="d_decl" id="TaskPool.size">const pure nothrow @property @safe size_t <strong id="size">size</strong>(); </dt> <dd>
<p>Returns the number of worker threads in the pool.</p> </dd> <dt class="d_decl" id="TaskPool.put">void <strong id="put">put</strong>(alias fun, Args...)(ref Task!(fun, Args) task)<br><small>  Constraints: if (!isSafeReturn!(typeof(task))); </small><br><br>void <strong id="put">put</strong>(alias fun, Args...)(Task!(fun, Args)* task)<br><small>  Constraints: if (!isSafeReturn!(typeof(*task))); </small>
</dt> <dd>
<p>Put a <code>Task</code> object on the back of the task queue. The <code>Task</code> object may be passed by pointer or reference. </p>
<dl>
<dt>Example</dt>

</dl>
<pre data-language="d">import std.file;

// Create a task.
auto t = task!read("foo.txt");

// Add it to the queue to be executed.
taskPool.put(t);
</pre>  <dl>
<dt>Notes</dt>
<dd> @trusted overloads of this function are called for <code>Task</code>s if <a href="std_traits.html#hasUnsharedAliasing"><code>std.traits.hasUnsharedAliasing</code></a> is false for the <code>Task</code>'s return type or the function the <code>Task</code> executes is <code>pure</code>. <code>Task</code> objects that meet all other requirements specified in the <code>@trusted</code> overloads of <code>task</code> and <code>scopedTask</code> may be created and executed from <code>@safe</code> code via <code>Task.executeInNewThread</code> but not via <code>TaskPool</code>. </dd>
</dl> While this function takes the address of variables that may be on the stack, some overloads are marked as @trusted. <code>Task</code> includes a destructor that waits for the task to complete before destroying the stack frame it is allocated on. Therefore, it is impossible for the stack frame to be destroyed before the task is complete and no longer referenced by a <code>TaskPool</code>. </dd> <dt class="d_decl" id="TaskPool.isDaemon">@property @trusted bool <strong id="isDaemon">isDaemon</strong>(); <br><br>@property @trusted void <strong id="isDaemon">isDaemon</strong>(bool newVal); </dt> <dd>
<p>These properties control whether the worker threads are daemon threads. A daemon thread is automatically terminated when all non-daemon threads have terminated. A non-daemon thread will prevent a program from terminating as long as it has not terminated. </p>
<p>If any <code>TaskPool</code> with non-daemon threads is active, either <code>stop</code> or <code>finish</code> must be called on it before the program can terminate. <br><br> The worker treads in the <code>TaskPool</code> instance returned by the <code>taskPool</code> property are daemon by default. The worker threads of manually instantiated task pools are non-daemon by default. </p> <dl>
<dt>Note</dt>
<dd> For a size zero pool, the getter arbitrarily returns true and the setter has no effect.</dd>
</dl> </dd> <dt class="d_decl" id="TaskPool.priority">@property @trusted int <strong id="priority">priority</strong>(); <br><br>@property @trusted void <strong id="priority">priority</strong>(int newPriority); </dt> <dd>
<p>These functions allow getting and setting the OS scheduling priority of the worker threads in this <code>TaskPool</code>. They forward to <code>core.thread.Thread.priority</code>, so a given priority value here means the same thing as an identical priority value in <code>core.thread</code>. </p>
<dl>
<dt>Note</dt>
<dd> For a size zero pool, the getter arbitrarily returns <code>core.thread.Thread.PRIORITY_MIN</code> and the setter has no effect.</dd>
</dl> </dd> </dl> </dd> <dt class="d_decl" id="taskPool">@property @trusted TaskPool <strong id="taskPool">taskPool</strong>(); </dt> <dd>
<p>Returns a lazily initialized global instantiation of <code>TaskPool</code>. This function can safely be called concurrently from multiple non-worker threads. The worker threads in this pool are daemon threads, meaning that it is not necessary to call <code>TaskPool.stop</code> or <code>TaskPool.finish</code> before terminating the main thread.</p> </dd> <dt class="d_decl" id="defaultPoolThreads">@property @trusted uint <strong id="defaultPoolThreads">defaultPoolThreads</strong>(); <br><br>@property @trusted void <strong id="defaultPoolThreads">defaultPoolThreads</strong>(uint newVal); </dt> <dd>
<p>These properties get and set the number of worker threads in the <code>TaskPool</code> instance returned by <code>taskPool</code>. The default value is <code>totalCPUs</code> - 1. Calling the setter after the first call to <code>taskPool</code> does not changes number of worker threads in the instance returned by <code>taskPool</code>.</p> </dd> <dt class="d_decl" id="parallel">ParallelForeach!R <strong id="parallel">parallel</strong>(R)(R range); <br><br>ParallelForeach!R <strong id="parallel">parallel</strong>(R)(R range, size_t workUnitSize); </dt> <dd>
<p>Convenience functions that forwards to <code>taskPool.parallel</code>. The purpose of these is to make parallel foreach less verbose and more readable. </p>
<dl>
<dt>Example</dt>

</dl>
<pre data-language="d">// Find the logarithm of every number from
// 1 to 1_000_000 in parallel, using the
// default TaskPool instance.
auto logs = new double[1_000_000];

foreach (i, ref elem; parallel(logs))
{
    elem = log(i + 1.0);
}
</pre>  </dd> </dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 1999&ndash;2021 The D Language Foundation<br>Licensed under the Boost License 1.0.<br>
    <a href="https://dlang.org/phobos/std_parallelism.html" class="_attribution-link">https://dlang.org/phobos/std_parallelism.html</a>
  </p>
</div>
