<h1>ovn-northd(8) — Linux manual page</h1>   <pre>
<span class="headline"><i>ovn-northd</i>(8)                  OVN Manual                  <i>ovn-northd</i>(8)</span>
</pre> <h2>
NAME </h2>
<pre>
       ovn-northd - Open Virtual Network central control daemon
</pre> <h2>
SYNOPSIS </h2>
<pre>
       <b>ovn-northd </b>[<i>options</i>]
</pre> <h2>
DESCRIPTION </h2>
<pre>
       <b>ovn-northd </b>is a centralized daemon responsible for translating
       the high-level OVN configuration into logical configuration
       consumable by daemons such as <b>ovn-controller</b>. It translates the
       logical network configuration in terms of conventional network
       concepts, taken from the OVN Northbound Database (see <a href="../man5/ovn-nb.5.html">ovn-nb(5)</a>),
       into logical datapath flows in the OVN Southbound Database (see
       <a href="../man5/ovn-sb.5.html">ovn-sb(5)</a>) below it.
</pre> <h2>
OPTIONS </h2>
<pre>
       <b>--ovnnb-db=</b><i>database</i>
              The OVSDB database containing the OVN Northbound Database.
              If the <b>OVN_NB_DB </b>environment variable is set, its value is
              used as the default. Otherwise, the default is
              <b>unix:/ovnnb_db.sock</b>.

       <b>--ovnsb-db=</b><i>database</i>
              The OVSDB database containing the OVN Southbound Database.
              If the <b>OVN_SB_DB </b>environment variable is set, its value is
              used as the default. Otherwise, the default is
              <b>unix:/ovnsb_db.sock</b>.

       <b>--dry-run</b>
              Causes <b>ovn-northd </b>to start paused. In the paused state,
              <b>ovn-northd </b>does not apply any changes to the databases,
              although it continues to monitor them. For more
              information, see the <b>pause </b>command, under <b>Runtime</b>
              <b>Management Commands </b>below.

       <b>n-threads N</b>
              In certain situations, it may be desirable to enable
              parallelization on a system to decrease latency (at the
              potential cost of increasing CPU usage).

              This option will cause ovn-northd to use N threads when
              building logical flows, when N is within [2-256]. If N is
              1, parallelization is disabled (default behavior). If N is
              less than 1, then N is set to 1, parallelization is
              disabled and a warning is logged. If N is more than 256,
              then N is set to 256, parallelization is enabled (with 256
              threads) and a warning is logged.

       <i>database</i> in the above options must be an OVSDB active or passive
       connection method, as described in <a href="../man7/ovsdb.7.html">ovsdb(7)</a>.

   <b>Daemon Options</b>
       <b>--pidfile</b>[<b>=</b><i>pidfile</i>]
              Causes a file (by default, <i>program</i><b>.pid</b>) to be created
              indicating the PID of the running process. If the <i>pidfile</i>
              argument is not specified, or if it does not begin with <b>/</b>,
              then it is created in .

              If <b>--pidfile </b>is not specified, no pidfile is created.

       <b>--overwrite-pidfile</b>
              By default, when <b>--pidfile </b>is specified and the specified
              pidfile already exists and is locked by a running process,
              the daemon refuses to start. Specify <b>--overwrite-pidfile</b>
              to cause it to instead overwrite the pidfile.

              When <b>--pidfile </b>is not specified, this option has no
              effect.

       <b>--detach</b>
              Runs this program as a background process. The process
              forks, and in the child it starts a new session, closes
              the standard file descriptors (which has the side effect
              of disabling logging to the console), and changes its
              current directory to the root (unless <b>--no-chdir </b>is
              specified). After the child completes its initialization,
              the parent exits.

       <b>--monitor</b>
              Creates an additional process to monitor this program. If
              it dies due to a signal that indicates a programming error
              (<b>SIGABRT</b>, <b>SIGALRM</b>, <b>SIGBUS</b>, <b>SIGFPE</b>, <b>SIGILL</b>, <b>SIGPIPE</b>,
              <b>SIGSEGV</b>, <b>SIGXCPU</b>, or <b>SIGXFSZ</b>) then the monitor process
              starts a new copy of it. If the daemon dies or exits for
              another reason, the monitor process exits.

              This option is normally used with <b>--detach</b>, but it also
              functions without it.

       <b>--no-chdir</b>
              By default, when <b>--detach </b>is specified, the daemon changes
              its current working directory to the root directory after
              it detaches. Otherwise, invoking the daemon from a
              carelessly chosen directory would prevent the
              administrator from unmounting the file system that holds
              that directory.

              Specifying <b>--no-chdir </b>suppresses this behavior, preventing
              the daemon from changing its current working directory.
              This may be useful for collecting core files, since it is
              common behavior to write core dumps into the current
              working directory and the root directory is not a good
              directory to use.

              This option has no effect when <b>--detach </b>is not specified.

       <b>--no-self-confinement</b>
              By default this daemon will try to self-confine itself to
              work with files under well-known directories determined at
              build time. It is better to stick with this default
              behavior and not to use this flag unless some other Access
              Control is used to confine daemon. Note that in contrast
              to other access control implementations that are typically
              enforced from kernel-space (e.g. DAC or MAC), self-
              confinement is imposed from the user-space daemon itself
              and hence should not be considered as a full confinement
              strategy, but instead should be viewed as an additional
              layer of security.

       <b>--user=</b><i>user</i><b>:</b><i>group</i>
              Causes this program to run as a different user specified
              in <i>user</i><b>:</b><i>group</i>, thus dropping most of the root privileges.
              Short forms <i>user</i> and <b>:</b><i>group</i> are also allowed, with current
              user or group assumed, respectively. Only daemons started
              by the root user accepts this argument.

              On Linux, daemons will be granted <b>CAP_IPC_LOCK </b>and
              <b>CAP_NET_BIND_SERVICES </b>before dropping root privileges.
              Daemons that interact with a datapath, such as
              <b>ovs-vswitchd</b>, will be granted three additional
              capabilities, namely <b>CAP_NET_ADMIN</b>, <b>CAP_NET_BROADCAST </b>and
              <b>CAP_NET_RAW</b>. The capability change will apply even if the
              new user is root.

              On Windows, this option is not currently supported. For
              security reasons, specifying this option will cause the
              daemon process not to start.

   <b>Logging Options</b>
       <b>-v</b>[<i>spec</i>]
       <b>--verbose=</b>[<i>spec</i>]
            Sets logging levels. Without any <i>spec</i>, sets the log level
            for every module and destination to <b>dbg</b>. Otherwise, <i>spec</i> is
            a list of words separated by spaces or commas or colons, up
            to one from each category below:

            •      A valid module name, as displayed by the <b>vlog/list</b>
                   command on <a href="ovs-appctl.8.html">ovs-appctl(8)</a>, limits the log level change
                   to the specified module.

            •      <b>syslog</b>, <b>console</b>, or <b>file</b>, to limit the log level
                   change to only to the system log, to the console, or
                   to a file, respectively. (If <b>--detach </b>is specified,
                   the daemon closes its standard file descriptors, so
                   logging to the console will have no effect.)

                   On Windows platform, <b>syslog </b>is accepted as a word and
                   is only useful along with the <b>--syslog-target </b>option
                   (the word has no effect otherwise).

            •      <b>off</b>, <b>emer</b>, <b>err</b>, <b>warn</b>, <b>info</b>, or <b>dbg</b>, to control the
                   log level. Messages of the given severity or higher
                   will be logged, and messages of lower severity will
                   be filtered out. <b>off </b>filters out all messages. See
                   <a href="ovs-appctl.8.html">ovs-appctl(8)</a> for a definition of each log level.

            Case is not significant within <i>spec</i>.

            Regardless of the log levels set for <b>file</b>, logging to a file
            will not take place unless <b>--log-file </b>is also specified (see
            below).

            For compatibility with older versions of OVS, <b>any </b>is
            accepted as a word but has no effect.

       <b>-v</b>
       <b>--verbose</b>
            Sets the maximum logging verbosity level, equivalent to
            <b>--verbose=dbg</b>.

       <b>-vPATTERN:</b><i>destination</i><b>:</b><i>pattern</i>
       <b>--verbose=PATTERN:</b><i>destination</i><b>:</b><i>pattern</i>
            Sets the log pattern for <i>destination</i> to <i>pattern</i>. Refer to
            <a href="ovs-appctl.8.html">ovs-appctl(8)</a> for a description of the valid syntax for
            <i>pattern</i>.

       <b>-vFACILITY:</b><i>facility</i>
       <b>--verbose=FACILITY:</b><i>facility</i>
            Sets the RFC5424 facility of the log message. <i>facility</i> can
            be one of <b>kern</b>, <b>user</b>, <b>mail</b>, <b>daemon</b>, <b>auth</b>, <b>syslog</b>, <b>lpr</b>, <b>news</b>,
            <b>uucp</b>, <b>clock</b>, <b>ftp</b>, <b>ntp</b>, <b>audit</b>, <b>alert</b>, <b>clock2</b>, <b>local0</b>, <b>local1</b>,
            <b>local2</b>, <b>local3</b>, <b>local4</b>, <b>local5</b>, <b>local6 </b>or <b>local7</b>. If this
            option is not specified, <b>daemon </b>is used as the default for
            the local system syslog and <b>local0 </b>is used while sending a
            message to the target provided via the <b>--syslog-target</b>
            option.

       <b>--log-file</b>[<b>=</b><i>file</i>]
            Enables logging to a file. If <i>file</i> is specified, then it is
            used as the exact name for the log file. The default log
            file name used if <i>file</i> is omitted is
            <b>/usr/local/var/log/ovn/</b><i>program</i><b>.log</b>.

       <b>--syslog-target=</b><i>host</i><b>:</b><i>port</i>
            Send syslog messages to UDP <i>port</i> on <i>host</i>, in addition to the
            system syslog. The <i>host</i> must be a numerical IP address, not
            a hostname.

       <b>--syslog-method=</b><i>method</i>
            Specify <i>method</i> as how syslog messages should be sent to
            syslog daemon. The following forms are supported:

            •      <b>libc</b>, to use the libc <b>syslog() </b>function. Downside of
                   using this options is that libc adds fixed prefix to
                   every message before it is actually sent to the
                   syslog daemon over <b>/dev/log </b>UNIX domain socket.

            •      <b>unix:</b><i>file</i>, to use a UNIX domain socket directly. It
                   is possible to specify arbitrary message format with
                   this option. However, <b>rsyslogd 8.9 </b>and older versions
                   use hard coded parser function anyway that limits
                   UNIX domain socket use. If you want to use arbitrary
                   message format with older <b>rsyslogd </b>versions, then use
                   UDP socket to localhost IP address instead.

            •      <b>udp:</b><i>ip</i><b>:</b><i>port</i>, to use a UDP socket. With this method it
                   is possible to use arbitrary message format also with
                   older <b>rsyslogd</b>. When sending syslog messages over UDP
                   socket extra precaution needs to be taken into
                   account, for example, syslog daemon needs to be
                   configured to listen on the specified UDP port,
                   accidental iptables rules could be interfering with
                   local syslog traffic and there are some security
                   considerations that apply to UDP sockets, but do not
                   apply to UNIX domain sockets.

            •      <b>null</b>, to discard all messages logged to syslog.

            The default is taken from the <b>OVS_SYSLOG_METHOD </b>environment
            variable; if it is unset, the default is <b>libc</b>.

   <b>PKI Options</b>
       PKI configuration is required in order to use SSL for the
       connections to the Northbound and Southbound databases.

              <b>-p </b><i>privkey.pem</i>
              <b>--private-key=</b><i>privkey.pem</i>
                   Specifies a PEM file containing the private key used
                   as identity for outgoing SSL connections.

              <b>-c </b><i>cert.pem</i>
              <b>--certificate=</b><i>cert.pem</i>
                   Specifies a PEM file containing a certificate that
                   certifies the private key specified on <b>-p </b>or
                   <b>--private-key </b>to be trustworthy. The certificate must
                   be signed by the certificate authority (CA) that the
                   peer in SSL connections will use to verify it.

              <b>-C </b><i>cacert.pem</i>
              <b>--ca-cert=</b><i>cacert.pem</i>
                   Specifies a PEM file containing the CA certificate
                   for verifying certificates presented to this program
                   by SSL peers. (This may be the same certificate that
                   SSL peers use to verify the certificate specified on
                   <b>-c </b>or <b>--certificate</b>, or it may be a different one,
                   depending on the PKI design in use.)

              <b>-C none</b>
              <b>--ca-cert=none</b>
                   Disables verification of certificates presented by
                   SSL peers. This introduces a security risk, because
                   it means that certificates cannot be verified to be
                   those of known trusted hosts.

   <b>Other Options</b>
       <b>--unixctl=</b><i>socket</i>
              Sets the name of the control socket on which <i>program</i>
              listens for runtime management commands (see <i>RUNTIME</i>
              <i>MANAGEMENT COMMANDS,</i> below). If <i>socket</i> does not begin with
              <b>/</b>, it is interpreted as relative to . If <b>--unixctl </b>is not
              used at all, the default socket is <b>/</b><i>program</i><b>.</b><i>pid</i><b>.ctl</b>, where
              <i>pid</i> is <i>program</i>’s process ID.

              On Windows a local named pipe is used to listen for
              runtime management commands. A file is created in the
              absolute path as pointed by <i>socket</i> or if <b>--unixctl </b>is not
              used at all, a file is created as <i>program</i> in the
              configured <i>OVS_RUNDIR</i> directory. The file exists just to
              mimic the behavior of a Unix domain socket.

              Specifying <b>none </b>for <i>socket</i> disables the control socket
              feature.

       <b>-h</b>
       <b>--help</b>
            Prints a brief help message to the console.

       <b>-V</b>
       <b>--version</b>
            Prints version information to the console.
</pre> <h2>
RUNTIME MANAGEMENT COMMANDS </h2>
<pre>
       <b>ovs-appctl </b>can send commands to a running <b>ovn-northd </b>process. The
       currently supported commands are described below.

              <b>exit   </b>Causes <b>ovn-northd </b>to gracefully terminate.

              <b>pause  </b>Pauses <b>ovn-northd</b>. When it is paused, <b>ovn-northd</b>
                     receives changes from the Northbound and Southbound
                     database changes as usual, but it does not send any
                     updates. A paused <b>ovn-northd </b>also drops database
                     locks, which allows any other non-paused instance
                     of <b>ovn-northd </b>to take over.

              <b>resume </b>Resumes the ovn-northd operation to process
                     Northbound and Southbound database contents and
                     generate logical flows. This will also instruct
                     ovn-northd to aspire for the lock on SB DB.

              <b>is-paused</b>
                     Returns "true" if ovn-northd is currently paused,
                     "false" otherwise.

              <b>status </b>Prints this server’s status. Status will be
                     "active" if ovn-northd has acquired OVSDB lock on
                     SB DB, "standby" if it has not or "paused" if this
                     instance is paused.

              <b>sb-cluster-state-reset</b>
                     Reset southbound database cluster status when
                     databases are destroyed and rebuilt.

                     If all databases in a clustered southbound database
                     are removed from disk, then the stored index of all
                     databases will be reset to zero. This will cause
                     ovn-northd to be unable to read or write to the
                     southbound database, because it will always detect
                     the data as stale. In such a case, run this command
                     so that ovn-northd will reset its local index so
                     that it can interact with the southbound database
                     again.

              <b>nb-cluster-state-reset</b>
                     Reset northbound database cluster status when
                     databases are destroyed and rebuilt.

                     This performs the same task as
                     <b>sb-cluster-state-reset </b>except for the northbound
                     database client.

              <b>set-n-threads N</b>
                     Set the number of threads used for building logical
                     flows. When N is within [2-256], parallelization is
                     enabled. When N is 1 parallelization is disabled.
                     When N is less than 1 or more than 256, an error is
                     returned. If ovn-northd fails to start
                     parallelization (e.g. fails to setup semaphores,
                     parallelization is disabled and an error is
                     returned.

              <b>get-n-threads</b>
                     Return the number of threads used for building
                     logical flows.

              <b>inc-engine/show-stats</b>
                     Display <b>ovn-northd </b>engine counters. For each engine
                     node the following counters have been added:

                     •      <b>recompute</b>

                     •      <b>compute</b>

                     •      <b>abort</b>

              <b>inc-engine/show-stats </b><i>engine_node_name counter_name</i>
                     Display the <b>ovn-northd </b>engine counter(s) for the
                     specified <i>engine_node_name</i>. <i>counter_name</i> is
                     optional and can be one of <b>recompute</b>, <b>compute </b>or
                     <b>abort</b>.

              <b>inc-engine/clear-stats</b>
                     Reset <b>ovn-northd </b>engine counters.
</pre> <h2>
ACTIVE-STANDBY FOR HIGH AVAILABILITY </h2>
<pre>
       You may run <b>ovn-northd </b>more than once in an OVN deployment. When
       connected to a standalone or clustered DB setup, OVN will
       automatically ensure that only one of them is active at a time.
       If multiple instances of <b>ovn-northd </b>are running and the active
       <b>ovn-northd </b>fails, one of the hot standby instances of <b>ovn-northd</b>
       will automatically take over.

   <b>Active-Standby with multiple OVN DB servers</b>
       You may run multiple OVN DB servers in an OVN deployment with:

              •      OVN DB servers deployed in active/passive mode with
                     one active and multiple passive ovsdb-servers.

              •      <b>ovn-northd </b>also deployed on all these nodes, using
                     unix ctl sockets to connect to the local OVN DB
                     servers.

       In such deployments, the ovn-northds on the passive nodes will
       process the DB changes and compute logical flows to be thrown out
       later, because write transactions are not allowed by the passive
       ovsdb-servers. It results in unnecessary CPU usage.

       With the help of runtime management command <b>pause</b>, you can pause
       <b>ovn-northd </b>on these nodes. When a passive node becomes master,
       you can use the runtime management command <b>resume </b>to resume the
       <b>ovn-northd </b>to process the DB changes.
</pre> <h2>
LOGICAL FLOW TABLE STRUCTURE </h2>
<pre>
       One of the main purposes of <b>ovn-northd </b>is to populate the
       <b>Logical_Flow </b>table in the <b>OVN_Southbound </b>database. This section
       describes how <b>ovn-northd </b>does this for switch and router logical
       datapaths.

   <b>Logical Switch Datapaths</b>
     <i>Ingress Table 0: Admission Control and Ingress Port Security check</i>

       Ingress table 0 contains these logical flows:

              •      Priority 100 flows to drop packets with VLAN tags
                     or multicast Ethernet source addresses.

              •      For each disabled logical port, a priority 100 flow
                     is added which matches on all packets and applies
                     the action <b>REGBIT_PORT_SEC_DROP" = 1; next;" </b>so
                     that the packets are dropped in the next stage.

              •      For each (enabled) vtep logical port, a priority 70
                     flow is added which matches on all packets and
                     applies the action <b>next(pipeline=ingress,</b>
                     <b>table=S_SWITCH_IN_L3_LKUP) = 1; </b>to skip most stages
                     of ingress pipeline and go directly to ingress L2
                     lookup table to determine the output port. Packets
                     from VTEP (RAMP) switch should not be subjected to
                     any ACL checks. Egress pipeline will do the ACL
                     checks.

              •      For each enabled logical port configured with qdisc
                     queue id in the <b>options:qdisc_queue_id </b>column of
                     <b>Logical_Switch_Port</b>, a priority 70 flow is added
                     which matches on all packets and applies the action
                     <b>set_queue(id); REGBIT_PORT_SEC_DROP" =</b>
                     <b>check_in_port_sec(); next;"</b>.

              •      A priority 1 flow is added which matches on all
                     packets for all the logical ports and applies the
                     action <b>REGBIT_PORT_SEC_DROP" = check_in_port_sec();</b>
                     <b>next; </b>to evaluate the port security. The action
                     <b>check_in_port_sec </b>applies the port security rules
                     defined in the <b>port_security </b>column of
                     <b>Logical_Switch_Port </b>table.

     <i>Ingress Table 1: Ingress Port Security - Apply</i>

       For each logical switch port <i>P</i> of type router connected to a gw
       router a priority-120 flow that matches ’recirculated’ icmp{4,6}
       error ’packet too big’ and <b>eth.src == </b><i>D</i> <b>&amp;&amp; outport == </b><i>P</i> <b>&amp;&amp;</b>
       <b>flags.tunnel_rx == 1 </b>where <i>D</i> is the peer logical router port <i>RP</i>
       mac address, swaps inport and outport and applies the action
       <b>next</b>.

       For each logical switch port <i>P</i> of type router connected to a
       distributed router a priority-120 flow that matches
       ’recirculated’ icmp{4,6} error ’packet too big’ and <b>eth.dst == </b><i>D</i>
       <b>&amp;&amp; flags.tunnel_rx == 1 </b>where <i>D</i> is the peer logical router port
       <i>RP</i> mac address, swaps inport and outport and applies the action
       <b>next(pipeline=S_SWITCH_IN_L2_LKUP)</b>.

       For each logical switch port <i>P</i> a priority-110 flow that matches
       ’recirculated’ icmp{4,6} error ’packet too big’ and <b>eth.src == </b><i>D</i>
       <b>&amp;&amp; outport == </b><i>P</i> <b>&amp;&amp; !is_chassis_resident("</b><i>P</i><b>") &amp;&amp; flags.tunnel_rx</b>
       <b>== 1</b>
        where <i>D</i> is the logical switch port mac address, swaps inport and
       outport and applies the action <b>next</b>.

       This table adds a priority-105 flow that matches ’recirculated’
       icmp{4,6} error ’packet too big’ to drop the packet.

       This table drops the packets if the port security check failed in
       the previous stage i.e the register bit <b>REGBIT_PORT_SEC_DROP </b>is
       set to 1.

       Ingress table 1 contains these logical flows:

              •      A priority-50 fallback flow that drops the packet
                     if the register bit <b>REGBIT_PORT_SEC_DROP </b>is set to
                     1.

              •      One priority-0 fallback flow that matches all
                     packets and advances to the next table.

     <i>Ingress Table 2: Lookup MAC address learning table</i>

       This table looks up the MAC learning table of the logical switch
       datapath to check if the <b>port-mac </b>pair is present or not. MAC is
       learnt for logical switch VIF ports whose port security is
       disabled and ’unknown’ address setn as well as for localnet ports
       with option localnet_learn_fdb. A localnet port entry does not
       overwrite a VIF port entry.

              •      For each such VIF logical port <i>p</i> whose port
                     security is disabled and ’unknown’ address set
                     following flow is added.

                     •      Priority 100 flow with the match <b>inport == </b><i>p</i>
                            and action <b>reg0[11] = lookup_fdb(inport,</b>
                            <b>eth.src); next;</b>

              •      For each such localnet logical port <i>p</i> following
                     flow is added.

                     •      Priority 100 flow with the match <b>inport == </b><i>p</i>
                            and action <b>flags.localnet = 1; reg0[11] =</b>
                            <b>lookup_fdb(inport, eth.src); next;</b>

              •      One priority-0 fallback flow that matches all
                     packets and advances to the next table.

     <i>Ingress Table 3: Learn MAC of ’unknown’ ports.</i>

       This table learns the MAC addresses seen on the VIF logical ports
       whose port security is disabled and ’unknown’ address set as well
       as on localnet ports with localnet_learn_fdb option set if the
       <b>lookup_fdb </b>action returned false in the previous table. For
       localnet ports (with flags.localnet = 1), lookup_fdb returns true
       if (port, mac) is found or if a mac is found for a port of type
       vif.

              •      For each such VIF logical port <i>p</i> whose port
                     security is disabled and ’unknown’ address set and
                     localnet port following flow is added.

                     •      Priority 100 flow with the match <b>inport == </b><i>p</i>
                            <b>&amp;&amp; reg0[11] == 0 </b>and action <b>put_fdb(inport,</b>
                            <b>eth.src); next; </b>which stores the <b>port-mac </b>in
                            the mac learning table of the logical switch
                            datapath and advances the packet to the next
                            table.

              •      One priority-0 fallback flow that matches all
                     packets and advances to the next table.

     <i>Ingress Table 4:</i> <b>from-lport </b><i>Pre-ACLs</i>

       This table prepares flows for possible stateful ACL processing in
       ingress table <b>ACLs</b>. It contains a priority-0 flow that simply
       moves traffic to the next table. If stateful ACLs are used in the
       logical datapath, a priority-100 flow is added that sets a hint
       (with <b>reg0[0] = 1; next;</b>) for table <b>Pre-stateful </b>to send IP
       packets to the connection tracker before eventually advancing to
       ingress table <b>ACLs</b>. If special ports such as route ports or
       localnet ports can’t use ct(), a priority-110 flow is added to
       skip over stateful ACLs. This priority-110 flow is not addd for
       router ports if the option enable_router_port_acl is set to true
       in <b>options:enable_router_port_acl </b>column of <b>Logical_Switch_Port</b>.
       Multicast, IPv6 Neighbor Discovery and MLD traffic also skips
       stateful ACLs. For "allow-stateless" ACLs, a flow is added to
       bypass setting the hint for connection tracker processing when
       there are stateful ACLs or LB rules; <b>REGBIT_ACL_STATELESS </b>is set
       for traffic matching stateless ACL flows.

       This table also has a priority-110 flow with the match <b>eth.dst ==</b>
       <i>E</i> for all logical switch datapaths to move traffic to the next
       table. Where <i>E</i> is the service monitor mac defined in the
       <b>options:svc_monitor_mac </b>column of <b>NB_Global </b>table.

     <i>Ingress Table 5: Pre-LB</i>

       This table prepares flows for possible stateful load balancing
       processing in ingress table <b>LB </b>and <b>Stateful</b>. It contains a
       priority-0 flow that simply moves traffic to the next table.
       Moreover it contains two priority-110 flows to move multicast,
       IPv6 Neighbor Discovery and MLD traffic to the next table. It
       also contains two priority-110 flows to move stateless traffic,
       i.e traffic for which <b>REGBIT_ACL_STATELESS </b>is set, to the next
       table. If load balancing rules with virtual IP addresses (and
       ports) are configured in <b>OVN_Northbound </b>database for a logical
       switch datapath, a priority-100 flow is added with the match <b>ip</b>
       to match on IP packets and sets the action <b>reg0[2] = 1; next; </b>to
       act as a hint for table <b>Pre-stateful </b>to send IP packets to the
       connection tracker for packet de-fragmentation (and to possibly
       do DNAT for already established load balanced traffic) before
       eventually advancing to ingress table <b>Stateful</b>. If
       controller_event has been enabled and load balancing rules with
       empty backends have been added in <b>OVN_Northbound</b>, a 130 flow is
       added to trigger ovn-controller events whenever the chassis
       receives a packet for that particular VIP. If <b>event-elb </b>meter has
       been previously created, it will be associated to the empty_lb
       logical flow

       Prior to <b>OVN 20.09 </b>we were setting the <b>reg0[0] = 1 </b>only if the IP
       destination matches the load balancer VIP. However this had few
       issues cases where a logical switch doesn’t have any ACLs with
       <b>allow-related </b>action. To understand the issue lets a take a TCP
       load balancer - <b>10.0.0.10:80=10.0.0.3:80</b>. If a logical port - p1
       with IP - 10.0.0.5 opens a TCP connection with the VIP -
       10.0.0.10, then the packet in the ingress pipeline of ’p1’ is
       sent to the p1’s conntrack zone id and the packet is load
       balanced to the backend - 10.0.0.3. For the reply packet from the
       backend lport, it is not sent to the conntrack of backend lport’s
       zone id. This is fine as long as the packet is valid. Suppose the
       backend lport sends an invalid TCP packet (like incorrect
       sequence number), the packet gets delivered to the lport ’p1’
       without unDNATing the packet to the VIP - 10.0.0.10. And this
       causes the connection to be reset by the lport p1’s VIF.

       We can’t fix this issue by adding a logical flow to drop ct.inv
       packets in the egress pipeline since it will drop all other
       connections not destined to the load balancers. To fix this
       issue, we send all the packets to the conntrack in the ingress
       pipeline if a load balancer is configured. We can now add a lflow
       to drop ct.inv packets.

       This table also has priority-120 flows that punt all IGMP/MLD
       packets to <b>ovn-controller </b>if the switch is an interconnect switch
       with multicast snooping enabled.

       This table also has a priority-110 flow with the match <b>eth.dst ==</b>
       <i>E</i> for all logical switch datapaths to move traffic to the next
       table. Where <i>E</i> is the service monitor mac defined in the
       <b>options:svc_monitor_mac </b>column of <b>NB_Global </b>table.

       This table also has a priority-110 flow with the match <b>inport ==</b>
       <i>I</i> for all logical switch datapaths to move traffic to the next
       table. Where <i>I</i> is the peer of a logical router port. This flow is
       added to skip the connection tracking of packets which enter from
       logical router datapath to logical switch datapath.

     <i>Ingress Table 6: Pre-stateful</i>

       This table prepares flows for all possible stateful processing in
       next tables. It contains a priority-0 flow that simply moves
       traffic to the next table.

              •      Priority-120 flows that send the packets to
                     connection tracker using <b>ct_lb_mark; </b>as the action
                     so that the already established traffic destined to
                     the load balancer VIP gets DNATted. These flows
                     match each VIPs IP and port. For IPv4 traffic the
                     flows also load the original destination IP and
                     transport port in registers <b>reg1 </b>and <b>reg2</b>. For IPv6
                     traffic the flows also load the original
                     destination IP and transport port in registers
                     <b>xxreg1 </b>and <b>reg2</b>.

              •      A priority-110 flow sends the packets that don’t
                     match the above flows to connection tracker based
                     on a hint provided by the previous tables (with a
                     match for <b>reg0[2] == 1</b>) by using the <b>ct_lb_mark;</b>
                     action.

              •      A priority-100 flow sends the packets to connection
                     tracker based on a hint provided by the previous
                     tables (with a match for <b>reg0[0] == 1</b>) by using the
                     <b>ct_next; </b>action.

     <i>Ingress Table 7:</i> <b>from-lport </b><i>ACL hints</i>

       This table consists of logical flows that set hints (<b>reg0 </b>bits)
       to be used in the next stage, in the ACL processing table, if
       stateful ACLs or load balancers are configured. Multiple hints
       can be set for the same packet. The possible hints are:

              •      <b>reg0[7]</b>: the packet might match an <b>allow-related</b>
                     ACL and might have to commit the connection to
                     conntrack.

              •      <b>reg0[8]</b>: the packet might match an <b>allow-related</b>
                     ACL but there will be no need to commit the
                     connection to conntrack because it already exists.

              •      <b>reg0[9]</b>: the packet might match a <b>drop/reject</b>.

              •      <b>reg0[10]</b>: the packet might match a <b>drop/reject </b>ACL
                     but the connection was previously allowed so it
                     might have to be committed again with <b>ct_label=1/1</b>.

       The table contains the following flows:

              •      A priority-65535 flow to advance to the next table
                     if the logical switch has <b>no </b>ACLs configured,
                     otherwise a priority-0 flow to advance to the next
                     table.

              •      A priority-7 flow that matches on packets that
                     initiate a new session. This flow sets <b>reg0[7] </b>and
                     <b>reg0[9] </b>and then advances to the next table.

              •      A priority-6 flow that matches on packets that are
                     in the request direction of an already existing
                     session that has been marked as blocked. This flow
                     sets <b>reg0[7] </b>and <b>reg0[9] </b>and then advances to the
                     next table.

              •      A priority-5 flow that matches untracked packets.
                     This flow sets <b>reg0[8] </b>and <b>reg0[9] </b>and then
                     advances to the next table.

              •      A priority-4 flow that matches on packets that are
                     in the request direction of an already existing
                     session that has not been marked as blocked. This
                     flow sets <b>reg0[8] </b>and <b>reg0[10] </b>and then advances to
                     the next table.

              •      A priority-3 flow that matches on packets that are
                     in not part of established sessions. This flow sets
                     <b>reg0[9] </b>and then advances to the next table.

              •      A priority-2 flow that matches on packets that are
                     part of an established session that has been marked
                     as blocked. This flow sets <b>reg0[9] </b>and then
                     advances to the next table.

              •      A priority-1 flow that matches on packets that are
                     part of an established session that has not been
                     marked as blocked. This flow sets <b>reg0[10] </b>and then
                     advances to the next table.

     <i>Ingress table 8:</i> <b>from-lport </b><i>ACL evaluation before LB</i>

       Logical flows in this table closely reproduce those in the <b>ACL</b>
       table in the <b>OVN_Northbound </b>database for the <b>from-lport </b>direction
       without the option <b>apply-after-lb </b>set or set to <b>false</b>. The
       <b>priority </b>values from the <b>ACL </b>table have a limited range and have
       1000 added to them to leave room for OVN default flows at both
       higher and lower priorities.

              •      This table is responsible for evaluating ACLs, and
                     setting a register bit to indicate whether the ACL
                     decided to allow, drop, or reject the traffic. The
                     allow bit is <b>reg8[16]</b>. The drop bit is <b>reg8[17]</b>.
                     All flows in this table will advance the packet to
                     the next table, where the bits from before are
                     evaluated to determine what to do with the packet.
                     Any flows in this table that intend for the packet
                     to pass will set <b>reg8[16] </b>to 1, even if an ACL with
                     an allow-type action was not matched. This lets the
                     next table know to allow the traffic to pass. These
                     bits will be referred to as the "allow", "drop",
                     and "reject" bits in the upcoming paragraphs.

              •      If the <b>tier </b>column has been configured on the ACL,
                     then OVN will also match the current tier counter
                     against the configured ACL tier. OVN keeps count of
                     the current tier in <b>reg8[30..31]</b>.

              •      <b>allow </b>ACLs translate into logical flows that set
                     the allow bit to 1 and advance the packet to the
                     next table. If there are any stateful ACLs on this
                     datapath, then <b>allow </b>ACLs set the allow bit to one
                     and in addition perform <b>ct_commit; </b>(which acts as a
                     hint for future tables to commit the connection to
                     conntrack). In case the <b>ACL </b>has a label then <b>reg3</b>
                     is loaded with the label value and <b>reg0[13] </b>bit is
                     set to 1 (which acts as a hint for the next tables
                     to commit the label to conntrack).

              •      <b>allow-related </b>ACLs translate into logical flows
                     that set the allow bit and additionally have
                     <b>ct_commit { ct_label=0/1; }; next; </b>actions for new
                     connections and <b>reg0[1] = 1; next; </b>for existing
                     connections. In case the <b>ACL </b>has a label then <b>reg3</b>
                     is loaded with the label value and <b>reg0[13] </b>bit is
                     set to 1 (which acts as a hint for the next tables
                     to commit the label to conntrack).

              •      <b>allow-stateless </b>ACLs translate into logical flows
                     that set the allow bit and advance to the next
                     table.

              •      <b>reject </b>ACLs translate into logical flows with that
                     set the reject bit and advance to the next table.

              •      <b>pass </b>ACLs translate into logical flows that do not
                     set the allow, drop, or reject bit and advance to
                     the next table.

              •      Other ACLs set the drop bit and advance to the next
                     table for new or untracked connections. For known
                     connections, they set the drop bit, as well as
                     running the <b>ct_commit { ct_label=1/1; }; </b>action.
                     Setting <b>ct_label </b>marks a connection as one that was
                     previously allowed, but should no longer be allowed
                     due to a policy change.

       This table contains a priority-65535 flow to set the allow bit
       and advance to the next table if the logical switch has <b>no </b>ACLs
       configured, otherwise a priority-0 flow to advance to the next
       table is added. This flow does not set the allow bit, so that the
       next table can decide whether to allow or drop the packet based
       on the value of the <b>options:default_acl_drop </b>column of the
       <b>NB_Global </b>table.

       A priority-65532 flow is added that sets the allow bit for IPv6
       Neighbor solicitation, Neighbor discover, Router solicitation,
       Router advertisement and MLD packets regardless of other ACLs
       defined.

       If the logical datapath has a stateful ACL or a load balancer
       with VIP configured, the following flows will also be added:

              •      If <b>options:default_acl_drop </b>column of <b>NB_Global </b>is
                     <b>false </b>or not set, a priority-1 flow that sets the
                     hint to commit IP traffic that is not part of
                     established sessions to the connection tracker
                     (with action <b>reg0[1] = 1; next;</b>). This is needed
                     for the default allow policy because, while the
                     initiator’s direction may not have any stateful
                     rules, the server’s may and then its return traffic
                     would not be known and marked as invalid.

              •      A priority-1 flow that sets the allow bit and sets
                     the hint to commit IP traffic to the connection
                     tracker (with action <b>reg0[1] = 1; next;</b>). This is
                     needed for the default allow policy because, while
                     the initiator’s direction may not have any stateful
                     rules, the server’s may and then its return traffic
                     would not be known and marked as invalid.

              •      A priority-65532 flow that sets the allow bit for
                     any traffic in the reply direction for a connection
                     that has been committed to the connection tracker
                     (i.e., established flows), as long as the committed
                     flow does not have <b>ct_mark.blocked </b>set. We only
                     handle traffic in the reply direction here because
                     we want all packets going in the request direction
                     to still go through the flows that implement the
                     currently defined policy based on ACLs. If a
                     connection is no longer allowed by policy,
                     <b>ct_mark.blocked </b>will get set and packets in the
                     reply direction will no longer be allowed, either.
                     This flow also clears the register bits <b>reg0[9] </b>and
                     <b>reg0[10] </b>and sets register bit <b>reg0[17]</b>. If ACL
                     logging and logging of related packets is enabled,
                     then a companion priority-65533 flow will be
                     installed that accomplishes the same thing but also
                     logs the traffic.

              •      A priority-65532 flow that sets the allow bit for
                     any traffic that is considered related to a
                     committed flow in the connection tracker (e.g., an
                     ICMP Port Unreachable from a non-listening UDP
                     port), as long as the committed flow does not have
                     <b>ct_mark.blocked </b>set. This flow also applies NAT to
                     the related traffic so that ICMP headers and the
                     inner packet have correct addresses. If ACL logging
                     and logging of related packets is enabled, then a
                     companion priority-65533 flow will be installed
                     that accomplishes the same thing but also logs the
                     traffic.

              •      A priority-65532 flow that sets the drop bit for
                     all traffic marked by the connection tracker as
                     invalid.

              •      A priority-65532 flow that sets the drop bit for
                     all traffic in the reply direction with
                     <b>ct_mark.blocked </b>set meaning that the connection
                     should no longer be allowed due to a policy change.
                     Packets in the request direction are skipped here
                     to let a newly created ACL re-allow this
                     connection.

       If the logical datapath has any ACL or a load balancer with VIP
       configured, the following flow will also be added:

              •      A priority 34000 logical flow is added for each
                     logical switch datapath with the match <b>eth.dst = </b><i>E</i>
                     to allow the service monitor reply packet destined
                     to <b>ovn-controller </b>that sets the allow bit, where <i>E</i>
                     is the service monitor mac defined in the
                     <b>options:svc_monitor_mac </b>column of <b>NB_Global </b>table.

     <i>Ingress Table 9:</i> <b>from-lport </b><i>ACL action</i>

       Logical flows in this table decide how to proceed based on the
       values of the allow, drop, and reject bits that may have been set
       in the previous table.

              •      If no ACLs are configured, then a priority 0 flow
                     is installed that matches everything and advances
                     to the next table.

              •      A priority 1000 flow is installed that will advance
                     the packet to the next table if the allow bit is
                     set.

              •      A priority 1000 flow is installed that will run the
                     <b>drop; </b>action if the drop bit is set.

              •      A priority 1000 flow is installed that will run the
                     <b>tcp_reset { output &lt;-&gt; inport;</b>
                     <b>next(pipeline=egress,table=5);} </b>action for TCP
                     connections,<b>icmp4/icmp6 </b>action for UDP connections,
                     and <b>sctp_abort {output &lt;-%gt; inport;</b>
                     <b>next(pipeline=egress,table=5);} </b>action for SCTP
                     associations.

              •      If any ACLs have tiers configured on them, then
                     three priority 500 flows are installed. If the
                     current tier counter is 0, 1, or 2, then the
                     current tier counter is incremented by one and the
                     packet is sent back to the previous table for re-
                     evaluation.

     <i>Ingress Table 10:</i> <b>from-lport </b><i>QoS Marking</i>

       Logical flows in this table closely reproduce those in the <b>QoS</b>
       table with the <b>action </b>column set in the <b>OVN_Northbound </b>database
       for the <b>from-lport </b>direction.

              •      For every qos_rules entry in a logical switch with
                     DSCP marking enabled, a flow will be added at the
                     priority mentioned in the QoS table.

              •      For every qos_rules entry in a logical switch with
                     packet marking enabled, a flow will be added at the
                     priority mentioned in the QoS table.

              •      One priority-0 fallback flow that matches all
                     packets and advances to the next table.

     <i>Ingress Table 11:</i> <b>from-lport </b><i>QoS Meter</i>

       Logical flows in this table closely reproduce those in the <b>QoS</b>
       table with the <b>bandwidth </b>column set in the <b>OVN_Northbound</b>
       database for the <b>from-lport </b>direction.

              •      For every qos_rules entry in a logical switch with
                     metering enabled, a flow will be added at the
                     priority mentioned in the QoS table.

              •      One priority-0 fallback flow that matches all
                     packets and advances to the next table.

     <i>Ingress Table 12: Load balancing affinity check</i>

       Load balancing affinity check table contains the following
       logical flows:

              •      For all the configured load balancing rules for a
                     switch in <b>OVN_Northbound </b>database where a positive
                     affinity timeout is specified in <b>options </b>column,
                     that includes a L4 port <i>PORT</i> of protocol <i>P</i> and IP
                     address <i>VIP</i>, a priority-100 flow is added. For IPv4
                     <i>VIPs</i>, the flow matches <b>ct.new &amp;&amp; ip &amp;&amp; ip4.dst ==</b>
                     <i>VIP</i> <b>&amp;&amp; </b><i>P</i><b>.dst == </b><i>PORT</i>. For IPv6 <i>VIPs</i>, the flow
                     matches <b>ct.new &amp;&amp; ip &amp;&amp; ip6.dst == </b><i>VIP</i><b>&amp;&amp; </b><i>P</i> <b>&amp;&amp; </b><i>P</i><b>.dst</b>
                     <b>==  </b><i>PORT</i>. The flow’s action is <b>reg9[6] =</b>
                     <b>chk_lb_aff(); next;</b>.

              •      A priority 0 flow is added which matches on all
                     packets and applies the action <b>next;</b>.

     <i>Ingress Table 13: LB</i>

              •      For all the configured load balancing rules for a
                     switch in <b>OVN_Northbound </b>database where a positive
                     affinity timeout is specified in <b>options </b>column,
                     that includes a L4 port <i>PORT</i> of protocol <i>P</i> and IP
                     address <i>VIP</i>, a priority-150 flow is added. For IPv4
                     <i>VIPs</i>, the flow matches <b>reg9[6] == 1 &amp;&amp; ct.new &amp;&amp; ip</b>
                     <b>&amp;&amp; ip4.dst == </b><i>VIP</i> <b>&amp;&amp; </b><i>P</i><b>.dst == </b><i>PORT</i> . For IPv6 <i>VIPs</i>,
                     the flow matches <b>reg9[6] == 1 &amp;&amp; ct.new &amp;&amp; ip &amp;&amp;</b>
                     <b>ip6.dst ==  </b><i>VIP</i> <b>&amp;&amp; </b><i>P</i> <b>&amp;&amp; </b><i>P</i><b>.dst ==  </b><i>PORT</i>. The flow’s
                     action is <b>ct_lb_mark(</b><i>args</i><b>)</b>, where <i>args</i> contains
                     comma separated IP addresses (and optional port
                     numbers) to load balance to. The address family of
                     the IP addresses of <i>args</i> is the same as the address
                     family of <i>VIP</i>.

              •      For all the configured load balancing rules for a
                     switch in <b>OVN_Northbound </b>database that includes a
                     L4 port <i>PORT</i> of protocol <i>P</i> and IP address <i>VIP</i>, a
                     priority-120 flow is added. For IPv4 <i>VIPs</i> , the
                     flow matches <b>ct.new &amp;&amp; ip &amp;&amp; ip4.dst == </b><i>VIP</i> <b>&amp;&amp;</b>
                     <i>P</i><b>.dst == </b><i>PORT</i>. For IPv6 <i>VIPs</i>, the flow matches
                     <b>ct.new &amp;&amp; ip &amp;&amp; ip6.dst == </b><i>VIP</i> <b>&amp;&amp; </b><i>P</i> <b>&amp;&amp; </b><i>P</i><b>.dst ==</b>
                     <i>PORT</i>. The flow’s action is <b>ct_lb_mark(</b><i>args</i><b>) </b>, where
                     <i>args</i> contains comma separated IP addresses (and
                     optional port numbers) to load balance to. The
                     address family of the IP addresses of <i>args</i> is the
                     same as the address family of <i>VIP</i>. If health check
                     is enabled, then <i>args</i> will only contain those
                     endpoints whose service monitor status entry in
                     <b>OVN_Southbound </b>db is either <b>online </b>or empty. For
                     IPv4 traffic the flow also loads the original
                     destination IP and transport port in registers <b>reg1</b>
                     and <b>reg2</b>. For IPv6 traffic the flow also loads the
                     original destination IP and transport port in
                     registers <b>xxreg1 </b>and <b>reg2</b>. The above flow is
                     created even if the load balancer is attached to a
                     logical router connected to the current logical
                     switch and the <b>install_ls_lb_from_router </b>variable
                     in <b>options </b>is set to true.

              •      For all the configured load balancing rules for a
                     switch in <b>OVN_Northbound </b>database that includes
                     just an IP address <i>VIP</i> to match on, OVN adds a
                     priority-110 flow. For IPv4 <i>VIPs</i>, the flow matches
                     <b>ct.new &amp;&amp; ip &amp;&amp; ip4.dst == </b><i>VIP</i>. For IPv6 <i>VIPs</i>, the
                     flow matches <b>ct.new &amp;&amp; ip &amp;&amp; ip6.dst == </b><i>VIP</i>. The
                     action on this flow is <b>ct_lb_mark(</b><i>args</i><b>)</b>, where <i>args</i>
                     contains comma separated IP addresses of the same
                     address family as <i>VIP</i>. For IPv4 traffic the flow
                     also loads the original destination IP and
                     transport port in registers <b>reg1 </b>and <b>reg2</b>. For IPv6
                     traffic the flow also loads the original
                     destination IP and transport port in registers
                     <b>xxreg1 </b>and <b>reg2</b>. The above flow is created even if
                     the load balancer is attached to a logical router
                     connected to the current logical switch and the
                     <b>install_ls_lb_from_router </b>variable in <b>options </b>is
                     set to true.

              •      If the load balancer is created with <b>--reject</b>
                     option and it has no active backends, a TCP reset
                     segment (for tcp) or an ICMP port unreachable
                     packet (for all other kind of traffic) will be sent
                     whenever an incoming packet is received for this
                     load-balancer. Please note using <b>--reject </b>option
                     will disable empty_lb SB controller event for this
                     load balancer.

     <i>Ingress Table 14: Load balancing affinity learn</i>

       Load balancing affinity learn table contains the following
       logical flows:

              •      For all the configured load balancing rules for a
                     switch in <b>OVN_Northbound </b>database where a positive
                     affinity timeout <i>T</i> is specified in <b>options </b>column,
                     that includes a L4 port <i>PORT</i> of protocol <i>P</i> and IP
                     address <i>VIP</i>, a priority-100 flow is added. For IPv4
                     <i>VIPs</i>, the flow matches <b>reg9[6] == 0 &amp;&amp; ct.new &amp;&amp; ip</b>
                     <b>&amp;&amp; ip4.dst == </b><i>VIP</i> <b>&amp;&amp; </b><i>P</i><b>.dst == </b><i>PORT</i>. For IPv6 <i>VIPs</i>,
                     the flow matches <b>ct.new &amp;&amp; ip &amp;&amp; ip6.dst == </b><i>VIP</i> <b>&amp;&amp;</b>
                     <i>P</i> <b>&amp;&amp; </b><i>P</i><b>.dst == </b><i>PORT</i> . The flow’s action is
                     <b>commit_lb_aff(vip = </b><i>VIP</i><b>:</b><i>PORT</i><b>, backend = </b><i>backend ip</i><b>:</b>
                     <i>backend port</i><b>, proto = </b><i>P</i><b>, timeout = </b><i>T</i><b>); </b>.

              •      A priority 0 flow is added which matches on all
                     packets and applies the action <b>next;</b>.

     <i>Ingress Table 15: Pre-Hairpin</i>

              •      If the logical switch has load balancer(s)
                     configured, then a priority-100 flow is added with
                     the match <b>ip &amp;&amp; ct.trk </b>to check if the packet needs
                     to be hairpinned (if after load balancing the
                     destination IP matches the source IP) or not by
                     executing the actions <b>reg0[6] = chk_lb_hairpin();</b>
                     and <b>reg0[12] = chk_lb_hairpin_reply(); </b>and advances
                     the packet to the next table.

              •      A priority-0 flow that simply moves traffic to the
                     next table.

     <i>Ingress Table 16: Nat-Hairpin</i>

              •      If the logical switch has load balancer(s)
                     configured, then a priority-100 flow is added with
                     the match <b>ip &amp;&amp; ct.new &amp;&amp; ct.trk &amp;&amp; reg0[6] == 1</b>
                     which hairpins the traffic by NATting source IP to
                     the load balancer VIP by executing the action
                     <b>ct_snat_to_vip </b>and advances the packet to the next
                     table.

              •      If the logical switch has load balancer(s)
                     configured, then a priority-100 flow is added with
                     the match <b>ip &amp;&amp; ct.est &amp;&amp; ct.trk &amp;&amp; reg0[6] == 1</b>
                     which hairpins the traffic by NATting source IP to
                     the load balancer VIP by executing the action
                     <b>ct_snat </b>and advances the packet to the next table.

              •      If the logical switch has load balancer(s)
                     configured, then a priority-90 flow is added with
                     the match <b>ip &amp;&amp; reg0[12] == 1 </b>which matches on the
                     replies of hairpinned traffic (i.e., destination IP
                     is VIP, source IP is the backend IP and source L4
                     port is backend port for L4 load balancers) and
                     executes <b>ct_snat </b>and advances the packet to the
                     next table.

              •      A priority-0 flow that simply moves traffic to the
                     next table.

     <i>Ingress Table 17: Hairpin</i>

              •      If logical switch has attached logical switch port
                     of <i>vtep</i> type, then for each distributed gateway
                     router port <i>RP</i> attached to this logical switch and
                     has chassis redirect port <i>cr-RP</i>, a priority-2000
                     flow is added with the match .IP
                     <b>reg0[14] == 1 &amp;&amp; is_chassis_resident(</b><i>cr-RP</i><b>)</b>

                     and action <b>next;</b>.

                     <b>reg0[14] </b>register bit is set in the ingress L2 port
                     security check table for traffic received from HW
                     VTEP (ramp) ports.

              •      If logical switch has attached logical switch port
                     of <i>vtep</i> type, then a priority-1000 flow that
                     matches on <b>reg0[14] </b>register bit for the traffic
                     received from HW VTEP (ramp) ports. This traffic is
                     passed to ingress table ls_in_l2_lkup.

              •      A priority-1 flow that hairpins traffic matched by
                     non-default flows in the Pre-Hairpin table.
                     Hairpinning is done at L2, Ethernet addresses are
                     swapped and the packets are looped back on the
                     input port.

              •      A priority-0 flow that simply moves traffic to the
                     next table.

     <i>Ingress table 18:</i> <b>from-lport </b><i>ACL evaluation after LB</i>

       Logical flows in this table closely reproduce those in the <b>ACL</b>
       <b>eval </b>table in the <b>OVN_Northbound </b>database for the <b>from-lport</b>
       direction with the option <b>apply-after-lb </b>set to <b>true</b>. The
       <b>priority </b>values from the <b>ACL </b>table have a limited range and have
       1000 added to them to leave room for OVN default flows at both
       higher and lower priorities. The flows in this table indicate the
       ACL verdict by setting <b>reg8[16] </b>for <b>allow-type </b>ACLs, <b>reg8[17] </b>for
       <b>drop </b>ACLs, and <b>reg8[17] </b>for <b>reject </b>ACLs, and then advancing the
       packet to the next table. These will be reffered to as the allow
       bit, drop bit, and reject bit throughout the documentation for
       this table and the next one.

       Like with ACLs that are evaluated before load balancers, if the
       ACL is configured with a tier value, then the current tier
       counter, supplied in reg8[30..31] is matched against the ACL’s
       configured tier in addition to the ACL’s match.

              •      <b>allow </b>apply-after-lb ACLs translate into logical
                     flows that set the allow bit. If there are any
                     stateful ACLs (including both before-lb and after-
                     lb ACLs) on this datapath, then <b>allow </b>ACLs also run
                     <b>ct_commit; next; </b>(which acts as a hint for an
                     upcoming table to commit the connection to
                     conntrack). In case the <b>ACL </b>has a label then <b>reg3</b>
                     is loaded with the label value and <b>reg0[13] </b>bit is
                     set to 1 (which acts as a hint for the next tables
                     to commit the label to conntrack).

              •      <b>allow-related </b>apply-after-lb ACLs translate into
                     logical flows that set the allow bit and run the
                     <b>ct_commit {ct_label=0/1; }; next; </b>actions for new
                     connections and <b>reg0[1] = 1; next; </b>for existing
                     connections. In case the <b>ACL </b>has a label then <b>reg3</b>
                     is loaded with the label value and <b>reg0[13] </b>bit is
                     set to 1 (which acts as a hint for the next tables
                     to commit the label to conntrack).

              •      <b>allow-stateless </b>apply-after-lb ACLs translate into
                     logical flows that set the allow bit and advance to
                     the next table.

              •      <b>reject </b>apply-after-lb ACLs translate into logical
                     flows that set the reject bit and advance to the
                     next table.

              •      <b>pass </b>apply-after-lb ACLs translate into logical
                     flows that do not set the allow, drop, or reject
                     bit and advance to the next table.

              •      Other apply-after-lb ACLs set the drop bit for new
                     or untracked connections and <b>ct_commit {</b>
                     <b>ct_label=1/1; } </b>for known connections. Setting
                     <b>ct_label </b>marks a connection as one that was
                     previously allowed, but should no longer be allowed
                     due to a policy change.

              •      One priority-65532 flow matching packets with
                     <b>reg0[17] </b>set (either replies to existing sessions
                     or traffic related to existing sessions) and allows
                     these by setting the allow bit and advancing to the
                     next table.

              •      One priority-0 fallback flow that matches all
                     packets and advances to the next table.

     <i>Ingress Table 19:</i> <b>from-lport </b><i>ACL action after LB</i>

       Logical flows in this table decide how to proceed based on the
       values of the allow, drop, and reject bits that may have been set
       in the previous table.

              •      If no ACLs are configured, then a priority 0 flow
                     is installed that matches everything and advances
                     to the next table.

              •      A priority 1000 flow is installed that will advance
                     the packet to the next table if the allow bit is
                     set.

              •      A priority 1000 flow is installed that will run the
                     <b>drop; </b>action if the drop bit is set.

              •      A priority 1000 flow is installed that will run the
                     <b>tcp_reset { output &lt;-&gt; inport;</b>
                     <b>next(pipeline=egress,table=5);} </b>action for TCP
                     connections,<b>icmp4/icmp6 </b>action for UDP connections,
                     and <b>sctp_abort {output &lt;-%gt; inport;</b>
                     <b>next(pipeline=egress,table=5);} </b>action for SCTP
                     associations.

              •      If any ACLs have tiers configured on them, then
                     three priority 500 flows are installed. If the
                     current tier counter is 0, 1, or 2, then the
                     current tier counter is incremented by one and the
                     packet is sent back to the previous table for re-
                     evaluation.

     <i>Ingress Table 20: Stateful</i>

              •      A priority 100 flow is added which commits the
                     packet to the conntrack and sets the most
                     significant 32-bits of <b>ct_label </b>with the <b>reg3 </b>value
                     based on the hint provided by previous tables (with
                     a match for <b>reg0[1] == 1 &amp;&amp; reg0[13] == 1</b>). This is
                     used by the <b>ACLs </b>with label to commit the label
                     value to conntrack.

              •      For <b>ACLs </b>without label, a second priority-100 flow
                     commits packets to connection tracker using
                     <b>ct_commit; next; </b>action based on a hint provided by
                     the previous tables (with a match for <b>reg0[1] == 1</b>
                     <b>&amp;&amp; reg0[13] == 0</b>).

              •      A priority-0 flow that simply moves traffic to the
                     next table.

     <i>Ingress Table 21: ARP/ND responder</i>

       This table implements ARP/ND responder in a logical switch for
       known IPs. The advantage of the ARP responder flow is to limit
       ARP broadcasts by locally responding to ARP requests without the
       need to send to other hypervisors. One common case is when the
       inport is a logical port associated with a VIF and the broadcast
       is responded to on the local hypervisor rather than broadcast
       across the whole network and responded to by the destination VM.
       This behavior is proxy ARP.

       ARP requests arrive from VMs from a logical switch inport of type
       default. For this case, the logical switch proxy ARP rules can be
       for other VMs or logical router ports. Logical switch proxy ARP
       rules may be programmed both for mac binding of IP addresses on
       other logical switch VIF ports (which are of the default logical
       switch port type, representing connectivity to VMs or
       containers), and for mac binding of IP addresses on logical
       switch router type ports, representing their logical router port
       peers. In order to support proxy ARP for logical router ports, an
       IP address must be configured on the logical switch router type
       port, with the same value as the peer logical router port. The
       configured MAC addresses must match as well. When a VM sends an
       ARP request for a distributed logical router port and if the peer
       router type port of the attached logical switch does not have an
       IP address configured, the ARP request will be broadcast on the
       logical switch. One of the copies of the ARP request will go
       through the logical switch router type port to the logical router
       datapath, where the logical router ARP responder will generate a
       reply. The MAC binding of a distributed logical router, once
       learned by an associated VM, is used for all that VM’s
       communication needing routing. Hence, the action of a VM re-
       arping for the mac binding of the logical router port should be
       rare.

       Logical switch ARP responder proxy ARP rules can also be hit when
       receiving ARP requests externally on a L2 gateway port. In this
       case, the hypervisor acting as an L2 gateway, responds to the ARP
       request on behalf of a destination VM.

       Note that ARP requests received from <b>localnet </b>logical inports can
       either go directly to VMs, in which case the VM responds or can
       hit an ARP responder for a logical router port if the packet is
       used to resolve a logical router port next hop address. In either
       case, logical switch ARP responder rules will not be hit. It
       contains these logical flows:

              •      If packet was received from HW VTEP (ramp switch),
                     and this packet is ARP or Neighbor Solicitation,
                     such packet is passed to next table with max
                     proirity. ARP/ND requests from HW VTEP must be
                     handled in logical router ingress pipeline.

              •      If the logical switch has no router ports with
                     options:arp_proxy configured add a priority-100
                     flows to skip the ARP responder if inport is of
                     type <b>localnet </b>advances directly to the next table.
                     ARP requests sent to <b>localnet </b>ports can be received
                     by multiple hypervisors. Now, because the same mac
                     binding rules are downloaded to all hypervisors,
                     each of the multiple hypervisors will respond. This
                     will confuse L2 learning on the source of the ARP
                     requests. ARP requests received on an inport of
                     type <b>router </b>are not expected to hit any logical
                     switch ARP responder flows. However, no skip flows
                     are installed for these packets, as there would be
                     some additional flow cost for this and the value
                     appears limited.

              •      If inport <b>V </b>is of type <b>virtual </b>adds a priority-100
                     logical flows for each <i>P</i> configured in the
                     <b>options:virtual-parents </b>column with the match

                     <b>inport == </b><i>P</i> <b>&amp;&amp; &amp;&amp; ((arp.op == 1 &amp;&amp; arp.spa == </b><i>VIP</i> <b>&amp;&amp; arp.tpa == </b><i>VIP</i><b>) || (arp.op == 2 &amp;&amp; arp.spa == </b><i>VIP</i><b>))</b>
                     <b>inport == </b><i>P</i> <b>&amp;&amp; &amp;&amp; ((nd_ns &amp;&amp; ip6.dst == </b><i>{VIP, NS_MULTICAST_ADDR}</i> <b>&amp;&amp; nd.target == </b><i>VIP</i><b>) || (nd_na &amp;&amp; nd.target == </b><i>VIP</i><b>))</b>

                     and applies the action

                     <b>bind_vport(</b><i>V</i><b>, inport);</b>

                     and advances the packet to the next table.

                     Where <i>VIP</i> is the virtual ip configured in the
                     column <b>options:virtual-ip </b>and NS_MULTICAST_ADDR is
                     solicited-node multicast address corresponding to
                     the VIP.

              •      Priority-50 flows that match ARP requests to each
                     known IP address <i>A</i> of every logical switch port,
                     and respond with ARP replies directly with
                     corresponding Ethernet address <i>E</i>:

                     <b>eth.dst = eth.src;</b>
                     <b>eth.src = </b><i>E</i>;
                     <b>arp.op = 2; /* ARP reply. */</b>
                     <b>arp.tha = arp.sha;</b>
                     <b>arp.sha = </b><i>E</i>;
                     <b>arp.tpa = arp.spa;</b>
                     <b>arp.spa = </b><i>A</i>;
                     <b>outport = inport;</b>
                     <b>flags.loopback = 1;</b>
                     <b>output;</b>

                     These flows are omitted for logical ports (other
                     than router ports or <b>localport </b>ports) that are down
                     (unless <b>ignore_lsp_down </b>is configured as true in
                     <b>options </b>column of <b>NB_Global </b>table of the <b>Northbound</b>
                     database), for logical ports of type <b>virtual</b>, for
                     logical ports with ’unknown’ address set, for
                     logical ports with the
                     <b>options:disable_arp_nd_rsp=true </b>and for logical
                     ports of a logical switch configured with
                     <b>other_config:vlan-passthru=true</b>.

                     The above ARP responder flows are added for the
                     list of IPv4 addresses if defined in
                     <b>options:arp_proxy </b>column of <b>Logical_Switch_Port</b>
                     table for logical switch ports of type <b>router</b>.

              •      Priority-50 flows that match IPv6 ND neighbor
                     solicitations to each known IP address <i>A</i> (and <i>A</i>’s
                     solicited node address) of every logical switch
                     port except of type router, and respond with
                     neighbor advertisements directly with corresponding
                     Ethernet address <i>E</i>:

                     <b>nd_na {</b>
                         <b>eth.src = </b><i>E</i>;
                         <b>ip6.src = </b><i>A</i>;
                         <b>nd.target = </b><i>A</i>;
                         <b>nd.tll = </b><i>E</i>;
                         <b>outport = inport;</b>
                         <b>flags.loopback = 1;</b>
                         <b>output;</b>
                     <b>};</b>

                     Priority-50 flows that match IPv6 ND neighbor
                     solicitations to each known IP address <i>A</i> (and <i>A</i>’s
                     solicited node address) of logical switch port of
                     type router, and respond with neighbor
                     advertisements directly with corresponding Ethernet
                     address <i>E</i>:

                     <b>nd_na_router {</b>
                         <b>eth.src = </b><i>E</i>;
                         <b>ip6.src = </b><i>A</i>;
                         <b>nd.target = </b><i>A</i>;
                         <b>nd.tll = </b><i>E</i>;
                         <b>outport = inport;</b>
                         <b>flags.loopback = 1;</b>
                         <b>output;</b>
                     <b>};</b>

                     These flows are omitted for logical ports (other
                     than router ports or <b>localport </b>ports) that are down
                     (unless <b>ignore_lsp_down </b>is configured as true in
                     <b>options </b>column of <b>NB_Global </b>table of the <b>Northbound</b>
                     database), for logical ports of type <b>virtual </b>and
                     for logical ports with ’unknown’ address set.

                     The above NDP responder flows are added for the
                     list of IPv6 addresses if defined in
                     <b>options:arp_proxy </b>column of <b>Logical_Switch_Port</b>
                     table for logical switch ports of type <b>router</b>.

              •      Priority-100 flows with match criteria like the ARP
                     and ND flows above, except that they only match
                     packets from the <b>inport </b>that owns the IP addresses
                     in question, with action <b>next;</b>. These flows prevent
                     OVN from replying to, for example, an ARP request
                     emitted by a VM for its own IP address. A VM only
                     makes this kind of request to attempt to detect a
                     duplicate IP address assignment, so sending a reply
                     will prevent the VM from accepting the IP address
                     that it owns.

                     In place of <b>next;</b>, it would be reasonable to use
                     <b>drop; </b>for the flows’ actions. If everything is
                     working as it is configured, then this would
                     produce equivalent results, since no host should
                     reply to the request. But ARPing for one’s own IP
                     address is intended to detect situations where the
                     network is not working as configured, so dropping
                     the request would frustrate that intent.

              •      For each <i>SVC_MON_SRC_IP</i> defined in the value of the
                     <b>ip_port_mappings:ENDPOINT_IP </b>column of
                     <b>Load_Balancer </b>table, priority-110 logical flow is
                     added with the match <b>arp.tpa == </b><i>SVC_MON_SRC_IP</i> <b>&amp;&amp;</b>
                     <b>&amp;&amp; arp.op == 1 </b>and applies the action

                     <b>eth.dst = eth.src;</b>
                     <b>eth.src = </b><i>E</i>;
                     <b>arp.op = 2; /* ARP reply. */</b>
                     <b>arp.tha = arp.sha;</b>
                     <b>arp.sha = </b><i>E</i>;
                     <b>arp.tpa = arp.spa;</b>
                     <b>arp.spa = </b><i>A</i>;
                     <b>outport = inport;</b>
                     <b>flags.loopback = 1;</b>
                     <b>output;</b>

                     where <i>E</i> is the service monitor source mac defined
                     in the <b>options:svc_monitor_mac </b>column in the
                     <b>NB_Global </b>table. This mac is used as the source mac
                     in the service monitor packets for the load
                     balancer endpoint IP health checks.

                     <i>SVC_MON_SRC_IP</i> is used as the source ip in the
                     service monitor IPv4 packets for the load balancer
                     endpoint IP health checks.

                     These flows are required if an ARP request is sent
                     for the IP <i>SVC_MON_SRC_IP</i>.

                     For IPv6 the similar flow is added with the
                     following action

                     <b>nd_na {</b>
                         <b>eth.dst = eth.src;</b>
                         <b>eth.src = </b><i>E</i>;
                         <b>ip6.src = </b><i>A</i>;
                         <b>nd.target = </b><i>A</i>;
                         <b>nd.tll = </b><i>E</i>;
                         <b>outport = inport;</b>
                         <b>flags.loopback = 1;</b>
                         <b>output;</b>
                     <b>};</b>

              •      For each <i>VIP</i> configured in the table
                     <b>Forwarding_Group </b>a priority-50 logical flow is
                     added with the match <b>arp.tpa == </b><i>vip</i> <b>&amp;&amp; &amp;&amp; arp.op ==</b>
                     <b>1</b>
                      and applies the action

                     <b>eth.dst = eth.src;</b>
                     <b>eth.src = </b><i>E</i>;
                     <b>arp.op = 2; /* ARP reply. */</b>
                     <b>arp.tha = arp.sha;</b>
                     <b>arp.sha = </b><i>E</i>;
                     <b>arp.tpa = arp.spa;</b>
                     <b>arp.spa = </b><i>A</i>;
                     <b>outport = inport;</b>
                     <b>flags.loopback = 1;</b>
                     <b>output;</b>

                     where <i>E</i> is the forwarding group’s mac defined in
                     the <b>vmac</b>.

                     <i>A</i> is used as either the destination ip for load
                     balancing traffic to child ports or as nexthop to
                     hosts behind the child ports.

                     These flows are required to respond to an ARP
                     request if an ARP request is sent for the IP <i>vip</i>.

              •      One priority-0 fallback flow that matches all
                     packets and advances to the next table.

     <i>Ingress Table 22: DHCP option processing</i>

       This table adds the DHCPv4 options to a DHCPv4 packet from the
       logical ports configured with IPv4 address(es) and DHCPv4
       options, and similarly for DHCPv6 options. This table also adds
       flows for the logical ports of type <b>external</b>.

              •      A priority-100 logical flow is added for these
                     logical ports which matches the IPv4 packet with
                     <b>udp.src </b>= 68 and <b>udp.dst </b>= 67 and applies the
                     action <b>put_dhcp_opts </b>and advances the packet to the
                     next table.

                     <b>reg0[3] = put_dhcp_opts(offer_ip = </b><i>ip</i>, <i>options</i>...);
                     <b>next;</b>

                     For DHCPDISCOVER and DHCPREQUEST, this transforms
                     the packet into a DHCP reply, adds the DHCP offer
                     IP <i>ip</i> and options to the packet, and stores 1 into
                     reg0[3]. For other kinds of packets, it just stores
                     0 into reg0[3]. Either way, it continues to the
                     next table.

              •      A priority-100 logical flow is added for these
                     logical ports which matches the IPv6 packet with
                     <b>udp.src </b>= 546 and <b>udp.dst </b>= 547 and applies the
                     action <b>put_dhcpv6_opts </b>and advances the packet to
                     the next table.

                     <b>reg0[3] = put_dhcpv6_opts(ia_addr = </b><i>ip</i>, <i>options</i>...);
                     <b>next;</b>

                     For DHCPv6 Solicit/Request/Confirm packets, this
                     transforms the packet into a DHCPv6
                     Advertise/Reply, adds the DHCPv6 offer IP <i>ip</i> and
                     options to the packet, and stores 1 into reg0[3].
                     For other kinds of packets, it just stores 0 into
                     reg0[3]. Either way, it continues to the next
                     table.

              •      A priority-0 flow that matches all packets to
                     advances to table 16.

     <i>Ingress Table 23: DHCP responses</i>

       This table implements DHCP responder for the DHCP replies
       generated by the previous table.

              •      A priority 100 logical flow is added for the
                     logical ports configured with DHCPv4 options which
                     matches IPv4 packets with <b>udp.src == 68 &amp;&amp; udp.dst</b>
                     <b>== 67 &amp;&amp; reg0[3] == 1 </b>and responds back to the
                     <b>inport </b>after applying these actions. If <b>reg0[3] </b>is
                     set to 1, it means that the action <b>put_dhcp_opts</b>
                     was successful.

                     <b>eth.dst = eth.src;</b>
                     <b>eth.src = </b><i>E</i>;
                     <b>ip4.src = </b><i>S</i>;
                     <b>udp.src = 67;</b>
                     <b>udp.dst = 68;</b>
                     <b>outport = </b><i>P</i>;
                     <b>flags.loopback = 1;</b>
                     <b>output;</b>

                     where <i>E</i> is the server MAC address and <i>S</i> is the
                     server IPv4 address defined in the DHCPv4 options.
                     Note that <b>ip4.dst </b>field is handled by
                     <b>put_dhcp_opts</b>.

                     (This terminates ingress packet processing; the
                     packet does not go to the next ingress table.)

              •      A priority 100 logical flow is added for the
                     logical ports configured with DHCPv6 options which
                     matches IPv6 packets with <b>udp.src == 546 &amp;&amp; udp.dst</b>
                     <b>== 547 &amp;&amp; reg0[3] == 1 </b>and responds back to the
                     <b>inport </b>after applying these actions. If <b>reg0[3] </b>is
                     set to 1, it means that the action <b>put_dhcpv6_opts</b>
                     was successful.

                     <b>eth.dst = eth.src;</b>
                     <b>eth.src = </b><i>E</i>;
                     <b>ip6.dst = </b><i>A</i>;
                     <b>ip6.src = </b><i>S</i>;
                     <b>udp.src = 547;</b>
                     <b>udp.dst = 546;</b>
                     <b>outport = </b><i>P</i>;
                     <b>flags.loopback = 1;</b>
                     <b>output;</b>

                     where <i>E</i> is the server MAC address and <i>S</i> is the
                     server IPv6 LLA address generated from the
                     <b>server_id </b>defined in the DHCPv6 options and <i>A</i> is
                     the IPv6 address defined in the logical port’s
                     addresses column.

                     (This terminates packet processing; the packet does
                     not go on the next ingress table.)

              •      A priority-0 flow that matches all packets to
                     advances to table 17.

     <i>Ingress Table 24 DNS Lookup</i>

       This table looks up and resolves the DNS names to the
       corresponding configured IP address(es).

              •      A priority-100 logical flow for each logical switch
                     datapath if it is configured with DNS records,
                     which matches the IPv4 and IPv6 packets with
                     <b>udp.dst </b>= 53 and applies the action <b>dns_lookup </b>and
                     advances the packet to the next table.

                     <b>reg0[4] = dns_lookup(); next;</b>

                     For valid DNS packets, this transforms the packet
                     into a DNS reply if the DNS name can be resolved,
                     and stores 1 into reg0[4]. For failed DNS
                     resolution or other kinds of packets, it just
                     stores 0 into reg0[4]. Either way, it continues to
                     the next table.

     <i>Ingress Table 25 DNS Responses</i>

       This table implements DNS responder for the DNS replies generated
       by the previous table.

              •      A priority-100 logical flow for each logical switch
                     datapath if it is configured with DNS records,
                     which matches the IPv4 and IPv6 packets with
                     <b>udp.dst = 53 &amp;&amp; reg0[4] == 1 </b>and responds back to
                     the <b>inport </b>after applying these actions. If <b>reg0[4]</b>
                     is set to 1, it means that the action <b>dns_lookup</b>
                     was successful.

                     <b>eth.dst &lt;-&gt; eth.src;</b>
                     <b>ip4.src &lt;-&gt; ip4.dst;</b>
                     <b>udp.dst = udp.src;</b>
                     <b>udp.src = 53;</b>
                     <b>outport = </b><i>P</i>;
                     <b>flags.loopback = 1;</b>
                     <b>output;</b>

                     (This terminates ingress packet processing; the
                     packet does not go to the next ingress table.)

     <i>Ingress table 26 External ports</i>

       Traffic from the <b>external </b>logical ports enter the ingress
       datapath pipeline via the <b>localnet </b>port. This table adds the
       below logical flows to handle the traffic from these ports.

              •      A priority-100 flow is added for each <b>external</b>
                     logical port which doesn’t reside on a chassis to
                     drop the ARP/IPv6 NS request to the router IP(s)
                     (of the logical switch) which matches on the <b>inport</b>
                     of the <b>external </b>logical port and the valid <b>eth.src</b>
                     address(es) of the <b>external </b>logical port.

                     This flow guarantees that the ARP/NS request to the
                     router IP address from the external ports is
                     responded by only the chassis which has claimed
                     these external ports. All the other chassis, drops
                     these packets.

                     A priority-100 flow is added for each <b>external</b>
                     logical port which doesn’t reside on a chassis to
                     drop any packet destined to the router mac - with
                     the match <b>inport == </b><i>external</i> <b>&amp;&amp; eth.src == </b><i>E</i> <b>&amp;&amp;</b>
                     <b>eth.dst == </b><i>R</i> <b>&amp;&amp; !is_chassis_resident("</b><i>external</i><b>")</b>
                     where <i>E</i> is the external port mac and <i>R</i> is the
                     router port mac.

              •      A priority-0 flow that matches all packets to
                     advances to table 20.

     <i>Ingress Table 27 Destination Lookup</i>

       This table implements switching behavior. It contains these
       logical flows:

              •      A priority-110 flow with the match <b>eth.src == </b><i>E</i> for
                     all logical switch datapaths and applies the action
                     <b>handle_svc_check(inport)</b>. Where <i>E</i> is the service
                     monitor mac defined in the <b>options:svc_monitor_mac</b>
                     column of <b>NB_Global </b>table.

              •      A priority-100 flow that punts all IGMP/MLD packets
                     to <b>ovn-controller </b>if multicast snooping is enabled
                     on the logical switch.

              •      A priority-100 flow that forwards all DHCP
                     broadcast packets coming from VIFs to the logical
                     router port’s MAC when DHCP relay is enabled on the
                     logical switch.

              •      Priority-90 flows for transit switches that forward
                     registered IP multicast traffic to their
                     corresponding multicast group , which <b>ovn-northd</b>
                     creates based on learnt <b>IGMP_Group </b>entries.

              •      Priority-90 flows for non-transit switches that
                     forward registered IP multicast traffic to their
                     corresponding multicast group, which <b>ovn-northd</b>
                     creates based on learnt <b>IGMP_Group </b>entries. The
                     flows also forward packets to the <b>MC_MROUTER_FLOOD</b>
                     multicast group, which <b>ovn-nortdh </b>populates with
                     all the logical ports that are connected to logical
                     routers with <b>options</b>:mcast_relay=’true’.

              •      A priority-85 flow that forwards all IP multicast
                     traffic destined to 224.0.0.X to the <b>MC_FLOOD_L2</b>
                     multicast group, which <b>ovn-northd </b>populates with
                     all non-router logical ports.

              •      A priority-85 flow that forwards all IP multicast
                     traffic destined to reserved multicast IPv6
                     addresses (RFC 4291, 2.7.1, e.g., Solicited-Node
                     multicast) to the <b>MC_FLOOD </b>multicast group, which
                     <b>ovn-northd </b>populates with all enabled logical
                     ports.

              •      A priority-80 flow that forwards all unregistered
                     IP multicast traffic to the <b>MC_STATIC </b>multicast
                     group, which <b>ovn-northd </b>populates with all the
                     logical ports that have <b>options</b>
                     <b>:mcast_flood=’true’</b>. The flow also forwards
                     unregistered IP multicast traffic to the
                     <b>MC_MROUTER_FLOOD </b>multicast group, which <b>ovn-northd</b>
                     populates with all the logical ports connected to
                     logical routers that have <b>options</b>
                     <b>:mcast_relay=’true’</b>.

              •      A priority-80 flow that drops all unregistered IP
                     multicast traffic if <b>other_config</b>
                     <b>:mcast_snoop=’true’ </b>and <b>other_config</b>
                     <b>:mcast_flood_unregistered=’false’ </b>and the switch is
                     not connected to a logical router that has <b>options</b>
                     <b>:mcast_relay=’true’ </b>and the switch doesn’t have any
                     logical port with <b>options :mcast_flood=’true’</b>.

              •      Priority-80 flows for each IP address/VIP/NAT
                     address owned by a router port connected to the
                     switch. These flows match ARP requests and ND
                     packets for the specific IP addresses. Matched
                     packets are forwarded only to the router that owns
                     the IP address and to the <b>MC_FLOOD_L2 </b>multicast
                     group which contains all non-router logical ports.

              •      Priority-75 flows for each port connected to a
                     logical router matching self originated ARP
                     request/RARP request/ND packets. These packets are
                     flooded to the <b>MC_FLOOD_L2 </b>which contains all non-
                     router logical ports.

              •      A priority-72 flow that outputs all ARP requests
                     and ND packets with an Ethernet broadcast or
                     multicast <b>eth.dst </b>to the <b>MC_FLOOD_L2 </b>multicast
                     group if
                     <b>other_config:broadcast-arps-to-all-routers=true</b>.

              •      A priority-70 flow that outputs all packets with an
                     Ethernet broadcast or multicast <b>eth.dst </b>to the
                     <b>MC_FLOOD </b>multicast group.

              •      One priority-50 flow that matches each known
                     Ethernet address against <b>eth.dst</b>. Action of this
                     flow outputs the packet to the single associated
                     output port if it is enabled. <b>drop; </b>action is
                     applied if LSP is disabled. If the logical switch
                     port of type VIF has the option
                     <b>options:pkt_clone_type </b>is set to the value
                     <b>mc_unknown</b>, then the packet is also forwarded to
                     the <b>MC_UNKNOWN </b>multicast group.

                     The above flow is not added if the logical switch
                     port is of type VIF, has <b>unknown </b>as one of its
                     address and has the option <b>options:force_fdb_lookup</b>
                     set to true.

                     For the Ethernet address on a logical switch port
                     of type <b>router</b>, when that logical switch port’s
                     <b>addresses </b>column is set to <b>router </b>and the connected
                     logical router port has a gateway chassis:

                     •      The flow for the connected logical router
                            port’s Ethernet address is only programmed
                            on the gateway chassis.

                     •      If the logical router has rules specified in
                            <b>nat </b>with <b>external_mac</b>, then those addresses
                            are also used to populate the switch’s
                            destination lookup on the chassis where
                            <b>logical_port </b>is resident.

                     For the Ethernet address on a logical switch port
                     of type <b>router</b>, when that logical switch port’s
                     <b>addresses </b>column is set to <b>router </b>and the connected
                     logical router port specifies a
                     <b>reside-on-redirect-chassis </b>and the logical router
                     to which the connected logical router port belongs
                     to has a distributed gateway LRP:

                     •      The flow for the connected logical router
                            port’s Ethernet address is only programmed
                            on the gateway chassis.

                     For each forwarding group configured on the logical
                     switch datapath, a priority-50 flow that matches on
                     <b>eth.dst == </b><i>VIP</i>
                      with an action of <b>fwd_group(childports=</b><i>args</i> <b>)</b>,
                     where <i>args</i> contains comma separated logical switch
                     child ports to load balance to. If <b>liveness </b>is
                     enabled, then action also includes  <b>liveness=true</b>.

              •      One priority-0 fallback flow that matches all
                     packets with the action <b>outport = get_fdb(eth.dst);</b>
                     <b>next;</b>. The action <b>get_fdb </b>gets the port for the
                     <b>eth.dst </b>in the MAC learning table of the logical
                     switch datapath. If there is no entry for <b>eth.dst</b>
                     in the MAC learning table, then it stores <b>none </b>in
                     the <b>outport</b>.

     <i>Ingress Table 28 Destination unknown</i>

       This table handles the packets whose destination was not found or
       and looked up in the MAC learning table of the logical switch
       datapath. It contains the following flows.

              •      Priority 50 flow with the match <b>outport == </b><i>P</i> is
                     added for each disabled Logical Switch Port <b>P</b>. This
                     flow has action <b>drop;</b>.

              •      If the logical switch has logical ports with
                     ’unknown’ addresses set, then the below logical
                     flow is added

                     •      Priority 50 flow with the match <b>outport ==</b>
                            <b>"none" </b>then outputs them to the <b>MC_UNKNOWN</b>
                            multicast group, which <b>ovn-northd </b>populates
                            with all enabled logical ports that accept
                            unknown destination packets. As a small
                            optimization, if no logical ports accept
                            unknown destination packets, <b>ovn-northd</b>
                            omits this multicast group and logical flow.

                     If the logical switch has no logical ports with
                     ’unknown’ address set, then the below logical flow
                     is added

                     •      Priority 50 flow with the match <b>outport ==</b>
                            <b>none </b>and drops the packets.

              •      One priority-0 fallback flow that outputs the
                     packet to the egress stage with the outport learnt
                     from <b>get_fdb </b>action.

     <i>Egress Table 0:</i> <b>to-lport </b><i>Pre-ACLs</i>

       This is similar to ingress table <b>Pre-ACLs </b>except for <b>to-lport</b>
       traffic.

       This table also has a priority-110 flow with the match <b>eth.src ==</b>
       <i>E</i> for all logical switch datapaths to move traffic to the next
       table. Where <i>E</i> is the service monitor mac defined in the
       <b>options:svc_monitor_mac </b>column of <b>NB_Global </b>table.

       This table also has a priority-110 flow with the match <b>outport ==</b>
       <i>I</i> for all logical switch datapaths to move traffic to the next
       table. Where <i>I</i> is the peer of a logical router port. This flow is
       added to skip the connection tracking of packets which will be
       entering logical router datapath from logical switch datapath for
       routing.

     <i>Egress Table 1: Pre-LB</i>

       This table is similar to ingress table <b>Pre-LB</b>. It contains a
       priority-0 flow that simply moves traffic to the next table.
       Moreover it contains two priority-110 flows to move multicast,
       IPv6 Neighbor Discovery and MLD traffic to the next table. If any
       load balancing rules exist for the datapath, a priority-100 flow
       is added with a match of <b>ip </b>and action of <b>reg0[2] = 1; next; </b>to
       act as a hint for table <b>Pre-stateful </b>to send IP packets to the
       connection tracker for packet de-fragmentation and possibly DNAT
       the destination VIP to one of the selected backend for already
       committed load balanced traffic.

       This table also has a priority-110 flow with the match <b>eth.src ==</b>
       <i>E</i> for all logical switch datapaths to move traffic to the next
       table. Where <i>E</i> is the service monitor mac defined in the
       <b>options:svc_monitor_mac </b>column of <b>NB_Global </b>table.

       This table also has a priority-110 flow with the match <b>outport ==</b>
       <i>I</i> for all logical switch datapaths to move traffic to the next
       table, and, if there are no stateful_acl, clear the ct_state.
       Where <i>I</i> is the peer of a logical router port. This flow is added
       to skip the connection tracking of packets which will be entering
       logical router datapath from logical switch datapath for routing.

     <i>Egress Table 2: Pre-stateful</i>

       This is similar to ingress table <b>Pre-stateful</b>. This table adds
       the below 3 logical flows.

              •      A Priority-120 flow that send the packets to
                     connection tracker using <b>ct_lb_mark; </b>as the action
                     so that the already established traffic gets
                     unDNATted from the backend IP to the load balancer
                     VIP based on a hint provided by the previous tables
                     with a match for <b>reg0[2] == 1</b>. If the packet was
                     not DNATted earlier, then <b>ct_lb_mark </b>functions like
                     <b>ct_next</b>.

              •      A priority-100 flow sends the packets to connection
                     tracker based on a hint provided by the previous
                     tables (with a match for <b>reg0[0] == 1</b>) by using the
                     <b>ct_next; </b>action.

              •      A priority-0 flow that matches all packets to
                     advance to the next table.

     <i>Egress Table 3:</i> <b>from-lport </b><i>ACL hints</i>

       This is similar to ingress table <b>ACL hints</b>.

     <i>Egress Table 4:</i> <b>to-lport </b><i>ACL evaluation</i>

       This is similar to ingress table <b>ACL eval </b>except for <b>to-lport</b>
       ACLs. As a reminder, these flows use the following register bits
       to indicate their verdicts. <b>Allow-type </b>ACLs set <b>reg8[16]</b>, <b>drop</b>
       ACLs set <b>reg8[17]</b>, and <b>reject </b>ACLs set <b>reg8[18]</b>.

       Also like with ingress ACLs, egress ACLs can have a configured
       <b>tier</b>. If a tier is configured, then the current tier counter is
       evaluated against the ACL’s configured tier in addition to the
       ACL’s match. The current tier counter is stored in <b>reg8[30..31]</b>.

       Similar to ingress table, a priority-65532 flow is added to allow
       IPv6 Neighbor solicitation, Neighbor discover, Router
       solicitation, Router advertisement and MLD packets regardless of
       other ACLs defined.

       In addition, the following flows are added.

              •      A priority 34000 logical flow is added for each
                     logical port which has DHCPv4 options defined to
                     allow the DHCPv4 reply packet and which has DHCPv6
                     options defined to allow the DHCPv6 reply packet
                     from the <b>Ingress Table 18: DHCP responses</b>. This is
                     indicated by setting the allow bit.

              •      A priority 34000 logical flow is added for each
                     logical switch datapath configured with DNS records
                     with the match <b>udp.dst = 53 </b>to allow the DNS reply
                     packet from the <b>Ingress Table 20: DNS responses</b>.
                     This is indicated by setting the allow bit.

              •      A priority 34000 logical flow is added for each
                     logical switch datapath with the match <b>eth.src = </b><i>E</i>
                     to allow the service monitor request packet
                     generated by <b>ovn-controller </b>with the action <b>next</b>,
                     where <i>E</i> is the service monitor mac defined in the
                     <b>options:svc_monitor_mac </b>column of <b>NB_Global </b>table.
                     This is indicated by setting the allow bit.

     <i>Egress Table 5:</i> <b>to-lport </b><i>ACL action</i>

       This is similar to ingress table <b>ACL action</b>.

     <i>Egress Table 6:</i> <b>to-lport </b><i>QoS Marking</i>

       This is similar to ingress table <b>QoS marking </b>except they apply to
       <b>to-lport </b>QoS rules.

     <i>Egress Table 7:</i> <b>to-lport </b><i>QoS Meter</i>

       This is similar to ingress table <b>QoS meter </b>except they apply to
       <b>to-lport </b>QoS rules.

     <i>Egress Table 8: Stateful</i>

       This is similar to ingress table <b>Stateful </b>except that there are
       no rules added for load balancing new connections.

     <i>Egress Table 9: Egress Port Security - check</i>

       This is similar to the port security logic in table <b>Ingress Port</b>
       <b>Security check </b>except that action <b>check_out_port_sec </b>is used to
       check the port security rules. This table adds the below logical
       flows.

              •      A priority 100 flow which matches on the multicast
                     traffic and applies the action
                     <b>REGBIT_PORT_SEC_DROP" = 0; next;" </b>to skip the out
                     port security checks.

              •      A priority 0 logical flow is added which matches on
                     all the packets and applies the action
                     <b>REGBIT_PORT_SEC_DROP" = check_out_port_sec();</b>
                     <b>next;"</b>. The action <b>check_out_port_sec </b>applies the
                     port security rules based on the addresses defined
                     in the <b>port_security </b>column of <b>Logical_Switch_Port</b>
                     table before delivering the packet to the <b>outport</b>.

     <i>Egress Table 10: Egress Port Security - Apply</i>

       This is similar to the ingress port security logic in ingress
       table <b>A Ingress Port Security - Apply</b>. This table drops the
       packets if the port security check failed in the previous stage
       i.e the register bit <b>REGBIT_PORT_SEC_DROP </b>is set to 1.

       The following flows are added.

              •      For each port configured with egress qos in the
                     <b>options:qdisc_queue_id </b>column of
                     <b>Logical_Switch_Port</b>, running a localnet port on the
                     same logical switch, a priority 110 flow is added
                     which matches on the localnet <b>outport </b>and on the
                     port <b>inport </b>and applies the action <b>set_queue(id);</b>
                     <b>output;"</b>.

              •      For each localnet port configured with egress qos
                     in the <b>options:qdisc_queue_id </b>column of
                     <b>Logical_Switch_Port</b>, a priority 100 flow is added
                     which matches on the localnet <b>outport </b>and applies
                     the action <b>set_queue(id); output;"</b>.

                     Please remember to mark the corresponding physical
                     interface with <b>ovn-egress-iface </b>set to true in
                     <b>external_ids</b>.

              •      A priority-50 flow that drops the packet if the
                     register bit <b>REGBIT_PORT_SEC_DROP </b>is set to 1.

              •      A priority-0 flow that outputs the packet to the
                     <b>outport</b>.

   <b>Logical Router Datapaths</b>
       Logical router datapaths will only exist for <b>Logical_Router </b>rows
       in the <b>OVN_Northbound </b>database that do not have <b>enabled </b>set to
       <b>false</b>

     <i>Ingress Table 0: L2 Admission Control</i>

       This table drops packets that the router shouldn’t see at all
       based on their Ethernet headers. It contains the following flows:

              •      Priority-100 flows to drop packets with VLAN tags
                     or multicast Ethernet source addresses.

              •      For each enabled router port <i>P</i> with Ethernet
                     address <i>E</i>, a priority-50 flow that matches <b>inport</b>
                     <b>== </b><i>P</i> <b>&amp;&amp; (eth.mcast || eth.dst == </b><i>E</i>), stores the
                     router port ethernet address and advances to next
                     table, with action <b>xreg0[0..47]=E; next;</b>.

                     For the gateway port on a distributed logical
                     router (where one of the logical router ports
                     specifies a gateway chassis), the above flow
                     matching <b>eth.dst == </b><i>E</i> is only programmed on the
                     gateway port instance on the gateway chassis. If
                     LRP’s logical switch has attached LSP of <b>vtep </b>type,
                     the <b>is_chassis_resident() </b>part is not added to
                     lflow to allow traffic originated from logical
                     switch to reach LR services (LBs, NAT).

                     For each gateway port <i>GW</i> on a distributed logical
                     router a priority-120 flow that matches
                     ’recirculated’ icmp{4,6} error ’packet too big’ and
                     <b>eth.dst == </b><i>D</i> <b>&amp;&amp; !is_chassis_resident( </b><i>cr-GW</i><b>) </b>where
                     <i>D</i> is the gateway port mac address and <i>cr-GW</i> is the
                     chassis resident port of <i>GW</i>, swap inport and
                     outport and stores <i>GW</i> as inport.

                     This table adds a priority-105 flow that matches
                     ’recirculated’ icmp{4,6} error ’packet too big’ to
                     drop the packet.

                     For a distributed logical router or for gateway
                     router where the port is configured with
                     <b>options:gateway_mtu </b>the action of the above flow is
                     modified adding <b>check_pkt_larger </b>in order to mark
                     the packet setting <b>REGBIT_PKT_LARGER </b>if the size is
                     greater than the MTU. If the port is also
                     configured with <b>options:gateway_mtu_bypass </b>then
                     another flow is added, with priority-55, to bypass
                     the <b>check_pkt_larger </b>flow. This is useful for
                     traffic that normally doesn’t need to be fragmented
                     and for which check_pkt_larger, which might not be
                     offloadable, is not really needed. One such example
                     is TCP traffic.

              •      For each <b>dnat_and_snat </b>NAT rule on a distributed
                     router that specifies an external Ethernet address
                     <i>E</i>, a priority-50 flow that matches <b>inport == </b><i>GW</i> <b>&amp;&amp;</b>
                     <b>eth.dst == </b><i>E</i>, where <i>GW</i> is the logical router
                     distributed gateway port corresponding to the NAT
                     rule (specified or inferred), with action
                     <b>xreg0[0..47]=E; next;</b>.

                     This flow is only programmed on the gateway port
                     instance on the chassis where the <b>logical_port</b>
                     specified in the NAT rule resides.

              •      A priority-0 logical flow that matches all packets
                     not already handled (match <b>1</b>) and drops them
                     (action <b>drop;</b>).

       Other packets are implicitly dropped.

     <i>Ingress Table 1: Neighbor lookup</i>

       For ARP and IPv6 Neighbor Discovery packets, this table looks
       into the <b>MAC_Binding </b>records to determine if OVN needs to learn
       the mac bindings. Following flows are added:

              •      For each router port <i>P</i> that owns IP address <i>A</i>,
                     which belongs to subnet <i>S</i> with prefix length <i>L</i>, if
                     the option <b>always_learn_from_arp_request </b>is <b>true</b>
                     for this router, a priority-100 flow is added which
                     matches <b>inport == </b><i>P</i> <b>&amp;&amp; arp.spa == </b><i>S</i><b>/</b><i>L</i> <b>&amp;&amp; arp.op ==</b>
                     <b>1 </b>(ARP request) with the following actions:

                     <b>reg9[2] = lookup_arp(inport, arp.spa, arp.sha);</b>
                     <b>next;</b>

                     If the option <b>always_learn_from_arp_request </b>is
                     <b>false</b>, the following two flows are added.

                     A priority-110 flow is added which matches <b>inport</b>
                     <b>== </b><i>P</i> <b>&amp;&amp; arp.spa == </b><i>S</i><b>/</b><i>L</i> <b>&amp;&amp; arp.tpa == </b><i>A</i> <b>&amp;&amp; arp.op ==</b>
                     <b>1 </b>(ARP request) with the following actions:

                     <b>reg9[2] = lookup_arp(inport, arp.spa, arp.sha);</b>
                     <b>reg9[3] = 1;</b>
                     <b>next;</b>

                     A priority-100 flow is added which matches <b>inport</b>
                     <b>== </b><i>P</i> <b>&amp;&amp; arp.spa == </b><i>S</i><b>/</b><i>L</i> <b>&amp;&amp; arp.op == 1 </b>(ARP request)
                     with the following actions:

                     <b>reg9[2] = lookup_arp(inport, arp.spa, arp.sha);</b>
                     <b>reg9[3] = lookup_arp_ip(inport, arp.spa);</b>
                     <b>next;</b>

                     If the logical router port <i>P</i> is a distributed
                     gateway router port, additional match
                     <b>is_chassis_resident(cr-</b><i>P</i><b>) </b>is added for all these
                     flows.

              •      A priority-100 flow which matches on ARP reply
                     packets and applies the actions if the option
                     <b>always_learn_from_arp_request </b>is <b>true</b>:

                     <b>reg9[2] = lookup_arp(inport, arp.spa, arp.sha);</b>
                     <b>next;</b>

                     If the option <b>always_learn_from_arp_request </b>is
                     <b>false</b>, the above actions will be:

                     <b>reg9[2] = lookup_arp(inport, arp.spa, arp.sha);</b>
                     <b>reg9[3] = 1;</b>
                     <b>next;</b>

              •      A priority-100 flow which matches on IPv6 Neighbor
                     Discovery advertisement packet and applies the
                     actions if the option <b>always_learn_from_arp_request</b>
                     is <b>true</b>:

                     <b>reg9[2] = lookup_nd(inport, nd.target, nd.tll);</b>
                     <b>next;</b>

                     If the option <b>always_learn_from_arp_request </b>is
                     <b>false</b>, the above actions will be:

                     <b>reg9[2] = lookup_nd(inport, nd.target, nd.tll);</b>
                     <b>reg9[3] = 1;</b>
                     <b>next;</b>

              •      A priority-100 flow which matches on IPv6 Neighbor
                     Discovery solicitation packet and applies the
                     actions if the option <b>always_learn_from_arp_request</b>
                     is <b>true</b>:

                     <b>reg9[2] = lookup_nd(inport, ip6.src, nd.sll);</b>
                     <b>next;</b>

                     If the option <b>always_learn_from_arp_request </b>is
                     <b>false</b>, the above actions will be:

                     <b>reg9[2] = lookup_nd(inport, ip6.src, nd.sll);</b>
                     <b>reg9[3] = lookup_nd_ip(inport, ip6.src);</b>
                     <b>next;</b>

              •      A priority-0 fallback flow that matches all packets
                     and applies the action <b>reg9[2] = 1; next; </b>advancing
                     the packet to the next table.

     <i>Ingress Table 2: Neighbor learning</i>

       This table adds flows to learn the mac bindings from the ARP and
       IPv6 Neighbor Solicitation/Advertisement packets if it is needed
       according to the lookup results from the previous stage.

       reg9[2] will be <b>1 </b>if the <b>lookup_arp/lookup_nd </b>in the previous
       table was successful or skipped, meaning no need to learn mac
       binding from the packet.

       reg9[3] will be <b>1 </b>if the <b>lookup_arp_ip/lookup_nd_ip </b>in the
       previous table was successful or skipped, meaning it is ok to
       learn mac binding from the packet (if reg9[2] is 0).

              •      A priority-100 flow with the match <b>reg9[2] == 1 ||</b>
                     <b>reg9[3] == 0 </b>and advances the packet to the next
                     table as there is no need to learn the neighbor.

              •      A priority-95 flow with the match <b>nd_ns &amp;&amp; (ip6.src</b>
                     <b>== 0 || nd.sll == 0) </b>and applies the action <b>next;</b>

              •      A priority-90 flow with the match <b>arp </b>and applies
                     the action <b>put_arp(inport, arp.spa, arp.sha); next;</b>

              •      A priority-95 flow with the match <b>nd_na  &amp;&amp; nd.tll</b>
                     <b>== 0 </b>and applies the action <b>put_nd(inport,</b>
                     <b>nd.target, eth.src); next;</b>

              •      A priority-90 flow with the match <b>nd_na </b>and applies
                     the action <b>put_nd(inport, nd.target, nd.tll); next;</b>

              •      A priority-90 flow with the match <b>nd_ns </b>and applies
                     the action <b>put_nd(inport, ip6.src, nd.sll); next;</b>

              •      A priority-0 logical flow that matches all packets
                     not already handled (match <b>1</b>) and drops them
                     (action <b>drop;</b>).

     <i>Ingress Table 3: IP Input</i>

       This table is the core of the logical router datapath
       functionality. It contains the following flows to implement very
       basic IP host functionality.

              •      For each <b>dnat_and_snat </b>NAT rule on a distributed
                     logical routers or gateway routers with gateway
                     port configured with <b>options:gateway_mtu </b>to a valid
                     integer value <i>M</i>, a priority-160 flow with the match
                     <b>inport == </b><i>LRP</i> <b>&amp;&amp; REGBIT_PKT_LARGER &amp;&amp;</b>
                     <b>REGBIT_EGRESS_LOOPBACK == 0</b>, where <i>LRP</i> is the
                     logical router port and applies the following
                     action for ipv4 and ipv6 respectively:

                     <b>icmp4_error {</b>
                         <b>icmp4.type = 3; /* Destination Unreachable. */</b>
                         <b>icmp4.code = 4;  /* Frag Needed and DF was Set. */</b>
                         <b>icmp4.frag_mtu = </b><i>M</i>;
                         <b>eth.dst = eth.src;</b>
                         <b>eth.src = </b><i>E</i>;
                         <b>ip4.dst = ip4.src;</b>
                         <b>ip4.src = </b><i>I</i>;
                         <b>ip.ttl = 255;</b>
                         <b>REGBIT_EGRESS_LOOPBACK = 1;</b>
                         <b>REGBIT_PKT_LARGER 0;</b>
                         <b>outport = </b><i>LRP</i>;
                         <b>flags.loopback = 1;</b>
                         <b>output;</b>
                     <b>};</b>
                     <b>icmp6_error {</b>
                         <b>icmp6.type = 2;</b>
                         <b>icmp6.code = 0;</b>
                         <b>icmp6.frag_mtu = </b><i>M</i>;
                         <b>eth.dst = eth.src;</b>
                         <b>eth.src = </b><i>E</i>;
                         <b>ip6.dst = ip6.src;</b>
                         <b>ip6.src = </b><i>I</i>;
                         <b>ip.ttl = 255;</b>
                         <b>REGBIT_EGRESS_LOOPBACK = 1;</b>
                         <b>REGBIT_PKT_LARGER 0;</b>
                         <b>outport = </b><i>LRP</i>;
                         <b>flags.loopback = 1;</b>
                         <b>output;</b>
                     <b>};</b>

                     where <i>E</i> and <i>I</i> are the NAT rule external mac and IP
                     respectively.

              •      For distributed logical routers or gateway routers
                     with gateway port configured with
                     <b>options:gateway_mtu </b>to a valid integer value, a
                     priority-150 flow with the match <b>inport == </b><i>LRP</i> <b>&amp;&amp;</b>
                     <b>REGBIT_PKT_LARGER &amp;&amp; REGBIT_EGRESS_LOOPBACK == 0</b>,
                     where <i>LRP</i> is the logical router port and applies
                     the following action for ipv4 and ipv6
                     respectively:

                     <b>icmp4_error {</b>
                         <b>icmp4.type = 3; /* Destination Unreachable. */</b>
                         <b>icmp4.code = 4;  /* Frag Needed and DF was Set. */</b>
                         <b>icmp4.frag_mtu = </b><i>M</i>;
                         <b>eth.dst = </b><i>E</i>;
                         <b>ip4.dst = ip4.src;</b>
                         <b>ip4.src = </b><i>I</i>;
                         <b>ip.ttl = 255;</b>
                         <b>REGBIT_EGRESS_LOOPBACK = 1;</b>
                         <b>REGBIT_PKT_LARGER 0;</b>
                         <b>next(pipeline=ingress, table=0);</b>
                     <b>};</b>
                     <b>icmp6_error {</b>
                         <b>icmp6.type = 2;</b>
                         <b>icmp6.code = 0;</b>
                         <b>icmp6.frag_mtu = </b><i>M</i>;
                         <b>eth.dst = </b><i>E</i>;
                         <b>ip6.dst = ip6.src;</b>
                         <b>ip6.src = </b><i>I</i>;
                         <b>ip.ttl = 255;</b>
                         <b>REGBIT_EGRESS_LOOPBACK = 1;</b>
                         <b>REGBIT_PKT_LARGER 0;</b>
                         <b>next(pipeline=ingress, table=0);</b>
                     <b>};</b>

              •      For each NAT entry of a distributed logical router
                     (with distributed gateway router port(s)) of type
                     <b>snat</b>, a priority-120 flow with the match <b>inport ==</b>
                     <i>P</i> <b>&amp;&amp; ip4.src == </b><i>A</i> advances the packet to the next
                     pipeline, where <i>P</i> is the distributed logical router
                     port corresponding to the NAT entry (specified or
                     inferred) and <i>A</i> is the <b>external_ip </b>set in the NAT
                     entry. If <i>A</i> is an IPv6 address, then <b>ip6.src </b>is
                     used for the match.

                     The above flow is required to handle the routing of
                     the East/west NAT traffic.

              •      For each BFD port the two following priority-110
                     flows are added to manage BFD traffic:

                     •      if <b>ip4.src </b>or <b>ip6.src </b>is any IP address
                            owned by the router port and <b>udp.dst == 3784</b>
                            , the packet is advanced to the next
                            pipeline stage.

                     •      if <b>ip4.dst </b>or <b>ip6.dst </b>is any IP address
                            owned by the router port and <b>udp.dst == 3784</b>
                            , the <b>handle_bfd_msg </b>action is executed.

              •      For each logical router port configured with DHCP
                     relay the following priority-110 flows are added to
                     manage the DHCP relay traffic:

                     •      if <b>inport </b>is lrp and <b>ip4.src == 0.0.0.0</b>
                             and <b>ip4.dst == 255.255.255.255 </b>and <b>ip4.frag</b>
                            <b>== 0  </b>and <b>udp.src == 68 </b>and <b>udp.dst == 67</b>,
                            the <b>dhcp_relay_req_chk</b>
                             action is executed.

                                            <b>reg9[7] = dhcp_relay_req_chk(</b><i>lrp_ip</i>,
                                                                        <i>dhcp_server_ip</i>);next

                            if action is successful then, GIADDR in the
                            dhcp header is updated with lrp ip and
                            stores 1 into reg9[7] else stores 0 into
                            reg9[7].

                     •      if <b>ip4.src </b>is DHCP server ip and <b>ip4.dst</b>
                             is lrp IP and <b>udp.src == 67 </b>and <b>udp.dst ==</b>
                            <b>67</b>, the packet is advanced to the next
                            pipeline stage.

              •      L3 admission control: Priority-120 flows allows
                     IGMP and MLD packets if the router has logical
                     ports that have <b>options :mcast_flood=’true’</b>.

              •      L3 admission control: A priority-100 flow drops
                     packets that match any of the following:

                     •      <b>ip4.src[28..31] == 0xe </b>(multicast source)

                     •      <b>ip4.src == 255.255.255.255 </b>(broadcast
                            source)

                     •      <b>ip4.src == 127.0.0.0/8 || ip4.dst ==</b>
                            <b>127.0.0.0/8 </b>(localhost source or
                            destination)

                     •      <b>ip4.src == 0.0.0.0/8 || ip4.dst == 0.0.0.0/8</b>
                            (zero network source or destination)

                     •      <b>ip4.src </b>or <b>ip6.src </b>is any IP address owned
                            by the router, unless the packet was
                            recirculated due to egress loopback as
                            indicated by <b>REGBIT_EGRESS_LOOPBACK</b>.

                     •      <b>ip4.src </b>is the broadcast address of any IP
                            network known to the router.

              •      A priority-100 flow parses DHCPv6 replies from IPv6
                     prefix delegation routers (<b>udp.src == 547 &amp;&amp;</b>
                     <b>udp.dst == 546</b>). The <b>handle_dhcpv6_reply </b>is used to
                     send IPv6 prefix delegation messages to the
                     delegation router.

              •      ICMP echo reply. These flows reply to ICMP echo
                     requests received for the router’s IP address. Let
                     <i>A</i> be an IP address owned by a router port. Then,
                     for each <i>A</i> that is an IPv4 address, a priority-90
                     flow matches on <b>ip4.dst == </b><i>A</i> and <b>icmp4.type == 8 &amp;&amp;</b>
                     <b>icmp4.code == 0 </b>(ICMP echo request). For each <i>A</i>
                     that is an IPv6 address, a priority-90 flow matches
                     on <b>ip6.dst == </b><i>A</i> and <b>icmp6.type == 128 &amp;&amp; icmp6.code</b>
                     <b>== 0 </b>(ICMPv6 echo request). The port of the router
                     that receives the echo request does not matter.
                     Also, the <b>ip.ttl </b>of the echo request packet is not
                     checked, so it complies with RFC 1812, section
                     4.2.2.9. Flows for ICMPv4 echo requests use the
                     following actions:

                     <b>ip4.dst &lt;-&gt; ip4.src;</b>
                     <b>ip.ttl = 255;</b>
                     <b>icmp4.type = 0;</b>
                     <b>flags.loopback = 1;</b>
                     <b>next;</b>

                     Flows for ICMPv6 echo requests use the following
                     actions:

                     <b>ip6.dst &lt;-&gt; ip6.src;</b>
                     <b>ip.ttl = 255;</b>
                     <b>icmp6.type = 129;</b>
                     <b>flags.loopback = 1;</b>
                     <b>next;</b>

              •      Reply to ARP requests.

                     These flows reply to ARP requests for the router’s
                     own IP address. The ARP requests are handled only
                     if the requestor’s IP belongs to the same subnets
                     of the logical router port. For each router port <i>P</i>
                     that owns IP address <i>A</i>, which belongs to subnet <i>S</i>
                     with prefix length <i>L</i>, and Ethernet address <i>E</i>, a
                     priority-90 flow matches <b>inport == </b><i>P</i> <b>&amp;&amp; arp.spa ==</b>
                     <i>S</i><b>/</b><i>L</i> <b>&amp;&amp; arp.op == 1 &amp;&amp; arp.tpa == </b><i>A</i> (ARP request)
                     with the following actions:

                     <b>eth.dst = eth.src;</b>
                     <b>eth.src = xreg0[0..47];</b>
                     <b>arp.op = 2; /* ARP reply. */</b>
                     <b>arp.tha = arp.sha;</b>
                     <b>arp.sha = xreg0[0..47];</b>
                     <b>arp.tpa = arp.spa;</b>
                     <b>arp.spa = </b><i>A</i>;
                     <b>outport = inport;</b>
                     <b>flags.loopback = 1;</b>
                     <b>output;</b>

                     For the gateway port on a distributed logical
                     router (where one of the logical router ports
                     specifies a gateway chassis), the above flows are
                     only programmed on the gateway port instance on the
                     gateway chassis. This behavior avoids generation of
                     multiple ARP responses from different chassis, and
                     allows upstream MAC learning to point to the
                     gateway chassis.

                     For the logical router port with the option
                     <b>reside-on-redirect-chassis </b>set (which is
                     centralized), the above flows are only programmed
                     on the gateway port instance on the gateway chassis
                     (if the logical router has a distributed gateway
                     port). This behavior avoids generation of multiple
                     ARP responses from different chassis, and allows
                     upstream MAC learning to point to the gateway
                     chassis.

              •      Reply to IPv6 Neighbor Solicitations. These flows
                     reply to Neighbor Solicitation requests for the
                     router’s own IPv6 address and populate the logical
                     router’s mac binding table.

                     For each router port <i>P</i> that owns IPv6 address <i>A</i>,
                     solicited node address <i>S</i>, and Ethernet address <i>E</i>, a
                     priority-90 flow matches <b>inport == </b><i>P</i> <b>&amp;&amp; nd_ns &amp;&amp;</b>
                     <b>ip6.dst == {</b><i>A</i><b>, </b><i>E</i><b>} &amp;&amp; nd.target == </b><i>A</i> with the
                     following actions:

                     <b>nd_na_router {</b>
                         <b>eth.src = xreg0[0..47];</b>
                         <b>ip6.src = </b><i>A</i>;
                         <b>nd.target = </b><i>A</i>;
                         <b>nd.tll = xreg0[0..47];</b>
                         <b>outport = inport;</b>
                         <b>flags.loopback = 1;</b>
                         <b>output;</b>
                     <b>};</b>

                     For the gateway port on a distributed logical
                     router (where one of the logical router ports
                     specifies a gateway chassis), the above flows
                     replying to IPv6 Neighbor Solicitations are only
                     programmed on the gateway port instance on the
                     gateway chassis. This behavior avoids generation of
                     multiple replies from different chassis, and allows
                     upstream MAC learning to point to the gateway
                     chassis.

              •      These flows reply to ARP requests or IPv6 neighbor
                     solicitation for the virtual IP addresses
                     configured in the router for NAT (both DNAT and
                     SNAT) or load balancing.

                     IPv4: For a configured NAT (both DNAT and SNAT) IP
                     address or a load balancer IPv4 VIP <i>A</i>, for each
                     router port <i>P</i> with Ethernet address <i>E</i>, a
                     priority-90 flow matches <b>arp.op == 1 &amp;&amp; arp.tpa ==</b>
                     <i>A</i> (ARP request) with the following actions:

                     <b>eth.dst = eth.src;</b>
                     <b>eth.src = xreg0[0..47];</b>
                     <b>arp.op = 2; /* ARP reply. */</b>
                     <b>arp.tha = arp.sha;</b>
                     <b>arp.sha = xreg0[0..47];</b>
                     <b>arp.tpa &lt;-&gt; arp.spa;</b>
                     <b>outport = inport;</b>
                     <b>flags.loopback = 1;</b>
                     <b>output;</b>

                     IPv4: For a configured load balancer IPv4 VIP, a
                     similar flow is added with the additional match
                     <b>inport == </b><i>P</i> if the VIP is reachable from any
                     logical router port of the logical router.

                     If the router port <i>P</i> is a distributed gateway
                     router port, then the <b>is_chassis_resident(</b><i>P</i><b>) </b>is
                     also added in the match condition for the load
                     balancer IPv4 VIP <i>A</i>.

                     IPv6: For a configured NAT (both DNAT and SNAT) IP
                     address or a load balancer IPv6 VIP <i>A</i> (if the VIP
                     is reachable from any logical router port of the
                     logical router), solicited node address <i>S</i>, for each
                     router port <i>P</i> with Ethernet address <i>E</i>, a
                     priority-90 flow matches <b>inport == </b><i>P</i> <b>&amp;&amp; nd_ns &amp;&amp;</b>
                     <b>ip6.dst == {</b><i>A</i><b>, </b><i>S</i><b>} &amp;&amp; nd.target == </b><i>A</i> with the
                     following actions:

                     <b>eth.dst = eth.src;</b>
                     <b>nd_na {</b>
                         <b>eth.src = xreg0[0..47];</b>
                         <b>nd.tll = xreg0[0..47];</b>
                         <b>ip6.src = </b><i>A</i>;
                         <b>nd.target = </b><i>A</i>;
                         <b>outport = inport;</b>
                         <b>flags.loopback = 1;</b>
                         <b>output;</b>
                     <b>}</b>

                     If the router port <i>P</i> is a distributed gateway
                     router port, then the <b>is_chassis_resident(</b><i>P</i><b>) </b>is
                     also added in the match condition for the load
                     balancer IPv6 VIP <i>A</i>.

                     For the gateway port on a distributed logical
                     router with NAT (where one of the logical router
                     ports specifies a gateway chassis):

                     •      If the corresponding NAT rule cannot be
                            handled in a distributed manner, then a
                            priority-92 flow is programmed on the
                            gateway port instance on the gateway
                            chassis. A priority-91 drop flow is
                            programmed on the other chassis when ARP
                            requests/NS packets are received on the
                            gateway port. This behavior avoids
                            generation of multiple ARP responses from
                            different chassis, and allows upstream MAC
                            learning to point to the gateway chassis.

                     •      If the corresponding NAT rule can be handled
                            in a distributed manner, then this flow is
                            only programmed on the gateway port instance
                            where the <b>logical_port </b>specified in the NAT
                            rule resides.

                            Some of the actions are different for this
                            case, using the <b>external_mac </b>specified in
                            the NAT rule rather than the gateway port’s
                            Ethernet address <i>E</i>:

                            <b>eth.src = </b><i>external_mac</i>;
                            <b>arp.sha = </b><i>external_mac</i>;

                            or in the case of IPv6 neighbor solicition:

                            <b>eth.src = </b><i>external_mac</i>;
                            <b>nd.tll = </b><i>external_mac</i>;

                            This behavior avoids generation of multiple
                            ARP responses from different chassis, and
                            allows upstream MAC learning to point to the
                            correct chassis.

              •      Priority-85 flows which drops the ARP and IPv6
                     Neighbor Discovery packets.

              •      A priority-84 flow explicitly allows IPv6 multicast
                     traffic that is supposed to reach the router
                     pipeline (i.e., router solicitation and router
                     advertisement packets).

              •      A priority-83 flow explicitly drops IPv6 multicast
                     traffic that is destined to reserved multicast
                     groups.

              •      A priority-82 flow allows IP multicast traffic if
                     <b>options</b>:mcast_relay=’true’, otherwise drops it.

              •      UDP port unreachable. Priority-80 flows generate
                     ICMP port unreachable messages in reply to UDP
                     datagrams directed to the router’s IP address,
                     except in the special case of gateways, which
                     accept traffic directed to a router IP for load
                     balancing and NAT purposes.

                     These flows should not match IP fragments with
                     nonzero offset.

              •      TCP reset. Priority-80 flows generate TCP reset
                     messages in reply to TCP datagrams directed to the
                     router’s IP address, except in the special case of
                     gateways, which accept traffic directed to a router
                     IP for load balancing and NAT purposes.

                     These flows should not match IP fragments with
                     nonzero offset.

              •      Protocol or address unreachable. Priority-70 flows
                     generate ICMP protocol or address unreachable
                     messages for IPv4 and IPv6 respectively in reply to
                     packets directed to the router’s IP address on IP
                     protocols other than UDP, TCP, and ICMP, except in
                     the special case of gateways, which accept traffic
                     directed to a router IP for load balancing
                     purposes.

                     These flows should not match IP fragments with
                     nonzero offset.

              •      Drop other IP traffic to this router. These flows
                     drop any other traffic destined to an IP address of
                     this router that is not already handled by one of
                     the flows above, which amounts to ICMP (other than
                     echo requests) and fragments with nonzero offsets.
                     For each IP address <i>A</i> owned by the router, a
                     priority-60 flow matches <b>ip4.dst == </b><i>A</i> or <b>ip6.dst ==</b>
                     <i>A</i> and drops the traffic. An exception is made and
                     the above flow is not added if the router port’s
                     own IP address is used to SNAT packets passing
                     through that router or if it is used as a load
                     balancer VIP.

       The flows above handle all of the traffic that might be directed
       to the router itself. The following flows (with lower priorities)
       handle the remaining traffic, potentially for forwarding:

              •      Drop Ethernet local broadcast. A priority-50 flow
                     with match <b>eth.bcast </b>drops traffic destined to the
                     local Ethernet broadcast address. By definition
                     this traffic should not be forwarded.

              •      Avoid ICMP time exceeded for multicast. A
                     priority-32 flow with match <b>ip.ttl == {0, 1} &amp;&amp;</b>
                     <b>!ip.later_frag &amp;&amp; (ip4.mcast || ip6.mcast) </b>and
                     actions <b>drop; </b>drops multicast packets whose TTL has
                     expired without sending ICMP time exceeded.

              •      ICMP time exceeded. For each router port <i>P</i>, whose
                     IP address is <i>A</i>, a priority-31 flow with match
                     <b>inport == </b><i>P</i> <b>&amp;&amp; ip.ttl == {0, 1} &amp;&amp; !ip.later_frag</b>
                     matches packets whose TTL has expired, with the
                     following actions to send an ICMP time exceeded
                     reply for IPv4 and IPv6 respectively:

                     <b>icmp4 {</b>
                         <b>icmp4.type = 11; /* Time exceeded. */</b>
                         <b>icmp4.code = 0;  /* TTL exceeded in transit. */</b>
                         <b>ip4.dst = ip4.src;</b>
                         <b>ip4.src = </b><i>A</i>;
                         <b>ip.ttl = 254;</b>
                         <b>next;</b>
                     <b>};</b>
                     <b>icmp6 {</b>
                         <b>icmp6.type = 3; /* Time exceeded. */</b>
                         <b>icmp6.code = 0;  /* TTL exceeded in transit. */</b>
                         <b>ip6.dst = ip6.src;</b>
                         <b>ip6.src = </b><i>A</i>;
                         <b>ip.ttl = 254;</b>
                         <b>next;</b>
                     <b>};</b>

              •      TTL discard. A priority-30 flow with match <b>ip.ttl</b>
                     <b>== {0, 1} </b>and actions <b>drop; </b>drops other packets
                     whose TTL has expired, that should not receive a
                     ICMP error reply (i.e. fragments with nonzero
                     offset).

              •      Next table. A priority-0 flows match all packets
                     that aren’t already handled and uses actions <b>next;</b>
                     to feed them to the next table.

     <i>Ingress Table 4: DHCP Relay Request</i>

       This stage process the DHCP request packets on which
       <b>dhcp_relay_req_chk </b>action is applied in the IP input stage.

              •      A priority-100 logical flow is added for each
                     logical router port configured with DHCP relay that
                     matches <b>inport </b>is lrp and <b>ip4.src == 0.0.0.0 </b>and
                     <b>ip4.dst == 255.255.255.255 </b>and <b>udp.src == 68</b>
                      and <b>udp.dst == 67 </b>and <b>reg9[7] == 1 </b>and applies
                     following actions. If <b>reg9[7] </b>is set to 1 then,
                     <b>dhcp_relay_req_chk </b>action was successful.

                     <b>ip4.src=</b><i>lrp ip</i>;
                     <b>ip4.dst=</b><i>dhcp server ip</i>;
                     <b>udp.src = 67;</b>
                     <b>next;</b>

              •      A priority-1 logical flow is added for each logical
                     router port configured with DHCP relay that matches
                     <b>inport </b>is lrp and <b>ip4.src == 0.0.0.0 </b>and <b>ip4.dst ==</b>
                     <b>255.255.255.255 </b>and <b>udp.src == 68</b>
                      and <b>udp.dst == 67 </b>and <b>reg9[7] == 0 </b>and drops the
                     packet. If <b>reg9[7] </b>is set to 0 then,
                     <b>dhcp_relay_req_chk </b>action was unsuccessful.

              •      A priority-0 flow that matches all packets to
                     advance to the next table.

     <i>Ingress Table 5: UNSNAT</i>

       This is for already established connections’ reverse traffic.
       i.e., SNAT has already been done in egress pipeline and now the
       packet has entered the ingress pipeline as part of a reply. It is
       unSNATted here.

       Ingress Table 5: UNSNAT on Gateway and Distributed Routers

              •      If the Router (Gateway or Distributed) is
                     configured with load balancers, then below lflows
                     are added:

                     For each IPv4 address <i>A</i> defined as load balancer
                     VIP with the protocol <i>P</i> (and the protocol port <i>T</i> if
                     defined) is also present as an <b>external_ip </b>in the
                     NAT table, a priority-120 logical flow is added
                     with the match <b>ip4 &amp;&amp; ip4.dst == </b><i>A</i> <b>&amp;&amp; </b><i>P</i> with the
                     action <b>next; </b>to advance the packet to the next
                     table. If the load balancer has protocol port <b>B</b>
                     defined, then the match also has <i>P</i><b>.dst == </b><i>B</i>.

                     The above flows are also added for IPv6 load
                     balancers.

       Ingress Table 5: UNSNAT on Gateway Routers

              •      If the Gateway router has been configured to force
                     SNAT any previously DNATted packets to <i>B</i>, a
                     priority-110 flow matches <b>ip &amp;&amp; ip4.dst == </b><i>B</i> or <b>ip</b>
                     <b>&amp;&amp; ip6.dst == </b><i>B</i> with an action <b>ct_snat; </b>.

                     If the Gateway router is configured with
                     <b>lb_force_snat_ip=router_ip </b>then for every logical
                     router port <i>P</i> attached to the Gateway router with
                     the router ip <i>B</i>, a priority-110 flow is added with
                     the match <b>inport == </b><i>P</i> <b>&amp;&amp; ip4.dst == </b><i>B</i> or <b>inport ==</b>
                     <i>P</i> <b>&amp;&amp; ip6.dst == </b><i>B</i> with an action <b>ct_snat; </b>.

                     If the Gateway router has been configured to force
                     SNAT any previously load-balanced packets to <i>B</i>, a
                     priority-100 flow matches <b>ip &amp;&amp; ip4.dst == </b><i>B</i> or <b>ip</b>
                     <b>&amp;&amp; ip6.dst == </b><i>B</i> with an action <b>ct_snat; </b>.

                     For each NAT configuration in the OVN Northbound
                     database, that asks to change the source IP address
                     of a packet from <i>A</i> to <i>B</i>, a priority-90 flow matches
                     <b>ip &amp;&amp; ip4.dst == </b><i>B</i> or <b>ip &amp;&amp; ip6.dst == </b><i>B</i> with an
                     action <b>ct_snat; </b>. If the NAT rule is of type
                     dnat_and_snat and has <b>stateless=true </b>in the
                     options, then the action would be <b>next;</b>.

                     A priority-0 logical flow with match <b>1 </b>has actions
                     <b>next;</b>.

       Ingress Table 5: UNSNAT on Distributed Routers

              •      For each configuration in the OVN Northbound
                     database, that asks to change the source IP address
                     of a packet from <i>A</i> to <i>B</i>, two priority-100 flows are
                     added.

                     If the NAT rule cannot be handled in a distributed
                     manner, then the below priority-100 flows are only
                     programmed on the gateway chassis.

                     •      The first flow matches <b>ip &amp;&amp; ip4.dst == </b><i>B</i> <b>&amp;&amp;</b>
                            <b>inport == </b><i>GW</i>
                             or <b>ip &amp;&amp; ip6.dst == </b><i>B</i> <b>&amp;&amp; inport == </b><i>GW</i> where
                            <i>GW</i> is the distributed gateway port
                            corresponding to the NAT rule (specified or
                            inferred), with an action <b>ct_snat; </b>to unSNAT
                            in the common zone. If the NAT rule is of
                            type dnat_and_snat and has <b>stateless=true </b>in
                            the options, then the action would be <b>next;</b>.

                            If the NAT entry is of type <b>snat</b>, then there
                            is an additional match
                            <b>is_chassis_resident(</b><i>cr-GW</i><b>)</b>
                             where <i>cr-GW</i> is the chassis resident port of
                            <i>GW</i>.

                     A priority-0 logical flow with match <b>1 </b>has actions
                     <b>next;</b>.

     <i>Ingress Table 6: DEFRAG</i>

       This is to send packets to connection tracker for tracking and
       defragmentation. It contains a priority-0 flow that simply moves
       traffic to the next table.

       For all load balancing rules that are configured in
       <b>OVN_Northbound </b>database for a Gateway router, a priority-100 flow
       is added for each configured virtual IP address <i>VIP</i>. For IPv4
       <i>VIPs</i> the flow matches <b>ip &amp;&amp; ip4.dst == </b><i>VIP</i>. For IPv6 <i>VIPs</i>, the
       flow matches <b>ip &amp;&amp; ip6.dst == </b><i>VIP</i>. The flow applies the action
       <b>ct_dnat; </b>to send IP packets to the connection tracker for packet
       de-fragmentation and to dnat the destination IP for the committed
       connection before sending it to the next table.

       If ECMP routes with symmetric reply are configured in the
       <b>OVN_Northbound </b>database for a gateway router, a priority-100 flow
       is added for each router port on which symmetric replies are
       configured. The matching logic for these ports essentially
       reverses the configured logic of the ECMP route. So for instance,
       a route with a destination routing policy will instead match if
       the source IP address matches the static route’s prefix. The flow
       uses the actions <b>chk_ecmp_nh_mac(); ct_next </b>or <b>chk_ecmp_nh();</b>
       <b>ct_next </b>to send IP packets to table <b>76 </b>or to table <b>77 </b>in order to
       check if source info are already stored by OVN and then to the
       connection tracker for packet de-fragmentation and tracking
       before sending it to the next table.

       If load balancing rules are configured in <b>OVN_Northbound </b>database
       for a Gateway router, a priority 50 flow that matches <b>icmp ||</b>
       <b>icmp6 </b>with an action of <b>ct_dnat;</b>, this allows potentially related
       ICMP traffic to pass through CT.

     <i>Ingress Table 7: Load balancing affinity check</i>

       Load balancing affinity check table contains the following
       logical flows:

              •      For all the configured load balancing rules for a
                     logical router where a positive affinity timeout is
                     specified in <b>options </b>column, that includes a L4
                     port <i>PORT</i> of protocol <i>P</i> and IPv4 or IPv6 address
                     <i>VIP</i>, a priority-100 flow that matches on <b>ct.new &amp;&amp;</b>
                     <b>ip &amp;&amp; ip.dst == </b><i>VIP</i> <b>&amp;&amp; </b><i>P</i> <b>&amp;&amp; P.dst ==  </b><i>PORT</i> (<b>xxreg0</b>
                     <b>== </b><i>VIP</i>
                      in the IPv6 case) with an action of <b>reg0 = ip.dst;</b>
                     <b>reg9[16..31] = P.dst; reg9[6] = chk_lb_aff(); next;</b>
                     (<b>xxreg0 == </b><i>ip6.dst</i>  in the IPv6 case)

              •      A priority 0 flow is added which matches on all
                     packets and applies the action <b>next;</b>.

     <i>Ingress Table 8: DNAT</i>

       Packets enter the pipeline with destination IP address that needs
       to be DNATted from a virtual IP address to a real IP address.
       Packets in the reverse direction needs to be unDNATed.

       Ingress Table 8: Load balancing DNAT rules

       Following load balancing DNAT flows are added for Gateway router
       or Router with gateway port. These flows are programmed only on
       the gateway chassis. These flows do not get programmed for load
       balancers with IPv6 <i>VIPs</i>.

              •      For all the configured load balancing rules for a
                     logical router where a positive affinity timeout is
                     specified in <b>options </b>column, that includes a L4
                     port <i>PORT</i> of protocol <i>P</i> and IPv4 or IPv6 address
                     <i>VIP</i>, a priority-150 flow that matches on <b>reg9[6] ==</b>
                     <b>1 &amp;&amp; ct.new &amp;&amp; ip &amp;&amp; ip.dst == </b><i>VIP</i> <b>&amp;&amp; </b><i>P</i> <b>&amp;&amp; P.dst ==</b>
                     <i>PORT</i> with an action of <b>ct_lb_mark(</b><i>args</i><b>) </b>, where
                     <i>args</i> contains comma separated IP addresses (and
                     optional port numbers) to load balance to. The
                     address family of the IP addresses of <i>args</i> is the
                     same as the address family of <i>VIP</i>.

              •      If controller_event has been enabled for all the
                     configured load balancing rules for a Gateway
                     router or Router with gateway port in
                     <b>OVN_Northbound </b>database that does not have
                     configured backends, a priority-130 flow is added
                     to trigger ovn-controller events whenever the
                     chassis receives a packet for that particular VIP.
                     If <b>event-elb </b>meter has been previously created, it
                     will be associated to the empty_lb logical flow

              •      For all the configured load balancing rules for a
                     Gateway router or Router with gateway port in
                     <b>OVN_Northbound </b>database that includes a L4 port
                     <i>PORT</i> of protocol <i>P</i> and IPv4 or IPv6 address <i>VIP</i>, a
                     priority-120 flow that matches on <b>ct.new &amp;&amp; !ct.rel</b>
                     <b>&amp;&amp; ip &amp;&amp; ip.dst == </b><i>VIP</i> <b>&amp;&amp; </b><i>P</i> <b>&amp;&amp; P.dst ==</b>
                      <i>PORT</i> with an action of <b>ct_lb_mark(</b><i>args</i><b>)</b>, where
                     <i>args</i> contains comma separated IPv4 or IPv6
                     addresses (and optional port numbers) to load
                     balance to. If the router is configured to force
                     SNAT any load-balanced packets, the above action
                     will be replaced by <b>flags.force_snat_for_lb = 1;</b>
                     <b>ct_lb_mark(</b><i>args</i><b>; force_snat);</b>. If the load
                     balancing rule is configured with <b>skip_snat </b>set to
                     true, the above action will be replaced by
                     <b>flags.skip_snat_for_lb = 1; ct_lb_mark(</b><i>args</i><b>;</b>
                     <b>skip_snat);</b>. If health check is enabled, then <i>args</i>
                     will only contain those endpoints whose service
                     monitor status entry in <b>OVN_Southbound </b>db is either
                     <b>online </b>or empty.

              •      For all the configured load balancing rules for a
                     router in <b>OVN_Northbound </b>database that includes
                     just an IP address <i>VIP</i> to match on, a priority-110
                     flow that matches on <b>ct.new &amp;&amp; !ct.rel &amp;&amp; ip4 &amp;&amp;</b>
                     <b>ip.dst == </b><i>VIP</i> with an action of <b>ct_lb_mark(</b><i>args</i><b>)</b>,
                     where <i>args</i> contains comma separated IPv4 or IPv6
                     addresses. If the router is configured to force
                     SNAT any load-balanced packets, the above action
                     will be replaced by <b>flags.force_snat_for_lb = 1;</b>
                     <b>ct_lb_mark(</b><i>args</i><b>; force_snat);</b>. If the load
                     balancing rule is configured with <b>skip_snat </b>set to
                     true, the above action will be replaced by
                     <b>flags.skip_snat_for_lb = 1; ct_lb_mark(</b><i>args</i><b>;</b>
                     <b>skip_snat);</b>.

                     The previous table <b>lr_in_defrag </b>sets the register
                     <b>reg0 </b>(or <b>xxreg0 </b>for IPv6) and does <b>ct_dnat</b>. Hence
                     for established traffic, this table just advances
                     the packet to the next stage.

              •      If the load balancer is created with <b>--reject</b>
                     option and it has no active backends, a TCP reset
                     segment (for tcp) or an ICMP port unreachable
                     packet (for all other kind of traffic) will be sent
                     whenever an incoming packet is received for this
                     load-balancer. Please note using <b>--reject </b>option
                     will disable empty_lb SB controller event for this
                     load balancer.

              •      For the related traffic, a priority 50 flow that
                     matches <b>ct.rel &amp;&amp; !ct.est &amp;&amp; !ct.new  </b>with an
                     action of <b>ct_commit_nat;</b>, if the router has load
                     balancer assigned to it. Along with two priority 70
                     flows that match <b>skip_snat </b>and <b>force_snat </b>flags,
                     setting the <b>flags.force_snat_for_lb = 1 </b>or
                     <b>flags.skip_snat_for_lb = 1 </b>accordingly.

              •      For the established traffic, a priority 50 flow
                     that matches <b>ct.est &amp;&amp; !ct.rel &amp;&amp; !ct.new &amp;&amp;</b>
                     <b>ct_mark.natted </b>with an action of <b>next;</b>, if the
                     router has load balancer assigned to it. Along with
                     two priority 70 flows that match <b>skip_snat </b>and
                     <b>force_snat </b>flags, setting the
                     <b>flags.force_snat_for_lb = 1 </b>or
                     <b>flags.skip_snat_for_lb = 1 </b>accordingly.

       Ingress Table 8: DNAT on Gateway Routers

              •      For each configuration in the OVN Northbound
                     database, that asks to change the destination IP
                     address of a packet from <i>A</i> to <i>B</i>, a priority-100
                     flow matches <b>ip &amp;&amp; ip4.dst == </b><i>A</i> or <b>ip &amp;&amp; ip6.dst ==</b>
                     <i>A</i> with an action <b>flags.loopback = 1; ct_dnat(</b><i>B</i><b>);</b>.
                     If the Gateway router is configured to force SNAT
                     any DNATed packet, the above action will be
                     replaced by <b>flags.force_snat_for_dnat = 1;</b>
                     <b>flags.loopback = 1; ct_dnat(</b><i>B</i><b>);</b>. If the NAT rule is
                     of type dnat_and_snat and has <b>stateless=true </b>in the
                     options, then the action would be <b>ip4/6.dst= (</b><i>B</i><b>)</b>.

                     If the NAT rule has <b>allowed_ext_ips </b>configured,
                     then there is an additional match <b>ip4.src ==</b>
                     <i>allowed_ext_ips</i> . Similarly, for IPV6, match would
                     be <b>ip6.src == </b><i>allowed_ext_ips</i>.

                     If the NAT rule has <b>exempted_ext_ips </b>set, then
                     there is an additional flow configured at priority
                     101. The flow matches if source ip is an
                     <b>exempted_ext_ip </b>and the action is <b>next; </b>. This flow
                     is used to bypass the ct_dnat action for a packet
                     originating from <b>exempted_ext_ips</b>.

              •      A priority-0 logical flow with match <b>1 </b>has actions
                     <b>next;</b>.

       Ingress Table 8: DNAT on Distributed Routers

       On distributed routers, the DNAT table only handles packets with
       destination IP address that needs to be DNATted from a virtual IP
       address to a real IP address. The unDNAT processing in the
       reverse direction is handled in a separate table in the egress
       pipeline.

              •      For each configuration in the OVN Northbound
                     database, that asks to change the destination IP
                     address of a packet from <i>A</i> to <i>B</i>, a priority-100
                     flow matches <b>ip &amp;&amp; ip4.dst == </b><i>B</i> <b>&amp;&amp; inport == </b><i>GW</i>,
                     where <i>GW</i> is the logical router gateway port
                     corresponding to the NAT rule (specified or
                     inferred), with an action <b>ct_dnat(</b><i>B</i><b>);</b>. The match
                     will include <b>ip6.dst == </b><i>B</i> in the IPv6 case. If the
                     NAT rule is of type dnat_and_snat and has
                     <b>stateless=true </b>in the options, then the action
                     would be <b>ip4/6.dst=(</b><i>B</i><b>)</b>.

                     If the NAT rule cannot be handled in a distributed
                     manner, then the priority-100 flow above is only
                     programmed on the gateway chassis.

                     If the NAT rule has <b>allowed_ext_ips </b>configured,
                     then there is an additional match <b>ip4.src ==</b>
                     <i>allowed_ext_ips</i> . Similarly, for IPV6, match would
                     be <b>ip6.src == </b><i>allowed_ext_ips</i>.

                     If the NAT rule has <b>exempted_ext_ips </b>set, then
                     there is an additional flow configured at priority
                     101. The flow matches if source ip is an
                     <b>exempted_ext_ip </b>and the action is <b>next; </b>. This flow
                     is used to bypass the ct_dnat action for a packet
                     originating from <b>exempted_ext_ips</b>.

                     A priority-0 logical flow with match <b>1 </b>has actions
                     <b>next;</b>.

     <i>Ingress Table 9: Load balancing affinity learn</i>

       Load balancing affinity learn table contains the following
       logical flows:

              •      For all the configured load balancing rules for a
                     logical router where a positive affinity timeout <i>T</i>
                     is specified in <b>options</b>
                      column, that includes a L4 port <i>PORT</i> of protocol <i>P</i>
                     and IPv4 or IPv6 address <i>VIP</i>, a priority-100 flow
                     that matches on <b>reg9[6] == 0 &amp;&amp; ct.new &amp;&amp; ip &amp;&amp;</b>
                     <b>reg0 == </b><i>VIP</i> <b>&amp;&amp; </b><i>P</i> <b>&amp;&amp; reg9[16..31] ==  </b><i>PORT</i> (<b>xxreg0</b>
                     <b>== </b><i>VIP</i>  in the IPv6 case) with an action of
                     <b>commit_lb_aff(vip = </b><i>VIP</i><b>:</b><i>PORT</i><b>, backend = </b><i>backend ip</i><b>:</b>
                     <i>backend port</i><b>, proto = </b><i>P</i><b>, timeout = </b><i>T</i><b>);</b>.

              •      A priority 0 flow is added which matches on all
                     packets and applies the action <b>next;</b>.

     <i>Ingress Table 10: ECMP symmetric reply processing</i>

              •      If ECMP routes with symmetric reply are configured
                     in the <b>OVN_Northbound </b>database for a gateway
                     router, a priority-100 flow is added for each
                     router port on which symmetric replies are
                     configured. The matching logic for these ports
                     essentially reverses the configured logic of the
                     ECMP route. So for instance, a route with a
                     destination routing policy will instead match if
                     the source IP address matches the static route’s
                     prefix. The flow uses the action <b>ct_commit {</b>
                     <b>ct_label.ecmp_reply_eth = eth.src;" "</b>
                     <b>ct_mark.ecmp_reply_port = </b><i>K</i><b>;}; commit_ecmp_nh();</b>
                     <b>next;</b>
                      to commit the connection and storing <b>eth.src </b>and
                     the ECMP reply port binding tunnel key <i>K</i> in the
                     <b>ct_label </b>and the traffic pattern to table <b>76 </b>or <b>77</b>.

     <i>Ingress Table 11: IPv6 ND RA option processing</i>

              •      A priority-50 logical flow is added for each
                     logical router port configured with IPv6 ND RA
                     options which matches IPv6 ND Router Solicitation
                     packet and applies the action <b>put_nd_ra_opts </b>and
                     advances the packet to the next table.

                     <b>reg0[5] = put_nd_ra_opts(</b><i>options</i>);next;

                     For a valid IPv6 ND RS packet, this transforms the
                     packet into an IPv6 ND RA reply and sets the RA
                     options to the packet and stores 1 into reg0[5].
                     For other kinds of packets, it just stores 0 into
                     reg0[5]. Either way, it continues to the next
                     table.

              •      A priority-0 logical flow with match <b>1 </b>has actions
                     <b>next;</b>.

     <i>Ingress Table 12: IPv6 ND RA responder</i>

       This table implements IPv6 ND RA responder for the IPv6 ND RA
       replies generated by the previous table.

              •      A priority-50 logical flow is added for each
                     logical router port configured with IPv6 ND RA
                     options which matches IPv6 ND RA packets and
                     <b>reg0[5] == 1 </b>and responds back to the <b>inport </b>after
                     applying these actions. If <b>reg0[5] </b>is set to 1, it
                     means that the action <b>put_nd_ra_opts </b>was
                     successful.

                     <b>eth.dst = eth.src;</b>
                     <b>eth.src = </b><i>E</i>;
                     <b>ip6.dst = ip6.src;</b>
                     <b>ip6.src = </b><i>I</i>;
                     <b>outport = </b><i>P</i>;
                     <b>flags.loopback = 1;</b>
                     <b>output;</b>

                     where <i>E</i> is the MAC address and <i>I</i> is the IPv6 link
                     local address of the logical router port.

                     (This terminates packet processing in ingress
                     pipeline; the packet does not go to the next
                     ingress table.)

              •      A priority-0 logical flow with match <b>1 </b>has actions
                     <b>next;</b>.

     <i>Ingress Table 13: IP Routing Pre</i>

       If a packet arrived at this table from Logical Router Port <i>P</i>
       which has <b>options:route_table </b>value set, a logical flow with
       match <b>inport == "</b><i>P</i><b>" </b>with priority 100 and action setting unique-
       generated per-datapath 32-bit value (non-zero) in OVS register 7.
       This register’s value is checked in next table. If packet didn’t
       match any configured inport (<i>&lt;main&gt;</i> route table), register 7
       value is set to 0.

       This table contains the following logical flows:

              •      Priority-100 flow with match <b>inport == "LRP_NAME"</b>
                     value and action, which set route table identifier
                     in reg7.

                     A priority-0 logical flow with match <b>1 </b>has actions
                     <b>reg7 = 0; next;</b>.

     <i>Ingress Table 14: IP Routing</i>

       A packet that arrives at this table is an IP packet that should
       be routed to the address in <b>ip4.dst </b>or <b>ip6.dst</b>. This table
       implements IP routing, setting <b>reg0 </b>(or <b>xxreg0 </b>for IPv6) to the
       next-hop IP address (leaving <b>ip4.dst </b>or <b>ip6.dst</b>, the packet’s
       final destination, unchanged) and advances to the next table for
       ARP resolution. It also sets <b>reg1 </b>(or <b>xxreg1</b>) to the IP address
       owned by the selected router port (ingress table <b>ARP Request </b>will
       generate an ARP request, if needed, with <b>reg0 </b>as the target
       protocol address and <b>reg1 </b>as the source protocol address).

       For ECMP routes, i.e. multiple static routes with same policy and
       prefix but different nexthops, the above actions are deferred to
       next table. This table, instead, is responsible for determine the
       ECMP group id and select a member id within the group based on
       5-tuple hashing. It stores group id in <b>reg8[0..15] </b>and member id
       in <b>reg8[16..31]</b>. This step is skipped with a priority-10300 rule
       if the traffic going out the ECMP route is reply traffic, and the
       ECMP route was configured to use symmetric replies. Instead, the
       stored values in conntrack is used to choose the destination. The
       <b>ct_label.ecmp_reply_eth </b>tells the destination MAC address to
       which the packet should be sent. The <b>ct_mark.ecmp_reply_port</b>
       tells the logical router port on which the packet should be sent.
       These values saved to the conntrack fields when the initial
       ingress traffic is received over the ECMP route and committed to
       conntrack. If <b>REGBIT_KNOWN_ECMP_NH </b>is set, the priority-10300
       flows in this stage set the <b>outport</b>, while the <b>eth.dst </b>is set by
       flows at the ARP/ND Resolution stage.

       This table contains the following logical flows:

              •      Priority-10550 flow that drops IPv6 Router
                     Solicitation/Advertisement packets that were not
                     processed in previous tables.

              •      Priority-10550 flows that drop IGMP and MLD packets
                     with source MAC address owned by the router. These
                     are used to prevent looping statically forwarded
                     IGMP and MLD packets for which TTL is not
                     decremented (it is always 1).

              •      Priority-10500 flows that match IP multicast
                     traffic destined to groups registered on any of the
                     attached switches and sets <b>outport </b>to the
                     associated multicast group that will eventually
                     flood the traffic to all interested attached
                     logical switches. The flows also decrement TTL.

              •      Priority-10460 flows that match IGMP and MLD
                     control packets, set <b>outport </b>to the <b>MC_STATIC</b>
                     multicast group, which <b>ovn-northd </b>populates with
                     the logical ports that have <b>options</b>
                     <b>:mcast_flood=’true’</b>. If no router ports are
                     configured to flood multicast traffic the packets
                     are dropped.

              •      Priority-10450 flow that matches unregistered IP
                     multicast traffic decrements TTL and sets <b>outport</b>
                     to the <b>MC_STATIC </b>multicast group, which <b>ovn-northd</b>
                     populates with the logical ports that have <b>options</b>
                     <b>:mcast_flood=’true’</b>. If no router ports are
                     configured to flood multicast traffic the packets
                     are dropped.

              •      IPv4 routing table. For each route to IPv4 network
                     <i>N</i> with netmask <i>M</i>, on router port <i>P</i> with IP address
                     <i>A</i> and Ethernet address <i>E</i>, a logical flow with match
                     <b>ip4.dst == </b><i>N</i><b>/</b><i>M</i>, whose priority is the number of
                     1-bits in <i>M</i>, has the following actions:

                     <b>ip.ttl--;</b>
                     <b>reg8[0..15] = 0;</b>
                     <b>reg0 = </b><i>G</i>;
                     <b>reg1 = </b><i>A</i>;
                     <b>eth.src = </b><i>E</i>;
                     <b>outport = </b><i>P</i>;
                     <b>flags.loopback = 1;</b>
                     <b>next;</b>

                     (Ingress table 1 already verified that <b>ip.ttl--;</b>
                     will not yield a TTL exceeded error.)

                     If the route has a gateway, <i>G</i> is the gateway IP
                     address. Instead, if the route is from a configured
                     static route, <i>G</i> is the next hop IP address. Else it
                     is <b>ip4.dst</b>.

              •      IPv6 routing table. For each route to IPv6 network
                     <i>N</i> with netmask <i>M</i>, on router port <i>P</i> with IP address
                     <i>A</i> and Ethernet address <i>E</i>, a logical flow with match
                     in CIDR notation <b>ip6.dst == </b><i>N</i><b>/</b><i>M</i>, whose priority is
                     the integer value of <i>M</i>, has the following actions:

                     <b>ip.ttl--;</b>
                     <b>reg8[0..15] = 0;</b>
                     <b>xxreg0 = </b><i>G</i>;
                     <b>xxreg1 = </b><i>A</i>;
                     <b>eth.src = </b><i>E</i>;
                     <b>outport = inport;</b>
                     <b>flags.loopback = 1;</b>
                     <b>next;</b>

                     (Ingress table 1 already verified that <b>ip.ttl--;</b>
                     will not yield a TTL exceeded error.)

                     If the route has a gateway, <i>G</i> is the gateway IP
                     address. Instead, if the route is from a configured
                     static route, <i>G</i> is the next hop IP address. Else it
                     is <b>ip6.dst</b>.

                     If the address <i>A</i> is in the link-local scope, the
                     route will be limited to sending on the ingress
                     port.

                     For each static route the <b>reg7 == id &amp;&amp; </b>is prefixed
                     in logical flow match portion. For routes with
                     <b>route_table </b>value set a unique non-zero id is used.
                     For routes within <b>&lt;main&gt; </b>route table (no route
                     table set), this id value is 0.

                     For each <i>connected</i> route (route to the LRP’s subnet
                     CIDR) the logical flow match portion has no <b>reg7 ==</b>
                     <b>id &amp;&amp; </b>prefix to have route to LRP’s subnets in all
                     routing tables.

              •      For ECMP routes, they are grouped by policy and
                     prefix. An unique id (non-zero) is assigned to each
                     group, and each member is also assigned an unique
                     id (non-zero) within each group.

                     For each IPv4/IPv6 ECMP group with group id <i>GID</i> and
                     member ids <i>MID1</i>, <i>MID2</i>, ..., a logical flow with
                     match in CIDR notation <b>ip4.dst == </b><i>N</i><b>/</b><i>M</i>, or <b>ip6.dst</b>
                     <b>== </b><i>N</i><b>/</b><i>M</i>, whose priority is the integer value of <i>M</i>,
                     has the following actions:

                     <b>ip.ttl--;</b>
                     <b>flags.loopback = 1;</b>
                     <b>reg8[0..15] = </b><i>GID</i>;
                     <b>select(reg8[16..31], </b><i>MID1</i>, <i>MID2</i>, ...);

              •      A priority-0 logical flow that matches all packets
                     not already handled (match <b>1</b>) and drops them
                     (action <b>drop;</b>).

     <i>Ingress Table 15: IP_ROUTING_ECMP</i>

       This table implements the second part of IP routing for ECMP
       routes following the previous table. If a packet matched a ECMP
       group in the previous table, this table matches the group id and
       member id stored from the previous table, setting <b>reg0 </b>(or <b>xxreg0</b>
       for IPv6) to the next-hop IP address (leaving <b>ip4.dst </b>or <b>ip6.dst</b>,
       the packet’s final destination, unchanged) and advances to the
       next table for ARP resolution. It also sets <b>reg1 </b>(or <b>xxreg1</b>) to
       the IP address owned by the selected router port (ingress table
       <b>ARP Request </b>will generate an ARP request, if needed, with <b>reg0 </b>as
       the target protocol address and <b>reg1 </b>as the source protocol
       address).

       This processing is skipped for reply traffic being sent out of an
       ECMP route if the route was configured to use symmetric replies.

       This table contains the following logical flows:

              •      A priority-150 flow that matches <b>reg8[0..15] == 0</b>
                     with action <b>next; </b>directly bypasses packets of non-
                     ECMP routes.

              •      For each member with ID <i>MID</i> in each ECMP group with
                     ID <i>GID</i>, a priority-100 flow with match <b>reg8[0..15]</b>
                     <b>== </b><i>GID</i> <b>&amp;&amp; reg8[16..31] == </b><i>MID</i> has following
                     actions:

                     <b>[xx]reg0 = </b><i>G</i>;
                     <b>[xx]reg1 = </b><i>A</i>;
                     <b>eth.src = </b><i>E</i>;
                     <b>outport = </b><i>P</i>;

              •      A priority-0 logical flow that matches all packets
                     not already handled (match <b>1</b>) and drops them
                     (action <b>drop;</b>).

     <i>Ingress Table 16: Router policies</i>

       This table adds flows for the logical router policies configured
       on the logical router. Please see the <b>OVN_Northbound </b>database
       <b>Logical_Router_Policy </b>table documentation in <b>ovn-nb </b>for supported
       actions.

              •      For each router policy configured on the logical
                     router, a logical flow is added with specified
                     priority, match and actions.

              •      If the policy action is <b>reroute </b>with 2 or more
                     nexthops defined, then the logical flow is added
                     with the following actions:

                     <b>reg8[0..15] = </b><i>GID</i>;
                     <b>reg8[16..31] = select(1,..n);</b>

                     where <i>GID</i> is the ECMP group id generated by
                     <b>ovn-northd </b>for this policy and <i>n</i> is the number of
                     nexthops. <b>select </b>action selects one of the nexthop
                     member id, stores it in the register <b>reg8[16..31]</b>
                     and advances the packet to the next stage.

              •      If the policy action is <b>reroute </b>with just one
                     nexhop, then the logical flow is added with the
                     following actions:

                     <b>[xx]reg0 = </b><i>H</i>;
                     <b>eth.src = </b><i>E</i>;
                     <b>outport = </b><i>P</i>;
                     <b>reg8[0..15] = 0;</b>
                     <b>flags.loopback = 1;</b>
                     <b>next;</b>

                     where <i>H</i> is the <b>nexthop  </b>defined in the router
                     policy, <i>E</i> is the ethernet address of the logical
                     router port from which the <b>nexthop </b>is reachable and
                     <i>P</i> is the logical router port from which the <b>nexthop</b>
                     is reachable.

              •      If a router policy has the option <b>pkt_mark=</b><i>m</i> set
                     and if the action is <b>not </b>drop, then the action also
                     includes <b>pkt.mark = </b><i>m</i> to mark the packet with the
                     marker <i>m</i>.

     <i>Ingress Table 17: ECMP handling for router policies</i>

       This table handles the ECMP for the router policies configured
       with multiple nexthops.

              •      A priority-150 flow is added to advance the packet
                     to the next stage if the ECMP group id register
                     <b>reg8[0..15] </b>is 0.

              •      For each ECMP reroute router policy with multiple
                     nexthops, a priority-100 flow is added for each
                     nexthop <i>H</i> with the match <b>reg8[0..15] == </b><i>GID</i> <b>&amp;&amp;</b>
                     <b>reg8[16..31] == </b><i>M</i> where <i>GID</i> is the router policy
                     group id generated by <b>ovn-northd </b>and <i>M</i> is the
                     member id of the nexthop <i>H</i> generated by <b>ovn-northd</b>.
                     The following actions are added to the flow:

                     <b>[xx]reg0 = </b><i>H</i>;
                     <b>eth.src = </b><i>E</i>;
                     <b>outport = </b><i>P</i>
                     <b>"flags.loopback = 1; "</b>
                     <b>"next;"</b>

                     where <i>H</i> is the <b>nexthop  </b>defined in the router
                     policy, <i>E</i> is the ethernet address of the logical
                     router port from which the <b>nexthop </b>is reachable and
                     <i>P</i> is the logical router port from which the <b>nexthop</b>
                     is reachable.

              •      A priority-0 logical flow that matches all packets
                     not already handled (match <b>1</b>) and drops them
                     (action <b>drop;</b>).

     <i>Ingress Table 18: DHCP Relay Response Check</i>

       This stage process the DHCP response packets coming from the DHCP
       server.

              •      A priority 100 logical flow is added for each
                     logical router port configured with DHCP relay that
                     matches <b>ip4.src </b>is DHCP server ip and <b>ip4.dst </b>is
                     lrp IP and <b>ip4.frag == 0 </b>and <b>udp.src == 67 </b>and
                     <b>udp.dst == 67 </b>and applies <b>dhcp_relay_resp_chk</b>
                      action. Original destination ip is stored in reg2.

                             <b>reg9[8] = dhcp_relay_resp_chk(</b><i>lrp_ip</i>,
                                                           <i>dhcp_server_ip</i>);next

                     if action is successful then, dest mac and dest IP
                     addresses are updated in the packet and stores 1
                     into reg9[8] else stores 0 into reg9[8].

              •      A priority-0 flow that matches all packets to
                     advance to the next table.

     <i>Ingress Table 19: DHCP Relay Response</i>

       This stage process the DHCP response packets on which
       <b>dhcp_relay_resp_chk </b>action is applied in the previous stage.

              •      A priority 100 logical flow is added for each
                     logical router port configured with DHCP relay that
                     matches <b>ip4.src </b>is DHCP server ip and <b>reg2 </b>is lrp
                     IP and <b>udp.src == 67 </b>and <b>udp.dst == 67 </b>and <b>reg9[8]</b>
                     <b>== 1 </b>and applies following actions. If <b>reg9[8] </b>is
                     set to 1 then, <b>dhcp_relay_resp_chk </b>was successful.

                     <b>ip4.src = </b><i>lrp ip</i>;
                     <b>udp.dst = 68;</b>
                     <b>outport = </b><i>lrp port</i>;
                     <b>output;</b>

              •      A priority 1 logical flow is added for the logical
                     router port on which DHCP relay is enabled that
                     matches <b>ip4.src </b>is DHCP server ip and <b>reg2 </b>is lrp
                     IP and <b>udp.src == 67 </b>and <b>udp.dst == 67 </b>and <b>reg9[8]</b>
                     <b>== 0 </b>and drops the packet. If <b>reg9[8] </b>is set to 0
                     then, <b>dhcp_relay_resp_chk </b>was unsuccessful.

              •      A priority-0 flow that matches all packets to
                     advance to the next table.

     <i>Ingress Table 20: ARP/ND Resolution</i>

       Any packet that reaches this table is an IP packet whose next-hop
       IPv4 address is in <b>reg0 </b>or IPv6 address is in <b>xxreg0</b>. (<b>ip4.dst </b>or
       <b>ip6.dst </b>contains the final destination.) This table resolves the
       IP address in <b>reg0 </b>(or <b>xxreg0</b>) into an output port in <b>outport </b>and
       an Ethernet address in <b>eth.dst</b>, using the following flows:

              •      A priority-500 flow that matches IP multicast
                     traffic that was allowed in the routing pipeline.
                     For this kind of traffic the <b>outport </b>was already
                     set so the flow just advances to the next table.

              •      Priority-200 flows that match ECMP reply traffic
                     for the routes configured to use symmetric replies,
                     with actions <b>push(xxreg1); xxreg1 = ct_label;</b>
                     <b>eth.dst = xxreg1[32..79]; pop(xxreg1); next;</b>.
                     <b>xxreg1 </b>is used here to avoid masked access to
                     ct_label, to make the flow HW-offloading friendly.

              •      Static MAC bindings. MAC bindings can be known
                     statically based on data in the <b>OVN_Northbound</b>
                     database. For router ports connected to logical
                     switches, MAC bindings can be known statically from
                     the <b>addresses </b>column in the <b>Logical_Switch_Port</b>
                     table. (Note: the flow is not installed for IPs of
                     logical switch ports of type <b>virtual</b>, and dynamic
                     MAC binding is used for those IPs instead, so that
                     virtual parent failover does not depend on
                     <b>ovn-northd</b>, to achieve better failover
                     performance.) For router ports connected to other
                     logical routers, MAC bindings can be known
                     statically from the <b>mac </b>and <b>networks </b>column in the
                     <b>Logical_Router_Port </b>table. (Note: the flow is NOT
                     installed for the IP addresses that belong to a
                     neighbor logical router port if the current router
                     has the <b>options:dynamic_neigh_routers </b>set to <b>true</b>)

                     For each IPv4 address <i>A</i> whose host is known to have
                     Ethernet address <i>E</i> on router port <i>P</i>, a priority-100
                     flow with match <b>outport === </b><i>P</i> <b>&amp;&amp; reg0 == </b><i>A</i> has
                     actions <b>eth.dst = </b><i>E</i><b>; next;</b>.

                     For each IPv6 address <i>A</i> whose host is known to have
                     Ethernet address <i>E</i> on router port <i>P</i>, a priority-100
                     flow with match <b>outport === </b><i>P</i> <b>&amp;&amp; xxreg0 == </b><i>A</i> has
                     actions <b>eth.dst = </b><i>E</i><b>; next;</b>.

                     For each logical router port with an IPv4 address <i>A</i>
                     and a mac address of <i>E</i> that is reachable via a
                     different logical router port <i>P</i>, a priority-100
                     flow with match <b>outport === </b><i>P</i> <b>&amp;&amp; reg0 == </b><i>A</i> has
                     actions <b>eth.dst = </b><i>E</i><b>; next;</b>.

                     For each logical router port with an IPv6 address <i>A</i>
                     and a mac address of <i>E</i> that is reachable via a
                     different logical router port <i>P</i>, a priority-100
                     flow with match <b>outport === </b><i>P</i> <b>&amp;&amp; xxreg0 == </b><i>A</i> has
                     actions <b>eth.dst = </b><i>E</i><b>; next;</b>.

              •      Static MAC bindings from NAT entries. MAC bindings
                     can also be known for the entries in the <b>NAT </b>table.
                     Below flows are programmed for distributed logical
                     routers i.e with a distributed router port.

                     For each row in the <b>NAT </b>table with IPv4 address <i>A</i>
                     in the <b>external_ip </b>column of <b>NAT </b>table, below two
                     flows are programmed:

                     A priority-100 flow with the match <b>outport == </b><i>P</i> <b>&amp;&amp;</b>
                     <b>reg0 == </b><i>A</i> has actions <b>eth.dst = </b><i>E</i><b>; next;</b>, where <b>P</b>
                     is the distributed logical router port, <i>E</i> is the
                     Ethernet address if set in the <b>external_mac </b>column
                     of <b>NAT </b>table for of type <b>dnat_and_snat</b>, otherwise
                     the Ethernet address of the distributed logical
                     router port. Note that if the <b>external_ip </b>is not
                     within a subnet on the owning logical router, then
                     OVN will only create ARP resolution flows if the
                     <b>options:add_route </b>is set to <b>true</b>. Otherwise, no ARP
                     resolution flows will be added.

                     Corresponding to the above flow, a priority-150
                     flow with the match <b>inport == </b><i>P</i> <b>&amp;&amp; outport == </b><i>P</i> <b>&amp;&amp;</b>
                     <b>ip4.dst == </b><i>A</i> has actions <b>drop; </b>to exclude packets
                     that have gone through DNAT/unSNAT stage but failed
                     to convert the destination, to avoid loop.

                     For IPv6 NAT entries, same flows are added, but
                     using the register <b>xxreg0 </b>and field <b>ip6 </b>for the
                     match.

              •      If the router datapath runs a port with
                     <b>redirect-type </b>set to <b>bridged</b>, for each distributed
                     NAT rule with IP <i>A</i> in the <b>logical_ip </b>column and
                     logical port <i>P</i> in the <b>logical_port </b>column of <b>NAT</b>
                     table, a priority-90 flow with the match <b>outport ==</b>
                     <i>Q</i> <b>&amp;&amp; ip.src === </b><i>A</i> <b>&amp;&amp; is_chassis_resident(</b><i>P</i><b>)</b>, where
                     <b>Q </b>is the distributed logical router port and action
                     <b>get_arp(outport, reg0); next; </b>for IPv4 and
                     <b>get_nd(outport, xxreg0); next; </b>for IPv6.

              •      Traffic with IP destination an address owned by the
                     router should be dropped. Such traffic is normally
                     dropped in ingress table <b>IP Input </b>except for IPs
                     that are also shared with SNAT rules. However, if
                     there was no unSNAT operation that happened
                     successfully until this point in the pipeline and
                     the destination IP of the packet is still a router
                     owned IP, the packets can be safely dropped.

                     A priority-2 logical flow with match <b>ip4.dst = {..}</b>
                     matches on traffic destined to router owned IPv4
                     addresses which are also SNAT IPs. This flow has
                     action <b>drop;</b>.

                     A priority-2 logical flow with match <b>ip6.dst = {..}</b>
                     matches on traffic destined to router owned IPv6
                     addresses which are also SNAT IPs. This flow has
                     action <b>drop;</b>.

                     A priority-0 logical that flow matches all packets
                     not already handled (match <b>1</b>) and drops them
                     (action <b>drop;</b>).

              •      Dynamic MAC bindings. These flows resolve MAC-to-IP
                     bindings that have become known dynamically through
                     ARP or neighbor discovery. (The ingress table <b>ARP</b>
                     <b>Request </b>will issue an ARP or neighbor solicitation
                     request for cases where the binding is not yet
                     known.)

                     A priority-0 logical flow with match <b>ip4 </b>has
                     actions <b>get_arp(outport, reg0); next;</b>.

                     A priority-0 logical flow with match <b>ip6 </b>has
                     actions <b>get_nd(outport, xxreg0); next;</b>.

              •      For a distributed gateway LRP with <b>redirect-type</b>
                     set to <b>bridged</b>, a priority-50 flow will match
                     <b>outport == "ROUTER_PORT" and !is_chassis_resident</b>
                     <b>("cr-ROUTER_PORT") </b>has actions <b>eth.dst = </b><i>E</i><b>; next;</b>,
                     where <i>E</i> is the ethernet address of the logical
                     router port.

     <i>Ingress Table 21: Check packet length</i>

       For distributed logical routers or gateway routers with gateway
       port configured with <b>options:gateway_mtu </b>to a valid integer
       value, this table adds a priority-50 logical flow with the match
       <b>outport == </b><i>GW_PORT</i> where <i>GW_PORT</i> is the gateway router port and
       applies the action <b>check_pkt_larger </b>and advances the packet to
       the next table.

       <b>REGBIT_PKT_LARGER = check_pkt_larger(</b><i>L</i>); next;

       where <i>L</i> is the packet length to check for. If the packet is
       larger than <i>L</i>, it stores 1 in the register bit <b>REGBIT_PKT_LARGER</b>.
       The value of <i>L</i> is taken from <b>options:gateway_mtu </b>column of
       <b>Logical_Router_Port </b>row.

       If the port is also configured with <b>options:gateway_mtu_bypass</b>
       then another flow is added, with priority-55, to bypass the
       <b>check_pkt_larger </b>flow.

       This table adds one priority-0 fallback flow that matches all
       packets and advances to the next table.

     <i>Ingress Table 22: Handle larger packets</i>

       For distributed logical routers or gateway routers with gateway
       port configured with <b>options:gateway_mtu </b>to a valid integer
       value, this table adds the following priority-150 logical flow
       for each logical router port with the match <b>inport == </b><i>LRP</i> <b>&amp;&amp;</b>
       <b>outport == </b><i>GW_PORT</i> <b>&amp;&amp; REGBIT_PKT_LARGER &amp;&amp;</b>
       <b>!REGBIT_EGRESS_LOOPBACK</b>, where <i>LRP</i> is the logical router port and
       <i>GW_PORT</i> is the gateway port and applies the following action for
       ipv4 and ipv6 respectively:

       <b>icmp4 {</b>
           <b>icmp4.type = 3; /* Destination Unreachable. */</b>
           <b>icmp4.code = 4;  /* Frag Needed and DF was Set. */</b>
           <b>icmp4.frag_mtu = </b><i>M</i>;
           <b>eth.dst = </b><i>E</i>;
           <b>ip4.dst = ip4.src;</b>
           <b>ip4.src = </b><i>I</i>;
           <b>ip.ttl = 255;</b>
           <b>REGBIT_EGRESS_LOOPBACK = 1;</b>
           <b>REGBIT_PKT_LARGER = 0;</b>
           <b>next(pipeline=ingress, table=0);</b>
       <b>};</b>
       <b>icmp6 {</b>
           <b>icmp6.type = 2;</b>
           <b>icmp6.code = 0;</b>
           <b>icmp6.frag_mtu = </b><i>M</i>;
           <b>eth.dst = </b><i>E</i>;
           <b>ip6.dst = ip6.src;</b>
           <b>ip6.src = </b><i>I</i>;
           <b>ip.ttl = 255;</b>
           <b>REGBIT_EGRESS_LOOPBACK = 1;</b>
           <b>REGBIT_PKT_LARGER = 0;</b>
           <b>next(pipeline=ingress, table=0);</b>
       <b>};</b>

              •      Where <i>M</i> is the (fragment MTU - 58) whose value is
                     taken from <b>options:gateway_mtu </b>column of
                     <b>Logical_Router_Port </b>row.

              •      <i>E</i> is the Ethernet address of the logical router
                     port.

              •      <i>I</i> is the IPv4/IPv6 address of the logical router
                     port.

       This table adds one priority-0 fallback flow that matches all
       packets and advances to the next table.

     <i>Ingress Table 23: Gateway Redirect</i>

       For distributed logical routers where one or more of the logical
       router ports specifies a gateway chassis, this table redirects
       certain packets to the distributed gateway port instances on the
       gateway chassises. This table has the following flows:

              •      For all the configured load balancing rules that
                     include an IPv4 address <i>VIP</i>, and a list of IPv4
                     backend addresses <i>B0</i>, <i>B1</i> .. <i>Bn</i> defined for the <i>VIP</i>
                     a priority-200 flow is added that matches <b>ip4 &amp;&amp;</b>
                     <b>(ip4.src == </b><i>B0</i> <b>|| ip4.src == </b><i>B1</i> <b>|| ... || ip4.src</b>
                     <b>== </b><i>Bn</i><b>) </b>with an action <b>outport = </b><i>CR</i><b>; next; </b>where <i>CR</i>
                     is the <b>chassisredirect </b>port representing the
                     instance of the logical router distributed gateway
                     port on the gateway chassis. If the backend IPv4
                     address <i>Bx</i> is also configured with L4 port <i>PORT</i> of
                     protocol <i>P</i>, then the match also includes <b>P.src </b>==
                     <i>PORT</i>. Similar flows are added for IPv6.

              •      For each NAT rule in the OVN Northbound database
                     that can be handled in a distributed manner, a
                     priority-100 logical flow with match <b>ip4.src == </b><i>B</i>
                     <b>&amp;&amp; outport == </b><i>GW</i> &amp;&amp; is_chassis_resident(<i>P</i>), where
                     <i>GW</i> is the distributed gateway port specified in the
                     NAT rule and <i>P</i> is the NAT logical port. IP traffic
                     matching the above rule will be managed locally
                     setting <b>reg1 </b>to <i>C</i> and <b>eth.src </b>to <i>D</i>, where <i>C</i> is NAT
                     external ip and <i>D</i> is NAT external mac.

              •      For each <b>dnat_and_snat </b>NAT rule with <b>stateless=true</b>
                     and <b>allowed_ext_ips </b>configured, a priority-75 flow
                     is programmed with match <b>ip4.dst == </b><i>B</i> and action
                     <b>outport = </b><i>CR</i><b>; next; </b>where <i>B</i> is the NAT rule
                     external IP and <i>CR</i> is the <b>chassisredirect </b>port
                     representing the instance of the logical router
                     distributed gateway port on the gateway chassis.
                     Moreover a priority-70 flow is programmed with same
                     match and action <b>drop;</b>. For each <b>dnat_and_snat </b>NAT
                     rule with <b>stateless=true </b>and <b>exempted_ext_ips</b>
                     configured, a priority-75 flow is programmed with
                     match <b>ip4.dst == </b><i>B</i> and action <b>drop; </b>where <i>B</i> is the
                     NAT rule external IP. A similar flow is added for
                     IPv6 traffic.

              •      For each NAT rule in the OVN Northbound database
                     that can be handled in a distributed manner, a
                     priority-80 logical flow with drop action if the
                     NAT logical port is a virtual port not claimed by
                     any chassis yet.

              •      A priority-50 logical flow with match <b>outport == </b><i>GW</i>
                     has actions <b>outport = </b><i>CR</i><b>; next;</b>, where <i>GW</i> is the
                     logical router distributed gateway port and <i>CR</i> is
                     the <b>chassisredirect </b>port representing the instance
                     of the logical router distributed gateway port on
                     the gateway chassis.

              •      A priority-0 logical flow with match <b>1 </b>has actions
                     <b>next;</b>.

     <i>Ingress Table 24: ARP Request</i>

       In the common case where the Ethernet destination has been
       resolved, this table outputs the packet. Otherwise, it composes
       and sends an ARP or IPv6 Neighbor Solicitation request. It holds
       the following flows:

              •      Unknown MAC address. A priority-100 flow for IPv4
                     packets with match <b>eth.dst == 00:00:00:00:00:00 </b>has
                     the following actions:

                     <b>arp {</b>
                         <b>eth.dst = ff:ff:ff:ff:ff:ff;</b>
                         <b>arp.spa = reg1;</b>
                         <b>arp.tpa = reg0;</b>
                         <b>arp.op = 1;  /* ARP request. */</b>
                         <b>output;</b>
                     <b>};</b>

                     Unknown MAC address. For each IPv6 static route
                     associated with the router with the nexthop IP: <i>G</i>,
                     a priority-200 flow for IPv6 packets with match
                     <b>eth.dst == 00:00:00:00:00:00 &amp;&amp; xxreg0 == </b><i>G</i> with
                     the following actions is added:

                     <b>nd_ns {</b>
                         <b>eth.dst = </b><i>E</i>;
                         <b>ip6.dst = </b><i>I</i>
                         <b>nd.target = </b><i>G</i>;
                         <b>output;</b>
                     <b>};</b>

                     Where <i>E</i> is the multicast mac derived from the
                     Gateway IP, <i>I</i> is the solicited-node multicast
                     address corresponding to the target address <i>G</i>.

                     Unknown MAC address. A priority-100 flow for IPv6
                     packets with match <b>eth.dst == 00:00:00:00:00:00 </b>has
                     the following actions:

                     <b>nd_ns {</b>
                         <b>nd.target = xxreg0;</b>
                         <b>output;</b>
                     <b>};</b>

                     (Ingress table <b>IP Routing </b>initialized <b>reg1 </b>with the
                     IP address owned by <b>outport </b>and <b>(xx)reg0 </b>with the
                     next-hop IP address)

                     The IP packet that triggers the ARP/IPv6 NS request
                     is dropped.

              •      Known MAC address. A priority-0 flow with match <b>1</b>
                     has actions <b>output;</b>.

     <i>Egress Table 0: Check DNAT local</i>

       This table checks if the packet needs to be DNATed in the router
       ingress table <b>lr_in_dnat </b>after it is SNATed and looped back to
       the ingress pipeline. This check is done only for routers
       configured with distributed gateway ports and NAT entries. This
       check is done so that SNAT and DNAT is done in different zones
       instead of a common zone.

              •      A priority-0 logical flow with match <b>1 </b>has actions
                     <b>REGBIT_DST_NAT_IP_LOCAL = 0; next;</b>.

     <i>Egress Table 1: UNDNAT</i>

       This is for already established connections’ reverse traffic.
       i.e., DNAT has already been done in ingress pipeline and now the
       packet has entered the egress pipeline as part of a reply. This
       traffic is unDNATed here.

              •      A priority-0 logical flow with match <b>1 </b>has actions
                     <b>next;</b>.

     <i>Egress Table 1: UNDNAT on Gateway Routers</i>

              •      For IPv6 Neighbor Discovery or Router
                     Solicitation/Advertisement traffic, a priority-100
                     flow with action <b>next;</b>.

              •      For all IP packets, a priority-50 flow with an
                     action <b>flags.loopback = 1; ct_dnat;</b>.

     <i>Egress Table 1: UNDNAT on Distributed Routers</i>

              •      For all the configured load balancing rules for a
                     router with gateway port in <b>OVN_Northbound </b>database
                     that includes an IPv4 address <b>VIP</b>, for every
                     backend IPv4 address <i>B</i> defined for the <b>VIP </b>a
                     priority-120 flow is programmed on gateway chassis
                     that matches <b>ip &amp;&amp; ip4.src == </b><i>B</i> <b>&amp;&amp; outport == </b><i>GW</i>,
                     where <i>GW</i> is the logical router gateway port with an
                     action <b>ct_dnat;</b>. If the backend IPv4 address <i>B</i> is
                     also configured with L4 port <i>PORT</i> of protocol <i>P</i>,
                     then the match also includes <b>P.src </b>== <i>PORT</i>. These
                     flows are not added for load balancers with IPv6
                     <i>VIPs</i>.

                     If the router is configured to force SNAT any load-
                     balanced packets, above action will be replaced by
                     <b>flags.force_snat_for_lb = 1; ct_dnat;</b>.

              •      For each configuration in the OVN Northbound
                     database that asks to change the destination IP
                     address of a packet from an IP address of <i>A</i> to <i>B</i>, a
                     priority-100 flow matches <b>ip &amp;&amp; ip4.src == </b><i>B</i> <b>&amp;&amp;</b>
                     <b>outport == </b><i>GW</i>, where <i>GW</i> is the logical router
                     gateway port, with an action <b>ct_dnat;</b>. If the NAT
                     rule is of type dnat_and_snat and has
                     <b>stateless=true </b>in the options, then the action
                     would be <b>next;</b>.

                     If the NAT rule cannot be handled in a distributed
                     manner, then the priority-100 flow above is only
                     programmed on the gateway chassis with the action
                     <b>ct_dnat</b>.

                     If the NAT rule can be handled in a distributed
                     manner, then there is an additional action <b>eth.src</b>
                     <b>= </b><i>EA</i><b>;</b>, where <i>EA</i> is the ethernet address associated
                     with the IP address <i>A</i> in the NAT rule. This allows
                     upstream MAC learning to point to the correct
                     chassis.

     <i>Egress Table 2: Post UNDNAT</i>

              •      A priority-70 logical flow is added that initiates
                     CT state for traffic that is configured to be
                     SNATed on Distributed routers. This allows the next
                     table, <b>lr_out_snat</b>, to effectively match on various
                     CT states.

              •      A priority-50 logical flow is added that commits
                     any untracked flows from the previous table
                     <b>lr_out_undnat </b>for Gateway routers. This flow
                     matches on <b>ct.new &amp;&amp; ip </b>with action <b>ct_commit { } ;</b>
                     <b>next; </b>.

              •      A priority-0 logical flow with match <b>1 </b>has actions
                     <b>next;</b>.

     <i>Egress Table 3: SNAT</i>

       Packets that are configured to be SNATed get their source IP
       address changed based on the configuration in the OVN Northbound
       database.

              •      A priority-120 flow to advance the IPv6 Neighbor
                     solicitation packet to next table to skip SNAT. In
                     the case where ovn-controller injects an IPv6
                     Neighbor Solicitation packet (for <b>nd_ns </b>action) we
                     don’t want the packet to go through conntrack.

       Egress Table 3: SNAT on Gateway Routers

              •      If the Gateway router in the OVN Northbound
                     database has been configured to force SNAT a packet
                     (that has been previously DNATted) to <i>B</i>, a
                     priority-100 flow matches <b>flags.force_snat_for_dnat</b>
                     <b>== 1 &amp;&amp; ip </b>with an action <b>ct_snat(</b><i>B</i><b>);</b>.

              •      If a load balancer configured to skip snat has been
                     applied to the Gateway router pipeline, a
                     priority-120 flow matches <b>flags.skip_snat_for_lb ==</b>
                     <b>1 &amp;&amp; ip </b>with an action <b>next;</b>.

              •      If the Gateway router in the OVN Northbound
                     database has been configured to force SNAT a packet
                     (that has been previously load-balanced) using
                     router IP (i.e <b>options</b>:lb_force_snat_ip=router_ip),
                     then for each logical router port <i>P</i> attached to the
                     Gateway router, a priority-110 flow matches
                     <b>flags.force_snat_for_lb == 1 &amp;&amp; outport == </b><i>P</i>
                      with an action <b>ct_snat(</b><i>R</i><b>); </b>where <i>R</i> is the IP
                     configured on the router port. If <b>R </b>is an IPv4
                     address then the match will also include <b>ip4 </b>and if
                     it is an IPv6 address, then the match will also
                     include <b>ip6</b>.

                     If the logical router port <i>P</i> is configured with
                     multiple IPv4 and multiple IPv6 addresses, only the
                     first IPv4 and first IPv6 address is considered.

              •      If the Gateway router in the OVN Northbound
                     database has been configured to force SNAT a packet
                     (that has been previously load-balanced) to <i>B</i>, a
                     priority-100 flow matches <b>flags.force_snat_for_lb</b>
                     <b>== 1 &amp;&amp; ip </b>with an action <b>ct_snat(</b><i>B</i><b>);</b>.

              •      For each configuration in the OVN Northbound
                     database, that asks to change the source IP address
                     of a packet from an IP address of <i>A</i> or to change
                     the source IP address of a packet that belongs to
                     network <i>A</i> to <i>B</i>, a flow matches <b>ip &amp;&amp; ip4.src == </b><i>A</i>
                     <b>&amp;&amp; (!ct.trk || !ct.rpl) </b>with an action <b>ct_snat(</b><i>B</i><b>);</b>.
                     The priority of the flow is calculated based on the
                     mask of <i>A</i>, with matches having larger masks getting
                     higher priorities. If the NAT rule is of type
                     dnat_and_snat and has <b>stateless=true </b>in the
                     options, then the action would be <b>ip4/6.src= (</b><i>B</i><b>)</b>.

              •      If the NAT rule has <b>allowed_ext_ips </b>configured,
                     then there is an additional match <b>ip4.dst ==</b>
                     <i>allowed_ext_ips</i> . Similarly, for IPV6, match would
                     be <b>ip6.dst == </b><i>allowed_ext_ips</i>.

              •      If the NAT rule has <b>exempted_ext_ips </b>set, then
                     there is an additional flow configured at the
                     priority + 1 of corresponding NAT rule. The flow
                     matches if destination ip is an <b>exempted_ext_ip </b>and
                     the action is <b>next; </b>. This flow is used to bypass
                     the ct_snat action for a packet which is destinted
                     to <b>exempted_ext_ips</b>.

              •      A priority-0 logical flow with match <b>1 </b>has actions
                     <b>next;</b>.

       Egress Table 3: SNAT on Distributed Routers

              •      For each configuration in the OVN Northbound
                     database, that asks to change the source IP address
                     of a packet from an IP address of <i>A</i> or to change
                     the source IP address of a packet that belongs to
                     network <i>A</i> to <i>B</i>, two flows are added. The priority <i>P</i>
                     of these flows are calculated based on the mask of
                     <i>A</i>, with matches having larger masks getting higher
                     priorities.

                     If the NAT rule cannot be handled in a distributed
                     manner, then the below flows are only programmed on
                     the gateway chassis increasing flow priority by 128
                     in order to be run first.

                     •      The first flow is added with the calculated
                            priority <i>P</i> and match <b>ip &amp;&amp; ip4.src == </b><i>A</i> <b>&amp;&amp;</b>
                            <b>outport == </b><i>GW</i>, where <i>GW</i> is the logical
                            router gateway port, with an action
                            <b>ct_snat(</b><i>B</i><b>); </b>to SNATed in the common zone. If
                            the NAT rule is of type dnat_and_snat and
                            has <b>stateless=true </b>in the options, then the
                            action would be <b>ip4/6.src=(</b><i>B</i><b>)</b>.

                     If the NAT rule can be handled in a distributed
                     manner, then there is an additional action (for
                     both the flows) <b>eth.src = </b><i>EA</i><b>;</b>, where <i>EA</i> is the
                     ethernet address associated with the IP address <i>A</i>
                     in the NAT rule. This allows upstream MAC learning
                     to point to the correct chassis.

                     If the NAT rule has <b>allowed_ext_ips </b>configured,
                     then there is an additional match <b>ip4.dst ==</b>
                     <i>allowed_ext_ips</i> . Similarly, for IPV6, match would
                     be <b>ip6.dst == </b><i>allowed_ext_ips</i>.

                     If the NAT rule has <b>exempted_ext_ips </b>set, then
                     there is an additional flow configured at the
                     priority <i>P</i> <b>+ 2  </b>of corresponding NAT rule. The flow
                     matches if destination ip is an <b>exempted_ext_ip </b>and
                     the action is <b>next; </b>. This flow is used to bypass
                     the ct_snat action for a flow which is destinted to
                     <b>exempted_ext_ips</b>.

              •      An additional flow is added for traffic that goes
                     in opposite direction (i.e. it enters a network
                     with configured SNAT). Where the flow above matched
                     on <b>ip4.src == </b><i>A</i> <b>&amp;&amp; outport == </b><i>GW</i>, this flow matches
                     on  <b>ip4.dst == </b><i>A</i> <b>&amp;&amp; inport == </b><i>GW</i>. A CT state is
                     initiated for this traffic so that the following
                     table, <b>lr_out_post_snat</b>, can identify whether the
                     traffic flow was initiated from the internal or
                     external network.

              •      A priority-0 logical flow with match <b>1 </b>has actions
                     <b>next;</b>.

     <i>Egress Table 4: Post SNAT</i>

       Packets reaching this table are processed according to the flows
       below:

              •      Traffic that goes directly into a network
                     configured with SNAT on Distributed routers, and
                     was initiated from an external network (i.e. it
                     matches <b>ct.new</b>), is committed to the SNAT CT zone.
                     This ensures that replies returning from the SNATed
                     network do not have their source address
                     translated. For details about match rules and
                     priority see section "Egress Table 3: SNAT on
                     Distributed Routers".

              •      A priority-0 logical flow that matches all packets
                     not already handled (match <b>1</b>) and action <b>next;</b>.

     <i>Egress Table 5: Egress Loopback</i>

       For distributed logical routers where one of the logical router
       ports specifies a gateway chassis.

       While UNDNAT and SNAT processing have already occurred by this
       point, this traffic needs to be forced through egress loopback on
       this distributed gateway port instance, in order for UNSNAT and
       DNAT processing to be applied, and also for IP routing and ARP
       resolution after all of the NAT processing, so that the packet
       can be forwarded to the destination.

       This table has the following flows:

              •      For each NAT rule in the OVN Northbound database on
                     a distributed router, a priority-100 logical flow
                     with match <b>ip4.dst == </b><i>E</i> <b>&amp;&amp; outport == </b><i>GW</i> <b>&amp;&amp;</b>
                     <b>is_chassis_resident(</b><i>P</i><b>)</b>, where <i>E</i> is the external IP
                     address specified in the NAT rule, <i>GW</i> is the
                     distributed gateway port corresponding to the NAT
                     rule (specified or inferred). For dnat_and_snat NAT
                     rule, <i>P</i> is the logical port specified in the NAT
                     rule. If <b>logical_port </b>column of <b>NAT </b>table is NOT
                     set, then <i>P</i> is the <b>chassisredirect port </b>of <i>GW</i> with
                     the following actions:

                     <b>clone {</b>
                         <b>ct_clear;</b>
                         <b>inport = outport;</b>
                         <b>outport = "";</b>
                         <b>flags = 0;</b>
                         <b>flags.loopback = 1;</b>
                         <b>reg0 = 0;</b>
                         <b>reg1 = 0;</b>
                         <b>...</b>
                         <b>reg9 = 0;</b>
                         <b>REGBIT_EGRESS_LOOPBACK = 1;</b>
                         <b>next(pipeline=ingress, table=0);</b>
                     <b>};</b>

                     <b>flags.loopback </b>is set since in_port is unchanged
                     and the packet may return back to that port after
                     NAT processing. <b>REGBIT_EGRESS_LOOPBACK </b>is set to
                     indicate that egress loopback has occurred, in
                     order to skip the source IP address check against
                     the router address.

              •      A priority-0 logical flow with match <b>1 </b>has actions
                     <b>next;</b>.

     <i>Egress Table 6: Delivery</i>

       Packets that reach this table are ready for delivery. It
       contains:

              •      Priority-110 logical flows that match IP multicast
                     packets on each enabled logical router port and
                     modify the Ethernet source address of the packets
                     to the Ethernet address of the port and then
                     execute action <b>output;</b>.

              •      Priority-100 logical flows that match packets on
                     each enabled logical router port, with action
                     <b>output;</b>.

              •      A priority-0 logical flow that matches all packets
                     not already handled (match <b>1</b>) and drops them
                     (action <b>drop;</b>).
</pre> <h2>
DROP SAMPLING </h2>
<pre>
       As described in the previous section, there are several places
       where ovn-northd might decided to drop a packet by explicitly
       creating a <b>Logical_Flow </b>with the <b>drop; </b>action.

       When debug drop-sampling has been cofigured in the OVN Northbound
       database, the ovn-northd will replace all the <b>drop; </b>actions with
       a <b>sample(priority=65535, collector_set=</b><i>id</i><b>, obs_domain=</b><i>obs_id</i><b>,</b>
       <b>obs_point=@cookie) </b>action, where:

              •      <i>id</i> is the value the <b>debug_drop_collector_set </b>option
                     configured in the OVN Northbound.

              •      <i>obs_id</i> has it’s 8 most significant bits equal to
                     the value of the <b>debug_drop_domain_id </b>option in the
                     OVN Northbound and it’s 24 least significant bits
                     equal to the datapath’s tunnel key.
</pre> <h2>
COLOPHON </h2>
<pre>
       This page is part of the <i>Open Virtual Network</i> (Daemons for Open
       vSwitch that translate virtual network configurations into
       OpenFlow) project.  Information about the project can be found at
       ⟨<a href="https://www.ovn.org/">https://www.ovn.org/</a>⟩.  If you have a bug report for this manual
       page, send it to bugs@openvswitch.org.  This page was obtained
       from the project's upstream Git repository
       ⟨<a href="https://github.com/ovn-org/ovn">https://github.com/ovn-org/ovn</a>⟩ on 2024-06-14.  (At that time,
       the date of the most recent commit that was found in the
       repository was 2024-06-12.)  If you discover any rendering
       problems in this HTML version of the page, or you believe there
       is a better or more up-to-date source for the page, or you have
       corrections or improvements to the information in this COLOPHON
       (which is <i>not</i> part of the original manual page), send a mail to
       man-pages@man7.org

<span class="footline">OVN 24.03.90                   ovn-northd                  <i>ovn-northd</i>(8)</span>
</pre>  <p>Pages that refer to this page: <a href="../man5/ovn-sb.5.html">ovn-sb(5)</a>, <a href="../man7/ovn-architecture.7.html">ovn-architecture(7)</a> </p> <hr>         <div class="_attribution">
  <p class="_attribution-p">
    ...<br>
    <a href="https://man7.org/linux/man-pages/man8/ovn-northd.8.html" class="_attribution-link">https://man7.org/linux/man-pages/man8/ovn-northd.8.html</a>
  </p>
</div>
