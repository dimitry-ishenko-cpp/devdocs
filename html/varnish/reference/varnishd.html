<h1 id="varnishd-1">varnishd</h1> <section id="http-accelerator-daemon"> <h2>HTTP accelerator daemon</h2> <dl class="field-list simple"> <dt class="field-odd">Manual section<span class="colon">:</span>
</dt> <dd class="field-odd">
<p>1</p> </dd> </dl> <section id="synopsis"> <h3>SYNOPSIS</h3> <dl class="simple"> <dt>varnishd</dt>
<dd>
<p>[-a [name=][listen_address[,PROTO]] [-b [host[:port]|path]] [-C] [-d] [-F] [-f config] [-h type[,options]] [-I clifile] [-i identity] [-j jail[,jailoptions]] [-l vsl] [-M address:port] [-n workdir] [-P file] [-p param=value] [-r param[,param…]] [-S secret-file] [-s [name=]kind[,options]] [-T address[:port]] [-t TTL] [-V] [-W waiter]</p> </dd> </dl> <p>varnishd [-x parameter|vsl|cli|builtin|optstring]</p> <p>varnishd [-?]</p> </section> <section id="description"> <h3>DESCRIPTION</h3> <p>The <code>varnishd</code> daemon accepts HTTP requests from clients, passes them on to a backend server and caches the returned documents to better satisfy future requests for the same document.</p> </section> <section id="options"> <h3 id="ref-varnishd-options">OPTIONS</h3> <section id="basic-options"> <h4>Basic options</h4> <dl class="option-list"> <dt><kbd><span class="option">-a <var>&lt;[name=][listen_address[,PROTO]]&gt;</var></span></kbd></dt> <dd>
<p>Accept for client requests on the specified listen_address (see below).</p> <p>Name is referenced in logs. If name is not specified, “a0”, “a1”, etc. is used.</p> <p>PROTO can be “HTTP” (the default) or “PROXY”. Both version 1 and 2 of the proxy protocol can be used.</p> <p>Multiple -a arguments are allowed.</p> <p>If no -a argument is given, the default <code>-a :80</code> will listen to all IPv4 and IPv6 interfaces.</p> </dd> <dt><kbd><span class="option">-a <var>&lt;[name=][ip_address][:port][,PROTO]&gt;</var></span></kbd></dt> <dd>
<p>The ip_address can be a host name (“localhost”), an IPv4 dotted-quad (“127.0.0.1”) or an IPv6 address enclosed in square brackets (“[::1]”)</p> <p>If port is not specified, port 80 (http) is used.</p> <p>At least one of ip_address or port is required.</p> </dd> <dt><kbd><span class="option">-a <var>&lt;[name=][path][,PROTO][,user=name][,group=name][,mode=octal]&gt;</var></span></kbd></dt> <dd>
<p>(VCL4.1 and higher)</p> <p>Accept connections on a Unix domain socket. Path must be absolute (“/path/to/listen.sock”) or “@” followed by the name of an abstract socket (“@myvarnishd”).</p> <p>The user, group and mode sub-arguments may be used to specify the permissions of the socket file – use names for user and group, and a 3-digit octal value for mode. These sub-arguments do not apply to abstract sockets.</p> </dd> <dt><kbd><span class="option">-b <var>&lt;[host[:port]|path]&gt;</var></span></kbd></dt> <dd>
<p>Use the specified host as backend server. If port is not specified, the default is 8080.</p> <p>If the value of <code>-b</code> begins with <code>/</code>, it is interpreted as the absolute path of a Unix domain socket to which Varnish connects. In that case, the value of <code>-b</code> must satisfy the conditions required for the <code>.path</code> field of a backend declaration, see <a class="reference internal" href="vcl.html#vcl-7"><span class="std std-ref">VCL</span></a>. Backends with Unix socket addresses may only be used with VCL versions &gt;= 4.1.</p> <p>-b can be used only once, and not together with f.</p> </dd> <dt><kbd><span class="option">-f <var>config</var></span></kbd></dt> <dd>
<p>Use the specified VCL configuration file instead of the builtin default. See <a class="reference internal" href="vcl.html#vcl-7"><span class="std std-ref">VCL</span></a> for details on VCL syntax.</p> <p>If a single -f option is used, then the VCL instance loaded from the file is named “boot” and immediately becomes active. If more than one -f option is used, the VCL instances are named “boot0”, “boot1” and so forth, in the order corresponding to the -f arguments, and the last one is named “boot”, which becomes active.</p> <p>Either -b or one or more -f options must be specified, but not both, and they cannot both be left out, unless -d is used to start <code>varnishd</code> in debugging mode. If the empty string is specified as the sole -f option, then <code>varnishd</code> starts without starting the worker process, and the management process will accept CLI commands. You can also combine an empty -f option with an initialization script (-I option) and the child process will be started if there is an active VCL at the end of the initialization.</p> <p>When used with a relative file name, config is searched in the <code>vcl_path</code>. It is possible to set this path prior to using <code>-f</code> options with a <code>-p</code> option. During startup, <code>varnishd</code> doesn’t complain about unsafe VCL paths: unlike the <code>varnish-cli(7)</code> that could later be accessed remotely, starting <code>varnishd</code> requires local privileges.</p> </dd> </dl> <dl class="option-list" id="opt-n"> <dt><kbd><span class="option">-n <var>workdir</var></span></kbd></dt> <dd>
<p>Runtime directory for the shared memory, compiled VCLs etc.</p> <p>In performance critical applications, this directory should be on a RAM backed filesystem.</p> <p>Relative paths will be appended to <code>/var/run/</code> (NB: Binary packages of Varnish may have adjusted this to the platform.)</p> <p>The default value is <code>/var/run/varnishd</code> (NB: as above.)</p> </dd> </dl> </section> <section id="documentation-options"> <h4>Documentation options</h4> <p>For these options, <code>varnishd</code> prints information to standard output and exits. When a -x option is used, it must be the only option (it outputs documentation in reStructuredText, aka RST).</p> <p>-?</p>  <p>Print the usage message.</p>  <dl class="option-list"> <dt><kbd><span class="option">-x <var>parameter</var></span></kbd></dt> <dd>
<p>Print documentation of the runtime parameters (-p options), see <a class="reference internal" href="#list-of-parameters">List of Parameters</a>.</p> </dd> <dt><kbd><span class="option">-x <var>vsl</var></span></kbd></dt> <dd>
<p>Print documentation of the tags used in the Varnish shared memory log, see <a class="reference internal" href="vsl.html#vsl-7"><span class="std std-ref">VSL</span></a>.</p> </dd> <dt><kbd><span class="option">-x <var>cli</var></span></kbd></dt> <dd>
<p>Print documentation of the command line interface, see <a class="reference internal" href="varnish-cli.html#varnish-cli-7"><span class="std std-ref">varnish-cli</span></a>.</p> </dd> <dt><kbd><span class="option">-x <var>builtin</var></span></kbd></dt> <dd>
<p>Print the contents of the default VCL program <code>builtin.vcl</code>.</p> </dd> <dt><kbd><span class="option">-x <var>optstring</var></span></kbd></dt> <dd>
<p>Print the optstring parameter to <code>getopt(3)</code> to help writing wrapper scripts.</p> </dd> </dl> </section> <section id="operations-options"> <h4>Operations options</h4> <dl class="option-list"> <dt><kbd><span class="option">-F</span></kbd></dt> <dd>
<p>Do not fork, run in the foreground. Only one of -F or -d can be specified, and -F cannot be used together with -C.</p> </dd> <dt><kbd><span class="option">-T <var>&lt;address[:port]&gt;</var></span></kbd></dt> <dd>
<p>Offer a management interface on the specified address and port. See <a class="reference internal" href="varnish-cli.html#varnish-cli-7"><span class="std std-ref">varnish-cli</span></a> for documentation of the management commands. To disable the management interface use <code>none</code>.</p> </dd> <dt><kbd><span class="option">-M <var>&lt;address:port&gt;</var></span></kbd></dt> <dd>
<p>Connect to this port and offer the command line interface. Think of it as a reverse shell. When running with -M and there is no backend defined the child process (the cache) will not start initially.</p> </dd> <dt><kbd><span class="option">-P <var>file</var></span></kbd></dt> <dd>
<p>Write the PID of the process to the specified file.</p> </dd> <dt><kbd><span class="option">-i <var>identity</var></span></kbd></dt> <dd>
<p>Specify the identity of the Varnish server. This can be accessed using <code>server.identity</code> from VCL.</p> <p>The server identity is used for the <code>received-by</code> field of <code>Via</code> headers generated by Varnish. For this reason, it must be a valid token as defined by the HTTP grammar.</p> <p>If not specified the output of <code>gethostname(3)</code> is used, in which case the syntax is assumed to be correct.</p> </dd> <dt><kbd><span class="option">-I <var>clifile</var></span></kbd></dt> <dd>
<p>Execute the management commands in the file given as <code>clifile</code> before the the worker process starts, see <a class="reference internal" href="#cli-command-file">CLI Command File</a>.</p> </dd> </dl> </section> <section id="tuning-options"> <h4>Tuning options</h4> <dl class="option-list"> <dt><kbd><span class="option">-t <var>TTL</var></span></kbd></dt> <dd>
<p>Specifies the default time to live (TTL) for cached objects. This is a shortcut for specifying the <em>default_ttl</em> run-time parameter.</p> </dd> <dt><kbd><span class="option">-p <var>&lt;param=value&gt;</var></span></kbd></dt> <dd>
<p>Set the parameter specified by param to the specified value, see <a class="reference internal" href="#list-of-parameters">List of Parameters</a> for details. This option can be used multiple times to specify multiple parameters.</p> </dd> <dt><kbd><span class="option">-s <var>&lt;[name=]type[,options]&gt;</var></span></kbd></dt> <dd>
<p>Use the specified storage backend. See <a class="reference internal" href="#storage-backend">Storage Backend</a> section.</p> <p>This option can be used multiple times to specify multiple storage files. Name is referenced in logs, VCL, statistics, etc. If name is not specified, “s0”, “s1” and so forth is used.</p> </dd> <dt><kbd><span class="option">-l <var>&lt;vsl&gt;</var></span></kbd></dt> <dd>
<p>Specifies size of the space for the VSL records, shorthand for <code>-p vsl_space=&lt;vsl&gt;</code>. Scaling suffixes like ‘K’ and ‘M’ can be used up to (G)igabytes. See <a class="reference internal" href="#vsl-space">vsl_space</a> for more information.</p> </dd> </dl> </section> <section id="security-options"> <h4>Security options</h4> <dl class="option-list"> <dt><kbd><span class="option">-r <var>&lt;param[,param…]&gt;</var></span></kbd></dt> <dd>
<p>Make the listed parameters read only. This gives the system administrator a way to limit what the Varnish CLI can do. Consider making parameters such as <em>cc_command</em>, <em>vcc_allow_inline_c</em> and <em>vmod_path</em> read only as these can potentially be used to escalate privileges from the CLI.</p> </dd> <dt><kbd><span class="option">-S <var>secret-file</var></span></kbd></dt> <dd>
<p>Path to a file containing a secret used for authorizing access to the management port. To disable authentication use <code>none</code>.</p> <p>If this argument is not provided, a secret drawn from the system PRNG will be written to a file called <code>_.secret</code> in the working directory (see <a class="reference internal" href="#opt-n">opt_n</a>) with default ownership and permissions of the user having started varnish.</p> <p>Thus, users wishing to delegate control over varnish will probably want to create a custom secret file with appropriate permissions (ie. readable by a unix group to delegate control to).</p> </dd> <dt><kbd><span class="option">-j <var>&lt;jail[,jailoptions]&gt;</var></span></kbd></dt> <dd>
<p>Specify the jailing mechanism to use. See <a class="reference internal" href="#jail">Jail</a> section.</p> </dd> </dl> </section> <section id="advanced-development-and-debugging-options"> <h4>Advanced, development and debugging options</h4> <dl class="option-list"> <dt><kbd><span class="option">-d</span></kbd></dt> <dd>
<p>Enables debugging mode: The parent process runs in the foreground with a CLI connection on stdin/stdout, and the child process must be started explicitly with a CLI command. Terminating the parent process will also terminate the child.</p> <p>Only one of -d or -F can be specified, and -d cannot be used together with -C.</p> </dd> <dt><kbd><span class="option">-C</span></kbd></dt> <dd>
<p>Print VCL code compiled to C language and exit. Specify the VCL file to compile with the -f option. Either -f or -b must be used with -C, and -C cannot be used with -F or -d.</p> </dd> <dt><kbd><span class="option">-V</span></kbd></dt> <dd>
<p>Display the version number and exit. This must be the only option.</p> </dd> <dt><kbd><span class="option">-h <var>&lt;type[,options]&gt;</var></span></kbd></dt> <dd>
<p>Specifies the hash algorithm. See <a class="reference internal" href="#hash-algorithm">Hash Algorithm</a> section for a list of supported algorithms.</p> </dd> <dt><kbd><span class="option">-W <var>waiter</var></span></kbd></dt> <dd>
<p>Specifies the waiter type to use.</p> </dd> </dl> </section> <section id="hash-algorithm"> <h4 id="opt-h">Hash Algorithm</h4> <p>The following hash algorithms are available:</p> <dl class="option-list"> <dt><kbd><span class="option">-h <var>critbit</var></span></kbd></dt> <dd>
<p>self-scaling tree structure. The default hash algorithm in Varnish Cache 2.1 and onwards. In comparison to a more traditional B tree the critbit tree is almost completely lockless. Do not change this unless you are certain what you’re doing.</p> </dd> <dt><kbd><span class="option">-h <var>simple_list</var></span></kbd></dt> <dd>
<p>A simple doubly-linked list. Not recommended for production use.</p> </dd> <dt><kbd><span class="option">-h <var>&lt;classic[,buckets]&gt;</var></span></kbd></dt> <dd>
<p>A standard hash table. The hash key is the CRC32 of the object’s URL modulo the size of the hash table. Each table entry points to a list of elements which share the same hash key. The buckets parameter specifies the number of entries in the hash table. The default is 16383.</p> </dd> </dl> </section> <section id="storage-backend"> <h4 id="ref-varnishd-opt-s">Storage Backend</h4> <p>The argument format to define storage backends is:</p> <dl class="option-list"> <dt><kbd><span class="option">-s <var>&lt;[name]=kind[,options]&gt;</var></span></kbd></dt> <dd>
<p>If <em>name</em> is omitted, Varnish will name storages <code>s</code><em>N</em>, starting with <code>s0</code> and incrementing <em>N</em> for every new storage.</p> <p>For <em>kind</em> and <em>options</em> see details below.</p> </dd> </dl> <p>Storages can be used in vcl as <code>storage.</code><em>name</em>, so, for example if <code>myStorage</code> was defined by <code>-s myStorage=malloc,5G</code>, it could be used in VCL like so:</p> <pre data-language="python">set beresp.storage = storage.myStorage;
</pre> <p>A special <em>name</em> is <code>Transient</code> which is the default storage for uncacheable objects as resulting from a pass, hit-for-miss or hit-for-pass.</p> <p>If no <code>-s</code> options are given, the default is:</p> <pre data-language="python">-s default,100m
</pre> <p>If no <code>Transient</code> storage is defined, the default is an unbound <code>default</code> storage as if defined as:</p> <pre data-language="python">-s Transient=default
</pre> <p>The following storage types and options are available:</p> <dl class="option-list"> <dt><kbd><span class="option">-s <var>&lt;default[,size]&gt;</var></span></kbd></dt> <dd>
<p>The default storage type resolves to <code>umem</code> where available and <code>malloc</code> otherwise.</p> </dd> <dt><kbd><span class="option">-s <var>&lt;malloc[,size]&gt;</var></span></kbd></dt> <dd>
<p>malloc is a memory based backend.</p> </dd> <dt><kbd><span class="option">-s <var>&lt;umem[,size]&gt;</var></span></kbd></dt> <dd>
<p>umem is a storage backend which is more efficient than malloc on platforms where it is available.</p> <p>See the section on umem in chapter <code>Storage backends</code> of <code>The Varnish Users Guide</code> for details.</p> </dd> <dt><kbd><span class="option">-s <var>&lt;file,path[,size[,granularity[,advice]]]&gt;</var></span></kbd></dt> <dd>
<p>The file backend stores data in a file on disk. The file will be accessed using mmap. Note that this storage provide no cache persistence.</p> <p>The path is mandatory. If path points to a directory, a temporary file will be created in that directory and immediately unlinked. If path points to a non-existing file, the file will be created.</p> <p>If size is omitted, and path points to an existing file with a size greater than zero, the size of that file will be used. If not, an error is reported.</p> <p>Granularity sets the allocation block size. Defaults to the system page size or the filesystem block size, whichever is larger.</p> <p>Advice tells the kernel how <code>varnishd</code> expects to use this mapped region so that the kernel can choose the appropriate read-ahead and caching techniques. Possible values are <code>normal</code>, <code>random</code> and <code>sequential</code>, corresponding to MADV_NORMAL, MADV_RANDOM and MADV_SEQUENTIAL madvise() advice argument, respectively. Defaults to <code>random</code>.</p> </dd> <dt><kbd><span class="option">-s <var>&lt;persistent,path,size&gt;</var></span></kbd></dt> <dd>
<p>Persistent storage. Varnish will store objects in a file in a manner that will secure the survival of <em>most</em> of the objects in the event of a planned or unplanned shutdown of Varnish. The persistent storage backend has multiple issues with it and will likely be removed from a future version of Varnish.</p> </dd> </dl> </section> <section id="jail"> <h4 id="ref-varnishd-opt-j">Jail</h4> <p>Varnish jails are a generalization over various platform specific methods to reduce the privileges of varnish processes. They may have specific options. Available jails are:</p> <dl class="option-list"> <dt><kbd><span class="option">-j <var>&lt;solaris[,worker=`privspec`]&gt;</var></span></kbd></dt> <dd>
<p>Reduce <code>privileges(5)</code> for <code>varnishd</code> and sub-processes to the minimally required set. Only available on platforms which have the <code>setppriv(2)</code> call.</p> <p>The optional <code>worker</code> argument can be used to pass a privilege-specification (see <code>ppriv(1)</code>) by which to extend the effective set of the varnish worker process. While extended privileges may be required by custom vmods, <em>not</em> using the <code>worker</code> option is always more secure.</p> <p>Example to grant basic privileges to the worker process:</p> <pre data-language="python">-j solaris,worker=basic
</pre> </dd> <dt><kbd><span class="option">-j <var>&lt;unix[,user=`user`][,ccgroup=`group`][,workuser=`user`]&gt;</var></span></kbd></dt> <dd>
<p>Default on all other platforms when <code>varnishd</code> is started with an effective uid of 0 (“as root”).</p> <p>With the <code>unix</code> jail mechanism activated, varnish will switch to an alternative user for subprocesses and change the effective uid of the master process whenever possible.</p> <p>The optional <code>user</code> argument specifies which alternative user to use. It defaults to <code>varnish</code>.</p> <p>The optional <code>ccgroup</code> argument specifies a group to add to varnish subprocesses requiring access to a c-compiler. There is no default.</p> <p>The optional <code>workuser</code> argument specifies an alternative user to use for the worker process. It defaults to <code>vcache</code>.</p> <p>The users given for the <code>user</code> and <code>workuser</code> arguments need to have the same primary (“login”) group.</p> <p>To set up a system for the default users with a group name <code>varnish</code>, shell commands similar to these may be used:</p> <pre data-language="python">groupadd varnish
useradd -g varnish -d /nonexistent -s /bin/false \
  -c "Varnish-Cache Daemon User" varnish
useradd -g varnish -d /nonexistent -s /bin/false \
  -c "Varnish-Cache Worker User" vcache
</pre> </dd> <dt><kbd><span class="option">-j <var>none</var></span></kbd></dt> <dd>
<p>last resort jail choice: With jail mechanism <code>none</code>, varnish will run all processes with the privileges it was started with.</p> </dd> </dl> </section> <section id="management-interface"> <h4 id="ref-varnishd-opt-t">Management Interface</h4> <p>If the -T option was specified, <code>varnishd</code> will offer a command-line management interface on the specified address and port. The recommended way of connecting to the command-line management interface is through <a class="reference internal" href="varnishadm.html#varnishadm-1"><span class="std std-ref">varnishadm</span></a>.</p> <p>The commands available are documented in <a class="reference internal" href="varnish-cli.html#varnish-cli-7"><span class="std std-ref">varnish-cli</span></a>.</p> </section> <section id="cli-command-file"> <h4>CLI Command File</h4> <p>The -I option makes it possible to run arbitrary management commands when <code>varnishd</code> is launched, before the worker process is started. In particular, this is the way to load configurations, apply labels to them, and make a VCL instance active that uses those labels on startup:</p> <pre data-language="python">vcl.load panic /etc/varnish_panic.vcl
vcl.load siteA0 /etc/varnish_siteA.vcl
vcl.load siteB0 /etc/varnish_siteB.vcl
vcl.load siteC0 /etc/varnish_siteC.vcl
vcl.label siteA siteA0
vcl.label siteB siteB0
vcl.label siteC siteC0
vcl.load main /etc/varnish_main.vcl
vcl.use main
</pre> <p>Every line in the file, including the last line, must be terminated by a newline or carriage return.</p> <p>If a command in the file is prefixed with ‘-’, failure will not abort the startup.</p> <p>Note that it is necessary to include an explicit <code>vcl.use</code> command to select which VCL should be the active VCL when relying on CLI Command File to load the configurations at startup.</p> </section> </section> <section id="run-time-parameters"> <h3 id="ref-varnishd-params">RUN TIME PARAMETERS</h3> <section id="run-time-parameter-flags"> <h4>Run Time Parameter Flags</h4> <p>Runtime parameters are marked with shorthand flags to avoid repeating the same text over and over in the table below. The meaning of the flags are:</p> <ul> <li>
<p><code>experimental</code></p> <p>We have no solid information about good/bad/optimal values for this parameter. Feedback with experience and observations are most welcome.</p> </li> <li>
<p><code>delayed</code></p> <p>This parameter can be changed on the fly, but will not take effect immediately.</p> </li> <li>
<p><code>restart</code></p> <p>The worker process must be stopped and restarted, before this parameter takes effect.</p> </li> <li>
<p><code>reload</code></p> <p>The VCL programs must be reloaded for this parameter to take effect.</p> </li> <li>
<p><code>wizard</code></p> <p>Do not touch unless you <em>really</em> know what you’re doing.</p> </li> <li>
<p><code>only_root</code></p> <p>Only works if <code>varnishd</code> is running as root.</p> </li> </ul> </section> <section id="default-value-exceptions-on-32-bit-systems"> <h4>Default Value Exceptions on 32 bit Systems</h4> <p>Be aware that on 32 bit systems, certain default or maximum values are reduced relative to the values listed below, in order to conserve VM space:</p> <ul class="simple"> <li>workspace_client: 24k</li> <li>workspace_backend: 20k</li> <li>http_resp_size: 8k</li> <li>http_req_size: 12k</li> <li>gzip_buffer: 4k</li> <li>vsl_buffer: 4k</li> <li>vsl_space: 1G (maximum)</li> <li>thread_pool_stack: 64k</li> </ul> </section> <section id="list-of-parameters"> <h4 id="id1">List of Parameters</h4> <p>This text is produced from the same text you will find in the CLI if you use the param.show command:</p> <section id="accept-filter"> <h5 id="ref-param-accept-filter">accept_filter</h5> <p>NB: This parameter depends on a feature which is not available on all platforms.</p>  <ul class="simple"> <li>Units: bool</li> <li>Default: on (if your platform supports accept filters)</li> <li>Flags: must_restart</li> </ul>  <p>Enable kernel accept-filters. This may require a kernel module to be loaded to have an effect when enabled.</p> <p>Enabling accept_filter may prevent some requests to reach Varnish in the first place. Malformed requests may go unnoticed and not increase the client_req_400 counter. GET or HEAD requests with a body may be blocked altogether.</p> </section> <section id="acceptor-sleep-decay"> <h5 id="ref-param-acceptor-sleep-decay">acceptor_sleep_decay</h5>  <ul class="simple"> <li>Default: 0.9</li> <li>Minimum: 0</li> <li>Maximum: 1</li> <li>Flags: experimental</li> </ul>  <p>If we run out of resources, such as file descriptors or worker threads, the acceptor will sleep between accepts. This parameter (multiplicatively) reduce the sleep duration for each successful accept. (ie: 0.9 = reduce by 10%)</p> </section> <section id="acceptor-sleep-incr"> <h5 id="ref-param-acceptor-sleep-incr">acceptor_sleep_incr</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 0.000</li> <li>Minimum: 0.000</li> <li>Maximum: 1.000</li> <li>Flags: experimental</li> </ul>  <p>If we run out of resources, such as file descriptors or worker threads, the acceptor will sleep between accepts. This parameter control how much longer we sleep, each time we fail to accept a new connection.</p> </section> <section id="acceptor-sleep-max"> <h5 id="ref-param-acceptor-sleep-max">acceptor_sleep_max</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 0.050</li> <li>Minimum: 0.000</li> <li>Maximum: 10.000</li> <li>Flags: experimental</li> </ul>  <p>If we run out of resources, such as file descriptors or worker threads, the acceptor will sleep between accepts. This parameter limits how long it can sleep between attempts to accept new connections.</p> </section> <section id="auto-restart"> <h5 id="ref-param-auto-restart">auto_restart</h5>  <ul class="simple"> <li>Units: bool</li> <li>Default: on</li> </ul>  <p>Automatically restart the child/worker process if it dies.</p> </section> <section id="backend-idle-timeout"> <h5 id="ref-param-backend-idle-timeout">backend_idle_timeout</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 60.000</li> <li>Minimum: 1.000</li> </ul>  <p>Timeout before we close unused backend connections.</p> </section> <section id="backend-local-error-holddown"> <h5 id="ref-param-backend-local-error-holddown">backend_local_error_holddown</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 10.000</li> <li>Minimum: 0.000</li> <li>Flags: experimental</li> </ul>  <p>When connecting to backends, certain error codes (EADDRNOTAVAIL, EACCESS, EPERM) signal a local resource shortage or configuration issue for which retrying connection attempts may worsen the situation due to the complexity of the operations involved in the kernel. This parameter prevents repeated connection attempts for the configured duration.</p> </section> <section id="backend-remote-error-holddown"> <h5 id="ref-param-backend-remote-error-holddown">backend_remote_error_holddown</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 0.250</li> <li>Minimum: 0.000</li> <li>Flags: experimental</li> </ul>  <p>When connecting to backends, certain error codes (ECONNREFUSED, ENETUNREACH) signal fundamental connection issues such as the backend not accepting connections or routing problems for which repeated connection attempts are considered useless This parameter prevents repeated connection attempts for the configured duration.</p> </section> <section id="ban-cutoff"> <h5 id="ref-param-ban-cutoff">ban_cutoff</h5>  <ul class="simple"> <li>Units: bans</li> <li>Default: 0</li> <li>Minimum: 0</li> <li>Flags: experimental</li> </ul>  <p>Expurge long tail content from the cache to keep the number of bans below this value. 0 disables.</p> <p>When this parameter is set to a non-zero value, the ban lurker continues to work the ban list as usual top to bottom, but when it reaches the ban_cutoff-th ban, it treats all objects as if they matched a ban and expurges them from cache. As actively used objects get tested against the ban list at request time and thus are likely to be associated with bans near the top of the ban list, with ban_cutoff, least recently accessed objects (the “long tail”) are removed.</p> <p>This parameter is a safety net to avoid bad response times due to bans being tested at lookup time. Setting a cutoff trades response time for cache efficiency. The recommended value is proportional to rate(bans_lurker_tests_tested) / n_objects while the ban lurker is working, which is the number of bans the system can sustain. The additional latency due to request ban testing is in the order of ban_cutoff / rate(bans_lurker_tests_tested). For example, for rate(bans_lurker_tests_tested) = 2M/s and a tolerable latency of 100ms, a good value for ban_cutoff may be 200K.</p> </section> <section id="ban-dups"> <h5 id="ref-param-ban-dups">ban_dups</h5>  <ul class="simple"> <li>Units: bool</li> <li>Default: on</li> </ul>  <p>Eliminate older identical bans when a new ban is added. This saves CPU cycles by not comparing objects to identical bans. This is a waste of time if you have many bans which are never identical.</p> </section> <section id="ban-lurker-age"> <h5 id="ref-param-ban-lurker-age">ban_lurker_age</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 60.000</li> <li>Minimum: 0.000</li> </ul>  <p>The ban lurker will ignore bans until they are this old. When a ban is added, the active traffic will be tested against it as part of object lookup. Because many applications issue bans in bursts, this parameter holds the ban-lurker off until the rush is over. This should be set to the approximate time which a ban-burst takes.</p> </section> <section id="ban-lurker-batch"> <h5 id="ref-param-ban-lurker-batch">ban_lurker_batch</h5>  <ul class="simple"> <li>Default: 1000</li> <li>Minimum: 1</li> </ul>  <p>The ban lurker sleeps ${ban_lurker_sleep} after examining this many objects. Use this to pace the ban-lurker if it eats too many resources.</p> </section> <section id="ban-lurker-holdoff"> <h5 id="ref-param-ban-lurker-holdoff">ban_lurker_holdoff</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 0.010</li> <li>Minimum: 0.000</li> <li>Flags: experimental</li> </ul>  <p>How long the ban lurker sleeps when giving way to lookup due to lock contention.</p> </section> <section id="ban-lurker-sleep"> <h5 id="ref-param-ban-lurker-sleep">ban_lurker_sleep</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 0.010</li> <li>Minimum: 0.000</li> </ul>  <p>How long the ban lurker sleeps after examining ${ban_lurker_batch} objects. Use this to pace the ban-lurker if it eats too many resources. A value of zero will disable the ban lurker entirely.</p> </section> <section id="between-bytes-timeout"> <h5 id="ref-param-between-bytes-timeout">between_bytes_timeout</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 60.000</li> <li>Minimum: 0.000</li> </ul>  <p>We only wait for this many seconds between bytes received from the backend before giving up the fetch. VCL values, per backend or per backend request take precedence. This parameter does not apply to pipe’ed requests.</p> </section> <section id="cc-command"> <h5 id="ref-param-cc-command">cc_command</h5> <p>NB: The actual default value for this parameter depends on the Varnish build environment and options.</p>  <ul class="simple"> <li>Default: exec $CC $CFLAGS %w -shared -o %o %s</li> <li>Flags: must_reload</li> </ul>  <p>The command used for compiling the C source code to a dlopen(3) loadable object. The following expansions can be used:</p> <ul class="simple"> <li>%s: the source file name</li> <li>%o: the output file name</li> <li>%w: the cc_warnings parameter</li> <li>%d: the raw default cc_command</li> <li>%D: the expanded default cc_command</li> <li>%n: the working directory (-n option)</li> <li>%%: a percent sign</li> </ul> <p>Unknown percent expansion sequences are ignored, and to avoid future incompatibilities percent characters should be escaped with a double percent sequence.</p> <p>The %d and %D expansions allow passing the parameter’s default value to a wrapper script to perform additional processing.</p> </section> <section id="cc-warnings"> <h5 id="ref-param-cc-warnings">cc_warnings</h5> <p>NB: The actual default value for this parameter depends on the Varnish build environment and options.</p>  <ul class="simple"> <li>Default: -Wall -Werror</li> <li>Flags: must_reload</li> </ul>  <p>Warnings used when compiling the C source code with the cc_command parameter. By default, VCL is compiled with the same set of warnings as Varnish itself.</p> </section> <section id="cli-limit"> <h5 id="ref-param-cli-limit">cli_limit</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 48k</li> <li>Minimum: 128b</li> <li>Maximum: 99999999b</li> </ul>  <p>Maximum size of CLI response. If the response exceeds this limit, the response code will be 201 instead of 200 and the last line will indicate the truncation.</p> </section> <section id="cli-timeout"> <h5 id="ref-param-cli-timeout">cli_timeout</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 60.000</li> <li>Minimum: 0.000</li> </ul>  <p>Timeout for the child’s replies to CLI requests.</p> </section> <section id="clock-skew"> <h5 id="ref-param-clock-skew">clock_skew</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 10</li> <li>Minimum: 0</li> </ul>  <p>How much clockskew we are willing to accept between the backend and our own clock.</p> </section> <section id="clock-step"> <h5 id="ref-param-clock-step">clock_step</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 1.000</li> <li>Minimum: 0.000</li> </ul>  <p>How much observed clock step we are willing to accept before we panic.</p> </section> <section id="connect-timeout"> <h5 id="ref-param-connect-timeout">connect_timeout</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 3.500</li> <li>Minimum: 0.000</li> </ul>  <p>Default connection timeout for backend connections. We only try to connect to the backend for this many seconds before giving up. VCL can override this default value for each backend and backend request.</p> </section> <section id="critbit-cooloff"> <h5 id="ref-param-critbit-cooloff">critbit_cooloff</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 180.000</li> <li>Minimum: 60.000</li> <li>Maximum: 254.000</li> <li>Flags: wizard</li> </ul>  <p>How long the critbit hasher keeps deleted objheads on the cooloff list.</p> </section> <section id="debug"> <h5 id="ref-param-debug">debug</h5>  <ul class="simple"> <li>Default: none</li> </ul>  <p>Enable/Disable various kinds of debugging.</p>  <dl class="simple"> <dt><em>none</em></dt>
<dd>
<p>Disable all debugging</p> </dd> </dl>  <p>Use +/- prefix to set/reset individual bits:</p>  <dl class="simple"> <dt><em>req_state</em></dt>
<dd>
<p>VSL Request state engine</p> </dd> <dt><em>workspace</em></dt>
<dd>
<p>VSL Workspace operations</p> </dd> <dt><em>waitinglist</em></dt>
<dd>
<p>VSL Waitinglist events</p> </dd> <dt><em>syncvsl</em></dt>
<dd>
<p>Make VSL synchronous</p> </dd> <dt><em>hashedge</em></dt>
<dd>
<p>Edge cases in Hash</p> </dd> <dt><em>vclrel</em></dt>
<dd>
<p>Rapid VCL release</p> </dd> <dt><em>lurker</em></dt>
<dd>
<p>VSL Ban lurker</p> </dd> <dt><em>esi_chop</em></dt>
<dd>
<p>Chop ESI fetch to bits</p> </dd> <dt><em>flush_head</em></dt>
<dd>
<p>Flush after http1 head</p> </dd> <dt><em>vtc_mode</em></dt>
<dd>
<p>Varnishtest Mode</p> </dd> <dt><em>witness</em></dt>
<dd>
<p>Emit WITNESS lock records</p> </dd> <dt><em>vsm_keep</em></dt>
<dd>
<p>Keep the VSM file on restart</p> </dd> <dt><em>slow_acceptor</em></dt>
<dd>
<p>Slow down Acceptor</p> </dd> <dt><em>h2_nocheck</em></dt>
<dd>
<p>Disable various H2 checks</p> </dd> <dt><em>vmod_so_keep</em></dt>
<dd>
<p>Keep copied VMOD libraries</p> </dd> <dt><em>processors</em></dt>
<dd>
<p>Fetch/Deliver processors</p> </dd> <dt><em>protocol</em></dt>
<dd>
<p>Protocol debugging</p> </dd> <dt><em>vcl_keep</em></dt>
<dd>
<p>Keep VCL C and so files</p> </dd> <dt><em>lck</em></dt>
<dd>
<p>Additional lock statistics</p> </dd> </dl>  </section> <section id="default-grace"> <h5 id="ref-param-default-grace">default_grace</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 10s</li> <li>Minimum: 0.000</li> <li>Flags: obj_sticky</li> </ul>  <p>Default grace period. We will deliver an object this long after it has expired, provided another thread is attempting to get a new copy.</p> </section> <section id="default-keep"> <h5 id="ref-param-default-keep">default_keep</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 0s</li> <li>Minimum: 0.000</li> <li>Flags: obj_sticky</li> </ul>  <p>Default keep period. We will keep a useless object around this long, making it available for conditional backend fetches. That means that the object will be removed from the cache at the end of ttl+grace+keep.</p> </section> <section id="default-ttl"> <h5 id="ref-param-default-ttl">default_ttl</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 2m</li> <li>Minimum: 0.000</li> <li>Flags: obj_sticky</li> </ul>  <p>The TTL assigned to objects if neither the backend nor the VCL code assigns one.</p> </section> <section id="experimental"> <h5 id="ref-param-experimental">experimental</h5>  <ul class="simple"> <li>Default: none</li> </ul>  <p>Enable/Disable experimental features.</p>  <dl class="simple"> <dt><em>none</em></dt>
<dd>
<p>Disable all experimental features</p> </dd> </dl>  <p>Use +/- prefix to set/reset individual bits:</p>  <dl class="simple"> <dt><em>drop_pools</em></dt>
<dd>
<p>Drop thread pools</p> </dd> </dl>  </section> <section id="feature"> <h5 id="ref-param-feature">feature</h5>  <ul class="simple"> <li>Default: +validate_headers</li> </ul>  <p>Enable/Disable various minor features.</p>  <dl class="simple"> <dt><em>default</em></dt>
<dd>
<p>Set default value</p> </dd> <dt><em>none</em></dt>
<dd>
<p>Disable all features.</p> </dd> </dl>  <p>Use +/- prefix to enable/disable individual feature:</p>  <dl class="simple"> <dt><em>http2</em></dt>
<dd>
<p>Enable HTTP/2 protocol support.</p> </dd> <dt><em>short_panic</em></dt>
<dd>
<p>Short panic message.</p> </dd> <dt><em>no_coredump</em></dt>
<dd>
<p>No coredumps. Must be set before child process starts.</p> </dd> <dt><em>https_scheme</em></dt>
<dd>
<p>Extract host from full URI in the HTTP/1 request line, if the scheme is https.</p> </dd> <dt><em>http_date_postel</em></dt>
<dd>
<p>Tolerate non compliant timestamp headers like <code>Date</code>, <code>Last-Modified</code>, <code>Expires</code> etc.</p> </dd> <dt><em>esi_ignore_https</em></dt>
<dd>
<p>Convert <code>&lt;esi:include src”https://…</code> to <code>http://…</code></p> </dd> <dt><em>esi_disable_xml_check</em></dt>
<dd>
<p>Allow ESI processing on non-XML ESI bodies</p> </dd> <dt><em>esi_ignore_other_elements</em></dt>
<dd>
<p>Ignore XML syntax errors in ESI bodies.</p> </dd> <dt><em>esi_remove_bom</em></dt>
<dd>
<p>Ignore UTF-8 BOM in ESI bodies.</p> </dd> <dt><em>esi_include_onerror</em></dt>
<dd>
<p>Parse the onerror attribute of &lt;esi:include&gt; tags.</p> </dd> <dt><em>wait_silo</em></dt>
<dd>
<p>Wait for persistent silos to completely load before serving requests.</p> </dd> <dt><em>validate_headers</em></dt>
<dd>
<p>Validate all header set operations to conform to RFC7230.</p> </dd> <dt><em>busy_stats_rate</em></dt>
<dd>
<p>Make busy workers comply with thread_stats_rate.</p> </dd> <dt><em>trace</em></dt>
<dd>
<p>Enable VCL tracing by default (enable (be)req.trace). Required for tracing vcl_init / vcl_fini</p> </dd> </dl>  </section> <section id="fetch-chunksize"> <h5 id="ref-param-fetch-chunksize">fetch_chunksize</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 16k</li> <li>Minimum: 4k</li> <li>Flags: experimental</li> </ul>  <p>The default chunksize used by fetcher. This should be bigger than the majority of objects with short TTLs. Internal limits in the storage_file module makes increases above 128kb a dubious idea.</p> </section> <section id="fetch-maxchunksize"> <h5 id="ref-param-fetch-maxchunksize">fetch_maxchunksize</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 0.25G</li> <li>Minimum: 64k</li> <li>Flags: experimental</li> </ul>  <p>The maximum chunksize we attempt to allocate from storage. Making this too large may cause delays and storage fragmentation.</p> </section> <section id="first-byte-timeout"> <h5 id="ref-param-first-byte-timeout">first_byte_timeout</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 60.000</li> <li>Minimum: 0.000</li> </ul>  <p>Default timeout for receiving first byte from backend. We only wait for this many seconds for the first byte before giving up. VCL can override this default value for each backend and backend request. This parameter does not apply to pipe’ed requests.</p> </section> <section id="gzip-buffer"> <h5 id="ref-param-gzip-buffer">gzip_buffer</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 32k</li> <li>Minimum: 2k</li> <li>Flags: experimental</li> </ul>  <p>Size of malloc buffer used for gzip processing. These buffers are used for in-transit data, for instance gunzip’ed data being sent to a client.Making this space to small results in more overhead, writes to sockets etc, making it too big is probably just a waste of memory.</p> </section> <section id="gzip-level"> <h5 id="ref-param-gzip-level">gzip_level</h5>  <ul class="simple"> <li>Default: 6</li> <li>Minimum: 0</li> <li>Maximum: 9</li> </ul>  <p>Gzip compression level: 0=debug, 1=fast, 9=best</p> </section> <section id="gzip-memlevel"> <h5 id="ref-param-gzip-memlevel">gzip_memlevel</h5>  <ul class="simple"> <li>Default: 8</li> <li>Minimum: 1</li> <li>Maximum: 9</li> </ul>  <p>Gzip memory level 1=slow/least, 9=fast/most compression. Memory impact is 1=1k, 2=2k, … 9=256k.</p> </section> <section id="h2-header-table-size"> <h5 id="ref-param-h2-header-table-size">h2_header_table_size</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 4k</li> <li>Minimum: 0b</li> </ul>  <p>HTTP2 header table size. This is the size that will be used for the HPACK dynamic decoding table.</p> </section> <section id="h2-initial-window-size"> <h5 id="ref-param-h2-initial-window-size">h2_initial_window_size</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 65535b</li> <li>Minimum: 65535b</li> <li>Maximum: 2147483647b</li> </ul>  <p>HTTP2 initial flow control window size.</p> </section> <section id="h2-max-concurrent-streams"> <h5 id="ref-param-h2-max-concurrent-streams">h2_max_concurrent_streams</h5>  <ul class="simple"> <li>Units: streams</li> <li>Default: 100</li> <li>Minimum: 0</li> </ul>  <p>HTTP2 Maximum number of concurrent streams. This is the number of requests that can be active at the same time for a single HTTP2 connection.</p> </section> <section id="h2-max-frame-size"> <h5 id="ref-param-h2-max-frame-size">h2_max_frame_size</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 16k</li> <li>Minimum: 16k</li> <li>Maximum: 16777215b</li> </ul>  <p>HTTP2 maximum per frame payload size we are willing to accept.</p> </section> <section id="h2-max-header-list-size"> <h5 id="ref-param-h2-max-header-list-size">h2_max_header_list_size</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 2147483647b</li> <li>Minimum: 0b</li> </ul>  <p>HTTP2 maximum size of an uncompressed header list.</p> </section> <section id="h2-rx-window-increment"> <h5 id="ref-param-h2-rx-window-increment">h2_rx_window_increment</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 1M</li> <li>Minimum: 1M</li> <li>Maximum: 1G</li> <li>Flags: wizard</li> </ul>  <p>HTTP2 Receive Window Increments. How big credits we send in WINDOW_UPDATE frames Only affects incoming request bodies (ie: POST, PUT etc.)</p> </section> <section id="h2-rx-window-low-water"> <h5 id="ref-param-h2-rx-window-low-water">h2_rx_window_low_water</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 10M</li> <li>Minimum: 65535b</li> <li>Maximum: 1G</li> <li>Flags: wizard</li> </ul>  <p>HTTP2 Receive Window low water mark. We try to keep the window at least this big Only affects incoming request bodies (ie: POST, PUT etc.)</p> </section> <section id="h2-rxbuf-storage"> <h5 id="ref-param-h2-rxbuf-storage">h2_rxbuf_storage</h5>  <ul class="simple"> <li>Default: Transient</li> <li>Flags: must_restart</li> </ul>  <p>The name of the storage backend that HTTP/2 receive buffers should be allocated from.</p> </section> <section id="http1-iovs"> <h5 id="ref-param-http1-iovs">http1_iovs</h5>  <ul class="simple"> <li>Units: struct iovec (=16 bytes)</li> <li>Default: 64</li> <li>Minimum: 5</li> <li>Maximum: 1024</li> <li>Flags: wizard</li> </ul>  <p>Number of io vectors to allocate for HTTP1 protocol transmission. A HTTP1 header needs 7 + 2 per HTTP header field. Allocated from workspace_thread. This parameter affects only io vectors used for client delivery. For backend fetches, the maximum number of io vectors (up to IOV_MAX) is allocated from available workspace_thread memory.</p> </section> <section id="http-gzip-support"> <h5 id="ref-param-http-gzip-support">http_gzip_support</h5>  <ul class="simple"> <li>Units: bool</li> <li>Default: on</li> </ul>  <dl class="simple"> <dt>Enable gzip support. When enabled Varnish request compressed objects from the backend and store them compressed. If a client does not support gzip encoding Varnish will uncompress compressed objects on demand. Varnish will also rewrite the Accept-Encoding header of clients indicating support for gzip to:</dt>
<dd>
<p>Accept-Encoding: gzip</p> </dd> </dl> <p>Clients that do not support gzip will have their Accept-Encoding header removed. For more information on how gzip is implemented please see the chapter on gzip in the Varnish reference.</p> <p>When gzip support is disabled the variables beresp.do_gzip and beresp.do_gunzip have no effect in VCL.</p> </section> <section id="http-max-hdr"> <h5 id="ref-param-http-max-hdr">http_max_hdr</h5>  <ul class="simple"> <li>Units: header lines</li> <li>Default: 64</li> <li>Minimum: 32</li> <li>Maximum: 65535</li> </ul>  <p>Maximum number of HTTP header lines we allow in {req|resp|bereq|beresp}.http (obj.http is autosized to the exact number of headers). Cheap, ~20 bytes, in terms of workspace memory. Note that the first line occupies five header lines.</p> </section> <section id="http-range-support"> <h5 id="ref-param-http-range-support">http_range_support</h5>  <ul class="simple"> <li>Units: bool</li> <li>Default: on</li> </ul>  <p>Enable support for HTTP Range headers.</p> </section> <section id="http-req-hdr-len"> <h5 id="ref-param-http-req-hdr-len">http_req_hdr_len</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 8k</li> <li>Minimum: 40b</li> </ul>  <p>Maximum length of any HTTP client request header we will allow. The limit is inclusive its continuation lines.</p> </section> <section id="http-req-size"> <h5 id="ref-param-http-req-size">http_req_size</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 32k</li> <li>Minimum: 0.25k</li> </ul>  <p>Maximum number of bytes of HTTP client request we will deal with. This is a limit on all bytes up to the double blank line which ends the HTTP request. The memory for the request is allocated from the client workspace (param: workspace_client) and this parameter limits how much of that the request is allowed to take up.</p> </section> <section id="http-resp-hdr-len"> <h5 id="ref-param-http-resp-hdr-len">http_resp_hdr_len</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 8k</li> <li>Minimum: 40b</li> </ul>  <p>Maximum length of any HTTP backend response header we will allow. The limit is inclusive its continuation lines.</p> </section> <section id="http-resp-size"> <h5 id="ref-param-http-resp-size">http_resp_size</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 32k</li> <li>Minimum: 0.25k</li> </ul>  <p>Maximum number of bytes of HTTP backend response we will deal with. This is a limit on all bytes up to the double blank line which ends the HTTP response. The memory for the response is allocated from the backend workspace (param: workspace_backend) and this parameter limits how much of that the response is allowed to take up.</p> </section> <section id="ref-param-idle-send-timeout"> <h5 id="idle-send-timeout">idle_send_timeout</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 60.000</li> <li>Minimum: 0.000</li> <li>Flags: delayed</li> </ul>  <p>Send timeout for individual pieces of data on client connections. May get extended if ‘send_timeout’ applies.</p> <p>When this timeout is hit, the session is closed.</p> <p>See the man page for <code>setsockopt(2)</code> or <code>socket(7)</code> under <code>SO_SNDTIMEO</code> for more information.</p> </section> <section id="listen-depth"> <h5 id="ref-param-listen-depth">listen_depth</h5>  <ul class="simple"> <li>Units: connections</li> <li>Default: 1024</li> <li>Minimum: 0</li> <li>Flags: must_restart</li> </ul>  <p>Listen queue depth.</p> </section> <section id="lru-interval"> <h5 id="ref-param-lru-interval">lru_interval</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 2.000</li> <li>Minimum: 0.000</li> <li>Flags: experimental</li> </ul>  <p>Grace period before object moves on LRU list. Objects are only moved to the front of the LRU list if they have not been moved there already inside this timeout period. This reduces the amount of lock operations necessary for LRU list access.</p> </section> <section id="max-esi-depth"> <h5 id="ref-param-max-esi-depth">max_esi_depth</h5>  <ul class="simple"> <li>Units: levels</li> <li>Default: 5</li> <li>Minimum: 0</li> </ul>  <p>Maximum depth of esi:include processing.</p> </section> <section id="max-restarts"> <h5 id="ref-param-max-restarts">max_restarts</h5>  <ul class="simple"> <li>Units: restarts</li> <li>Default: 4</li> <li>Minimum: 0</li> </ul>  <p>Upper limit on how many times a request can restart.</p> </section> <section id="max-retries"> <h5 id="ref-param-max-retries">max_retries</h5>  <ul class="simple"> <li>Units: retries</li> <li>Default: 4</li> <li>Minimum: 0</li> </ul>  <p>Upper limit on how many times a backend fetch can retry.</p> </section> <section id="max-vcl"> <h5 id="ref-param-max-vcl">max_vcl</h5>  <ul class="simple"> <li>Default: 100</li> <li>Minimum: 0</li> </ul>  <p>Threshold of loaded VCL programs. (VCL labels are not counted.) Parameter max_vcl_handling determines behaviour.</p> </section> <section id="max-vcl-handling"> <h5 id="ref-param-max-vcl-handling">max_vcl_handling</h5>  <ul class="simple"> <li>Default: 1</li> <li>Minimum: 0</li> <li>Maximum: 2</li> </ul>  <p>Behaviour when attempting to exceed max_vcl loaded VCL.</p> <ul class="simple"> <li>0 - Ignore max_vcl parameter.</li> <li>1 - Issue warning.</li> <li>2 - Refuse loading VCLs.</li> </ul> </section> <section id="nuke-limit"> <h5 id="ref-param-nuke-limit">nuke_limit</h5>  <ul class="simple"> <li>Units: allocations</li> <li>Default: 50</li> <li>Minimum: 0</li> <li>Flags: experimental</li> </ul>  <p>Maximum number of objects we attempt to nuke in order to make space for a object body.</p> </section> <section id="pcre2-depth-limit"> <h5 id="ref-param-pcre2-depth-limit">pcre2_depth_limit</h5>  <ul class="simple"> <li>Default: 20</li> <li>Minimum: 1</li> </ul>  <p>The recursion depth-limit for the internal match logic in a pcre2_match().</p> <p>(See: pcre2_set_depth_limit() in pcre2 docs.)</p> <p>This puts an upper limit on the amount of stack used by PCRE2 for certain classes of regular expressions.</p> <p>We have set the default value low in order to prevent crashes, at the cost of possible regexp matching failures.</p> <p>Matching failures will show up in the log as VCL_Error messages.</p> </section> <section id="pcre2-jit-compilation"> <h5 id="ref-param-pcre2-jit-compilation">pcre2_jit_compilation</h5>  <ul class="simple"> <li>Units: bool</li> <li>Default: on</li> </ul>  <p>Use the pcre2 JIT compiler if available.</p> </section> <section id="pcre2-match-limit"> <h5 id="ref-param-pcre2-match-limit">pcre2_match_limit</h5>  <ul class="simple"> <li>Default: 10000</li> <li>Minimum: 1</li> </ul>  <p>The limit for the number of calls to the internal match logic in pcre2_match().</p> <p>(See: pcre2_set_match_limit() in pcre2 docs.)</p> <p>This parameter limits how much CPU time regular expression matching can soak up.</p> </section> <section id="ping-interval"> <h5 id="ref-param-ping-interval">ping_interval</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 3</li> <li>Minimum: 0</li> <li>Flags: must_restart</li> </ul>  <p>Interval between pings from parent to child. Zero will disable pinging entirely, which makes it possible to attach a debugger to the child.</p> </section> <section id="pipe-sess-max"> <h5 id="ref-param-pipe-sess-max">pipe_sess_max</h5>  <ul class="simple"> <li>Units: connections</li> <li>Default: 0</li> <li>Minimum: 0</li> </ul>  <p>Maximum number of sessions dedicated to pipe transactions.</p> </section> <section id="pipe-timeout"> <h5 id="ref-param-pipe-timeout">pipe_timeout</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 60.000</li> <li>Minimum: 0.000</li> </ul>  <p>Idle timeout for PIPE sessions. If nothing have been received in either direction for this many seconds, the session is closed.</p> </section> <section id="pool-req"> <h5 id="ref-param-pool-req">pool_req</h5>  <ul class="simple"> <li>Default: 10,100,10</li> </ul>  <p>Parameters for per worker pool request memory pool.</p> <p>The three numbers are:</p>  <dl class="simple"> <dt><em>min_pool</em></dt>
<dd>
<p>minimum size of free pool.</p> </dd> <dt><em>max_pool</em></dt>
<dd>
<p>maximum size of free pool.</p> </dd> <dt><em>max_age</em></dt>
<dd>
<p>max age of free element.</p> </dd> </dl>  </section> <section id="pool-sess"> <h5 id="ref-param-pool-sess">pool_sess</h5>  <ul class="simple"> <li>Default: 10,100,10</li> </ul>  <p>Parameters for per worker pool session memory pool.</p> <p>The three numbers are:</p>  <dl class="simple"> <dt><em>min_pool</em></dt>
<dd>
<p>minimum size of free pool.</p> </dd> <dt><em>max_pool</em></dt>
<dd>
<p>maximum size of free pool.</p> </dd> <dt><em>max_age</em></dt>
<dd>
<p>max age of free element.</p> </dd> </dl>  </section> <section id="pool-vbo"> <h5 id="ref-param-pool-vbo">pool_vbo</h5>  <ul class="simple"> <li>Default: 10,100,10</li> </ul>  <p>Parameters for backend object fetch memory pool.</p> <p>The three numbers are:</p>  <dl class="simple"> <dt><em>min_pool</em></dt>
<dd>
<p>minimum size of free pool.</p> </dd> <dt><em>max_pool</em></dt>
<dd>
<p>maximum size of free pool.</p> </dd> <dt><em>max_age</em></dt>
<dd>
<p>max age of free element.</p> </dd> </dl>  </section> <section id="prefer-ipv6"> <h5 id="ref-param-prefer-ipv6">prefer_ipv6</h5>  <ul class="simple"> <li>Units: bool</li> <li>Default: off</li> </ul>  <p>Prefer IPv6 address when connecting to backends which have both IPv4 and IPv6 addresses.</p> </section> <section id="rush-exponent"> <h5 id="ref-param-rush-exponent">rush_exponent</h5>  <ul class="simple"> <li>Units: requests per request</li> <li>Default: 3</li> <li>Minimum: 2</li> <li>Flags: experimental</li> </ul>  <p>How many parked request we start for each completed request on the object. NB: Even with the implict delay of delivery, this parameter controls an exponential increase in number of worker threads.</p> </section> <section id="send-timeout"> <h5 id="ref-param-send-timeout">send_timeout</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 600.000</li> <li>Minimum: 0.000</li> <li>Flags: delayed</li> </ul>  <p>Total timeout for ordinary HTTP1 responses. Does not apply to some internally generated errors and pipe mode.</p> <p>When ‘idle_send_timeout’ is hit while sending an HTTP1 response, the timeout is extended unless the total time already taken for sending the response in its entirety exceeds this many seconds.</p> <p>When this timeout is hit, the session is closed</p> </section> <section id="shortlived"> <h5 id="ref-param-shortlived">shortlived</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 10.000</li> <li>Minimum: 0.000</li> </ul>  <p>Objects created with (ttl+grace+keep) shorter than this are always put in transient storage.</p> </section> <section id="sigsegv-handler"> <h5 id="ref-param-sigsegv-handler">sigsegv_handler</h5>  <ul class="simple"> <li>Units: bool</li> <li>Default: on</li> <li>Flags: must_restart</li> </ul>  <p>Install a signal handler which tries to dump debug information on segmentation faults, bus errors and abort signals.</p> </section> <section id="startup-timeout"> <h5 id="ref-param-startup-timeout">startup_timeout</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 0.000</li> <li>Minimum: 0.000</li> </ul>  <p>Alternative timeout for the initial worker process startup. If cli_timeout is longer than startup_timeout, it is used instead.</p> </section> <section id="syslog-cli-traffic"> <h5 id="ref-param-syslog-cli-traffic">syslog_cli_traffic</h5>  <ul class="simple"> <li>Units: bool</li> <li>Default: on</li> </ul>  <p>Log all CLI traffic to syslog(LOG_INFO).</p> </section> <section id="tcp-fastopen"> <h5 id="ref-param-tcp-fastopen">tcp_fastopen</h5> <p>NB: This parameter depends on a feature which is not available on all platforms.</p>  <ul class="simple"> <li>Units: bool</li> <li>Default: off</li> <li>Flags: must_restart</li> </ul>  <p>Enable TCP Fast Open extension.</p> </section> <section id="tcp-keepalive-intvl"> <h5 id="ref-param-tcp-keepalive-intvl">tcp_keepalive_intvl</h5> <p>NB: This parameter depends on a feature which is not available on all platforms.</p>  <ul class="simple"> <li>Units: seconds</li> <li>Default: platform dependent</li> <li>Minimum: 1.000</li> <li>Maximum: 100.000</li> <li>Flags: experimental</li> </ul>  <p>The number of seconds between TCP keep-alive probes. Ignored for Unix domain sockets.</p> </section> <section id="tcp-keepalive-probes"> <h5 id="ref-param-tcp-keepalive-probes">tcp_keepalive_probes</h5> <p>NB: This parameter depends on a feature which is not available on all platforms.</p>  <ul class="simple"> <li>Units: probes</li> <li>Default: platform dependent</li> <li>Minimum: 1</li> <li>Maximum: 100</li> <li>Flags: experimental</li> </ul>  <p>The maximum number of TCP keep-alive probes to send before giving up and killing the connection if no response is obtained from the other end. Ignored for Unix domain sockets.</p> </section> <section id="tcp-keepalive-time"> <h5 id="ref-param-tcp-keepalive-time">tcp_keepalive_time</h5> <p>NB: This parameter depends on a feature which is not available on all platforms.</p>  <ul class="simple"> <li>Units: seconds</li> <li>Default: platform dependent</li> <li>Minimum: 1.000</li> <li>Maximum: 7200.000</li> <li>Flags: experimental</li> </ul>  <p>The number of seconds a connection needs to be idle before TCP begins sending out keep-alive probes. Ignored for Unix domain sockets.</p> </section> <section id="thread-pool-add-delay"> <h5 id="ref-param-thread-pool-add-delay">thread_pool_add_delay</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 0.000</li> <li>Minimum: 0.000</li> <li>Flags: experimental</li> </ul>  <p>Wait at least this long after creating a thread.</p> <p>Some (buggy) systems may need a short (sub-second) delay between creating threads. Set this to a few milliseconds if you see the ‘threads_failed’ counter grow too much.</p> <p>Setting this too high results in insufficient worker threads.</p> </section> <section id="thread-pool-destroy-delay"> <h5 id="ref-param-thread-pool-destroy-delay">thread_pool_destroy_delay</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 1.000</li> <li>Minimum: 0.010</li> <li>Flags: delayed, experimental</li> </ul>  <p>Wait this long after destroying a thread.</p> <p>This controls the decay of thread pools when idle(-ish).</p> </section> <section id="thread-pool-fail-delay"> <h5 id="ref-param-thread-pool-fail-delay">thread_pool_fail_delay</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 0.200</li> <li>Minimum: 0.010</li> <li>Flags: experimental</li> </ul>  <p>Wait at least this long after a failed thread creation before trying to create another thread.</p> <p>Failure to create a worker thread is often a sign that the end is near, because the process is running out of some resource. This delay tries to not rush the end on needlessly.</p> <p>If thread creation failures are a problem, check that thread_pool_max is not too high.</p> <p>It may also help to increase thread_pool_timeout and thread_pool_min, to reduce the rate at which treads are destroyed and later recreated.</p> </section> <section id="thread-pool-max"> <h5 id="ref-param-thread-pool-max">thread_pool_max</h5>  <ul class="simple"> <li>Units: threads</li> <li>Default: 5000</li> <li>Minimum: thread_pool_min</li> <li>Flags: delayed</li> </ul>  <p>The maximum number of worker threads in each pool.</p> <p>Do not set this higher than you have to, since excess worker threads soak up RAM and CPU and generally just get in the way of getting work done.</p> </section> <section id="thread-pool-min"> <h5 id="ref-param-thread-pool-min">thread_pool_min</h5>  <ul class="simple"> <li>Units: threads</li> <li>Default: 100</li> <li>Minimum: 5</li> <li>Maximum: thread_pool_max</li> <li>Flags: delayed</li> </ul>  <p>The minimum number of worker threads in each pool.</p> <p>Increasing this may help ramp up faster from low load situations or when threads have expired.</p> <p>Technical minimum is 5 threads, but this parameter is strongly recommended to be at least 10</p> </section> <section id="thread-pool-reserve"> <h5 id="ref-param-thread-pool-reserve">thread_pool_reserve</h5>  <ul class="simple"> <li>Units: threads</li> <li>Default: 0 (auto-tune: 5% of thread_pool_min)</li> <li>Maximum: 95% of thread_pool_min</li> <li>Flags: delayed</li> </ul>  <p>The number of worker threads reserved for vital tasks in each pool.</p> <p>Tasks may require other tasks to complete (for example, client requests may require backend requests, http2 sessions require streams, which require requests). This reserve is to ensure that lower priority tasks do not prevent higher priority tasks from running even under high load.</p> <p>The effective value is at least 5 (the number of internal priority classes), irrespective of this parameter.</p> </section> <section id="thread-pool-stack"> <h5 id="ref-param-thread-pool-stack">thread_pool_stack</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 80k</li> <li>Minimum: sysconf(_SC_THREAD_STACK_MIN)</li> <li>Flags: delayed</li> </ul>  <p>Worker thread stack size. This will likely be rounded up to a multiple of 4k (or whatever the page_size might be) by the kernel.</p> <p>The required stack size is primarily driven by the depth of the call-tree. The most common relevant determining factors in varnish core code are GZIP (un)compression, ESI processing and regular expression matches. VMODs may also require significant amounts of additional stack. The nesting depth of VCL subs is another factor, although typically not predominant.</p> <p>The stack size is per thread, so the maximum total memory required for worker thread stacks is in the order of size = thread_pools x thread_pool_max x thread_pool_stack.</p> <p>Thus, in particular for setups with many threads, keeping the stack size at a minimum helps reduce the amount of memory required by Varnish.</p> <p>On the other hand, thread_pool_stack must be large enough under all circumstances, otherwise varnish will crash due to a stack overflow. Usually, a stack overflow manifests itself as a segmentation fault (aka segfault / SIGSEGV) with the faulting address being near the stack pointer (sp).</p> <p>Unless stack usage can be reduced, thread_pool_stack must be increased when a stack overflow occurs. Setting it in 150%-200% increments is recommended until stack overflows cease to occur.</p> </section> <section id="thread-pool-timeout"> <h5 id="ref-param-thread-pool-timeout">thread_pool_timeout</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 300.000</li> <li>Minimum: 10.000</li> <li>Flags: delayed, experimental</li> </ul>  <p>Thread idle threshold.</p> <p>Threads in excess of thread_pool_min, which have been idle for at least this long, will be destroyed.</p> </section> <section id="thread-pool-watchdog"> <h5 id="ref-param-thread-pool-watchdog">thread_pool_watchdog</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 60.000</li> <li>Minimum: 0.100</li> <li>Flags: experimental</li> </ul>  <p>Thread queue stuck watchdog.</p> <p>If no queued work have been released for this long, the worker process panics itself.</p> </section> <section id="thread-pools"> <h5 id="ref-param-thread-pools">thread_pools</h5>  <ul class="simple"> <li>Units: pools</li> <li>Default: 2</li> <li>Minimum: 1</li> <li>Maximum: 32</li> <li>Flags: delayed, experimental</li> </ul>  <p>Number of worker thread pools.</p> <p>Increasing the number of worker pools decreases lock contention. Each worker pool also has a thread accepting new connections, so for very high rates of incoming new connections on systems with many cores, increasing the worker pools may be required.</p> <p>Too many pools waste CPU and RAM resources, and more than one pool for each CPU is most likely detrimental to performance.</p> <p>Can be increased on the fly, but decreases require a restart to take effect, unless the drop_pools experimental debug flag is set.</p> </section> <section id="thread-queue-limit"> <h5 id="ref-param-thread-queue-limit">thread_queue_limit</h5>  <ul class="simple"> <li>Units: requests</li> <li>Default: 20</li> <li>Minimum: 0</li> <li>Flags: experimental</li> </ul>  <p>Permitted request queue length per thread-pool.</p> <p>This sets the number of requests we will queue, waiting for an available thread. Above this limit sessions will be dropped instead of queued.</p> </section> <section id="thread-stats-rate"> <h5 id="ref-param-thread-stats-rate">thread_stats_rate</h5>  <ul class="simple"> <li>Units: requests</li> <li>Default: 10</li> <li>Minimum: 0</li> <li>Flags: experimental</li> </ul>  <p>Worker threads accumulate statistics, and dump these into the global stats counters if the lock is free when they finish a job (request/fetch etc.) This parameters defines the maximum number of jobs a worker thread may handle, before it is forced to dump its accumulated stats into the global counters.</p> </section> <section id="timeout-idle"> <h5 id="ref-param-timeout-idle">timeout_idle</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 5.000</li> <li>Minimum: 0.000</li> </ul>  <p>Idle timeout for client connections.</p> <p>A connection is considered idle until we have received the full request headers.</p> <p>This parameter is particularly relevant for HTTP1 keepalive connections which are closed unless the next request is received before this timeout is reached.</p> </section> <section id="timeout-linger"> <h5 id="ref-param-timeout-linger">timeout_linger</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 0.050</li> <li>Minimum: 0.000</li> <li>Flags: experimental</li> </ul>  <p>How long the worker thread lingers on an idle session before handing it over to the waiter. When sessions are reused, as much as half of all reuses happen within the first 100 msec of the previous request completing. Setting this too high results in worker threads not doing anything for their keep, setting it too low just means that more sessions take a detour around the waiter.</p> </section> <section id="transit-buffer"> <h5 id="ref-param-transit-buffer">transit_buffer</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 0b</li> <li>Minimum: 0b</li> </ul>  <p>The number of bytes which Varnish buffers for uncacheable backend streaming fetches - in other words, how many bytes Varnish reads from the backend ahead of what has been sent to the client. A zero value means no limit, the object is fetched as fast as possible.</p> <p>When dealing with slow clients, setting this parameter to non-zero can prevent large uncacheable objects from being stored in full when the intent is to simply stream them to the client. As a result, a slow client transaction holds onto a backend connection until the end of the delivery.</p> <p>This parameter is the default to the VCL variable <code>beresp.transit_buffer</code>, which can be used to control the transit buffer per backend request.</p> </section> <section id="vary-notice"> <h5 id="ref-param-vary-notice">vary_notice</h5>  <ul class="simple"> <li>Units: variants</li> <li>Default: 10</li> <li>Minimum: 1</li> </ul>  <p>How many variants need to be evaluated to log a Notice that there might be too many variants.</p> </section> <section id="vcc-allow-inline-c"> <h5 id="ref-param-vcc-allow-inline-c">vcc_allow_inline_c</h5> <p>Deprecated alias for the vcc_feature parameter.</p> </section> <section id="vcc-err-unref"> <h5 id="ref-param-vcc-err-unref">vcc_err_unref</h5> <p>Deprecated alias for the vcc_feature parameter.</p> </section> <section id="vcc-feature"> <h5 id="ref-param-vcc-feature">vcc_feature</h5>  <ul class="simple"> <li>Default: +err_unref,+unsafe_path</li> </ul>  <p>Enable/Disable various VCC behaviors.</p>  <dl class="simple"> <dt><em>default</em></dt>
<dd>
<p>Set default value</p> </dd> <dt><em>none</em></dt>
<dd>
<p>Disable all behaviors.</p> </dd> </dl>  <p>Use +/- prefix to enable/disable individual behavior:</p>  <dl class="simple"> <dt><em>err_unref</em></dt>
<dd>
<p>Unreferenced VCL objects result in error.</p> </dd> <dt><em>allow_inline_c</em></dt>
<dd>
<p>Allow inline C code in VCL.</p> </dd> <dt><em>unsafe_path</em></dt>
<dd>
<p>Allow ‘/’ in vmod &amp; include paths. Allow ‘import … from …’.</p> </dd> </dl>  </section> <section id="vcc-unsafe-path"> <h5 id="ref-param-vcc-unsafe-path">vcc_unsafe_path</h5> <p>Deprecated alias for the vcc_feature parameter.</p> </section> <section id="vcl-cooldown"> <h5 id="ref-param-vcl-cooldown">vcl_cooldown</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 600.000</li> <li>Minimum: 1.000</li> </ul>  <p>How long a VCL is kept warm after being replaced as the active VCL (granularity approximately 30 seconds).</p> </section> <section id="vcl-path"> <h5 id="ref-param-vcl-path">vcl_path</h5> <p>NB: The actual default value for this parameter depends on the Varnish build environment and options.</p>  <ul class="simple"> <li>Default: ${sysconfdir}/varnish:${datadir}/varnish/vcl</li> </ul>  <p>Directory (or colon separated list of directories) from which relative VCL filenames (vcl.load and include) are to be found. By default Varnish searches VCL files in both the system configuration and shared data directories to allow packages to drop their VCL files in a standard location where relative includes would work.</p> </section> <section id="vmod-path"> <h5 id="ref-param-vmod-path">vmod_path</h5> <p>NB: The actual default value for this parameter depends on the Varnish build environment and options.</p>  <ul class="simple"> <li>Default: ${libdir}/varnish/vmods</li> </ul>  <p>Directory (or colon separated list of directories) where VMODs are to be found.</p> </section> <section id="vsl-buffer"> <h5 id="ref-param-vsl-buffer">vsl_buffer</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 16k</li> <li>Minimum: vsl_reclen + 12 bytes</li> </ul>  <p>Bytes of (req-/backend-)workspace dedicated to buffering VSL records. When this parameter is adjusted, most likely workspace_client and workspace_backend will have to be adjusted by the same amount.</p> <p>Setting this too high costs memory, setting it too low will cause more VSL flushes and likely increase lock-contention on the VSL mutex.</p> </section> <section id="vsl-mask"> <h5 id="ref-param-vsl-mask">vsl_mask</h5>  <ul class="simple"> <li>Default: -Debug,-ObjProtocol,-ObjStatus,-ObjReason,-ObjHeader,-ExpKill,-WorkThread,-Hash,-VfpAcct,-H2RxHdr,-H2RxBody,-H2TxHdr,-H2TxBody,-VdpAcct</li> </ul>  <p>Mask individual VSL messages from being logged.</p>  <dl class="simple"> <dt><em>default</em></dt>
<dd>
<p>Set default value</p> </dd> </dl>  <p>Use +/- prefix in front of VSL tag name to unmask/mask individual VSL messages.</p> </section> <section id="vsl-reclen"> <h5 id="ref-param-vsl-reclen">vsl_reclen</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 255b</li> <li>Minimum: 16b</li> <li>Maximum: vsl_buffer - 12 bytes</li> </ul>  <p>Maximum number of bytes in SHM log record.</p> </section> <section id="vsl-space"> <h5 id="ref-param-vsl-space">vsl_space</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 80M</li> <li>Minimum: 1M</li> <li>Maximum: 4G</li> <li>Flags: must_restart</li> </ul>  <p>The amount of space to allocate for the VSL fifo buffer in the VSM memory segment. If you make this too small, varnish{ncsa|log} etc will not be able to keep up. Making it too large just costs memory resources.</p> </section> <section id="vsm-free-cooldown"> <h5 id="ref-param-vsm-free-cooldown">vsm_free_cooldown</h5>  <ul class="simple"> <li>Units: seconds</li> <li>Default: 60.000</li> <li>Minimum: 10.000</li> <li>Maximum: 600.000</li> </ul>  <p>How long VSM memory is kept warm after a deallocation (granularity approximately 2 seconds).</p> </section> <section id="workspace-backend"> <h5 id="ref-param-workspace-backend">workspace_backend</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 96k</li> <li>Minimum: 1k</li> <li>Flags: delayed</li> </ul>  <p>Bytes of HTTP protocol workspace for backend HTTP req/resp. If larger than 4k, use a multiple of 4k for VM efficiency.</p> </section> <section id="workspace-client"> <h5 id="ref-param-workspace-client">workspace_client</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 96k</li> <li>Minimum: 9k</li> <li>Flags: delayed</li> </ul>  <p>Bytes of HTTP protocol workspace for clients HTTP req/resp. Use a multiple of 4k for VM efficiency. For HTTP/2 compliance this must be at least 20k, in order to receive fullsize (=16k) frames from the client. That usually happens only in POST/PUT bodies. For other traffic-patterns smaller values work just fine.</p> </section> <section id="workspace-session"> <h5 id="ref-param-workspace-session">workspace_session</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 0.75k</li> <li>Minimum: 384b</li> <li>Flags: delayed</li> </ul>  <p>Allocation size for session structure and workspace. The workspace is primarily used for TCP connection addresses. If larger than 4k, use a multiple of 4k for VM efficiency.</p> </section> <section id="workspace-thread"> <h5 id="ref-param-workspace-thread">workspace_thread</h5>  <ul class="simple"> <li>Units: bytes</li> <li>Default: 2k</li> <li>Minimum: 0.25k</li> <li>Maximum: 8k</li> <li>Flags: delayed</li> </ul>  <p>Bytes of auxiliary workspace per thread. This workspace is used for certain temporary data structures during the operation of a worker thread. One use is for the IO-vectors used during delivery. Setting this parameter too low may increase the number of writev() syscalls, setting it too high just wastes space. ~0.1k + UIO_MAXIOV * sizeof(struct iovec) (typically = ~16k for 64bit) is considered the maximum sensible value under any known circumstances (excluding exotic vmod use).</p> </section> </section> </section> <section id="exit-codes"> <h3>EXIT CODES</h3> <p>Varnish and bundled tools will, in most cases, exit with one of the following codes</p> <ul class="simple"> <li>
<code>0</code> OK</li> <li>
<code>1</code> Some error which could be system-dependent and/or transient</li> <li>
<code>2</code> Serious configuration / parameter error - retrying with the same configuration / parameters is most likely useless</li> </ul> <p>The <code>varnishd</code> master process may also OR its exit code</p> <ul class="simple"> <li>with <code>0x20</code> when the <code>varnishd</code> child process died,</li> <li>with <code>0x40</code> when the <code>varnishd</code> child process was terminated by a signal and</li> <li>with <code>0x80</code> when a core was dumped.</li> </ul> </section> <section id="see-also"> <h3>SEE ALSO</h3> <ul class="simple"> <li><a class="reference internal" href="varnishlog.html#varnishlog-1"><span class="std std-ref">varnishlog</span></a></li> <li><a class="reference internal" href="varnishhist.html#varnishhist-1"><span class="std std-ref">varnishhist</span></a></li> <li><a class="reference internal" href="varnishncsa.html#varnishncsa-1"><span class="std std-ref">varnishncsa</span></a></li> <li><a class="reference internal" href="varnishstat.html#varnishstat-1"><span class="std std-ref">varnishstat</span></a></li> <li><a class="reference internal" href="varnishtop.html#varnishtop-1"><span class="std std-ref">varnishtop</span></a></li> <li><a class="reference internal" href="varnish-cli.html#varnish-cli-7"><span class="std std-ref">varnish-cli</span></a></li> <li><a class="reference internal" href="vcl.html#vcl-7"><span class="std std-ref">VCL</span></a></li> </ul> </section> <section id="history"> <h3>HISTORY</h3> <p>The <code>varnishd</code> daemon was developed by Poul-Henning Kamp in cooperation with Verdens Gang AS and Varnish Software.</p> <p>This manual page was written by Dag-Erling Smørgrav with updates by Stig Sandbeck Mathisen &lt;<a class="reference external" href="mailto:ssm%40debian.org.html">ssm<span>@</span>debian<span>.</span>org</a>&gt;, Nils Goroll and others.</p> </section> <section id="copyright"> <h3>COPYRIGHT</h3> <p>This document is licensed under the same licence as Varnish itself. See LICENCE for details.</p> <ul class="simple"> <li>Copyright (c) 2007-2015 Varnish Software AS</li> </ul> </section> </section><div class="_attribution">
  <p class="_attribution-p">
    Copyright &copy; 2006 Verdens Gang AS<br>Copyright &copy; 2006&ndash;2020 Varnish Software AS<br>Licensed under the BSD-2-Clause License.<br>
    <a href="https://varnish-cache.org/docs/7.4/reference/varnishd.html" class="_attribution-link">https://varnish-cache.org/docs/7.4/reference/varnishd.html</a>
  </p>
</div>
