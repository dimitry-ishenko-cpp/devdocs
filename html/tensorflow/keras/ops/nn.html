<h1 class="devsite-page-title" tabindex="-1"> Module: tf.keras.ops.nn </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.ops.nn"> <meta itemprop="path" content="Stable"> </div>   <p>DO NOT EDIT.</p> <p>This file was autogenerated. Do not edit it by hand, since your modifications would be overwritten.</p> <h2 id="functions" data-text="Functions" tabindex="-1">Functions</h2> <p><a href="average_pool.html"><code translate="no" dir="ltr">average_pool(...)</code></a>: Average pooling operation.</p> <p><a href="batch_normalization.html"><code translate="no" dir="ltr">batch_normalization(...)</code></a>: Normalizes <code translate="no" dir="ltr">x</code> by <code translate="no" dir="ltr">mean</code> and <code translate="no" dir="ltr">variance</code>.</p> <p><a href="binary_crossentropy.html"><code translate="no" dir="ltr">binary_crossentropy(...)</code></a>: Computes binary cross-entropy loss between target and output tensor.</p> <p><a href="categorical_crossentropy.html"><code translate="no" dir="ltr">categorical_crossentropy(...)</code></a>: Computes categorical cross-entropy loss between target and output tensor.</p> <p><a href="conv.html"><code translate="no" dir="ltr">conv(...)</code></a>: General N-D convolution.</p> <p><a href="conv_transpose.html"><code translate="no" dir="ltr">conv_transpose(...)</code></a>: General N-D convolution transpose.</p> <p><a href="ctc_decode.html"><code translate="no" dir="ltr">ctc_decode(...)</code></a>: Decodes the output of a CTC model.</p> <p><a href="ctc_loss.html"><code translate="no" dir="ltr">ctc_loss(...)</code></a>: CTC (Connectionist Temporal Classification) loss.</p> <p><a href="depthwise_conv.html"><code translate="no" dir="ltr">depthwise_conv(...)</code></a>: General N-D depthwise convolution.</p> <p><a href="elu.html"><code translate="no" dir="ltr">elu(...)</code></a>: Exponential Linear Unit activation function.</p> <p><a href="gelu.html"><code translate="no" dir="ltr">gelu(...)</code></a>: Gaussian Error Linear Unit (GELU) activation function.</p> <p><a href="hard_sigmoid.html"><code translate="no" dir="ltr">hard_sigmoid(...)</code></a>: Hard sigmoid activation function.</p> <p><a href="hard_silu.html"><code translate="no" dir="ltr">hard_silu(...)</code></a>: Hard SiLU activation function, also known as Hard Swish.</p> <p><a href="hard_silu.html"><code translate="no" dir="ltr">hard_swish(...)</code></a>: Hard SiLU activation function, also known as Hard Swish.</p> <p><a href="leaky_relu.html"><code translate="no" dir="ltr">leaky_relu(...)</code></a>: Leaky version of a Rectified Linear Unit activation function.</p> <p><a href="log_sigmoid.html"><code translate="no" dir="ltr">log_sigmoid(...)</code></a>: Logarithm of the sigmoid activation function.</p> <p><a href="log_softmax.html"><code translate="no" dir="ltr">log_softmax(...)</code></a>: Log-softmax activation function.</p> <p><a href="max_pool.html"><code translate="no" dir="ltr">max_pool(...)</code></a>: Max pooling operation.</p> <p><a href="moments.html"><code translate="no" dir="ltr">moments(...)</code></a>: Calculates the mean and variance of <code translate="no" dir="ltr">x</code>.</p> <p><a href="multi_hot.html"><code translate="no" dir="ltr">multi_hot(...)</code></a>: Encodes integer labels as multi-hot vectors.</p> <p><a href="normalize.html"><code translate="no" dir="ltr">normalize(...)</code></a>: Normalizes <code translate="no" dir="ltr">x</code> over the specified axis.</p> <p><a href="one_hot.html"><code translate="no" dir="ltr">one_hot(...)</code></a>: Converts integer tensor <code translate="no" dir="ltr">x</code> into a one-hot tensor.</p> <p><a href="psnr.html"><code translate="no" dir="ltr">psnr(...)</code></a>: Peak Signal-to-Noise Ratio (PSNR) function.</p> <p><a href="relu.html"><code translate="no" dir="ltr">relu(...)</code></a>: Rectified linear unit activation function.</p> <p><a href="relu6.html"><code translate="no" dir="ltr">relu6(...)</code></a>: Rectified linear unit activation function with upper bound of 6.</p> <p><a href="selu.html"><code translate="no" dir="ltr">selu(...)</code></a>: Scaled Exponential Linear Unit (SELU) activation function.</p> <p><a href="separable_conv.html"><code translate="no" dir="ltr">separable_conv(...)</code></a>: General N-D separable convolution.</p> <p><a href="sigmoid.html"><code translate="no" dir="ltr">sigmoid(...)</code></a>: Sigmoid activation function.</p> <p><a href="silu.html"><code translate="no" dir="ltr">silu(...)</code></a>: Sigmoid Linear Unit (SiLU) activation function, also known as Swish.</p> <p><a href="softmax.html"><code translate="no" dir="ltr">softmax(...)</code></a>: Softmax activation function.</p> <p><a href="softplus.html"><code translate="no" dir="ltr">softplus(...)</code></a>: Softplus activation function.</p> <p><a href="softsign.html"><code translate="no" dir="ltr">softsign(...)</code></a>: Softsign activation function.</p> <p><a href="sparse_categorical_crossentropy.html"><code translate="no" dir="ltr">sparse_categorical_crossentropy(...)</code></a>: Computes sparse categorical cross-entropy loss.</p> <p><a href="silu.html"><code translate="no" dir="ltr">swish(...)</code></a>: Sigmoid Linear Unit (SiLU) activation function, also known as Swish.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/ops/nn" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/ops/nn</a>
  </p>
</div>
