<h1 class="devsite-page-title" tabindex="-1"> tf.keras.ops.image.affine_transform </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.ops.image.affine_transform"> <meta itemprop="path" content="Stable"> </div>   <p>Applies the given transform(s) to the image(s).</p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.keras.ops.image.affine_transform(
    image,
    transform,
    interpolation='bilinear',
    fill_mode='constant',
    fill_value=0,
    data_format='channels_last'
)
</pre></devsite-code>   
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">image</code> </td> <td> Input image or batch of images. Must be 3D or 4D. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">transform</code> </td> <td> Projective transform matrix/matrices. A vector of length 8 or tensor of size N x 8. If one row of transform is <code translate="no" dir="ltr">[a0, a1, a2, b0, b1, b2, c0, c1]</code>, then it maps the output point <code translate="no" dir="ltr">(x, y)</code> to a transformed input point <code translate="no" dir="ltr">(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)</code>, where <code translate="no" dir="ltr">k = c0 x + c1 y + 1</code>. The transform is inverted compared to the transform mapping input points to output points. Note that gradients are not backpropagated into transformation parameters. Note that <code translate="no" dir="ltr">c0</code> and <code translate="no" dir="ltr">c1</code> are only effective when using TensorFlow backend and will be considered as <code translate="no" dir="ltr">0</code> when using other backends. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">interpolation</code> </td> <td> Interpolation method. Available methods are <code translate="no" dir="ltr">"nearest"</code>, and <code translate="no" dir="ltr">"bilinear"</code>. Defaults to <code translate="no" dir="ltr">"bilinear"</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">fill_mode</code> </td> <td> Points outside the boundaries of the input are filled according to the given mode. Available methods are <code translate="no" dir="ltr">"constant"</code>, <code translate="no" dir="ltr">"nearest"</code>, <code translate="no" dir="ltr">"wrap"</code> and <code translate="no" dir="ltr">"reflect"</code>. Defaults to <code translate="no" dir="ltr">"constant"</code>. <ul> <li>
<code translate="no" dir="ltr">"reflect"</code>: <code translate="no" dir="ltr">(d c b a | a b c d | d c b a)</code> The input is extended by reflecting about the edge of the last pixel.</li> <li>
<code translate="no" dir="ltr">"constant"</code>: <code translate="no" dir="ltr">(k k k k | a b c d | k k k k)</code> The input is extended by filling all values beyond the edge with the same constant value k specified by <code translate="no" dir="ltr">fill_value</code>.</li> <li>
<code translate="no" dir="ltr">"wrap"</code>: <code translate="no" dir="ltr">(a b c d | a b c d | a b c d)</code> The input is extended by wrapping around to the opposite edge.</li> <li>
<code translate="no" dir="ltr">"nearest"</code>: <code translate="no" dir="ltr">(a a a a | a b c d | d d d d)</code> The input is extended by the nearest pixel. </li>
</ul>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">fill_value</code> </td> <td> Value used for points outside the boundaries of the input if <code translate="no" dir="ltr">fill_mode="constant"</code>. Defaults to <code translate="no" dir="ltr">0</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">data_format</code> </td> <td> string, either <code translate="no" dir="ltr">"channels_last"</code> or <code translate="no" dir="ltr">"channels_first"</code>. The ordering of the dimensions in the inputs. <code translate="no" dir="ltr">"channels_last"</code> corresponds to inputs with shape <code translate="no" dir="ltr">(batch, height, width, channels)</code> while <code translate="no" dir="ltr">"channels_first"</code> corresponds to inputs with shape <code translate="no" dir="ltr">(batch, channels, height, weight)</code>. It defaults to the <code translate="no" dir="ltr">image_data_format</code> value found in your Keras config file at <code translate="no" dir="ltr">~/.keras/keras.json</code>. If you never set it, then it will be <code translate="no" dir="ltr">"channels_last"</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Applied affine transform image or batch of images. </td> </tr> 
</table> <h4 id="examples" data-text="Examples:" tabindex="-1">Examples:</h4> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">x = np.random.random((2, 64, 80, 3)) # batch of 2 RGB images
transform = np.array(
    [
        [1.5, 0, -20, 0, 1.5, -16, 0, 0],  # zoom
        [1, 0, -20, 0, 1, -16, 0, 0],  # translation
    ]
)
y = keras.ops.image.affine_transform(x, transform)
y.shape
(2, 64, 80, 3)</pre></devsite-code> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">x = np.random.random((64, 80, 3)) # single RGB image
transform = np.array([1.0, 0.5, -20, 0.5, 1.0, -16, 0, 0])  # shear
y = keras.ops.image.affine_transform(x, transform)
y.shape
(64, 80, 3)</pre></devsite-code> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">x = np.random.random((2, 3, 64, 80)) # batch of 2 RGB images
transform = np.array(
    [
        [1.5, 0, -20, 0, 1.5, -16, 0, 0],  # zoom
        [1, 0, -20, 0, 1, -16, 0, 0],  # translation
    ]
)
y = keras.ops.image.affine_transform(x, transform,
    data_format="channels_first")
y.shape
(2, 3, 64, 80)</pre></devsite-code>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/ops/image/affine_transform" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/ops/image/affine_transform</a>
  </p>
</div>
