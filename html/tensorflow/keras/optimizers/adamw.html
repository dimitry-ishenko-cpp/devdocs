<h1 class="devsite-page-title" tabindex="-1"> tf.keras.optimizers.AdamW </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.optimizers.AdamW"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="add_variable"> <meta itemprop="property" content="add_variable_from_reference"> <meta itemprop="property" content="apply"> <meta itemprop="property" content="apply_gradients"> <meta itemprop="property" content="assign"> <meta itemprop="property" content="assign_add"> <meta itemprop="property" content="assign_sub"> <meta itemprop="property" content="build"> <meta itemprop="property" content="exclude_from_weight_decay"> <meta itemprop="property" content="finalize_variable_values"> <meta itemprop="property" content="from_config"> <meta itemprop="property" content="get_config"> <meta itemprop="property" content="load_own_variables"> <meta itemprop="property" content="save_own_variables"> <meta itemprop="property" content="scale_loss"> <meta itemprop="property" content="set_weights"> <meta itemprop="property" content="stateless_apply"> <meta itemprop="property" content="update_step"> </div>   <p>Optimizer that implements the AdamW algorithm.</p> <p>Inherits From: <a href="adam.html"><code translate="no" dir="ltr">Adam</code></a>, <a href="../optimizer.html"><code translate="no" dir="ltr">Optimizer</code></a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.keras.optimizers.AdamW(
    learning_rate=0.001,
    weight_decay=0.004,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    loss_scale_factor=None,
    gradient_accumulation_steps=None,
    name='adamw',
    **kwargs
)
</pre></devsite-code>  <p>AdamW optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments with an added method to decay weights per the techniques discussed in the paper, 'Decoupled Weight Decay Regularization' by <a href="https://arxiv.org/abs/1711.05101">Loshchilov, Hutter et al., 2019</a>.</p> <p>According to <a href="http://arxiv.org/abs/1412.6980">Kingma et al., 2014</a>, the underying Adam method is "<em>computationally efficient, has little memory requirement, invariant to diagonal rescaling of gradients, and is well suited for problems that are large in terms of data/parameters</em>".</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">learning_rate</code> </td> <td> A float, a <a href="schedules/learningrateschedule.html"><code translate="no" dir="ltr">keras.optimizers.schedules.LearningRateSchedule</code></a> instance, or a callable that takes no arguments and returns the actual value to use. The learning rate. Defaults to <code translate="no" dir="ltr">0.001</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">beta_1</code> </td> <td> A float value or a constant float tensor, or a callable that takes no arguments and returns the actual value to use. The exponential decay rate for the 1st moment estimates. Defaults to <code translate="no" dir="ltr">0.9</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">beta_2</code> </td> <td> A float value or a constant float tensor, or a callable that takes no arguments and returns the actual value to use. The exponential decay rate for the 2nd moment estimates. Defaults to <code translate="no" dir="ltr">0.999</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">epsilon</code> </td> <td> A small constant for numerical stability. This epsilon is "epsilon hat" in the Kingma and Ba paper (in the formula just before Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to 1e-7. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">amsgrad</code> </td> <td> Boolean. Whether to apply AMSGrad variant of this algorithm from the paper "On the Convergence of Adam and beyond". Defaults to <code translate="no" dir="ltr">False</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> String. The name to use for momentum accumulator weights created by the optimizer. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">weight_decay</code> </td> <td> Float. If set, weight decay is applied. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">clipnorm</code> </td> <td> Float. If set, the gradient of each weight is individually clipped so that its norm is no higher than this value. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">clipvalue</code> </td> <td> Float. If set, the gradient of each weight is clipped to be no higher than this value. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">global_clipnorm</code> </td> <td> Float. If set, the gradient of all weights is clipped so that their global norm is no higher than this value. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">use_ema</code> </td> <td> Boolean, defaults to <code translate="no" dir="ltr">False</code>. If <code translate="no" dir="ltr">True</code>, exponential moving average (EMA) is applied. EMA consists of computing an exponential moving average of the weights of the model (as the weight values change after each training batch), and periodically overwriting the weights with their moving average. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ema_momentum</code> </td> <td> Float, defaults to 0.99. Only used if <code translate="no" dir="ltr">use_ema=True</code>. This is the momentum to use when computing the EMA of the model's weights: <code translate="no" dir="ltr">new_average = ema_momentum * old_average + (1 - ema_momentum) * current_variable_value</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ema_overwrite_frequency</code> </td> <td> Int or None, defaults to None. Only used if <code translate="no" dir="ltr">use_ema=True</code>. Every <code translate="no" dir="ltr">ema_overwrite_frequency</code> steps of iterations, we overwrite the model variable by its moving average. If None, the optimizer does not overwrite model variables in the middle of training, and you need to explicitly overwrite the variables at the end of training by calling <code translate="no" dir="ltr">optimizer.finalize_variable_values()</code> (which updates the model variables in-place). When using the built-in <code translate="no" dir="ltr">fit()</code> training loop, this happens automatically after the last epoch, and you don't need to do anything. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">loss_scale_factor</code> </td> <td> Float or <code translate="no" dir="ltr">None</code>. If a float, the scale factor will be multiplied the loss before computing gradients, and the inverse of the scale factor will be multiplied by the gradients before updating variables. Useful for preventing underflow during mixed precision training. Alternately, <a href="../mixed_precision/lossscaleoptimizer.html"><code translate="no" dir="ltr">keras.optimizers.LossScaleOptimizer</code></a> will automatically set a loss scale factor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">gradient_accumulation_steps</code> </td> <td> Int or <code translate="no" dir="ltr">None</code>. If an int, model &amp; optimizer variables will not be updated at every step; instead they will be updated every <code translate="no" dir="ltr">gradient_accumulation_steps</code> steps, using the average value of the gradients since the last update. This is known as "gradient accumulation". This can be useful when your batch size is very small, in order to reduce gradient noise at each update step. </td> </tr> </table> <h4 id="references" data-text="References:" tabindex="-1">References:</h4> <ul> <li><a href="https://arxiv.org/abs/1711.05101">Loshchilov et al., 2019</a></li> <li>
<a href="http://arxiv.org/abs/1412.6980">Kingma et al., 2014</a> for <code translate="no" dir="ltr">adam</code>
</li> <li>
<a href="https://openreview.net/pdf?id=ryQu7f-RZ">Reddi et al., 2018</a> for <code translate="no" dir="ltr">amsgrad</code>.</li> </ul>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">learning_rate</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">variables</code> </td> <td> 
</td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="-1">Methods</h2> <h3 id="add_variable" data-text="add_variable" tabindex="-1"><code translate="no" dir="ltr">add_variable</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L181-L201">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">add_variable(
    shape,
    initializer='zeros',
    dtype=None,
    aggregation='mean',
    name=None
)
</pre></devsite-code> <h3 id="add_variable_from_reference" data-text="add_variable_from_reference" tabindex="-1"><code translate="no" dir="ltr">add_variable_from_reference</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/backend/tensorflow/optimizer.py#L25-L38">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">add_variable_from_reference(
    reference_variable, name=None, initializer='zeros'
)
</pre></devsite-code> <p>Add an all-zeros variable with the shape and dtype of a reference variable.</p> <h3 id="apply" data-text="apply" tabindex="-1"><code translate="no" dir="ltr">apply</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L286-L355">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">apply(
    grads, trainable_variables=None
)
</pre></devsite-code> <p>Update traininable variables according to provided gradient values.</p> <p><code translate="no" dir="ltr">grads</code> should be a list of gradient tensors with 1:1 mapping to the list of variables the optimizer was built with.</p> <p><code translate="no" dir="ltr">trainable_variables</code> can be provided on the first call to build the optimizer.</p> <h3 id="apply_gradients" data-text="apply_gradients" tabindex="-1"><code translate="no" dir="ltr">apply_gradients</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L280-L284">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">apply_gradients(
    grads_and_vars
)
</pre></devsite-code> <h3 id="assign" data-text="assign" tabindex="-1"><code translate="no" dir="ltr">assign</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/backend/tensorflow/optimizer.py#L48-L55">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">assign(
    variable, value
)
</pre></devsite-code> <p>Assign a value to a variable.</p> <p>This should be used in optimizers instead of <code translate="no" dir="ltr">variable.assign(value)</code> to support backend specific optimizations. Note that the variable can be a model variable or an optimizer variable; it can be a backend native variable or a Keras variable.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">variable</code> </td> <td> The variable to update. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> The value to add to the variable. </td> </tr> </table> <h3 id="assign_add" data-text="assign_add" tabindex="-1"><code translate="no" dir="ltr">assign_add</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/backend/tensorflow/optimizer.py#L57-L64">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">assign_add(
    variable, value
)
</pre></devsite-code> <p>Add a value to a variable.</p> <p>This should be used in optimizers instead of <code translate="no" dir="ltr">variable.assign_add(value)</code> to support backend specific optimizations. Note that the variable can be a model variable or an optimizer variable; it can be a backend native variable or a Keras variable.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">variable</code> </td> <td> The variable to update. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> The value to add to the variable. </td> </tr> </table> <h3 id="assign_sub" data-text="assign_sub" tabindex="-1"><code translate="no" dir="ltr">assign_sub</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/backend/tensorflow/optimizer.py#L66-L73">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">assign_sub(
    variable, value
)
</pre></devsite-code> <p>Subtract a value from a variable.</p> <p>This should be used in optimizers instead of <code translate="no" dir="ltr">variable.assign_sub(value)</code> to support backend specific optimizations. Note that the variable can be a model variable or an optimizer variable; it can be a backend native variable or a Keras variable.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">variable</code> </td> <td> The variable to update. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> The value to add to the variable. </td> </tr> </table> <h3 id="build" data-text="build" tabindex="-1"><code translate="no" dir="ltr">build</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/adam.py#L81-L113">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">build(
    var_list
)
</pre></devsite-code> <p>Initialize optimizer variables.</p> <p>Adam optimizer has 3 types of variables: momentums, velocities and velocity_hat (only set when amsgrad is applied),</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">var_list</code> </td> <td> list of model variables to build Adam variables on. </td> </tr> </table> <h3 id="exclude_from_weight_decay" data-text="exclude_from_weight_decay" tabindex="-1"><code translate="no" dir="ltr">exclude_from_weight_decay</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L685-L723">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">exclude_from_weight_decay(
    var_list=None, var_names=None
)
</pre></devsite-code> <p>Exclude variables from weight decay.</p> <p>This method must be called before the optimizer's <code translate="no" dir="ltr">build</code> method is called. You can set specific variables to exclude out, or set a list of strings as the anchor words, if any of which appear in a variable's name, then the variable is excluded.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">var_list</code> </td> <td> A list of <code translate="no" dir="ltr">Variable</code>s to exclude from weight decay. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">var_names</code> </td> <td> A list of strings. If any string in <code translate="no" dir="ltr">var_names</code> appear in the model variable's name, then this model variable is excluded from weight decay. For example, <code translate="no" dir="ltr">var_names=['bias']</code> excludes all bias variables from weight decay. </td> </tr> </table> <h3 id="finalize_variable_values" data-text="finalize_variable_values" tabindex="-1"><code translate="no" dir="ltr">finalize_variable_values</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L803-L816">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">finalize_variable_values(
    var_list
)
</pre></devsite-code> <p>Set the final value of model's trainable variables.</p> <p>Sometimes there are some extra steps before ending the variable updates, such as overriding the model variables with its average value.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">var_list</code> </td> <td> list of model variables. </td> </tr> </table> <h3 id="from_config" data-text="from_config" tabindex="-1"><code translate="no" dir="ltr">from_config</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L866-L888">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@classmethod
from_config(
    config, custom_objects=None
)
</pre></devsite-code> <p>Creates an optimizer from its config.</p> <p>This method is the reverse of <code translate="no" dir="ltr">get_config</code>, capable of instantiating the same optimizer from the config dictionary.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">config</code> </td> <td> A Python dictionary, typically the output of get_config. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">custom_objects</code> </td> <td> A Python dictionary mapping names to additional user-defined Python objects needed to recreate this optimizer. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An optimizer instance. </td> </tr> 
</table> <h3 id="get_config" data-text="get_config" tabindex="-1"><code translate="no" dir="ltr">get_config</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/adam.py#L152-L162">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">get_config()
</pre></devsite-code> <p>Returns the config of the optimizer.</p> <p>An optimizer config is a Python dictionary (serializable) containing the configuration of an optimizer. The same optimizer can be reinstantiated later (without any saved state) from this configuration.</p> <p>Subclass optimizer should override this method to include other hyperparameters.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Python dictionary. </td> </tr> 
</table> <h3 id="load_own_variables" data-text="load_own_variables" tabindex="-1"><code translate="no" dir="ltr">load_own_variables</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L567-L583">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">load_own_variables(
    store
)
</pre></devsite-code> <p>Set the state of this optimizer object.</p> <h3 id="save_own_variables" data-text="save_own_variables" tabindex="-1"><code translate="no" dir="ltr">save_own_variables</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L562-L565">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">save_own_variables(
    store
)
</pre></devsite-code> <p>Get the state of this optimizer object.</p> <h3 id="scale_loss" data-text="scale_loss" tabindex="-1"><code translate="no" dir="ltr">scale_loss</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L499-L508">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">scale_loss(
    loss
)
</pre></devsite-code> <p>Scale the loss before computing gradients.</p> <p>Scales the loss before gradients are computed in a <code translate="no" dir="ltr">train_step</code>. This is primarily useful during mixed precision training to prevent numeric underflow.</p> <h3 id="set_weights" data-text="set_weights" tabindex="-1"><code translate="no" dir="ltr">set_weights</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L544-L560">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">set_weights(
    weights
)
</pre></devsite-code> <p>Set the weights of the optimizer.</p> <h3 id="stateless_apply" data-text="stateless_apply" tabindex="-1"><code translate="no" dir="ltr">stateless_apply</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/backend/tensorflow/optimizer.py#L40-L46">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">stateless_apply(
    optimizer_variables, grads, trainable_variables
)
</pre></devsite-code> <h3 id="update_step" data-text="update_step" tabindex="-1"><code translate="no" dir="ltr">update_step</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/adam.py#L115-L150">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">update_step(
    gradient, variable, learning_rate
)
</pre></devsite-code> <p>Update step given gradient and the associated model variable.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/AdamW" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/AdamW</a>
  </p>
</div>
