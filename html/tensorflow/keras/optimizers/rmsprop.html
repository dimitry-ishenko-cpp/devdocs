<h1 class="devsite-page-title" tabindex="-1"> tf.keras.optimizers.RMSprop </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.optimizers.RMSprop"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="add_variable"> <meta itemprop="property" content="add_variable_from_reference"> <meta itemprop="property" content="apply"> <meta itemprop="property" content="apply_gradients"> <meta itemprop="property" content="assign"> <meta itemprop="property" content="assign_add"> <meta itemprop="property" content="assign_sub"> <meta itemprop="property" content="build"> <meta itemprop="property" content="exclude_from_weight_decay"> <meta itemprop="property" content="finalize_variable_values"> <meta itemprop="property" content="from_config"> <meta itemprop="property" content="get_config"> <meta itemprop="property" content="load_own_variables"> <meta itemprop="property" content="save_own_variables"> <meta itemprop="property" content="scale_loss"> <meta itemprop="property" content="set_weights"> <meta itemprop="property" content="stateless_apply"> <meta itemprop="property" content="update_step"> </div>   <p>Optimizer that implements the RMSprop algorithm.</p> <p>Inherits From: <a href="../optimizer.html"><code translate="no" dir="ltr">Optimizer</code></a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.keras.optimizers.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    loss_scale_factor=None,
    gradient_accumulation_steps=None,
    name='rmsprop',
    **kwargs
)
</pre></devsite-code> <h3 id="used-in-the-notebooks" data-text="Used in the notebooks" tabindex="-1">Used in the notebooks</h3> <table class="vertical-rules"> <thead> <tr> <th>Used in the guide</th> <th>Used in the tutorials</th> </tr> </thead> <tbody> <tr> <td> <ul> <li><a href="https://www.tensorflow.org/guide/mixed_precision">Mixed precision</a></li> </ul> </td> <td> <ul> <li><a href="https://www.tensorflow.org/tutorials/distribute/multi_worker_with_ctl">Custom training loop with Keras and MultiWorkerMirroredStrategy</a></li> <li><a href="https://www.tensorflow.org/tutorials/images/transfer_learning">Transfer learning and fine-tuning</a></li> <li><a href="https://www.tensorflow.org/tutorials/load_data/numpy">Load NumPy data</a></li> <li><a href="https://www.tensorflow.org/xla/tf2xla/tutorials/autoclustering_xla">Classifying CIFAR-10 with XLA</a></li> </ul> </td> </tr> </tbody> </table> <p>The gist of RMSprop is to:</p> <ul> <li>Maintain a moving (discounted) average of the square of gradients</li> <li>Divide the gradient by the root of this average</li> </ul> <p>This implementation of RMSprop uses plain momentum, not Nesterov momentum.</p> <p>The centered version additionally maintains a moving average of the gradients, and uses that average to estimate the variance.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">learning_rate</code> </td> <td> A float, a <a href="schedules/learningrateschedule.html"><code translate="no" dir="ltr">keras.optimizers.schedules.LearningRateSchedule</code></a> instance, or a callable that takes no arguments and returns the actual value to use. The learning rate. Defaults to <code translate="no" dir="ltr">0.001</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">rho</code> </td> <td> float, defaults to 0.9. Discounting factor for the old gradients. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">momentum</code> </td> <td> float, defaults to 0.0. If not 0.0., the optimizer tracks the momentum value, with a decay rate equals to <code translate="no" dir="ltr">1 - momentum</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">epsilon</code> </td> <td> A small constant for numerical stability. This epsilon is "epsilon hat" in the Kingma and Ba paper (in the formula just before Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to 1e-7. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">centered</code> </td> <td> Boolean. If <code translate="no" dir="ltr">True</code>, gradients are normalized by the estimated variance of the gradient; if False, by the uncentered second moment. Setting this to <code translate="no" dir="ltr">True</code> may help with training, but is slightly more expensive in terms of computation and memory. Defaults to <code translate="no" dir="ltr">False</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> String. The name to use for momentum accumulator weights created by the optimizer. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">weight_decay</code> </td> <td> Float. If set, weight decay is applied. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">clipnorm</code> </td> <td> Float. If set, the gradient of each weight is individually clipped so that its norm is no higher than this value. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">clipvalue</code> </td> <td> Float. If set, the gradient of each weight is clipped to be no higher than this value. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">global_clipnorm</code> </td> <td> Float. If set, the gradient of all weights is clipped so that their global norm is no higher than this value. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">use_ema</code> </td> <td> Boolean, defaults to <code translate="no" dir="ltr">False</code>. If <code translate="no" dir="ltr">True</code>, exponential moving average (EMA) is applied. EMA consists of computing an exponential moving average of the weights of the model (as the weight values change after each training batch), and periodically overwriting the weights with their moving average. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ema_momentum</code> </td> <td> Float, defaults to 0.99. Only used if <code translate="no" dir="ltr">use_ema=True</code>. This is the momentum to use when computing the EMA of the model's weights: <code translate="no" dir="ltr">new_average = ema_momentum * old_average + (1 - ema_momentum) * current_variable_value</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ema_overwrite_frequency</code> </td> <td> Int or None, defaults to None. Only used if <code translate="no" dir="ltr">use_ema=True</code>. Every <code translate="no" dir="ltr">ema_overwrite_frequency</code> steps of iterations, we overwrite the model variable by its moving average. If None, the optimizer does not overwrite model variables in the middle of training, and you need to explicitly overwrite the variables at the end of training by calling <code translate="no" dir="ltr">optimizer.finalize_variable_values()</code> (which updates the model variables in-place). When using the built-in <code translate="no" dir="ltr">fit()</code> training loop, this happens automatically after the last epoch, and you don't need to do anything. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">loss_scale_factor</code> </td> <td> Float or <code translate="no" dir="ltr">None</code>. If a float, the scale factor will be multiplied the loss before computing gradients, and the inverse of the scale factor will be multiplied by the gradients before updating variables. Useful for preventing underflow during mixed precision training. Alternately, <a href="../mixed_precision/lossscaleoptimizer.html"><code translate="no" dir="ltr">keras.optimizers.LossScaleOptimizer</code></a> will automatically set a loss scale factor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">gradient_accumulation_steps</code> </td> <td> Int or <code translate="no" dir="ltr">None</code>. If an int, model &amp; optimizer variables will not be updated at every step; instead they will be updated every <code translate="no" dir="ltr">gradient_accumulation_steps</code> steps, using the average value of the gradients since the last update. This is known as "gradient accumulation". This can be useful when your batch size is very small, in order to reduce gradient noise at each update step. </td> </tr> </table> <h4 id="example" data-text="Example:" tabindex="-1">Example:</h4> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">opt = keras.optimizers.RMSprop(learning_rate=0.1)
var1 = keras.backend.Variable(10.0)
loss = lambda: (var1 ** 2) / 2.0  # d(loss) / d(var1) = var1
opt.minimize(loss, [var1])
var1
9.683772</pre></devsite-code> <h4 id="reference" data-text="Reference:" tabindex="-1">Reference:</h4> <ul> <li><a href="http://www.cs.toronto.edu/%7Etijmen/csc321/slides/lecture_slides_lec6.pdf">Hinton, 2012</a></li> </ul>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">learning_rate</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">variables</code> </td> <td> 
</td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="-1">Methods</h2> <h3 id="add_variable" data-text="add_variable" tabindex="-1"><code translate="no" dir="ltr">add_variable</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L181-L201">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">add_variable(
    shape,
    initializer='zeros',
    dtype=None,
    aggregation='mean',
    name=None
)
</pre></devsite-code> <h3 id="add_variable_from_reference" data-text="add_variable_from_reference" tabindex="-1"><code translate="no" dir="ltr">add_variable_from_reference</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/backend/tensorflow/optimizer.py#L25-L38">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">add_variable_from_reference(
    reference_variable, name=None, initializer='zeros'
)
</pre></devsite-code> <p>Add an all-zeros variable with the shape and dtype of a reference variable.</p> <h3 id="apply" data-text="apply" tabindex="-1"><code translate="no" dir="ltr">apply</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L286-L355">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">apply(
    grads, trainable_variables=None
)
</pre></devsite-code> <p>Update traininable variables according to provided gradient values.</p> <p><code translate="no" dir="ltr">grads</code> should be a list of gradient tensors with 1:1 mapping to the list of variables the optimizer was built with.</p> <p><code translate="no" dir="ltr">trainable_variables</code> can be provided on the first call to build the optimizer.</p> <h3 id="apply_gradients" data-text="apply_gradients" tabindex="-1"><code translate="no" dir="ltr">apply_gradients</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L280-L284">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">apply_gradients(
    grads_and_vars
)
</pre></devsite-code> <h3 id="assign" data-text="assign" tabindex="-1"><code translate="no" dir="ltr">assign</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/backend/tensorflow/optimizer.py#L48-L55">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">assign(
    variable, value
)
</pre></devsite-code> <p>Assign a value to a variable.</p> <p>This should be used in optimizers instead of <code translate="no" dir="ltr">variable.assign(value)</code> to support backend specific optimizations. Note that the variable can be a model variable or an optimizer variable; it can be a backend native variable or a Keras variable.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">variable</code> </td> <td> The variable to update. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> The value to add to the variable. </td> </tr> </table> <h3 id="assign_add" data-text="assign_add" tabindex="-1"><code translate="no" dir="ltr">assign_add</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/backend/tensorflow/optimizer.py#L57-L64">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">assign_add(
    variable, value
)
</pre></devsite-code> <p>Add a value to a variable.</p> <p>This should be used in optimizers instead of <code translate="no" dir="ltr">variable.assign_add(value)</code> to support backend specific optimizations. Note that the variable can be a model variable or an optimizer variable; it can be a backend native variable or a Keras variable.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">variable</code> </td> <td> The variable to update. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> The value to add to the variable. </td> </tr> </table> <h3 id="assign_sub" data-text="assign_sub" tabindex="-1"><code translate="no" dir="ltr">assign_sub</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/backend/tensorflow/optimizer.py#L66-L73">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">assign_sub(
    variable, value
)
</pre></devsite-code> <p>Subtract a value from a variable.</p> <p>This should be used in optimizers instead of <code translate="no" dir="ltr">variable.assign_sub(value)</code> to support backend specific optimizations. Note that the variable can be a model variable or an optimizer variable; it can be a backend native variable or a Keras variable.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">variable</code> </td> <td> The variable to update. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> The value to add to the variable. </td> </tr> </table> <h3 id="build" data-text="build" tabindex="-1"><code translate="no" dir="ltr">build</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/rmsprop.py#L91-L115">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">build(
    var_list
)
</pre></devsite-code> <h3 id="exclude_from_weight_decay" data-text="exclude_from_weight_decay" tabindex="-1"><code translate="no" dir="ltr">exclude_from_weight_decay</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L685-L723">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">exclude_from_weight_decay(
    var_list=None, var_names=None
)
</pre></devsite-code> <p>Exclude variables from weight decay.</p> <p>This method must be called before the optimizer's <code translate="no" dir="ltr">build</code> method is called. You can set specific variables to exclude out, or set a list of strings as the anchor words, if any of which appear in a variable's name, then the variable is excluded.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">var_list</code> </td> <td> A list of <code translate="no" dir="ltr">Variable</code>s to exclude from weight decay. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">var_names</code> </td> <td> A list of strings. If any string in <code translate="no" dir="ltr">var_names</code> appear in the model variable's name, then this model variable is excluded from weight decay. For example, <code translate="no" dir="ltr">var_names=['bias']</code> excludes all bias variables from weight decay. </td> </tr> </table> <h3 id="finalize_variable_values" data-text="finalize_variable_values" tabindex="-1"><code translate="no" dir="ltr">finalize_variable_values</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L803-L816">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">finalize_variable_values(
    var_list
)
</pre></devsite-code> <p>Set the final value of model's trainable variables.</p> <p>Sometimes there are some extra steps before ending the variable updates, such as overriding the model variables with its average value.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">var_list</code> </td> <td> list of model variables. </td> </tr> </table> <h3 id="from_config" data-text="from_config" tabindex="-1"><code translate="no" dir="ltr">from_config</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L866-L888">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@classmethod
from_config(
    config, custom_objects=None
)
</pre></devsite-code> <p>Creates an optimizer from its config.</p> <p>This method is the reverse of <code translate="no" dir="ltr">get_config</code>, capable of instantiating the same optimizer from the config dictionary.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">config</code> </td> <td> A Python dictionary, typically the output of get_config. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">custom_objects</code> </td> <td> A Python dictionary mapping names to additional user-defined Python objects needed to recreate this optimizer. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An optimizer instance. </td> </tr> 
</table> <h3 id="get_config" data-text="get_config" tabindex="-1"><code translate="no" dir="ltr">get_config</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/rmsprop.py#L164-L175">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">get_config()
</pre></devsite-code> <p>Returns the config of the optimizer.</p> <p>An optimizer config is a Python dictionary (serializable) containing the configuration of an optimizer. The same optimizer can be reinstantiated later (without any saved state) from this configuration.</p> <p>Subclass optimizer should override this method to include other hyperparameters.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Python dictionary. </td> </tr> 
</table> <h3 id="load_own_variables" data-text="load_own_variables" tabindex="-1"><code translate="no" dir="ltr">load_own_variables</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L567-L583">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">load_own_variables(
    store
)
</pre></devsite-code> <p>Set the state of this optimizer object.</p> <h3 id="save_own_variables" data-text="save_own_variables" tabindex="-1"><code translate="no" dir="ltr">save_own_variables</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L562-L565">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">save_own_variables(
    store
)
</pre></devsite-code> <p>Get the state of this optimizer object.</p> <h3 id="scale_loss" data-text="scale_loss" tabindex="-1"><code translate="no" dir="ltr">scale_loss</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L499-L508">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">scale_loss(
    loss
)
</pre></devsite-code> <p>Scale the loss before computing gradients.</p> <p>Scales the loss before gradients are computed in a <code translate="no" dir="ltr">train_step</code>. This is primarily useful during mixed precision training to prevent numeric underflow.</p> <h3 id="set_weights" data-text="set_weights" tabindex="-1"><code translate="no" dir="ltr">set_weights</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/base_optimizer.py#L544-L560">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">set_weights(
    weights
)
</pre></devsite-code> <p>Set the weights of the optimizer.</p> <h3 id="stateless_apply" data-text="stateless_apply" tabindex="-1"><code translate="no" dir="ltr">stateless_apply</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/backend/tensorflow/optimizer.py#L40-L46">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">stateless_apply(
    optimizer_variables, grads, trainable_variables
)
</pre></devsite-code> <h3 id="update_step" data-text="update_step" tabindex="-1"><code translate="no" dir="ltr">update_step</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/optimizers/rmsprop.py#L117-L162">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">update_step(
    gradient, variable, learning_rate
)
</pre></devsite-code> <p>Update step given gradient and the associated model variable.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop</a>
  </p>
</div>
