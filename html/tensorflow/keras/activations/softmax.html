<h1 class="devsite-page-title">tf.keras.activations.softmax</h1> <devsite-bookmark></devsite-bookmark>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/activations.py#L38-L92">  View source on GitHub </a> </td> </table> <p>Softmax converts a vector of values to a probability distribution.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax"><code translate="no" dir="ltr">tf.compat.v1.keras.activations.softmax</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.keras.activations.softmax(
    x, axis=-1
)
</pre>  <p>The elements of the output vector are in range (0, 1) and sum to 1.</p> <p>Each vector is handled independently. The <code translate="no" dir="ltr">axis</code> argument sets which axis of the input the function is applied along.</p> <p>Softmax is often used as the activation for the last layer of a classification network because the result could be interpreted as a probability distribution.</p> <p>The softmax of each vector x is computed as <code translate="no" dir="ltr">exp(x) / tf.reduce_sum(exp(x))</code>.</p> <p>The input values in are the log-odds of the resulting probability.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">x</code> </td> <td> Input tensor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">axis</code> </td> <td> Integer, axis along which the softmax normalization is applied. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Tensor, output of softmax transformation (all values are non-negative and sum to 1). </td> </tr> 
</table> <h4 id="examples" data-text="Examples:">Examples:</h4> <p><strong>Example 1: standalone usage</strong></p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
inputs = tf.random.normal(shape=(32, 10))
outputs = tf.keras.activations.softmax(inputs)
tf.reduce_sum(outputs[0, :])  # Each sample in the batch now sums to 1
&lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0000001&gt;
</pre> <p><strong>Example 2: usage in a <code translate="no" dir="ltr">Dense</code> layer</strong></p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
layer = tf.keras.layers.Dense(32, activation=tf.keras.activations.softmax)
</pre>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/activations/softmax" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/activations/softmax</a>
  </p>
</div>
