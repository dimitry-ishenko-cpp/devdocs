<h1 class="devsite-page-title" tabindex="-1"> tf.keras.activations.relu </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.activations.relu"> <meta itemprop="path" content="Stable"> </div>   <p>Applies the rectified linear unit activation function.</p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.keras.activations.relu(
    x, negative_slope=0.0, max_value=None, threshold=0.0
)
</pre></devsite-code>  <p>With default values, this returns the standard ReLU activation: <code translate="no" dir="ltr">max(x, 0)</code>, the element-wise maximum of 0 and the input tensor.</p> <p>Modifying default parameters allows you to use non-zero thresholds, change the max value of the activation, and to use a non-zero multiple of the input for values below the threshold.</p> <h4 id="examples" data-text="Examples:" tabindex="-1">Examples:</h4> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">x = [-10, -5, 0.0, 5, 10]
keras.activations.relu(x)
[ 0.,  0.,  0.,  5., 10.]
keras.activations.relu(x, negative_slope=0.5)
[-5. , -2.5,  0. ,  5. , 10. ]
keras.activations.relu(x, max_value=5.)
[0., 0., 0., 5., 5.]
keras.activations.relu(x, threshold=5.)
[-0., -0.,  0.,  0., 10.]</pre></devsite-code>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">x</code> </td> <td> Input tensor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">negative_slope</code> </td> <td> A <code translate="no" dir="ltr">float</code> that controls the slope for values lower than the threshold. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">max_value</code> </td> <td> A <code translate="no" dir="ltr">float</code> that sets the saturation threshold (the largest value the function will return). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">threshold</code> </td> <td> A <code translate="no" dir="ltr">float</code> giving the threshold value of the activation function below which values will be damped or set to zero. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A tensor with the same shape and dtype as input <code translate="no" dir="ltr">x</code>. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu</a>
  </p>
</div>
