<h1 class="devsite-page-title" tabindex="-1"> tf.keras.layers.GroupQueryAttention </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.layers.GroupQueryAttention"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="from_config"> <meta itemprop="property" content="symbolic_call"> </div>   <p>Grouped Query Attention layer.</p> <p>Inherits From: <a href="../layer.html"><code translate="no" dir="ltr">Layer</code></a>, <a href="../operation.html"><code translate="no" dir="ltr">Operation</code></a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.keras.layers.GroupQueryAttention(
    head_dim,
    num_query_heads,
    num_key_value_heads,
    dropout=0.0,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
</pre></devsite-code>  <p>This is an implementation of grouped-query attention introduced by <a href="https://arxiv.org/abs/2305.13245">Ainslie et al., 2023</a>. Here <code translate="no" dir="ltr">num_key_value_heads</code> denotes number of groups, setting <code translate="no" dir="ltr">num_key_value_heads</code> to 1 is equivalent to multi-query attention, and when <code translate="no" dir="ltr">num_key_value_heads</code> is equal to <code translate="no" dir="ltr">num_query_heads</code> it is equivalent to multi-head attention.</p> <p>This layer first projects <code translate="no" dir="ltr">query</code>, <code translate="no" dir="ltr">key</code>, and <code translate="no" dir="ltr">value</code> tensors. Then, <code translate="no" dir="ltr">key</code> and <code translate="no" dir="ltr">value</code> are repeated to match the number of heads of <code translate="no" dir="ltr">query</code>.</p> <p>Then, the <code translate="no" dir="ltr">query</code> is scaled and dot-producted with <code translate="no" dir="ltr">key</code> tensors. These are softmaxed to obtain attention probabilities. The value tensors are then interpolated by these probabilities and concatenated back to a single tensor.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">head_dim</code> </td> <td> Size of each attention head. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_query_heads</code> </td> <td> Number of query attention heads. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_key_value_heads</code> </td> <td> Number of key and value attention heads. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dropout</code> </td> <td> Dropout probability. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">use_bias</code> </td> <td> Boolean, whether the dense layers use bias vectors/matrices. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">kernel_initializer</code> </td> <td> Initializer for dense layer kernels. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">bias_initializer</code> </td> <td> Initializer for dense layer biases. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">kernel_regularizer</code> </td> <td> Regularizer for dense layer kernels. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">bias_regularizer</code> </td> <td> Regularizer for dense layer biases. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">activity_regularizer</code> </td> <td> Regularizer for dense layer activity. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">kernel_constraint</code> </td> <td> Constraint for dense layer kernels. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">bias_constraint</code> </td> <td> Constraint for dense layer kernels. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Call arguments</th></tr> 
<tr> <td> <code translate="no" dir="ltr">query</code> </td> <td> Query tensor of shape <code translate="no" dir="ltr">(batch_dim, target_seq_len, feature_dim)</code>, where <code translate="no" dir="ltr">batch_dim</code> is batch size, <code translate="no" dir="ltr">target_seq_len</code> is the length of target sequence, and <code translate="no" dir="ltr">feature_dim</code> is dimension of feature. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> Value tensor of shape <code translate="no" dir="ltr">(batch_dim, source_seq_len, feature_dim)</code>, where <code translate="no" dir="ltr">batch_dim</code> is batch size, <code translate="no" dir="ltr">source_seq_len</code> is the length of source sequence, and <code translate="no" dir="ltr">feature_dim</code> is dimension of feature. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">key</code> </td> <td> Optional key tensor of shape <code translate="no" dir="ltr">(batch_dim, source_seq_len, feature_dim)</code>. If not given, will use <code translate="no" dir="ltr">value</code> for both <code translate="no" dir="ltr">key</code> and <code translate="no" dir="ltr">value</code>, which is most common case. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">attention_mask</code> </td> <td> A boolean mask of shape <code translate="no" dir="ltr">(batch_dim, target_seq_len, source_seq_len)</code>, that prevents attention to certain positions. The boolean mask specifies which query elements can attend to which key elements, where 1 indicates attention and 0 indicates no attention. Broadcasting can happen for the missing batch dimensions and the head dimension. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">return_attention_scores</code> </td> <td> A boolean to indicate whether the output should be <code translate="no" dir="ltr">(attention_output, attention_scores)</code> if <code translate="no" dir="ltr">True</code>, or <code translate="no" dir="ltr">attention_output</code> if <code translate="no" dir="ltr">False</code>. Defaults to <code translate="no" dir="ltr">False</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">training</code> </td> <td> Python boolean indicating whether the layer should behave in training mode (adding dropout) or in inference mode (no dropout). Will go with either using the training mode of the parent layer/model or <code translate="no" dir="ltr">False</code> (inference) if there is no parent layer. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">use_causal_mask</code> </td> <td> A boolean to indicate whether to apply a causal mask to prevent tokens from attending to future tokens (e.g., used in a decoder Transformer). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> 
<tr> <td> <code translate="no" dir="ltr">attention_output</code> </td> <td> Result of the computation, of shape <code translate="no" dir="ltr">(batch_dim, target_seq_len, feature_dim)</code>, where <code translate="no" dir="ltr">target_seq_len</code> is for target sequence length and <code translate="no" dir="ltr">feature_dim</code> is the query input last dim. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">attention_scores</code> </td> <td> (Optional) attention coefficients of shape <code translate="no" dir="ltr">(batch_dim, num_query_heads, target_seq_len, source_seq_len)</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input</code> </td> <td> Retrieves the input tensor(s) of a symbolic operation. <p>Only returns the tensor(s) corresponding to the <em>first time</em> the operation was called. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">output</code> </td> <td> Retrieves the output tensor(s) of a layer. <p>Only returns the tensor(s) corresponding to the <em>first time</em> the operation was called. </p>
</td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="-1">Methods</h2> <h3 id="from_config" data-text="from_config" tabindex="-1"><code translate="no" dir="ltr">from_config</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/ops/operation.py#L191-L213">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@classmethod
from_config(
    config
)
</pre></devsite-code> <p>Creates a layer from its config.</p> <p>This method is the reverse of <code translate="no" dir="ltr">get_config</code>, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by <code translate="no" dir="ltr">set_weights</code>).</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">config</code> </td> <td> A Python dictionary, typically the output of get_config. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A layer instance. </td> </tr> 
</table> <h3 id="symbolic_call" data-text="symbolic_call" tabindex="-1"><code translate="no" dir="ltr">symbolic_call</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/ops/operation.py#L58-L70">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">symbolic_call(
    *args, **kwargs
)
</pre></devsite-code>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/GroupQueryAttention" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/layers/GroupQueryAttention</a>
  </p>
</div>
