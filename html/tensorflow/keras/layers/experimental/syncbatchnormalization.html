<h1 class="devsite-page-title">tf.keras.layers.experimental.SyncBatchNormalization</h1> <devsite-bookmark></devsite-bookmark>       <p>Normalize and scale inputs or activations synchronously across replicas.</p> <p>Inherits From: <a href="../layer.html"><code translate="no" dir="ltr">Layer</code></a>, <a href="../../../module.html"><code translate="no" dir="ltr">Module</code></a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.keras.layers.experimental.SyncBatchNormalization(
    axis=-1,
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer='zeros',
    gamma_initializer='ones',
    moving_mean_initializer='zeros',
    moving_variance_initializer='ones',
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    **kwargs
)
</pre>  <p>Applies batch normalization to activations of the previous layer at each batch by synchronizing the global batch statistics across all devices that are training the model. For specific details about batch normalization please refer to the <a href="../batchnormalization.html"><code translate="no" dir="ltr">tf.keras.layers.BatchNormalization</code></a> layer docs.</p> <p>If this layer is used when using tf.distribute strategy to train models across devices/workers, there will be an allreduce call to aggregate batch statistics across all replicas at every training step. Without tf.distribute strategy, this layer behaves as a regular <a href="../batchnormalization.html"><code translate="no" dir="ltr">tf.keras.layers.BatchNormalization</code></a> layer.</p> <h4 id="example_usage" data-text="Example usage:">Example usage:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.Dense(16))
  model.add(tf.keras.layers.experimental.SyncBatchNormalization())
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">axis</code> </td> <td> Integer, the axis that should be normalized (typically the features axis). For instance, after a <code translate="no" dir="ltr">Conv2D</code> layer with <code translate="no" dir="ltr">data_format="channels_first"</code>, set <code translate="no" dir="ltr">axis=1</code> in <code translate="no" dir="ltr">BatchNormalization</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">momentum</code> </td> <td> Momentum for the moving average. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">epsilon</code> </td> <td> Small float added to variance to avoid dividing by zero. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">center</code> </td> <td> If True, add offset of <code translate="no" dir="ltr">beta</code> to normalized tensor. If False, <code translate="no" dir="ltr">beta</code> is ignored. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">scale</code> </td> <td> If True, multiply by <code translate="no" dir="ltr">gamma</code>. If False, <code translate="no" dir="ltr">gamma</code> is not used. When the next layer is linear (also e.g. <a href="../../../nn/relu.html"><code translate="no" dir="ltr">nn.relu</code></a>), this can be disabled since the scaling will be done by the next layer. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">beta_initializer</code> </td> <td> Initializer for the beta weight. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">gamma_initializer</code> </td> <td> Initializer for the gamma weight. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">moving_mean_initializer</code> </td> <td> Initializer for the moving mean. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">moving_variance_initializer</code> </td> <td> Initializer for the moving variance. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">beta_regularizer</code> </td> <td> Optional regularizer for the beta weight. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">gamma_regularizer</code> </td> <td> Optional regularizer for the gamma weight. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">beta_constraint</code> </td> <td> Optional constraint for the beta weight. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">gamma_constraint</code> </td> <td> Optional constraint for the gamma weight. </td> </tr> </table> <h4 id="call_arguments" data-text="Call arguments:">Call arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">inputs</code></b>: Input tensor (of any rank).</li> <li>
<b><code translate="no" dir="ltr">training</code></b>: Python boolean indicating whether the layer should behave in training mode or in inference mode. <ul> <li>
<code translate="no" dir="ltr">training=True</code>: The layer will normalize its inputs using the mean and variance of the current batch of inputs.</li> <li>
<code translate="no" dir="ltr">training=False</code>: The layer will normalize its inputs using the mean and variance of its moving statistics, learned during training.</li> </ul>
</li> </ul> <h4 id="input_shape" data-text="Input shape:">Input shape:</h4> <p>Arbitrary. Use the keyword argument <code translate="no" dir="ltr">input_shape</code> (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.</p> <h4 id="output_shape" data-text="Output shape:">Output shape:</h4> <p>Same shape as input.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/experimental/SyncBatchNormalization" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/experimental/SyncBatchNormalization</a>
  </p>
</div>
