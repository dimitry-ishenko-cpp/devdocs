<h1 class="devsite-page-title">tf.keras.layers.experimental.preprocessing.PreprocessingLayer</h1> <devsite-bookmark></devsite-bookmark>       <p>Base class for Preprocessing Layers.</p> <p>Inherits From: <a href="../../layer.html"><code translate="no" dir="ltr">Layer</code></a>, <a href="../../../../module.html"><code translate="no" dir="ltr">Module</code></a></p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/PreprocessingLayer"><code translate="no" dir="ltr">tf.compat.v1.keras.layers.experimental.preprocessing.PreprocessingLayer</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.keras.layers.experimental.preprocessing.PreprocessingLayer(
    **kwargs
)
</pre>  <p><strong>Don't use this class directly: it's an abstract base class!</strong> You may be looking for one of the many built-in <a href="https://keras.io/guides/preprocessing_layers/">preprocessing layers</a> instead.</p> <p>Preprocessing layers are layers whose state gets computed before model training starts. They do not get updated during training. Most preprocessing layers implement an <code translate="no" dir="ltr">adapt()</code> method for state computation.</p> <p>The <code translate="no" dir="ltr">PreprocessingLayer</code> class is the base class you would subclass to implement your own preprocessing layers.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">is_adapted</code> </td> <td> Whether the layer has been fit to data already. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="adapt" data-text="adapt"><code translate="no" dir="ltr">adapt</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_preprocessing_layer.py#L156-L253">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
adapt(
    data, batch_size=None, steps=None
)
</pre> <p>Fits the state of the preprocessing layer to the data being passed.</p> <p>After calling <code translate="no" dir="ltr">adapt</code> on a layer, a preprocessing layer's state will not update during training. In order to make preprocessing layers efficient in any distribution context, they are kept constant with respect to any compiled <a href="../../../../graph.html"><code translate="no" dir="ltr">tf.Graph</code></a>s that call the layer. This does not affect the layer use when adapting each layer only once, but if you adapt a layer multiple times you will need to take care to re-compile any compiled functions as follows:</p> <ul> <li>If you are adding a preprocessing layer to a <a href="../../../model.html"><code translate="no" dir="ltr">keras.Model</code></a>, you need to call <code translate="no" dir="ltr">model.compile</code> after each subsequent call to <code translate="no" dir="ltr">adapt</code>.</li> <li>If you are calling a preprocessing layer inside <a href="../../../../data/dataset.html#map"><code translate="no" dir="ltr">tf.data.Dataset.map</code></a>, you should call <code translate="no" dir="ltr">map</code> again on the input <a href="../../../../data/dataset.html"><code translate="no" dir="ltr">tf.data.Dataset</code></a> after each <code translate="no" dir="ltr">adapt</code>.</li> <li>If you are using a <a href="../../../../function.html"><code translate="no" dir="ltr">tf.function</code></a> directly which calls a preprocessing layer, you need to call <a href="../../../../function.html"><code translate="no" dir="ltr">tf.function</code></a> again on your callable after each subsequent call to <code translate="no" dir="ltr">adapt</code>.</li> </ul> <p><a href="../../../model.html"><code translate="no" dir="ltr">tf.keras.Model</code></a> example with multiple adapts:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
layer = tf.keras.layers.Normalization(
    axis=None)
layer.adapt([0, 2])
model = tf.keras.Sequential(layer)
model.predict([0, 1, 2])
array([-1.,  0.,  1.], dtype=float32)
layer.adapt([-1, 1])
model.compile() # This is needed to re-compile model.predict!
model.predict([0, 1, 2])
array([0., 1., 2.], dtype=float32)
</pre> <p><a href="../../../../data/dataset.html"><code translate="no" dir="ltr">tf.data.Dataset</code></a> example with multiple adapts:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
layer = tf.keras.layers.Normalization(
    axis=None)
layer.adapt([0, 2])
input_ds = tf.data.Dataset.range(3)
normalized_ds = input_ds.map(layer)
list(normalized_ds.as_numpy_iterator())
[array([-1.], dtype=float32),
 array([0.], dtype=float32),
 array([1.], dtype=float32)]
layer.adapt([-1, 1])
normalized_ds = input_ds.map(layer) # Re-map over the input dataset.
list(normalized_ds.as_numpy_iterator())
[array([0.], dtype=float32),
 array([1.], dtype=float32),
 array([2.], dtype=float32)]
</pre> <p><code translate="no" dir="ltr">adapt()</code> is meant only as a single machine utility to compute layer state. To analyze a dataset that cannot fit on a single machine, see <a href="https://www.tensorflow.org/tfx/transform/get_started">Tensorflow Transform</a> for a multi-machine, map-reduce solution.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Arguments</th></tr> 
<tr> <td> <code translate="no" dir="ltr">data</code> </td> <td> The data to train on. It can be passed either as a tf.data Dataset, or as a numpy array. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">batch_size</code> </td> <td> Integer or <code translate="no" dir="ltr">None</code>. Number of samples per state update. If unspecified, <code translate="no" dir="ltr">batch_size</code> will default to 32. Do not specify the <code translate="no" dir="ltr">batch_size</code> if your data is in the form of datasets, generators, or <a href="../../../utils/sequence.html"><code translate="no" dir="ltr">keras.utils.Sequence</code></a> instances (since they generate batches). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">steps</code> </td> <td> Integer or <code translate="no" dir="ltr">None</code>. Total number of steps (batches of samples) When training with input tensors such as TensorFlow data tensors, the default <code translate="no" dir="ltr">None</code> is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a <a href="../../../../data.html"><code translate="no" dir="ltr">tf.data</code></a> dataset, and 'steps' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the <code translate="no" dir="ltr">steps</code> argument. This argument is not supported with array inputs. </td> </tr> </table> <h3 id="compile" data-text="compile"><code translate="no" dir="ltr">compile</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/engine/base_preprocessing_layer.py#L134-L154">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
compile(
    run_eagerly=None, steps_per_execution=None
)
</pre> <p>Configures the layer for <code translate="no" dir="ltr">adapt</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Arguments</th></tr> 
<tr> <td> <code translate="no" dir="ltr">run_eagerly</code> </td> <td> Bool. Defaults to <code translate="no" dir="ltr">False</code>. If <code translate="no" dir="ltr">True</code>, this <code translate="no" dir="ltr">Model</code>'s logic will not be wrapped in a <a href="../../../../function.html"><code translate="no" dir="ltr">tf.function</code></a>. Recommended to leave this as <code translate="no" dir="ltr">None</code> unless your <code translate="no" dir="ltr">Model</code> cannot be run inside a <a href="../../../../function.html"><code translate="no" dir="ltr">tf.function</code></a>. steps_per_execution: Int. Defaults to 1. The number of batches to run during each <a href="../../../../function.html"><code translate="no" dir="ltr">tf.function</code></a> call. Running multiple batches inside a single <a href="../../../../function.html"><code translate="no" dir="ltr">tf.function</code></a> call can greatly improve performance on TPUs or small models with a large Python overhead. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/experimental/preprocessing/PreprocessingLayer" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/experimental/preprocessing/PreprocessingLayer</a>
  </p>
</div>
