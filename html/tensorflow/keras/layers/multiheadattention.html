<h1 class="devsite-page-title" tabindex="-1"> tf.keras.layers.MultiHeadAttention </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.layers.MultiHeadAttention"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="from_config"> <meta itemprop="property" content="symbolic_call"> </div>   <p>MultiHeadAttention layer.</p> <p>Inherits From: <a href="../layer.html"><code translate="no" dir="ltr">Layer</code></a>, <a href="../operation.html"><code translate="no" dir="ltr">Operation</code></a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.keras.layers.MultiHeadAttention(
    num_heads,
    key_dim,
    value_dim=None,
    dropout=0.0,
    use_bias=True,
    output_shape=None,
    attention_axes=None,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
</pre></devsite-code> <h3 id="used-in-the-notebooks" data-text="Used in the notebooks" tabindex="-1">Used in the notebooks</h3> <table class="vertical-rules"> <thead> <tr> <th>Used in the tutorials</th> </tr> </thead> <tbody> <tr> <td> <ul> <li><a href="https://www.tensorflow.org/text/tutorials/image_captioning">Image captioning with visual attention</a></li> <li><a href="https://www.tensorflow.org/text/tutorials/nmt_with_attention">Neural machine translation with attention</a></li> <li><a href="https://www.tensorflow.org/text/tutorials/transformer">Neural machine translation with a Transformer and Keras</a></li> </ul> </td> </tr> </tbody> </table> <p>This is an implementation of multi-headed attention as described in the paper "Attention is all you Need" <a href="https://arxiv.org/abs/1706.03762">Vaswani et al., 2017</a>. If <code translate="no" dir="ltr">query</code>, <code translate="no" dir="ltr">key,</code> <code translate="no" dir="ltr">value</code> are the same, then this is self-attention. Each timestep in <code translate="no" dir="ltr">query</code> attends to the corresponding sequence in <code translate="no" dir="ltr">key</code>, and returns a fixed-width vector.</p> <p>This layer first projects <code translate="no" dir="ltr">query</code>, <code translate="no" dir="ltr">key</code> and <code translate="no" dir="ltr">value</code>. These are (effectively) a list of tensors of length <code translate="no" dir="ltr">num_attention_heads</code>, where the corresponding shapes are <code translate="no" dir="ltr">(batch_size, &lt;query dimensions&gt;, key_dim)</code>, <code translate="no" dir="ltr">(batch_size, &lt;key/value dimensions&gt;, key_dim)</code>, <code translate="no" dir="ltr">(batch_size, &lt;key/value dimensions&gt;, value_dim)</code>.</p> <p>Then, the query and key tensors are dot-producted and scaled. These are softmaxed to obtain attention probabilities. The value tensors are then interpolated by these probabilities, then concatenated back to a single tensor.</p> <p>Finally, the result tensor with the last dimension as <code translate="no" dir="ltr">value_dim</code> can take a linear projection and return.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">num_heads</code> </td> <td> Number of attention heads. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">key_dim</code> </td> <td> Size of each attention head for query and key. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value_dim</code> </td> <td> Size of each attention head for value. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dropout</code> </td> <td> Dropout probability. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">use_bias</code> </td> <td> Boolean, whether the dense layers use bias vectors/matrices. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">output_shape</code> </td> <td> The expected shape of an output tensor, besides the batch and sequence dims. If not specified, projects back to the query feature dim (the query input's last dimension). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">attention_axes</code> </td> <td> axes over which the attention is applied. <code translate="no" dir="ltr">None</code> means attention over all axes, but batch, heads, and features. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">kernel_initializer</code> </td> <td> Initializer for dense layer kernels. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">bias_initializer</code> </td> <td> Initializer for dense layer biases. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">kernel_regularizer</code> </td> <td> Regularizer for dense layer kernels. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">bias_regularizer</code> </td> <td> Regularizer for dense layer biases. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">activity_regularizer</code> </td> <td> Regularizer for dense layer activity. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">kernel_constraint</code> </td> <td> Constraint for dense layer kernels. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">bias_constraint</code> </td> <td> Constraint for dense layer kernels. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Call arguments</th></tr> 
<tr> <td> <code translate="no" dir="ltr">query</code> </td> <td> Query tensor of shape <code translate="no" dir="ltr">(B, T, dim)</code>, where <code translate="no" dir="ltr">B</code> is the batch size, <code translate="no" dir="ltr">T</code> is the target sequence length, and dim is the feature dimension. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> Value tensor of shape <code translate="no" dir="ltr">(B, S, dim)</code>, where <code translate="no" dir="ltr">B</code> is the batch size, <code translate="no" dir="ltr">S</code> is the source sequence length, and dim is the feature dimension. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">key</code> </td> <td> Optional key tensor of shape <code translate="no" dir="ltr">(B, S, dim)</code>. If not given, will use <code translate="no" dir="ltr">value</code> for both <code translate="no" dir="ltr">key</code> and <code translate="no" dir="ltr">value</code>, which is the most common case. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">attention_mask</code> </td> <td> a boolean mask of shape <code translate="no" dir="ltr">(B, T, S)</code>, that prevents attention to certain positions. The boolean mask specifies which query elements can attend to which key elements, 1 indicates attention and 0 indicates no attention. Broadcasting can happen for the missing batch dimensions and the head dimension. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">return_attention_scores</code> </td> <td> A boolean to indicate whether the output should be <code translate="no" dir="ltr">(attention_output, attention_scores)</code> if <code translate="no" dir="ltr">True</code>, or <code translate="no" dir="ltr">attention_output</code> if <code translate="no" dir="ltr">False</code>. Defaults to <code translate="no" dir="ltr">False</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">training</code> </td> <td> Python boolean indicating whether the layer should behave in training mode (adding dropout) or in inference mode (no dropout). Will go with either using the training mode of the parent layer/model, or <code translate="no" dir="ltr">False</code> (inference) if there is no parent layer. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">use_causal_mask</code> </td> <td> A boolean to indicate whether to apply a causal mask to prevent tokens from attending to future tokens (e.g., used in a decoder Transformer). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> 
<tr> <td> <code translate="no" dir="ltr">attention_output</code> </td> <td> The result of the computation, of shape <code translate="no" dir="ltr">(B, T, E)</code>, where <code translate="no" dir="ltr">T</code> is for target sequence shapes and <code translate="no" dir="ltr">E</code> is the query input last dimension if <code translate="no" dir="ltr">output_shape</code> is <code translate="no" dir="ltr">None</code>. Otherwise, the multi-head outputs are projected to the shape specified by <code translate="no" dir="ltr">output_shape</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">attention_scores</code> </td> <td> (Optional) multi-head attention coefficients over attention axes. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">attention_axes</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">dropout</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">input</code> </td> <td> Retrieves the input tensor(s) of a symbolic operation. <p>Only returns the tensor(s) corresponding to the <em>first time</em> the operation was called. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">key_dense</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">key_dim</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_heads</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">output</code> </td> <td> Retrieves the output tensor(s) of a layer. <p>Only returns the tensor(s) corresponding to the <em>first time</em> the operation was called. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">output_dense</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">output_shape</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">query_dense</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">use_bias</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">value_dense</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">value_dim</code> </td> <td> 
</td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="-1">Methods</h2> <h3 id="from_config" data-text="from_config" tabindex="-1"><code translate="no" dir="ltr">from_config</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/ops/operation.py#L191-L213">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@classmethod
from_config(
    config
)
</pre></devsite-code> <p>Creates a layer from its config.</p> <p>This method is the reverse of <code translate="no" dir="ltr">get_config</code>, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by <code translate="no" dir="ltr">set_weights</code>).</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">config</code> </td> <td> A Python dictionary, typically the output of get_config. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A layer instance. </td> </tr> 
</table> <h3 id="symbolic_call" data-text="symbolic_call" tabindex="-1"><code translate="no" dir="ltr">symbolic_call</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/ops/operation.py#L58-L70">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">symbolic_call(
    *args, **kwargs
)
</pre></devsite-code>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention</a>
  </p>
</div>
