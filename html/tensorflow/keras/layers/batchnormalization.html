<h1 class="devsite-page-title" tabindex="-1"> tf.keras.layers.BatchNormalization </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.layers.BatchNormalization"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="from_config"> <meta itemprop="property" content="symbolic_call"> </div>   <p>Layer that normalizes its inputs.</p> <p>Inherits From: <a href="../layer.html"><code translate="no" dir="ltr">Layer</code></a>, <a href="../operation.html"><code translate="no" dir="ltr">Operation</code></a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.keras.layers.BatchNormalization(
    axis=-1,
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer='zeros',
    gamma_initializer='ones',
    moving_mean_initializer='zeros',
    moving_variance_initializer='ones',
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    synchronized=False,
    **kwargs
)
</pre></devsite-code> <h3 id="used-in-the-notebooks" data-text="Used in the notebooks" tabindex="-1">Used in the notebooks</h3> <table class="vertical-rules"> <thead> <tr> <th>Used in the guide</th> <th>Used in the tutorials</th> </tr> </thead> <tbody> <tr> <td> <ul> <li><a href="https://www.tensorflow.org/guide/effective_tf2">Effective Tensorflow 2</a></li> <li><a href="https://www.tensorflow.org/guide/advanced_autodiff">Advanced automatic differentiation</a></li> <li><a href="https://www.tensorflow.org/model_optimization/guide/pruning/pruning_for_on_device_inference">Pruning for on-device inference w/ XNNPACK</a></li> <li><a href="https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_sparsity_2_by_4">Sparse weights using structural pruning</a></li> </ul> </td> <td> <ul> <li><a href="https://www.tensorflow.org/tutorials/customization/custom_layers">Custom layers</a></li> <li><a href="https://www.tensorflow.org/tutorials/generative/dcgan">Deep Convolutional Generative Adversarial Network</a></li> <li><a href="https://www.tensorflow.org/tutorials/generative/pix2pix">pix2pix: Image-to-image translation with a conditional GAN</a></li> <li><a href="https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial">Distributed training with DTensors</a></li> </ul> </td> </tr> </tbody> </table> <p>Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.</p> <p>Importantly, batch normalization works differently during training and during inference.</p> <p><strong>During training</strong> (i.e. when using <code translate="no" dir="ltr">fit()</code> or when calling the layer/model with the argument <code translate="no" dir="ltr">training=True</code>), the layer normalizes its output using the mean and standard deviation of the current batch of inputs. That is to say, for each channel being normalized, the layer returns <code translate="no" dir="ltr">gamma * (batch - mean(batch)) / sqrt(var(batch) + epsilon) + beta</code>, where:</p> <ul> <li>
<code translate="no" dir="ltr">epsilon</code> is small constant (configurable as part of the constructor arguments)</li> <li>
<code translate="no" dir="ltr">gamma</code> is a learned scaling factor (initialized as 1), which can be disabled by passing <code translate="no" dir="ltr">scale=False</code> to the constructor.</li> <li>
<code translate="no" dir="ltr">beta</code> is a learned offset factor (initialized as 0), which can be disabled by passing <code translate="no" dir="ltr">center=False</code> to the constructor.</li> </ul> <p><strong>During inference</strong> (i.e. when using <code translate="no" dir="ltr">evaluate()</code> or <code translate="no" dir="ltr">predict()</code> or when calling the layer/model with the argument <code translate="no" dir="ltr">training=False</code> (which is the default), the layer normalizes its output using a moving average of the mean and standard deviation of the batches it has seen during training. That is to say, it returns <code translate="no" dir="ltr">gamma * (batch - self.moving_mean) / sqrt(self.moving_var+epsilon) + beta</code>.</p> <p><code translate="no" dir="ltr">self.moving_mean</code> and <code translate="no" dir="ltr">self.moving_var</code> are non-trainable variables that are updated each time the layer in called in training mode, as such:</p> <ul> <li><code translate="no" dir="ltr">moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)</code></li> <li><code translate="no" dir="ltr">moving_var = moving_var * momentum + var(batch) * (1 - momentum)</code></li> </ul> <p>As such, the layer will only normalize its inputs during inference <em>after having been trained on data that has similar statistics as the inference data</em>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">axis</code> </td> <td> Integer, the axis that should be normalized (typically the features axis). For instance, after a <code translate="no" dir="ltr">Conv2D</code> layer with <code translate="no" dir="ltr">data_format="channels_first"</code>, use <code translate="no" dir="ltr">axis=1</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">momentum</code> </td> <td> Momentum for the moving average. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">epsilon</code> </td> <td> Small float added to variance to avoid dividing by zero. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">center</code> </td> <td> If <code translate="no" dir="ltr">True</code>, add offset of <code translate="no" dir="ltr">beta</code> to normalized tensor. If <code translate="no" dir="ltr">False</code>, <code translate="no" dir="ltr">beta</code> is ignored. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">scale</code> </td> <td> If <code translate="no" dir="ltr">True</code>, multiply by <code translate="no" dir="ltr">gamma</code>. If <code translate="no" dir="ltr">False</code>, <code translate="no" dir="ltr">gamma</code> is not used. When the next layer is linear this can be disabled since the scaling will be done by the next layer. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">beta_initializer</code> </td> <td> Initializer for the beta weight. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">gamma_initializer</code> </td> <td> Initializer for the gamma weight. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">moving_mean_initializer</code> </td> <td> Initializer for the moving mean. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">moving_variance_initializer</code> </td> <td> Initializer for the moving variance. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">beta_regularizer</code> </td> <td> Optional regularizer for the beta weight. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">gamma_regularizer</code> </td> <td> Optional regularizer for the gamma weight. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">beta_constraint</code> </td> <td> Optional constraint for the beta weight. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">gamma_constraint</code> </td> <td> Optional constraint for the gamma weight. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">synchronized</code> </td> <td> Only applicable with the TensorFlow backend. If <code translate="no" dir="ltr">True</code>, synchronizes the global batch statistics (mean and variance) for the layer across all devices at each training step in a distributed training strategy. If <code translate="no" dir="ltr">False</code>, each replica uses its own local batch statistics. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> Base layer keyword arguments (e.g. <code translate="no" dir="ltr">name</code> and <code translate="no" dir="ltr">dtype</code>). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Call arguments</th></tr> 
<tr> <td> <code translate="no" dir="ltr">inputs</code> </td> <td> Input tensor (of any rank). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">training</code> </td> <td> Python boolean indicating whether the layer should behave in training mode or in inference mode. <ul> <li>
<code translate="no" dir="ltr">training=True</code>: The layer will normalize its inputs using the mean and variance of the current batch of inputs.</li> <li>
<code translate="no" dir="ltr">training=False</code>: The layer will normalize its inputs using the mean and variance of its moving statistics, learned during training. </li>
</ul>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">mask</code> </td> <td> Binary tensor of shape broadcastable to <code translate="no" dir="ltr">inputs</code> tensor, with <code translate="no" dir="ltr">True</code> values indicating the positions for which mean and variance should be computed. Masked elements of the current inputs are not taken into account for mean and variance computation during training. Any prior unmasked element values will be taken into account until their momentum expires. </td> </tr> </table> <h4 id="reference" data-text="Reference:" tabindex="-1">Reference:</h4> <ul> <li>
<a href="https://arxiv.org/abs/1502.03167">Ioffe and Szegedy, 2015</a>.</li> </ul> <p><strong>About setting <code translate="no" dir="ltr">layer.trainable = False</code> on a <code translate="no" dir="ltr">BatchNormalization</code> layer:</strong></p> <p>The meaning of setting <code translate="no" dir="ltr">layer.trainable = False</code> is to freeze the layer, i.e. its internal state will not change during training: its trainable weights will not be updated during <code translate="no" dir="ltr">fit()</code> or <code translate="no" dir="ltr">train_on_batch()</code>, and its state updates will not be run.</p> <p>Usually, this does not necessarily mean that the layer is run in inference mode (which is normally controlled by the <code translate="no" dir="ltr">training</code> argument that can be passed when calling a layer). "Frozen state" and "inference mode" are two separate concepts.</p> <p>However, in the case of the <code translate="no" dir="ltr">BatchNormalization</code> layer, <strong>setting <code translate="no" dir="ltr">trainable = False</code> on the layer means that the layer will be subsequently run in inference mode</strong> (meaning that it will use the moving mean and the moving variance to normalize the current batch, rather than using the mean and variance of the current batch).</p> <h4 id="note_that" data-text="Note that:" tabindex="-1">Note that:</h4> <ul> <li>Setting <code translate="no" dir="ltr">trainable</code> on an model containing other layers will recursively set the <code translate="no" dir="ltr">trainable</code> value of all inner layers.</li> <li>If the value of the <code translate="no" dir="ltr">trainable</code> attribute is changed after calling <code translate="no" dir="ltr">compile()</code> on a model, the new value doesn't take effect for this model until <code translate="no" dir="ltr">compile()</code> is called again.</li> </ul>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input</code> </td> <td> Retrieves the input tensor(s) of a symbolic operation. <p>Only returns the tensor(s) corresponding to the <em>first time</em> the operation was called. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">output</code> </td> <td> Retrieves the output tensor(s) of a layer. <p>Only returns the tensor(s) corresponding to the <em>first time</em> the operation was called. </p>
</td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="-1">Methods</h2> <h3 id="from_config" data-text="from_config" tabindex="-1"><code translate="no" dir="ltr">from_config</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/ops/operation.py#L191-L213">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@classmethod
from_config(
    config
)
</pre></devsite-code> <p>Creates a layer from its config.</p> <p>This method is the reverse of <code translate="no" dir="ltr">get_config</code>, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by <code translate="no" dir="ltr">set_weights</code>).</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">config</code> </td> <td> A Python dictionary, typically the output of get_config. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A layer instance. </td> </tr> 
</table> <h3 id="symbolic_call" data-text="symbolic_call" tabindex="-1"><code translate="no" dir="ltr">symbolic_call</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/ops/operation.py#L58-L70">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">symbolic_call(
    *args, **kwargs
)
</pre></devsite-code>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization</a>
  </p>
</div>
