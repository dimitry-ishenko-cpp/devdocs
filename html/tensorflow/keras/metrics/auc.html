<h1 class="devsite-page-title">tf.keras.metrics.AUC</h1> <devsite-bookmark></devsite-bookmark>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/metrics/metrics.py#L1466-L1944">  View source on GitHub </a> </td> </table> <p>Approximates the AUC (Area under the curve) of the ROC or PR curves.</p> <p>Inherits From: <a href="metric.html"><code translate="no" dir="ltr">Metric</code></a>, <a href="../layers/layer.html"><code translate="no" dir="ltr">Layer</code></a>, <a href="../../module.html"><code translate="no" dir="ltr">Module</code></a></p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC"><code translate="no" dir="ltr">tf.compat.v1.keras.metrics.AUC</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.keras.metrics.AUC(
    num_thresholds=200,
    curve='ROC',
    summation_method='interpolation',
    name=None,
    dtype=None,
    thresholds=None,
    multi_label=False,
    num_labels=None,
    label_weights=None,
    from_logits=False
)
</pre>  <p>The AUC (Area under the curve) of the ROC (Receiver operating characteristic; default) or PR (Precision Recall) curves are quality measures of binary classifiers. Unlike the accuracy, and like cross-entropy losses, ROC-AUC and PR-AUC evaluate all the operational points of a model.</p> <p>This class approximates AUCs using a Riemann sum. During the metric accumulation phrase, predictions are accumulated within predefined buckets by value. The AUC is then computed by interpolating per-bucket averages. These buckets define the evaluated operational points.</p> <p>This metric creates four local variables, <code translate="no" dir="ltr">true_positives</code>, <code translate="no" dir="ltr">true_negatives</code>, <code translate="no" dir="ltr">false_positives</code> and <code translate="no" dir="ltr">false_negatives</code> that are used to compute the AUC. To discretize the AUC curve, a linearly spaced set of thresholds is used to compute pairs of recall and precision values. The area under the ROC-curve is therefore computed using the height of the recall values by the false positive rate, while the area under the PR-curve is the computed using the height of the precision values by the recall.</p> <p>This value is ultimately returned as <code translate="no" dir="ltr">auc</code>, an idempotent operation that computes the area under a discretized curve of precision versus recall values (computed using the aforementioned variables). The <code translate="no" dir="ltr">num_thresholds</code> variable controls the degree of discretization with larger numbers of thresholds more closely approximating the true AUC. The quality of the approximation may vary dramatically depending on <code translate="no" dir="ltr">num_thresholds</code>. The <code translate="no" dir="ltr">thresholds</code> parameter can be used to manually specify thresholds which split the predictions more evenly.</p> <p>For a best approximation of the real AUC, <code translate="no" dir="ltr">predictions</code> should be distributed approximately uniformly in the range <a href="if%20%60from_logits=false%60.html">0, 1</a>. The quality of the AUC approximation may be poor if this is not the case. Setting <code translate="no" dir="ltr">summation_method</code> to 'minoring' or 'majoring' can help quantify the error in the approximation by providing lower or upper bound estimate of the AUC.</p> <p>If <code translate="no" dir="ltr">sample_weight</code> is <code translate="no" dir="ltr">None</code>, weights default to 1. Use <code translate="no" dir="ltr">sample_weight</code> of 0 to mask values.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">num_thresholds</code> </td> <td> (Optional) Defaults to 200. The number of thresholds to use when discretizing the roc curve. Values must be &gt; 1. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">curve</code> </td> <td> (Optional) Specifies the name of the curve to be computed, 'ROC' [default] or 'PR' for the Precision-Recall-curve. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">summation_method</code> </td> <td> (Optional) Specifies the <a href="https://en.wikipedia.org/wiki/Riemann_sum">Riemann summation method</a> used. 'interpolation' (default) applies mid-point summation scheme for <code translate="no" dir="ltr">ROC</code>. For PR-AUC, interpolates (true/false) positives but not the ratio that is precision (see Davis &amp; Goadrich 2006 for details); 'minoring' applies left summation for increasing intervals and right summation for decreasing intervals; 'majoring' does the opposite. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> (Optional) string name of the metric instance. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dtype</code> </td> <td> (Optional) data type of the metric result. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">thresholds</code> </td> <td> (Optional) A list of floating point values to use as the thresholds for discretizing the curve. If set, the <code translate="no" dir="ltr">num_thresholds</code> parameter is ignored. Values should be in [0, 1]. Endpoint thresholds equal to {-epsilon, 1+epsilon} for a small positive epsilon value will be automatically included with these to correctly handle predictions equal to exactly 0 or 1. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">multi_label</code> </td> <td> boolean indicating whether multilabel data should be treated as such, wherein AUC is computed separately for each label and then averaged across labels, or (when False) if the data should be flattened into a single label before AUC computation. In the latter case, when multilabel data is passed to AUC, each label-prediction pair is treated as an individual data point. Should be set to False for multi-class data. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_labels</code> </td> <td> (Optional) The number of labels, used when <code translate="no" dir="ltr">multi_label</code> is True. If <code translate="no" dir="ltr">num_labels</code> is not specified, then state variables get created on the first call to <code translate="no" dir="ltr">update_state</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">label_weights</code> </td> <td> (Optional) list, array, or tensor of non-negative weights used to compute AUCs for multilabel data. When <code translate="no" dir="ltr">multi_label</code> is True, the weights are applied to the individual label AUCs when they are averaged to produce the multi-label AUC. When it's False, they are used to weight the individual label predictions in computing the confusion matrix on the flattened data. Note that this is unlike class_weights in that class_weights weights the example depending on the value of its label, whereas label_weights depends only on the index of that label before flattening; therefore <code translate="no" dir="ltr">label_weights</code> should not be used for multi-class data. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">from_logits</code> </td> <td> boolean indicating whether the predictions (<code translate="no" dir="ltr">y_pred</code> in <code translate="no" dir="ltr">update_state</code>) are probabilities or sigmoid logits. As a rule of thumb, when using a keras loss, the <code translate="no" dir="ltr">from_logits</code> constructor argument of the loss should match the AUC <code translate="no" dir="ltr">from_logits</code> constructor argument. </td> </tr> </table> <h4 id="standalone_usage" data-text="Standalone usage:">Standalone usage:</h4> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
m = tf.keras.metrics.AUC(num_thresholds=3)
m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
# threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]
# tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]
# tp_rate = recall = [1, 0.5, 0], fp_rate = [1, 0, 0]
# auc = ((((1+0.5)/2)*(1-0)) + (((0.5+0)/2)*(0-0))) = 0.75
m.result().numpy()
0.75
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
m.reset_state()
m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],
               sample_weight=[1, 0, 0, 1])
m.result().numpy()
1.0
</pre> <p>Usage with <code translate="no" dir="ltr">compile()</code> API:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># Reports the AUC of a model outputting a probability.
model.compile(optimizer='sgd',
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[tf.keras.metrics.AUC()])

# Reports the AUC of a model outputting a logit.
model.compile(optimizer='sgd',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=[tf.keras.metrics.AUC(from_logits=True)])
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">thresholds</code> </td> <td> The thresholds used for evaluating AUC. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="interpolate_pr_auc" data-text="interpolate_pr_auc"><code translate="no" dir="ltr">interpolate_pr_auc</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/metrics/metrics.py#L1778-L1857">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
interpolate_pr_auc()
</pre> <p>Interpolation formula inspired by section 4 of Davis &amp; Goadrich 2006.</p> <p><a href="https://www.biostat.wisc.edu/~page/rocpr.pdf">https://www.biostat.wisc.edu/~page/rocpr.pdf</a></p> <p>Note here we derive &amp; use a closed formula not present in the paper as follows:</p> <p>Precision = TP / (TP + FP) = TP / P</p> <p>Modeling all of TP (true positive), FP (false positive) and their sum P = TP + FP (predicted positive) as varying linearly within each interval [A, B] between successive thresholds, we get</p> <p>Precision slope = dTP / dP = (TP_B - TP_A) / (P_B - P_A) = (TP - TP_A) / (P - P_A) Precision = (TP_A + slope * (P - P_A)) / P</p> <p>The area within the interval is (slope / total_pos_weight) times</p> <p>int_A^B{Precision.dP} = int_A^B{(TP_A + slope * (P - P_A)) * dP / P} int_A^B{Precision.dP} = int_A^B{slope * dP + intercept * dP / P}</p> <p>where intercept = TP_A - slope * P_A = TP_B - slope * P_B, resulting in</p> <p>int_A^B{Precision.dP} = TP_B - TP_A + intercept * log(P_B / P_A)</p> <p>Bringing back the factor (slope / total_pos_weight) we'd put aside, we get</p> <p>slope * [dTP + intercept * log(P_B / P_A)] / total_pos_weight</p> <p>where dTP == TP_B - TP_A.</p> <p>Note that when P_A == 0 the above calculation simplifies into</p> <p>int_A^B{Precision.dTP} = int_A^B{slope * dTP} = slope * (TP_B - TP_A)</p> <p>which is really equivalent to imputing constant precision throughout the first bucket having &gt;0 true positives.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> 
<tr> <td> <code translate="no" dir="ltr">pr_auc</code> </td> <td> an approximation of the area under the P-R curve. </td> </tr> </table> <h3 id="merge_state" data-text="merge_state"><code translate="no" dir="ltr">merge_state</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/metrics/base_metric.py#L275-L309">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
merge_state(
    metrics
)
</pre> <p>Merges the state from one or more metrics.</p> <p>This method can be used by distributed systems to merge the state computed by different metric instances. Typically the state will be stored in the form of the metric's weights. For example, a tf.keras.metrics.Mean metric contains a list of two weight values: a total and a count. If there were two instances of a tf.keras.metrics.Accuracy that each independently aggregated partial state for an overall accuracy calculation, these two metric's states could be combined as follows:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
m1 = tf.keras.metrics.Accuracy()
_ = m1.update_state([[1], [2]], [[0], [2]])
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
m2 = tf.keras.metrics.Accuracy()
_ = m2.update_state([[3], [4]], [[3], [4]])
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
m2.merge_state([m1])
m2.result().numpy()
0.75
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">metrics</code> </td> <td> an iterable of metrics. The metrics must have compatible state. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If the provided iterable does not contain metrics matching the metric's required specifications. </td> </tr> </table> <h3 id="reset_state" data-text="reset_state"><code translate="no" dir="ltr">reset_state</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/metrics/metrics.py#L1913-L1923">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
reset_state()
</pre> <p>Resets all of the metric state variables.</p> <p>This function is called between epochs/steps, when a metric is evaluated during training.</p> <h3 id="result" data-text="result"><code translate="no" dir="ltr">result</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/metrics/metrics.py#L1859-L1911">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
result()
</pre> <p>Computes and returns the scalar metric value tensor or a dict of scalars.</p> <p>Result computation is an idempotent operation that simply calculates the metric value using the state variables.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A scalar tensor, or a dictionary of scalar tensors. </td> </tr> 
</table> <h3 id="update_state" data-text="update_state"><code translate="no" dir="ltr">update_state</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v2.9.0/keras/metrics/metrics.py#L1717-L1776">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
update_state(
    y_true, y_pred, sample_weight=None
)
</pre> <p>Accumulates confusion matrix statistics.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">y_true</code> </td> <td> The ground truth values. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">y_pred</code> </td> <td> The predicted values. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">sample_weight</code> </td> <td> Optional weighting of each example. Defaults to 1. Can be a <code translate="no" dir="ltr">Tensor</code> whose rank is either 0, or the same rank as <code translate="no" dir="ltr">y_true</code>, and must be broadcastable to <code translate="no" dir="ltr">y_true</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Update op. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/metrics/AUC" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/metrics/AUC</a>
  </p>
</div>
