<h1 class="devsite-page-title" tabindex="-1"> tf.keras.distribution.ModelParallel </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.distribution.ModelParallel"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="distribute_dataset"> <meta itemprop="property" content="get_data_layout"> <meta itemprop="property" content="get_tensor_layout"> <meta itemprop="property" content="get_variable_layout"> <meta itemprop="property" content="scope"> </div>   <p>Distribution that shards model variables.</p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.keras.distribution.ModelParallel(
    device_mesh, layout_map, batch_dim_name=None
)
</pre></devsite-code>  <p>Compare to <code translate="no" dir="ltr">DataParallel</code> which replicates the variables across all devices, <code translate="no" dir="ltr">ModelParallel</code> allows you to shard variables in addition to the input data.</p> <p>To construct a <code translate="no" dir="ltr">ModelParallel</code> distribution, you need to provide a <code translate="no" dir="ltr">DeviceMesh</code> and a <code translate="no" dir="ltr">LayoutMap</code>.</p> <ol> <li>
<code translate="no" dir="ltr">DeviceMesh</code> contains physical device information. The axis names in the mesh will be used to map the variable and data layout.</li> <li>
<code translate="no" dir="ltr">LayoutMap</code> contains the mapping between variable paths to their corresponding <code translate="no" dir="ltr">TensorLayout</code>.</li> </ol> <h4 id="example" data-text="Example:" tabindex="-1">Example:</h4> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">devices = list_devices()    # Assume there are 8 devices.

# Create a mesh with 2 devices for data parallelism and 4 devices for
# model parallelism.
device_mesh = DeviceMesh(shape=(2, 4), axis_names=('batch', 'model'),
                         devices=devices)
# Create a layout map that shard the `Dense` layer and `Conv2D`
# layer variables on the last dimension.
# Based on the `device_mesh`, this means the variables
# will be split across 4 devices. Any other variable that doesn't
# match any key in the layout map will be fully replicated.
layout_map = LayoutMap(device_mesh)
layout_map['dense.*kernel'] = (None, 'model')
layout_map['dense.*bias'] = ('model',)
layout_map['conv2d.*kernel'] = (None, None, None, 'model')
layout_map['conv2d.*bias'] = ('model',)

distribution = ModelParallel(device_mesh=device_mesh,
                             layout_map=layout_map,
                             batch_dim_name='batch')
# Set the global distribution, or via `with distribution.scope():`
set_distribution(distribution)

model = model_creation()
model.compile()
model.fit(data)
</pre></devsite-code> <p>You can quickly update the device mesh shape to change the sharding factor of the variables. E.g.</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp"># With only the shape change for the device mesh, the variables will be
# sharded across 8 devices instead of 4, which further reduces the memory
# footprint of variables on each of the device.
device_mesh = DeviceMesh(shape=(1, 8), axis_names=('batch', 'model'),
                         devices=devices)
</pre></devsite-code> <p>To figure out a proper layout mapping rule for all the model variables, you can first list out all the model variable paths, which will be used as the key to map the variables to <code translate="no" dir="ltr">TensorLayout</code>.</p> <p>e.g.</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">model = create_model()
for v in model.variables:
    print(v.path)
</pre></devsite-code>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">device_mesh</code> </td> <td> <code translate="no" dir="ltr">DeviceMesh</code> instance for physical device and its logical mapping. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">layout_map</code> </td> <td> <code translate="no" dir="ltr">LayoutMap</code> instance which map the variable path to the corresponding <code translate="no" dir="ltr">TensorLayout</code>. The axis names of the <code translate="no" dir="ltr">TensorLayout</code>s should match to the axis names in the device_mesh, or exception will be raised. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">batch_dim_name</code> </td> <td> optional string, the axis name in the <code translate="no" dir="ltr">device_mesh</code> that will be used to distribute data. If unspecified, the first axis from the <code translate="no" dir="ltr">device_mesh</code> will be used. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">device_mesh</code> </td> <td> 
</td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="-1">Methods</h2> <h3 id="distribute_dataset" data-text="distribute_dataset" tabindex="-1"><code translate="no" dir="ltr">distribute_dataset</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/distribution/distribution_lib.py#L592-L673">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">distribute_dataset(
    dataset
)
</pre></devsite-code> <p>Create a distributed dataset instance from the original user dataset.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">dataset</code> </td> <td> the original global dataset instance. Only <a href="../../data/dataset.html"><code translate="no" dir="ltr">tf.data.Dataset</code></a> is supported at the moment. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> a sharded <a href="../../data/dataset.html"><code translate="no" dir="ltr">tf.data.Dataset</code></a> instance, which will produce data for the current local worker/process. </td> </tr> 
</table> <h3 id="get_data_layout" data-text="get_data_layout" tabindex="-1"><code translate="no" dir="ltr">get_data_layout</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/distribution/distribution_lib.py#L577-L580">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">get_data_layout(
    data_shape
)
</pre></devsite-code> <p>Retrieve the <code translate="no" dir="ltr">TensorLayout</code> for the input data.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">data_shape</code> </td> <td> shape for the input data in list or tuple format. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> The <code translate="no" dir="ltr">TensorLayout</code> for the data, which can be used by <code translate="no" dir="ltr">backend.distribute_value()</code> to redistribute a input data. </td> </tr> 
</table> <h3 id="get_tensor_layout" data-text="get_tensor_layout" tabindex="-1"><code translate="no" dir="ltr">get_tensor_layout</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/distribution/distribution_lib.py#L589-L590">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">get_tensor_layout(
    path
)
</pre></devsite-code> <p>Retrieve the <code translate="no" dir="ltr">TensorLayout</code> for the intermediate tensor.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">path</code> </td> <td> a string path for the corresponding tensor. </td> </tr> </table> <p>return: The <code translate="no" dir="ltr">TensorLayout</code> for the intermediate tensor, which can be used by <code translate="no" dir="ltr">backend.relayout()</code> to reshard the tensor. Could also return None.</p> <h3 id="get_variable_layout" data-text="get_variable_layout" tabindex="-1"><code translate="no" dir="ltr">get_variable_layout</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/distribution/distribution_lib.py#L582-L587">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">get_variable_layout(
    variable
)
</pre></devsite-code> <p>Retrieve the <code translate="no" dir="ltr">TensorLayout</code> for the variable.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">variable</code> </td> <td> A <code translate="no" dir="ltr">KerasVariable</code> instance. </td> </tr> </table> <p>return: The <code translate="no" dir="ltr">TensorLayout</code> for the variable, which can be used by <code translate="no" dir="ltr">backend.distribute_value()</code> to redistribute a variable.</p> <h3 id="scope" data-text="scope" tabindex="-1"><code translate="no" dir="ltr">scope</code></h3> <p><a target="_blank" class="external" href="https://github.com/keras-team/keras/tree/v3.3.3/keras/src/distribution/distribution_lib.py#L332-L340">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@contextlib.contextmanager
scope()
</pre></devsite-code> <p>Context manager to make the <code translate="no" dir="ltr">Distribution</code> current.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/distribution/ModelParallel" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/distribution/ModelParallel</a>
  </p>
</div>
