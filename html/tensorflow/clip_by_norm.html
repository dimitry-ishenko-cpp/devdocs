<h1 class="devsite-page-title" tabindex="-1"> tf.clip_by_norm </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.clip_by_norm"> <meta itemprop="path" content="Stable"> </div>   <p>Clips tensor values to a maximum L2-norm.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases" tabindex="-1">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="clip_by_norm.html"><code translate="no" dir="ltr">tf.compat.v1.clip_by_norm</code></a></p> </section> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.clip_by_norm(
    t, clip_norm, axes=None, name=None
)
</pre></devsite-code> <h3 id="used-in-the-notebooks" data-text="Used in the notebooks" tabindex="-1">Used in the notebooks</h3> <table class="vertical-rules"> <thead> <tr> <th>Used in the guide</th> </tr> </thead> <tbody> <tr> <td> <ul> <li><a href="https://www.tensorflow.org/guide/advanced_autodiff">Advanced automatic differentiation</a></li> </ul> </td> </tr> </tbody> </table> <p>Given a tensor <code translate="no" dir="ltr">t</code>, and a maximum clip value <code translate="no" dir="ltr">clip_norm</code>, this operation normalizes <code translate="no" dir="ltr">t</code> so that its L2-norm is less than or equal to <code translate="no" dir="ltr">clip_norm</code>, along the dimensions given in <code translate="no" dir="ltr">axes</code>. Specifically, in the default case where all dimensions are used for calculation, if the L2-norm of <code translate="no" dir="ltr">t</code> is already less than or equal to <code translate="no" dir="ltr">clip_norm</code>, then <code translate="no" dir="ltr">t</code> is not modified. If the L2-norm is greater than <code translate="no" dir="ltr">clip_norm</code>, then this operation returns a tensor of the same type and shape as <code translate="no" dir="ltr">t</code> with its values set to:</p> <p><code translate="no" dir="ltr">t * clip_norm / l2norm(t)</code></p> <p>In this case, the L2-norm of the output tensor is <code translate="no" dir="ltr">clip_norm</code>.</p> <p>As another example, if <code translate="no" dir="ltr">t</code> is a matrix and <code translate="no" dir="ltr">axes == [1]</code>, then each row of the output will have L2-norm less than or equal to <code translate="no" dir="ltr">clip_norm</code>. If <code translate="no" dir="ltr">axes == [0]</code> instead, each column of the output will be clipped.</p> <h4 id="code_example" data-text="Code example:" tabindex="-1">Code example:</h4> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">some_nums = tf.constant([[1, 2, 3, 4, 5]], dtype=tf.float32)
tf.clip_by_norm(some_nums, 2.0).numpy()
array([[0.26967996, 0.5393599 , 0.80903983, 1.0787199 , 1.3483998 ]],
      dtype=float32)</pre></devsite-code> <p>This operation is typically used to clip gradients before applying them with an optimizer. Most gradient data is a collection of different shaped tensors for different parts of the model. Thus, this is a common usage:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp"># Get your gradients after training
loss_value, grads = grad(model, features, labels)

# Apply some clipping
grads = [tf.clip_by_norm(g, norm)
             for g in grads]

# Continue on with training
optimizer.apply_gradients(grads)
</pre></devsite-code>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">t</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> or <code translate="no" dir="ltr">IndexedSlices</code>. This must be a floating point type. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">clip_norm</code> </td> <td> A 0-D (scalar) <code translate="no" dir="ltr">Tensor</code> &gt; 0. A maximum clipping value, also floating point. Note: If a negative clip_norm is provided, it will be treated as zero. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">axes</code> </td> <td> A 1-D (vector) <code translate="no" dir="ltr">Tensor</code> of type int32 containing the dimensions to use for computing the L2-norm. If <code translate="no" dir="ltr">None</code> (the default), uses all dimensions. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A clipped <code translate="no" dir="ltr">Tensor</code> or <code translate="no" dir="ltr">IndexedSlices</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If the clip_norm tensor is not a 0-D scalar tensor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If dtype of the input is not a floating point or complex type. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/clip_by_norm" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/clip_by_norm</a>
  </p>
</div>
