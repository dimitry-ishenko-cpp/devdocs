<h1 class="devsite-page-title" tabindex="-1"> tf.nn.embedding_lookup_sparse </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.nn.embedding_lookup_sparse"> <meta itemprop="path" content="Stable"> </div>   <p>Looks up embeddings for the given ids and weights from a list of tensors.</p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.nn.embedding_lookup_sparse(
    params,
    sp_ids,
    sp_weights,
    combiner=None,
    max_norm=None,
    name=None,
    allow_fast_lookup=False
)
</pre></devsite-code>  <p><code translate="no" dir="ltr">params</code> is a dense tensor or a list of dense tensors, and <code translate="no" dir="ltr">sp_ids</code> is a 2D <a href="../sparse/sparsetensor.html"><code translate="no" dir="ltr">tf.SparseTensor</code></a> or <a href="../raggedtensor.html"><code translate="no" dir="ltr">tf.RaggedTensor</code></a> indicating the indices of <code translate="no" dir="ltr">params</code> to gather.</p> <p>This op is best described with an example. Suppose <code translate="no" dir="ltr">params</code> is an embedding table of size <code translate="no" dir="ltr">(4, 2)</code> and <code translate="no" dir="ltr">sp_ids</code> has 3 rows. Since <code translate="no" dir="ltr">sp_ids</code> is sparse or ragged, not every row has the same number of elements. The output has shape (3, 2). Each row of <code translate="no" dir="ltr">sp_ids</code> is a list of indices, where each index selects a row of <code translate="no" dir="ltr">params</code>. For a given row of <code translate="no" dir="ltr">sp_ids</code>, the rows of <code translate="no" dir="ltr">params</code> are gathered based on the indices in <code translate="no" dir="ltr">sp_ids</code>, then combined by taking their sum or mean.</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">params = tf.constant([[1, 2], [3, 4], [5, 6], [7, 8]], dtype=tf.float32)
sp_ids = tf.SparseTensor(indices=[[0, 0], [0, 1], [1, 0], [2, 0]],
                         values=[0, 1, 3, 2], dense_shape=(3, 2))
tf.nn.embedding_lookup_sparse(params, sp_ids, sp_weights=None,
                              combiner='sum').numpy()
array([[4., 6.], [7., 8.], [5., 6.]], dtype=float32)</pre></devsite-code> <p>In this example, <code translate="no" dir="ltr">sp_ids</code> has 3 rows, so the output has 3 rows. Row 0 of <code translate="no" dir="ltr">sp_ids</code> has values 0 and 1, so it selects rows 0 and 1 from <code translate="no" dir="ltr">params</code>, which are <code translate="no" dir="ltr">[1, 2]</code> and <code translate="no" dir="ltr">[3, 4]</code>. The rows are summed since <code translate="no" dir="ltr">combiner='sum'</code>, resulting in the output row of <code translate="no" dir="ltr">[4, 6]</code>.</p> <p>Since row 1 and 2 of <code translate="no" dir="ltr">sp_ids</code> only have one value each, they simply select the corresponding row from <code translate="no" dir="ltr">params</code> as the output row. Row 1 has value <code translate="no" dir="ltr">3</code> so it selects the <code translate="no" dir="ltr">params</code> elements <code translate="no" dir="ltr">[7, 8]</code> and row 2 has the value 2 so it selects the <code translate="no" dir="ltr">params</code> elements <code translate="no" dir="ltr">[5, 6]</code>.</p> <p>If <code translate="no" dir="ltr">sparse_weights</code> is specified, it must have the same shape as <code translate="no" dir="ltr">sp_ids</code>. <code translate="no" dir="ltr">sparse_weights</code> is used to assign a weight to each slice of <code translate="no" dir="ltr">params</code>. For example:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">params = tf.constant([[1, 2], [3, 4], [5, 6], [7, 8]], dtype=tf.float32)
sp_ids = tf.SparseTensor(indices=[[0, 0], [0, 1], [1, 0], [2, 0]],
                         values=[0, 1, 3, 2], dense_shape=(3, 2))
sparse_weights = tf.SparseTensor(indices=[[0, 0], [0, 1], [1, 0], [2, 0]],
                                 values=[0.1, 1.0, 0.5, 2.0],
                                 dense_shape=(3, 2))
tf.nn.embedding_lookup_sparse(params, sp_ids, sp_weights=sparse_weights,
                              combiner='sum').numpy()
array([[3.1, 4.2], [3.5, 4.], [10., 12.]], dtype=float32)</pre></devsite-code> <p>In general, <code translate="no" dir="ltr">params</code> can have shape <code translate="no" dir="ltr">(p0, ..., pn)</code> and <code translate="no" dir="ltr">sp_ids</code> can have <code translate="no" dir="ltr">M</code> rows, where each row can have any number of elements. The output has shape <code translate="no" dir="ltr">(M, p1, ..., pn)</code>. Each slice of the output <code translate="no" dir="ltr">output[i, ...]</code> is obtained as follows: The <code translate="no" dir="ltr">combiner</code> argument is used to combine the values <code translate="no" dir="ltr">params[sp_ids[i, j], ...] * sparse_weights[i, j]</code> for each <code translate="no" dir="ltr">j</code> in <code translate="no" dir="ltr">range(0, len(sp_ids[i]))</code>, e.g. by taking the sum or mean of the values.</p> <p>This op assumes that there is at least one id for each row in the dense tensor represented by sp_ids (i.e. there are no rows with empty features), and that all the indices of sp_ids are in canonical row-major order.</p> <p><code translate="no" dir="ltr">sp_ids</code> and <code translate="no" dir="ltr">sp_weights</code> (if not None) are <code translate="no" dir="ltr">SparseTensor</code>s or <code translate="no" dir="ltr">RaggedTensor</code>s with rank of 2. For <code translate="no" dir="ltr">SpareTensor</code>s with left-aligned non-zero entries which can be described as <code translate="no" dir="ltr">RaggedTensor</code>s, use of <code translate="no" dir="ltr">RaggedTensor</code>s can yield higher performance.</p> <p>This op assumes that all id values lie in the range [0, p0), where p0 is <code translate="no" dir="ltr">params.shape[0]</code>. If you want a version of this op that prunes id values less than 0, see <a href="safe_embedding_lookup_sparse.html"><code translate="no" dir="ltr">tf.nn.safe_embedding_lookup_sparse</code></a></p> <p>If <code translate="no" dir="ltr">len(params) &gt; 1</code>, each element of <code translate="no" dir="ltr">sp_ids</code> is partitioned between the elements of <code translate="no" dir="ltr">params</code> according to the "div" partition strategy, which means we assign ids to partitions in a contiguous manner. For instance, 13 ids are split across 5 partitions as: <code translate="no" dir="ltr">[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]]</code>.</p> <p>If the id space does not evenly divide the number of partitions, each of the first <code translate="no" dir="ltr">(max_id + 1) % len(params)</code> partitions will be assigned one more id.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">params</code> </td> <td> A single tensor representing the complete embedding tensor, or a list of tensors all of same shape except for the first dimension, representing sharded embedding tensors following "div" partition strategy. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">sp_ids</code> </td> <td> N x M <code translate="no" dir="ltr">SparseTensor</code> of int64 ids where N is typically batch size and M is arbitrary or a <code translate="no" dir="ltr">RaggedTensor</code> with rank 2. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">sparse_weights</code> </td> <td> <code translate="no" dir="ltr">SparseTensor</code> or <code translate="no" dir="ltr">RaggedTensor</code> of same type and shape as <code translate="no" dir="ltr">sparse_ids</code>, containing float / double weights corresponding to <code translate="no" dir="ltr">sparse_ids</code>, or <code translate="no" dir="ltr">None</code> if all weights are assumed to be 1.0. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">combiner</code> </td> <td> A string specifying the reduction op. Currently "mean", "sqrtn" and "sum" are supported. "sum" computes the weighted sum of the embedding results for each row. "mean" is the weighted sum divided by the total weight. "sqrtn" is the weighted sum divided by the square root of the sum of the squares of the weights. Defaults to <code translate="no" dir="ltr">mean</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">max_norm</code> </td> <td> If not <code translate="no" dir="ltr">None</code>, each embedding is clipped if its l2-norm is larger than this value, before combining. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the op. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">allow_fast_lookup</code> </td> <td> An optional boolean specifying whether to allow simplified embedding lookups when <code translate="no" dir="ltr">params</code> is a single tensor and <code translate="no" dir="ltr">max_norm</code> is <code translate="no" dir="ltr">None</code>. Setting this flag to <code translate="no" dir="ltr">True</code> during training can cause the use of dense gradients with increased memory footprint. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A dense tensor representing the combined embeddings for the sparse ids. For each row in the dense tensor represented by <code translate="no" dir="ltr">sp_ids</code>, the op looks up the embeddings for all ids in that row, multiplies them by the corresponding weight, and combines these embeddings as specified. <p>In other words, if</p> <p><code translate="no" dir="ltr">shape(combined params) = [p0, p1, ..., pm]</code></p> <p>and</p> <p><code translate="no" dir="ltr">shape(sp_ids) = shape(sp_weights) = [d0, d1]</code></p> <p>then</p> <p><code translate="no" dir="ltr">shape(output) = [d0, p1, ..., pm]</code>.</p> <p>For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">[0, 0]: id 1, weight 2.0
[0, 1]: id 3, weight 0.5
[1, 0]: id 0, weight 1.0
[2, 3]: id 1, weight 3.0
</pre></devsite-code> <p>with <code translate="no" dir="ltr">combiner</code>="mean", then the output will be a 3x20 matrix where</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)
output[1, :] = (params[0, :] * 1.0) / 1.0
output[2, :] = (params[1, :] * 3.0) / 3.0
</pre></devsite-code> 
</td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If <code translate="no" dir="ltr">sp_ids</code> is not a <code translate="no" dir="ltr">SparseTensor</code>, or if <code translate="no" dir="ltr">sp_weights</code> is neither <code translate="no" dir="ltr">None</code> nor <code translate="no" dir="ltr">SparseTensor</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If <code translate="no" dir="ltr">combiner</code> is not one of {"mean", "sqrtn", "sum"}. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup_sparse" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup_sparse</a>
  </p>
</div>
