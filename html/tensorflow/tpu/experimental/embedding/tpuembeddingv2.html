<h1 class="devsite-page-title" tabindex="-1"> tf.tpu.experimental.embedding.TPUEmbeddingV2 </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.tpu.experimental.embedding.TPUEmbeddingV2"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__call__"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="apply_gradients"> <meta itemprop="property" content="build"> <meta itemprop="property" content="dequeue"> <meta itemprop="property" content="embedding_lookup"> <meta itemprop="property" content="enqueue"> <meta itemprop="property" content="preprocess_features"> </div>   <p>The TPUEmbedding mid level API running on TPU with sparse core accelerator.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases" tabindex="-1">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="tpuembeddingv2.html"><code translate="no" dir="ltr">tf.compat.v1.tpu.experimental.embedding.TPUEmbeddingV2</code></a></p> </section> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.tpu.experimental.embedding.TPUEmbeddingV2(
    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable],
    optimizer: Optional[tpu_embedding_v2_utils._Optimizer] = None,
    pipeline_execution_with_tensor_core: bool = False
)
</pre></devsite-code>   
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">feature_config</code> </td> <td> A nested structure of <a href="featureconfig.html"><code translate="no" dir="ltr">tf.tpu.experimental.embedding.FeatureConfig</code></a> configs. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">optimizer</code> </td> <td> An instance of one of <a href="sgd.html"><code translate="no" dir="ltr">tf.tpu.experimental.embedding.SGD</code></a>, <a href="adagrad.html"><code translate="no" dir="ltr">tf.tpu.experimental.embedding.Adagrad</code></a> or <a href="adam.html"><code translate="no" dir="ltr">tf.tpu.experimental.embedding.Adam</code></a>. When not created under TPUStrategy may be set to None to avoid the creation of the optimizer slot variables, useful for optimizing memory consumption when exporting the model for serving where slot variables aren't needed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">pipeline_execution_with_tensor_core</code> </td> <td> If True, the TPU embedding computations will overlap with the TensorCore computations (and hence will be one step old). Set to True for improved performance. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If optimizer is not one of tf.tpu.experimental.embedding.(SGD, Adam or Adagrad) or None when created under a TPUStrategy. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> If not created under TPUStrategy. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">embedding_table_shards</code> </td> <td> Returns a dict of embedding tables, keyed by <code translate="no" dir="ltr">TableConfig</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">embedding_tables</code> </td> <td> Returns a dict of embedding tables, keyed by <code translate="no" dir="ltr">TableConfig</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">variables</code> </td> <td> Returns a dict of variables, keyed by <code translate="no" dir="ltr">TableConfig</code>, then by slot name. </td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="-1">Methods</h2> <h3 id="apply_gradients" data-text="apply_gradients" tabindex="-1"><code translate="no" dir="ltr">apply_gradients</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/tpu/tpu_embedding_v3.py#L699-L873">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">apply_gradients(
    gradients: Any, preserved_outputs: Dict[str, PartitionedCsrFormatTensor]
)
</pre></devsite-code> <p>Applies the gradient update to the embedding tables.</p> <p>If a gradient of <code translate="no" dir="ltr">None</code> is passed in any position of the nested structure, then a gradient update with a zero gradient is applied for that feature. For optimizers like SGD or Adagrad, this is the same as applying no update at all. For lazy Adam and other sparsely applied optimizers with decay, ensure you understand the effect of applying a zero gradient.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">gradients</code> </td> <td> A nested structure of gradients, with structure matching the <code translate="no" dir="ltr">feature_config</code> passed to this object. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">preserved_outputs</code> </td> <td> A dicts of PartitionedCsrFormatTensor, coming from the second output of the embedding lookup call. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> if not built. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If a non-<a href="../../../tensor.html"><code translate="no" dir="ltr">tf.Tensor</code></a> non-<code translate="no" dir="ltr">None</code> gradient is passed in, or a <a href="../../../tensor.html"><code translate="no" dir="ltr">tf.Tensor</code></a> of the incorrect shape is passed in. Also if the size of any sequence in <code translate="no" dir="ltr">gradients</code> does not match corresponding sequence in <code translate="no" dir="ltr">feature_config</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If the type of any sequence in <code translate="no" dir="ltr">gradients</code> does not match corresponding sequence in <code translate="no" dir="ltr">feature_config</code>. </td> </tr> </table> <h3 id="build" data-text="build" tabindex="-1"><code translate="no" dir="ltr">build</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/tpu/tpu_embedding_v3.py#L692-L697">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">build()
</pre></devsite-code> <p>Create variables and slots variables for TPU embeddings.</p> <h3 id="dequeue" data-text="dequeue" tabindex="-1"><code translate="no" dir="ltr">dequeue</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/tpu/tpu_embedding_v3.py#L1425-L1489">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">dequeue(
    partitioned_tensors: Tuple[Dict[str, PartitionedCsrFormatTensor], int, int]
) -&gt; Tuple[Any, Dict[str, PartitionedCsrFormatTensor]]
</pre></devsite-code> <p>Perform embedding lookup.</p> <h3 id="embedding_lookup" data-text="embedding_lookup" tabindex="-1"><code translate="no" dir="ltr">embedding_lookup</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/tpu/tpu_embedding_v3.py#L1491-L1534">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">embedding_lookup(
    features: Any, weights: Optional[Any] = None
) -&gt; Tuple[Any, Dict[str, PartitionedCsrFormatTensor]]
</pre></devsite-code> <p>Perform embedding lookup on the input feature.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">features</code> </td> <td> A nested structure of <a href="../../../tensor.html"><code translate="no" dir="ltr">tf.Tensor</code></a>s, <a href="../../../sparse/sparsetensor.html"><code translate="no" dir="ltr">tf.SparseTensor</code></a>s or <a href="../../../raggedtensor.html"><code translate="no" dir="ltr">tf.RaggedTensor</code></a>s, with the same structure as <code translate="no" dir="ltr">feature_config</code>. Inputs will be downcast to <a href="../../../../tf.html#int32"><code translate="no" dir="ltr">tf.int32</code></a>. Only one type out of <a href="../../../sparse/sparsetensor.html"><code translate="no" dir="ltr">tf.SparseTensor</code></a> or <a href="../../../raggedtensor.html"><code translate="no" dir="ltr">tf.RaggedTensor</code></a> is supported per call. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">weights</code> </td> <td> If not <code translate="no" dir="ltr">None</code>, a nested structure of <a href="../../../tensor.html"><code translate="no" dir="ltr">tf.Tensor</code></a>s, <a href="../../../sparse/sparsetensor.html"><code translate="no" dir="ltr">tf.SparseTensor</code></a>s or <a href="../../../raggedtensor.html"><code translate="no" dir="ltr">tf.RaggedTensor</code></a>s, matching the above, except that the tensors should be of float type (and they will be downcast to <a href="../../../../tf.html#float32"><code translate="no" dir="ltr">tf.float32</code></a>). For <a href="../../../sparse/sparsetensor.html"><code translate="no" dir="ltr">tf.SparseTensor</code></a>s we assume the <code translate="no" dir="ltr">indices</code> are the same for the parallel entries from <code translate="no" dir="ltr">features</code> and similarly for <a href="../../../raggedtensor.html"><code translate="no" dir="ltr">tf.RaggedTensor</code></a>s we assume the row_splits are the same. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If the input feature is not one of the Tensor, SparseTensor or RaggedTensor type. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If the type of any sequence in <code translate="no" dir="ltr">features</code> does not match corresponding sequence in <code translate="no" dir="ltr">feature_config</code>. Similarly for <code translate="no" dir="ltr">weights</code>, if not <code translate="no" dir="ltr">None</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> 
<tr> <td> <code translate="no" dir="ltr">packed_activations</code> </td> <td> Embedding lookup results packed as the same sequence of the input feature. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">packed_output</code> </td> <td> A dict of PartitionedCsrFormatTensors. </td> </tr> </table> <h3 id="enqueue" data-text="enqueue" tabindex="-1"><code translate="no" dir="ltr">enqueue</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/tpu/tpu_embedding_v3.py#L1300-L1373">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">enqueue(
    features: Any, weights: Optional[Any] = None, device: Optional[str] = None
) -&gt; Any
</pre></devsite-code> <p>Preprocessing the features on host.</p> <h3 id="preprocess_features" data-text="preprocess_features" tabindex="-1"><code translate="no" dir="ltr">preprocess_features</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/tpu/tpu_embedding_v3.py#L1235-L1298">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@staticmethod
preprocess_features(
    num_replicas_in_sync: int,
    max_ids_per_chip_per_sample: int,
    max_minibatches_per_sc: int,
    num_sc_per_chip: int,
    num_sc_shards: int,
    stacked_table_to_tables: Dict[str, Any],
    table_to_stacked_table_offset: Dict[str, Tuple[str, int, int]],
    table_to_sample_count: Dict[str, int],
    feature_to_sample_offset: Dict[str, int],
    flat_features: Any,
    flat_inputs: Any,
    flat_weights: Optional[Any] = None
) -&gt; Any
</pre></devsite-code> <p>Function to preprocess features.</p> <h3 id="__call__" data-text="__call__" tabindex="-1"><code translate="no" dir="ltr">__call__</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/tpu/tpu_embedding_v3.py#L952-L956">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">__call__(
    features: Any, weights: Optional[Any] = None
) -&gt; Tuple[Any, Dict[str, PartitionedCsrFormatTensor]]
</pre></devsite-code> <p>Call the mid level api to do embedding lookup.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/tpu/experimental/embedding/TPUEmbeddingV2" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/tpu/experimental/embedding/TPUEmbeddingV2</a>
  </p>
</div>
