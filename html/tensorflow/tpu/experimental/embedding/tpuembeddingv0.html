<h1 class="devsite-page-title">tf.tpu.experimental.embedding.TPUEmbeddingV0</h1> <devsite-bookmark></devsite-bookmark>       <p>The TPUEmbedding mid level API running on TPU without Embedding accelerator.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/tpu/experimental/embedding/TPUEmbeddingV0"><code translate="no" dir="ltr">tf.compat.v1.tpu.experimental.embedding.TPUEmbeddingV0</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.tpu.experimental.embedding.TPUEmbeddingV0(
    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable],
    optimizer: Optional[tpu_embedding_v2_utils._Optimizer]
)
</pre>  <blockquote class="note">
<strong>Note:</strong><span> This mid level API is not intended for large embedding table lookup. Embedding tables will be replicated across devices rather than sharding across them. To do large embedding table lookup, please use the <a href="tpuembedding.html"><code translate="no" dir="ltr">tpu.experimental.embedding.TPUEmbedding</code></a> class. This class is an alternative way to do embedding lookups when the TPU doesn't support any version of embedding feature. See <code translate="no" dir="ltr">tpu.experimental.tpu_hardware_feature.embedding_feature</code> for a detailed explanation.</span>
</blockquote> <p>This class has to be created under the <code translate="no" dir="ltr">TPUStrategy</code>, Otherwise a RuntimeError will be raised.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">strategy = tf.distribute.TPUStrategy(...)
with strategy.scope():
  embedding = tf.tpu.experimental.embedding.TPUEmbeddingV0(
      feature_config=feature_config,
      optimizer=tf.tpu.experimental.embedding.SGD(0.1))
</pre> <p>When creating a distributed dataset that is to be passed to the lookup operation a special input option must be specified:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">distributed_dataset = (
    strategy.distribute_datasets_from_function(
        dataset_fn=...,
        options=tf.distribute.InputOptions(
            experimental_fetch_to_device=False))
dataset_iterator = iter(distributed_dataset)
</pre> <p>Below is an example of a training and evaluation step:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">optimizer = tf.keras.optimizers.SGD(0.1)

@tf.function
def training_step(dataset_iterator, num_steps):
  def tpu_step(embedding_features):
    with tf.GradientTape() as tape:
      tape.watch(embedding.embedding_table.values())
      activation = embedding(embedding_features)
      model_output = model(activations)
      loss = ...  # some function of labels and model_output

    embedding_gradients = tape.gradient(loss,
                                        embedding.embedding_table.values())
    optimizer.apply_gradients(list(zip(gradients,
                              mid_level_api.embedding_tables.values())))
    # Insert your model gradient and optimizer application here

  for _ in tf.range(num_steps):
    strategy.run(tpu_step, args=(next(dataset_iterator), ))

@tf.function
def evalution_step(dataset_iterator, num_steps):
  def tpu_step(embedding_features):
    activations = embedding(embedding_features)
    model_output = model(activations)
    # Insert your evaluation code here.

  for _ in tf.range(num_steps):
    strategy.run(tpu_step, args=(next(dataset_iterator), ))
</pre>
<blockquote class="note">
<strong>Note:</strong><span> The optimizer used here is a Keras optimizer. In order to make the slot variable creation stay consistent between Keras optimizers and embedding optimizers, the <code translate="no" dir="ltr">slot_variable_creation_fn</code> argument of the embedding optimizers has to be passed with the Keras <code translate="no" dir="ltr">add_slot</code> function. Also note that the slot names might be slightly different between them.</span>
</blockquote>
<pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.1)

def slot_variable_creation_fn(table, slot_names, slot_initializers):
    slots = {}
    for slot, initializer in zip(slot_names, slot_initializers):
      slots[slot] = optimizer.add_slot(table, slot, initializer)
    return slots

embedding_optimizer = tf.experimental.embedding.Adagrad(
    learning_rate=0.1,
    slot_variable_creation_fn=slot_variable_creation_fn)

# Use the embedding optimizer to create mid level api and keras optimizer to
# apply gradients.
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">embedding_tables</code> </td> <td> Returns a dict of embedding tables, keyed by <code translate="no" dir="ltr">TableConfig</code>. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="build" data-text="build"><code translate="no" dir="ltr">build</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/tpu/tpu_embedding_base.py#L140-L145">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
build()
</pre> <p>Create variables and slots variables for TPU embeddings.</p> <h3 id="embedding_lookup" data-text="embedding_lookup"><code translate="no" dir="ltr">embedding_lookup</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/tpu/tpu_embedding_v1.py#L239-L304">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
embedding_lookup(
    features: Any, weights: Optional[Any] = None
) -&gt; Any
</pre> <p>Apply embedding lookup on TPUs using Tensorcore.</p> <p>Note that all the sparse and ragged tensors will be converted to dense tensors on CPU and then passed to the TPU to do embedding look up. Large embedding lookup is not supported by this API, use the TPUEmbedding mid level api instead.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">features</code> </td> <td> a nested structure of Tensors, SparseTensors or RaggedTensors. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">weights</code> </td> <td> a nested structure of Tensors, SparseTensors or RaggedTensors or None for no weights. If not None, structure must match that of inputs, but entries are allowed to be None. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A nested structure of Tensors with the same structure as inputs. </td> </tr> 
</table> <h3 id="__call__" data-text="__call__"><code translate="no" dir="ltr">__call__</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/tpu/tpu_embedding_base.py#L147-L151">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
__call__(
    features: Any, weights: Optional[Any] = None
) -&gt; Any
</pre> <p>Call the mid level api to do embedding lookup.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/tpu/experimental/embedding/TPUEmbeddingV0" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/tpu/experimental/embedding/TPUEmbeddingV0</a>
  </p>
</div>
