<h1 class="devsite-page-title">tf.estimator.MultiHead</h1> <devsite-bookmark></devsite-bookmark>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator/head/multi_head.py#L53-L547">  View source on GitHub </a> </td> </table> <p>Creates a <code translate="no" dir="ltr">Head</code> for multi-objective learning.</p> <p>Inherits From: <a href="head.html"><code translate="no" dir="ltr">Head</code></a></p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/estimator/MultiHead"><code translate="no" dir="ltr">tf.compat.v1.estimator.MultiHead</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.estimator.MultiHead(
    heads, head_weights=None
)
</pre>  <p>This class merges the output of multiple <code translate="no" dir="ltr">Head</code> objects. Specifically:</p> <ul> <li>For training, sums losses of each head, calls <code translate="no" dir="ltr">train_op_fn</code> with this final loss.</li> <li>For eval, merges metrics by adding <code translate="no" dir="ltr">head.name</code> suffix to the keys in eval metrics, such as <code translate="no" dir="ltr">precision/head1.name</code>, <code translate="no" dir="ltr">precision/head2.name</code>.</li> <li>For prediction, merges predictions and updates keys in prediction dict to a 2-tuple, <code translate="no" dir="ltr">(head.name, prediction_key)</code>. Merges <code translate="no" dir="ltr">export_outputs</code> such that by default the first head is served.</li> </ul> <h4 id="usage" data-text="Usage:">Usage:</h4> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
head1 = tf.estimator.MultiLabelHead(n_classes=2, name='head1')
head2 = tf.estimator.MultiLabelHead(n_classes=3, name='head2')
multi_head = tf.estimator.MultiHead([head1, head2])
logits = {
   'head1': np.array([[-10., 10.], [-15., 10.]], dtype=np.float32),
   'head2': np.array([[20., -20., 20.], [-30., 20., -20.]],
   dtype=np.float32),}
labels = {
   'head1': np.array([[1, 0], [1, 1]], dtype=np.int64),
   'head2': np.array([[0, 1, 0], [1, 1, 0]], dtype=np.int64),}
features = {'x': np.array(((42,),), dtype=np.float32)}
# For large logits, sigmoid cross entropy loss is approximated as:
# loss = labels * (logits &lt; 0) * (-logits) +
#        (1 - labels) * (logits &gt; 0) * logits =&gt;
# head1: expected_unweighted_loss = [[10., 10.], [15., 0.]]
# loss1 = ((10 + 10) / 2 + (15 + 0) / 2) / 2 = 8.75
# head2: expected_unweighted_loss = [[20., 20., 20.], [30., 0., 0]]
# loss2 = ((20 + 20 + 20) / 3 + (30 + 0 + 0) / 3) / 2 = 15.00
# loss = loss1 + loss2 = 8.75 + 15.00 = 23.75
loss = multi_head.loss(labels, logits, features=features)
print('{:.2f}'.format(loss.numpy()))
23.75
eval_metrics = multi_head.metrics()
updated_metrics = multi_head.update_metrics(
  eval_metrics, features, logits, labels)
for k in sorted(updated_metrics):
 print('{} : {:.2f}'.format(k, updated_metrics[k].result().numpy()))
auc/head1 : 0.17
auc/head2 : 0.33
auc_precision_recall/head1 : 0.60
auc_precision_recall/head2 : 0.40
average_loss/head1 : 8.75
average_loss/head2 : 15.00
loss/head1 : 8.75
loss/head2 : 15.00
preds = multi_head.predictions(logits)
print(preds[('head1', 'logits')])
tf.Tensor(
  [[-10.  10.]
   [-15.  10.]], shape=(2, 2), dtype=float32)
</pre> <p>Usage with a canned estimator:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># In `input_fn`, specify labels as a dict keyed by head name:
def input_fn():
  features = ...
  labels1 = ...
  labels2 = ...
  return features, {'head1.name': labels1, 'head2.name': labels2}

# In `model_fn`, specify logits as a dict keyed by head name:
def model_fn(features, labels, mode):
  # Create simple heads and specify head name.
  head1 = tf.estimator.MultiClassHead(n_classes=3, name='head1')
  head2 = tf.estimator.BinaryClassHead(name='head2')
  # Create MultiHead from two simple heads.
  head = tf.estimator.MultiHead([head1, head2])
  # Create logits for each head, and combine them into a dict.
  logits1, logits2 = logit_fn()
  logits = {'head1.name': logits1, 'head2.name': logits2}
  # Return the merged EstimatorSpec
  return head.create_estimator_spec(..., logits=logits, ...)

# Create an estimator with this model_fn.
estimator = tf.estimator.Estimator(model_fn=model_fn)
estimator.train(input_fn=input_fn)
</pre> <p>Also supports <code translate="no" dir="ltr">logits</code> as a <code translate="no" dir="ltr">Tensor</code> of shape <code translate="no" dir="ltr">[D0, D1, ... DN, logits_dimension]</code>. It will split the <code translate="no" dir="ltr">Tensor</code> along the last dimension and distribute it appropriately among the heads. E.g.:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># Input logits.
logits = np.array([[-1., 1., 2., -2., 2.], [-1.5, 1., -3., 2., -2.]],
                  dtype=np.float32)
# Suppose head1 and head2 have the following logits dimension.
head1.logits_dimension = 2
head2.logits_dimension = 3
# After splitting, the result will be:
logits_dict = {'head1_name': [[-1., 1.], [-1.5, 1.]],
               'head2_name':  [[2., -2., 2.], [-3., 2., -2.]]}
</pre> <h4 id="usage_2" data-text="Usage:">Usage:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">def model_fn(features, labels, mode):
  # Create simple heads and specify head name.
  head1 = tf.estimator.MultiClassHead(n_classes=3, name='head1')
  head2 = tf.estimator.BinaryClassHead(name='head2')
  # Create multi-head from two simple heads.
  head = tf.estimator.MultiHead([head1, head2])
  # Create logits for the multihead. The result of logits is a `Tensor`.
  logits = logit_fn(logits_dimension=head.logits_dimension)
  # Return the merged EstimatorSpec
  return head.create_estimator_spec(..., logits=logits, ...)
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">heads</code> </td> <td> List or tuple of <code translate="no" dir="ltr">Head</code> instances. All heads must have <code translate="no" dir="ltr">name</code> specified. The first head in the list is the default used at serving time. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">head_weights</code> </td> <td> Optional list of weights, same length as <code translate="no" dir="ltr">heads</code>. Used when merging losses to calculate the weighted sum of losses from each head. If <code translate="no" dir="ltr">None</code>, all losses are weighted equally. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">logits_dimension</code> </td> <td> See <code translate="no" dir="ltr">base_head.Head</code> for details. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">loss_reduction</code> </td> <td> See <code translate="no" dir="ltr">base_head.Head</code> for details. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> See <code translate="no" dir="ltr">base_head.Head</code> for details. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="create_estimator_spec" data-text="create_estimator_spec"><code translate="no" dir="ltr">create_estimator_spec</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator/head/multi_head.py#L396-L511">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
create_estimator_spec(
    features,
    mode,
    logits,
    labels=None,
    optimizer=None,
    trainable_variables=None,
    train_op_fn=None,
    update_ops=None,
    regularization_losses=None
)
</pre> <p>Returns a <code translate="no" dir="ltr">model_fn.EstimatorSpec</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">features</code> </td> <td> Input <code translate="no" dir="ltr">dict</code> of <code translate="no" dir="ltr">Tensor</code> or <code translate="no" dir="ltr">SparseTensor</code> objects. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">mode</code> </td> <td> Estimator's <code translate="no" dir="ltr">ModeKeys</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">logits</code> </td> <td> Input <code translate="no" dir="ltr">dict</code> keyed by head name, or logits <code translate="no" dir="ltr">Tensor</code> with shape <code translate="no" dir="ltr">[D0, D1, ... DN, logits_dimension]</code>. For many applications, the <code translate="no" dir="ltr">Tensor</code> shape is <code translate="no" dir="ltr">[batch_size, logits_dimension]</code>. If logits is a <code translate="no" dir="ltr">Tensor</code>, it will split the <code translate="no" dir="ltr">Tensor</code> along the last dimension and distribute it appropriately among the heads. Check <code translate="no" dir="ltr">MultiHead</code> for examples. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">labels</code> </td> <td> Input <code translate="no" dir="ltr">dict</code> keyed by head name. For each head, the label value can be integer or string <code translate="no" dir="ltr">Tensor</code> with shape matching its corresponding <code translate="no" dir="ltr">logits</code>.<code translate="no" dir="ltr">labels</code> is a required argument when <code translate="no" dir="ltr">mode</code> equals <code translate="no" dir="ltr">TRAIN</code> or <code translate="no" dir="ltr">EVAL</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">optimizer</code> </td> <td> An <a href="../keras/optimizers/optimizer.html"><code translate="no" dir="ltr">tf.keras.optimizers.Optimizer</code></a> instance to optimize the loss in TRAIN mode. Namely, sets <code translate="no" dir="ltr">train_op = optimizer.get_updates(loss, trainable_variables)</code>, which updates variables to minimize <code translate="no" dir="ltr">loss</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">trainable_variables</code> </td> <td> A list or tuple of <code translate="no" dir="ltr">Variable</code> objects to update to minimize <code translate="no" dir="ltr">loss</code>. In Tensorflow 1.x, by default these are the list of variables collected in the graph under the key <code translate="no" dir="ltr">GraphKeys.TRAINABLE_VARIABLES</code>. As Tensorflow 2.x doesn't have collections and GraphKeys, trainable_variables need to be passed explicitly here. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">train_op_fn</code> </td> <td> Function that takes a scalar loss <code translate="no" dir="ltr">Tensor</code> and returns <code translate="no" dir="ltr">train_op</code>. Used if <code translate="no" dir="ltr">optimizer</code> is <code translate="no" dir="ltr">None</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">update_ops</code> </td> <td> A list or tuple of update ops to be run at training time. For example, layers such as BatchNormalization create mean and variance update ops that need to be run at training time. In Tensorflow 1.x, these are thrown into an UPDATE_OPS collection. As Tensorflow 2.x doesn't have collections, update_ops need to be passed explicitly here. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">regularization_losses</code> </td> <td> A list of additional scalar losses to be added to the training loss, such as regularization losses. These losses are usually expressed as a batch average, so for best results, in each head, users need to use the default <code translate="no" dir="ltr">loss_reduction=SUM_OVER_BATCH_SIZE</code> to avoid scaling errors. Compared to the regularization losses for each head, this loss is to regularize the merged loss of all heads in multi head, and will be added to the overall training loss of multi head. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <code translate="no" dir="ltr">model_fn.EstimatorSpec</code> instance. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If both <code translate="no" dir="ltr">train_op_fn</code> and <code translate="no" dir="ltr">optimizer</code> are <code translate="no" dir="ltr">None</code> in TRAIN mode, or if both are set. If <code translate="no" dir="ltr">mode</code> is not in Estimator's <code translate="no" dir="ltr">ModeKeys</code>. </td> </tr> </table> <h3 id="loss" data-text="loss"><code translate="no" dir="ltr">loss</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator/head/multi_head.py#L307-L341">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
loss(
    labels, logits, features=None, mode=None, regularization_losses=None
)
</pre> <p>Returns regularized training loss. See <code translate="no" dir="ltr">base_head.Head</code> for details.</p> <h3 id="metrics" data-text="metrics"><code translate="no" dir="ltr">metrics</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator/head/multi_head.py#L354-L366">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
metrics(
    regularization_losses=None
)
</pre> <p>Creates metrics. See <code translate="no" dir="ltr">base_head.Head</code> for details.</p> <h3 id="predictions" data-text="predictions"><code translate="no" dir="ltr">predictions</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator/head/multi_head.py#L343-L352">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
predictions(
    logits, keys=None
)
</pre> <p>Create predictions. See <code translate="no" dir="ltr">base_head.Head</code> for details.</p> <h3 id="update_metrics" data-text="update_metrics"><code translate="no" dir="ltr">update_metrics</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator/head/multi_head.py#L368-L394">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
update_metrics(
    eval_metrics, features, logits, labels, regularization_losses=None
)
</pre> <p>Updates eval metrics. See <code translate="no" dir="ltr">base_head.Head</code> for details.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/estimator/MultiHead" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/estimator/MultiHead</a>
  </p>
</div>
