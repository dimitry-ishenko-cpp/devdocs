<h1 class="devsite-page-title">tf.train.ExponentialMovingAverage</h1> <devsite-bookmark></devsite-bookmark>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/moving_averages.py#L282-L685">  View source on GitHub </a> </td> </table> <p>Maintains moving averages of variables by employing an exponential decay.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage"><code translate="no" dir="ltr">tf.compat.v1.train.ExponentialMovingAverage</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.train.ExponentialMovingAverage(
    decay,
    num_updates=None,
    zero_debias=False,
    name='ExponentialMovingAverage'
)
</pre>  <p>When training a model, it is often beneficial to maintain moving averages of the trained parameters. Evaluations that use averaged parameters sometimes produce significantly better results than the final trained values.</p> <p>The <code translate="no" dir="ltr">apply()</code> method adds shadow copies of trained variables the first time it is called, and maintains a moving average of the trained variables in their shadow copies at every additional invocation. It should generally be called immediately after creating the model weights, and then after each training step.</p> <p>The <code translate="no" dir="ltr">average()</code> method gives access to the shadow variables. It allows you to use the moving averages in place of the last trained values for evaluations, by loading the moving averages into your model via <code translate="no" dir="ltr">var.assign(ema.average(var))</code>. Additionally, although <code translate="no" dir="ltr">ExponentialMovingAverage</code> objects are not directly trackable by checkpoints, <code translate="no" dir="ltr">average()</code> returns the moving average variables for your model weights, which you can then checkpoint. (There is an example of this near the bottom of this docstring). So, <code translate="no" dir="ltr">average()</code> is useful when building an evaluation model, or when restoring a model from a checkpoint file.</p> <p>The moving averages are computed using exponential decay. You specify the decay value (as a scalar float value, <code translate="no" dir="ltr">Tensor</code>, or <code translate="no" dir="ltr">Variable</code>) when creating the <code translate="no" dir="ltr">ExponentialMovingAverage</code> object. The shadow variables are initialized with the same initial values as the trained variables. When you run <code translate="no" dir="ltr">apply</code> to update the moving averages, each shadow variable is updated with the formula:</p> <p><code translate="no" dir="ltr">shadow_variable -= (1 - decay) * (shadow_variable - variable)</code></p> <p>This is mathematically equivalent to the classic formula below, but the use of an <code translate="no" dir="ltr">assign_sub</code> op (the <code translate="no" dir="ltr">"-="</code> in the formula) allows concurrent lockless updates to the variables:</p> <p><code translate="no" dir="ltr">shadow_variable = decay * shadow_variable + (1 - decay) * variable</code></p> <p>Reasonable values for <code translate="no" dir="ltr">decay</code> are close to 1.0, typically in the multiple-nines range: 0.999, 0.9999, etc.</p> <p>To have fine-grained control over the value of the decay parameter during training, pass a scalar <a href="../variable.html"><code translate="no" dir="ltr">tf.Variable</code></a> as the <code translate="no" dir="ltr">decay</code> value to the constructor, and update the variable as needed.</p> <p>Example usage when creating a training model:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># Create variables.
var0 = tf.Variable(...)
var1 = tf.Variable(...)
# ... use the variables to build a training model...

# Create an ExponentialMovingAverage object
ema = tf.train.ExponentialMovingAverage(decay=0.9999)

# The first `apply` creates the shadow variables that hold the moving averages
ema.apply([var0, var1])

# grab the moving averages for checkpointing purposes or to be able to
# load the moving averages into the model weights
averages = [ema.average(var0), ema.average(var1)]

...
def train_step(...):
...
  # Apply the optimizer.
  opt.minimize(my_loss, [var0, var1])

  # Update the moving averages
  # of var0 and var1 with additional calls to `apply`
  ema.apply([var0, var1])

...train the model by running train_step multiple times...
</pre> <p>There are several ways to use the moving averages for evaluations:</p> <ol> <li>Assign the values of the shadow variables to your model variables with <a href="../variable.html#assign"><code translate="no" dir="ltr">Variable.assign(...)</code></a> before evaluating your model. You can use the <code translate="no" dir="ltr">average()</code> method to get the shadow variable for a given variable. To continue training after using this approach, make sure to record the unaveraged weights and restore them before continuing to train. You can see the tensorflow-addons' MovingAverage optimizer's <code translate="no" dir="ltr">swap_weights</code> method for one example of how to swap variables efficiently in distributed settings: https://github.com/tensorflow/addons/blob/v0.13.0/tensorflow_addons/optimizers/moving_average.py#L151</li> <li>Make sure to checkpoint out your moving average variables in your <a href="checkpoint.html"><code translate="no" dir="ltr">tf.train.Checkpoint</code></a>. At evaluation time, create your shadow variables and use <a href="checkpoint.html"><code translate="no" dir="ltr">tf.train.Checkpoint</code></a> to restore the moving averages into the shadow variables. Then, load the moving averages into the actual model weights via <code translate="no" dir="ltr">var.assign(moving_avg)</code>.</li> <li>Checkpoint out your moving average variables in your <a href="checkpoint.html"><code translate="no" dir="ltr">tf.train.Checkpoint</code></a>. For evaluation, restore your model weights directly from the moving averages instead of from the non-averaged weights. Caution: If you choose this approach, include only the object-graph paths to the averaged path in your checkpoint restore. If you point both the unaveraged and averaged paths in a checkpoint restore to the same variables, it is hard to reason about whether your model will restore the averaged or non-averaged variables.</li> </ol> <p>Example of saving out then restoring the shadow variable values:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># Create variables.
var0 = tf.Variable(...)
var1 = tf.Variable(...)
# ... use the variables to build a training model...

# Create an ExponentialMovingAverage object, create the shadow variables,
# and grab the moving averages for checkpointing purposes.
# (The ExponentialMovingAverage object itself is not checkpointable)
ema = tf.train.ExponentialMovingAverage(decay=0.9999)
ema.apply([var0, var1])
avg_var0 = ema.average(var0)
avg_var1 = ema.average(var1)

# Create a Checkpoint that will manage the model weights and the averages,
checkpoint = tf.train.Checkpoint(model_weights=[var0, var1],
                                 averaged_weights=[avg_var0, avg_var1])
... # Do training

# Save out the checkpoint including the model weights and the moving averages
checkpoint.save(...)
</pre> <p>Restore option: restore all averaged &amp; non-averaged weights, then load moving averages into the model via <code translate="no" dir="ltr">var.assign()</code></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># Create variables.
var0 = tf.Variable(...)
var1 = tf.Variable(...)
# ... use the variables to build a training model...

# Create an ExponentialMovingAverage object, create the shadow variables,
# and grab the moving averages for checkpoint restore purposes.
# (The ExponentialMovingAverage object itself is not checkpointable)
ema = tf.train.ExponentialMovingAverage(decay=0.9999)
ema.apply([var0, var1])
avg_var0 = ema.average(var0)
avg_var1 = ema.average(var1)

# Create a Checkpoint that will manage the model weights and the averages,
checkpoint = tf.train.Checkpoint(model_weights=[var0, var1],
                                 averaged_weights=[avg_var0, avg_var1])
checkpoint.restore(...)
var0.assign(avg_var0)
var1.assign(avg_var1)
# var0 and var1 now hold the moving average values
</pre> <p>Restore option: Directly restore the moving averages into the model weights.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># Create variables.
var0 = tf.Variable(...)
var1 = tf.Variable(...)
# ... use the variables to build a training model...

# Create a Checkpoint that will manage two objects with trackable state,
checkpoint = tf.train.Checkpoint(averaged_weights=[var0, var1])
checkpoint.restore(...)
# var0 and var1 now hold the moving average values
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">decay</code> </td> <td> A scalar float value, <code translate="no" dir="ltr">Tensor</code>, or <code translate="no" dir="ltr">Variable</code>. The decay parameter. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_updates</code> </td> <td> Optional count of number of updates applied to variables. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">zero_debias</code> </td> <td> If <code translate="no" dir="ltr">True</code>, zero debias moving-averages that are initialized with tensors. (Note: moving averages may not be initialized with non-variable tensors when eager execution is enabled). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> String. Optional prefix name to use for the name of ops added in <code translate="no" dir="ltr">apply()</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> The name of this ExponentialMovingAverage object. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="apply" data-text="apply"><code translate="no" dir="ltr">apply</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/moving_averages.py#L491-L588">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
apply(
    var_list=None
)
</pre> <p>Maintains moving averages of variables.</p> <p><code translate="no" dir="ltr">var_list</code> must be a list of <code translate="no" dir="ltr">Variable</code> objects. This method creates shadow variables (holding the moving averages) for all elements of <code translate="no" dir="ltr">var_list</code>, and updates the moving averages using the current <code translate="no" dir="ltr">var_list</code> values. Shadow variables for <code translate="no" dir="ltr">Variable</code> objects are initialized to the variable's initial value.</p> <p>Shadow variables are created with <code translate="no" dir="ltr">trainable=False</code>. To access them you can use the EMA object's <code translate="no" dir="ltr">average</code> method. Note that <code translate="no" dir="ltr">EMA</code> objects are not trackable by checkpoints, so if you want to checkpoint or restore the moving variables you will need to manually grab the shadow variables via <code translate="no" dir="ltr">average()</code> and assign them as <a href="../module.html"><code translate="no" dir="ltr">tf.Module</code></a> properties or directly pass them to your <a href="checkpoint.html"><code translate="no" dir="ltr">tf.train.Checkpoint</code></a>.</p> <p>Note that <code translate="no" dir="ltr">apply()</code> can be called multiple times. When eager execution is enabled each call to apply will update the variables once, so this needs to be called in a loop.</p> <p>In legacy TF 1.x graphs, this method returns an op that updates all shadow variables from the current value of their associated variables. In TF 1.x graphs without automatically control dependencies this op needs to be manually run.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">var_list</code> </td> <td> A list of Variable objects. The variables must be of types bfloat16, float16, float32, or float64. (In legacy TF 1.x graphs these may be tensors, but this is unsupported when eager execution is enabled.) </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An Operation that updates the moving averages. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If the arguments are not an allowed type. </td> </tr> </table> <h3 id="average" data-text="average"><code translate="no" dir="ltr">average</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/moving_averages.py#L590-L600">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
average(
    var
)
</pre> <p>Returns the <code translate="no" dir="ltr">Variable</code> holding the average of <code translate="no" dir="ltr">var</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">var</code> </td> <td> A <code translate="no" dir="ltr">Variable</code> object. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <code translate="no" dir="ltr">Variable</code> object or <code translate="no" dir="ltr">None</code> if the moving average of <code translate="no" dir="ltr">var</code> is not maintained. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/train/ExponentialMovingAverage" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/train/ExponentialMovingAverage</a>
  </p>
</div>
