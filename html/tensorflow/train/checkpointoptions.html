<h1 class="devsite-page-title" tabindex="-1"> tf.train.CheckpointOptions </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.train.CheckpointOptions"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> </div>   <p>Options for constructing a Checkpoint.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases" tabindex="-1">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="checkpointoptions.html"><code translate="no" dir="ltr">tf.compat.v1.train.CheckpointOptions</code></a></p> </section> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.train.CheckpointOptions(
    experimental_io_device=None,
    experimental_enable_async_checkpoint=False,
    experimental_write_callbacks=None,
    enable_async=False,
    experimental_skip_slot_variables=False,
    experimental_sharding_callback=None
)
</pre></devsite-code>  <p>Used as the <code translate="no" dir="ltr">options</code> argument to either <a href="checkpoint.html#save"><code translate="no" dir="ltr">tf.train.Checkpoint.save()</code></a> or <a href="checkpoint.html#restore"><code translate="no" dir="ltr">tf.train.Checkpoint.restore()</code></a> methods to adjust how variables are saved/restored.</p> <p>Example: Run IO ops on "localhost" while saving a checkpoint:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">step = tf.Variable(0, name="step")
checkpoint = tf.train.Checkpoint(step=step)
options = tf.train.CheckpointOptions(experimental_io_device="/job:localhost")
checkpoint.save("/tmp/ckpt", options=options)
</pre></devsite-code>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">experimental_io_device</code> </td> <td> string. Applies in a distributed setting. Tensorflow device to use to access the filesystem. If <code translate="no" dir="ltr">None</code> (default) then for each variable the filesystem is accessed from the CPU:0 device of the host where that variable is assigned. If specified, the filesystem is instead accessed from that device for all variables. This is for example useful if you want to save to a local directory, such as "/tmp" when running in a distributed setting. In that case pass a device for the host where the "/tmp" directory is accessible. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_enable_async_checkpoint</code> </td> <td> bool Type. Deprecated, please use the enable_async option. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_write_callbacks</code> </td> <td> List[Callable]. A list of callback functions that will be executed after each saving event finishes (i.e. after <code translate="no" dir="ltr">save()</code> or <code translate="no" dir="ltr">write()</code>). For async checkpoint, the callbacks will be executed only after the async thread finishes saving. The return values of the callback(s) will be ignored. The callback(s) can optionally take the <code translate="no" dir="ltr">save_path</code> (the result of <code translate="no" dir="ltr">save()</code> or <code translate="no" dir="ltr">write()</code>) as an argument. The callbacks will be executed in the same order of this list after the checkpoint has been written. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">enable_async</code> </td> <td> bool Type. Indicates whether async checkpointing is enabled. Default is False, i.e., no async checkpoint. Async checkpoint moves the checkpoint file writing off the main thread, so that the model can continue to train while the checkpoing file writing runs in the background. Async checkpoint reduces TPU device idle cycles and speeds up model training process, while memory consumption may increase. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_skip_slot_variables</code> </td> <td> bool Type. If true, ignores slot variables during restore. Context: TPU Embedding layers for Serving do not properly restore slot variables. This option is a way to omit restoring slot variables which are not required for Serving usecase anyways.(b/315912101) </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_sharding_callback</code> </td> <td> <a href="experimental/shardingcallback.html"><code translate="no" dir="ltr">tf.train.experimental.ShardingCallback</code></a>. A pre-made or custom callback that determines how checkpoints are sharded on disk. Pre-made callback options are <code translate="no" dir="ltr">tf.train.experimental.ShardByDevicePolicy</code> and <a href="experimental/maxshardsizepolicy.html"><code translate="no" dir="ltr">tf.train.experimental.MaxShardSizePolicy</code></a>. You may also write a custom callback, see <a href="experimental/shardingcallback.html"><code translate="no" dir="ltr">tf.train.experimental.ShardingCallback</code></a>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">enable_async</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_enable_async_checkpoint</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_io_device</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_sharding_callback</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_skip_slot_variables</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_write_callbacks</code> </td> <td> 
</td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/train/CheckpointOptions" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/train/CheckpointOptions</a>
  </p>
</div>
