<h1 class="devsite-page-title">Module: tf.data.experimental.service</h1> <devsite-bookmark></devsite-bookmark>       <p>API for using the tf.data service.</p> <h4 id="this_module_contains" data-text="This module contains:">This module contains:</h4> <ol> <li>tf.data server implementations for running the tf.data service.</li> <li>APIs for registering datasets with the tf.data service and reading from the registered datasets.</li> </ol> <p>The tf.data service provides the following benefits:</p> <ul> <li>Horizontal scaling of tf.data input pipeline processing to solve input bottlenecks.</li> <li>Data coordination for distributed training. Coordinated reads enable all replicas to train on similar-length examples across each global training step, improving step times in synchronous training.</li> <li>Dynamic balancing of data across training replicas.</li> </ul> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split("://")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(
        dispatcher_address=dispatcher_address))
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode=tf.data.experimental.service.ShardingPolicy.OFF,
    service=dispatcher.target))
print(list(dataset.as_numpy_iterator()))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
</pre> <h2 id="setup" data-text="Setup">Setup</h2> <p>This section goes over how to set up the tf.data service.</p> <h3 id="run_tfdata_servers" data-text="Run tf.data servers">Run tf.data servers</h3> <p>The tf.data service consists of one dispatch server and <code translate="no" dir="ltr">n</code> worker servers. tf.data servers should be brought up alongside your training jobs, then brought down when the jobs are finished. Use <a href="service/dispatchserver.html"><code translate="no" dir="ltr">tf.data.experimental.service.DispatchServer</code></a> to start a dispatch server, and <a href="service/workerserver.html"><code translate="no" dir="ltr">tf.data.experimental.service.WorkerServer</code></a> to start worker servers. Servers can be run in the same process for testing purposes, or scaled up on separate machines.</p> <p>See <a href="https://github.com/tensorflow/ecosystem/tree/master/data_service">https://github.com/tensorflow/ecosystem/tree/master/data_service</a> for an example of using Google Kubernetes Engine (GKE) to manage the tf.data service. Note that the server implementation in <a href="https://github.com/tensorflow/ecosystem/blob/master/data_service/tf_std_data_server.py">tf_std_data_server.py</a> is not GKE-specific, and can be used to run the tf.data service in other contexts.</p> <h3 id="custom_ops" data-text="Custom ops">Custom ops</h3> <p>If your dataset uses custom ops, these ops need to be made available to tf.data servers by calling <a href="https://www.tensorflow.org/api_docs/python/tf/load_op_library">load_op_library</a> from the dispatcher and worker processes at startup.</p> <h2 id="usage" data-text="Usage">Usage</h2> <p>Users interact with tf.data service by programmatically registering their datasets with tf.data service, then creating datasets that read from the registered datasets. The <a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/service/register_dataset">register_dataset</a> function registers a dataset, then the <a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/service/from_dataset_id">from_dataset_id</a> function creates a new dataset which reads from the registered dataset. The <a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/service/distribute">distribute</a> function wraps <code translate="no" dir="ltr">register_dataset</code> and <code translate="no" dir="ltr">from_dataset_id</code> into a single convenient transformation which registers its input dataset and then reads from it. <code translate="no" dir="ltr">distribute</code> enables tf.data service to be used with a one-line code change. However, it assumes that the dataset is created and consumed by the same entity and this assumption might not always be valid or desirable. In particular, in certain scenarios, such as distributed training, it might be desirable to decouple the creation and consumption of the dataset (via <code translate="no" dir="ltr">register_dataset</code> and <code translate="no" dir="ltr">from_dataset_id</code> respectively) to avoid having to create the dataset on each of the training workers.</p> <h3 id="example" data-text="Example">Example</h3> <h4 id="distribute" data-text="distribute"><code translate="no" dir="ltr">distribute</code></h4> <p>To use the <code translate="no" dir="ltr">distribute</code> transformation, apply the transformation after the prefix of your input pipeline that you would like to be executed using tf.data service (typically at the end).</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">dataset = ...  # Define your dataset here.
# Move dataset processing from the local machine to the tf.data service
dataset = dataset.apply(
    tf.data.experimental.service.distribute(
        processing_mode=tf.data.experimental.service.ShardingPolicy.OFF,
        service=FLAGS.tf_data_service_address,
        job_name="shared_job"))
# Any transformations added after `distribute` will be run on the local machine.
dataset = dataset.prefetch(1)
</pre> <p>The above code will create a tf.data service "job", which iterates through the dataset to generate data. To share the data from a job across multiple clients (e.g. when using TPUStrategy or MultiWorkerMirroredStrategy), set a common <code translate="no" dir="ltr">job_name</code> across all clients.</p> <h4 id="register_dataset_and_from_dataset_id" data-text="register_dataset and from_dataset_id">
<code translate="no" dir="ltr">register_dataset</code> and <code translate="no" dir="ltr">from_dataset_id</code>
</h4> <p><code translate="no" dir="ltr">register_dataset</code> registers a dataset with the tf.data service, returning a dataset id for the registered dataset. <code translate="no" dir="ltr">from_dataset_id</code> creates a dataset that reads from the registered dataset. These APIs can be used to reduce dataset building time for distributed training. Instead of building the dataset on all training workers, we can build the dataset just once and then register the dataset using <code translate="no" dir="ltr">register_dataset</code>. Then all workers can call <code translate="no" dir="ltr">from_dataset_id</code> without needing to build the dataset themselves.</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">dataset = ...  # Define your dataset here.
dataset_id = tf.data.experimental.service.register_dataset(
    service=FLAGS.tf_data_service_address,
    dataset=dataset)
# Use `from_dataset_id` to create per-worker datasets.
per_worker_datasets = {}
for worker in workers:
  per_worker_datasets[worker] = tf.data.experimental.service.from_dataset_id(
      processing_mode=tf.data.experimental.service.ShardingPolicy.OFF,
      service=FLAGS.tf_data_service_address,
      dataset_id=dataset_id,
      job_name="shared_job")
</pre> <h3 id="processing_modes" data-text="Processing Modes">Processing Modes</h3> <p><code translate="no" dir="ltr">processing_mode</code> specifies how to shard a dataset among tf.data service workers. tf.data service supports <code translate="no" dir="ltr">OFF</code>, <code translate="no" dir="ltr">DYNAMIC</code>, <code translate="no" dir="ltr">FILE</code>, <code translate="no" dir="ltr">DATA</code>, <code translate="no" dir="ltr">FILE_OR_DATA</code>, <code translate="no" dir="ltr">HINT</code> sharding policies.</p> <p>OFF: No sharding will be performed. The entire input dataset will be processed independently by each of the tf.data service workers. For this reason, it is important to shuffle data (e.g. filenames) non-deterministically, so that each worker will process the elements of the dataset in a different order. This mode can be used to distribute datasets that aren't splittable.</p> <p>If a worker is added or restarted during ShardingPolicy.OFF processing, the worker will instantiate a new copy of the dataset and begin producing data from the beginning.</p> <h4 id="dynamic_sharding" data-text="Dynamic Sharding">Dynamic Sharding</h4> <p>DYNAMIC: In this mode, tf.data service divides the dataset into two components: a source component that generates "splits" such as filenames, and a processing component that takes splits and outputs dataset elements. The source component is executed in a centralized fashion by the tf.data service dispatcher, which generates different splits of input data. The processing component is executed in a parallel fashion by the tf.data service workers, each operating on a different set of input data splits.</p> <p>For example, consider the following dataset:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">dataset = tf.data.Dataset.from_tensor_slices(filenames)
dataset = dataset.interleave(TFRecordDataset)
dataset = dataset.map(preprocess_fn)
dataset = dataset.batch(batch_size)
dataset = dataset.apply(
    tf.data.experimental.service.distribute(
        processing_mode=tf.data.experimental.service.ShardingPolicy.DYNAMIC,
        ...))
</pre> <p>The <code translate="no" dir="ltr">from_tensor_slices</code> will be run on the dispatcher, while the <code translate="no" dir="ltr">interleave</code>, <code translate="no" dir="ltr">map</code>, and <code translate="no" dir="ltr">batch</code> will be run on tf.data service workers. The workers will pull filenames from the dispatcher for processing. To process a dataset with dynamic sharding, the dataset must have a splittable source, and all of its transformations must be compatible with splitting. While most sources and transformations support splitting, there are exceptions, such as custom datasets which may not implement the splitting API. Please file a Github issue if you would like to use distributed epoch processing for a currently unsupported dataset source or transformation.</p> <p>If no workers are restarted during training, dynamic sharding mode will visit every example exactly once. If workers are restarted during training, the splits they were processing will not be fully visited. The dispatcher maintains a cursor through the dataset's splits. Assuming fault tolerance is enabled (See "Fault Tolerance" below), the dispatcher will store cursor state in write-ahead logs so that the cursor can be restored in case the dispatcher is restarted mid-training. This provides an at-most-once visitation guarantee in the presence of server restarts.</p> <h4 id="static_sharding" data-text="Static Sharding">Static Sharding</h4> <p>The following are static sharding policies. The semantics are similar to <a href="autoshardpolicy.html"><code translate="no" dir="ltr">tf.data.experimental.AutoShardPolicy</code></a>. These policies require:</p> <ul> <li>The tf.data service cluster is configured with a fixed list of workers in DispatcherConfig.</li> <li>Each client only reads from the local tf.data service worker.</li> </ul> <p>If a worker is restarted while performing static sharding, the worker will begin processing its shard again from the beginning.</p> <p>FILE: Shards by input files (i.e. each worker will get a fixed set of files to process). When this option is selected, make sure that there is at least as many files as workers. If there are fewer input files than workers, a runtime error will be raised.</p> <p>DATA: Shards by elements produced by the dataset. Each worker will process the whole dataset and discard the portion that is not for itself. Note that for this mode to correctly partition the dataset elements, the dataset needs to produce elements in a deterministic order.</p> <p>FILE_OR_DATA: Attempts FILE-based sharding, falling back to DATA-based sharding on failure.</p> <p>HINT: Looks for the presence of <code translate="no" dir="ltr">shard(SHARD_HINT, ...)</code> which is treated as a placeholder to replace with <code translate="no" dir="ltr">shard(num_workers, worker_index)</code>.</p> <p>For backwards compatibility, <code translate="no" dir="ltr">processing_mode</code> may also be set to the strings <code translate="no" dir="ltr">"parallel_epochs"</code> or <code translate="no" dir="ltr">"distributed_epoch"</code>, which are respectively equivalent to <a href="service/shardingpolicy.html#OFF"><code translate="no" dir="ltr">ShardingPolicy.OFF</code></a> and <a href="service/shardingpolicy.html#DYNAMIC"><code translate="no" dir="ltr">ShardingPolicy.DYNAMIC</code></a>.</p> <h3 id="coordinated_data_read" data-text="Coordinated Data Read">Coordinated Data Read</h3> <p>By default, when multiple consumers read from the same job, they receive data on a first-come first-served basis. In some use cases, it is advantageous to coordinate the consumers. At each step, consumers read data from the same worker.</p> <p>For example, the tf.data service can be used to coordinate example sizes across a cluster during synchronous training, so that during each step all replicas train on similar-sized elements. To achieve this, define a dataset which generates rounds of <code translate="no" dir="ltr">num_consumers</code> consecutive similar-sized batches, then enable coordinated reads by setting <code translate="no" dir="ltr">consumer_index</code> and <code translate="no" dir="ltr">num_consumers</code>.</p> <blockquote class="note">
<strong>Note:</strong><span> To keep consumers in sync, coordinated reads require that the dataset have infinite cardinality. You can get this by adding <code translate="no" dir="ltr">.repeat()</code> at the end of the dataset definition.</span>
</blockquote> <h3 id="jobs" data-text="Jobs">Jobs</h3> <p>A tf.data service "job" refers to the process of reading from a dataset managed by the tf.data service, using one or more data consumers. Jobs are created when iterating over datasets that read from tf.data service. The data produced by a job is determined by (1) dataset associated with the job and (2) the job's processing mode. For example, if a job is created for the dataset <a href="../dataset.html#range"><code translate="no" dir="ltr">Dataset.range(5)</code></a>, and the processing mode is <a href="service/shardingpolicy.html#OFF"><code translate="no" dir="ltr">ShardingPolicy.OFF</code></a>, each tf.data worker will produce the elements <code translate="no" dir="ltr">{0, 1, 2, 3, 4}</code> for the job, resulting in the job producing <code translate="no" dir="ltr">5 * num_workers</code> elements. If the processing mode is <a href="service/shardingpolicy.html#DYNAMIC"><code translate="no" dir="ltr">ShardingPolicy.DYNAMIC</code></a>, the job will only produce <code translate="no" dir="ltr">5</code> elements.</p> <p>One or more consumers can consume data from a job. By default, jobs are "anonymous", meaning that only the consumer which created the job can read from it. To share the output of a job across multiple consumers, you can set a common <code translate="no" dir="ltr">job_name</code>.</p> <h3 id="fault_tolerance" data-text="Fault Tolerance">Fault Tolerance</h3> <p>By default, the tf.data dispatch server stores its state in-memory, making it a single point of failure during training. To avoid this, pass <code translate="no" dir="ltr">fault_tolerant_mode=True</code> when creating your <code translate="no" dir="ltr">DispatchServer</code>. Dispatcher fault tolerance requires <code translate="no" dir="ltr">work_dir</code> to be configured and accessible from the dispatcher both before and after restart (e.g. a GCS path). With fault tolerant mode enabled, the dispatcher will journal its state to the work directory so that no state is lost when the dispatcher is restarted.</p> <p>WorkerServers may be freely restarted, added, or removed during training. At startup, workers will register with the dispatcher and begin processing all outstanding jobs from the beginning.</p> <h3 id="usage_with_tfdistribute" data-text="Usage with tf.distribute">Usage with tf.distribute</h3> <p>tf.distribute is the TensorFlow API for distributed training. There are several ways to use tf.data with tf.distribute: <code translate="no" dir="ltr">strategy.experimental_distribute_dataset</code>, <code translate="no" dir="ltr">strategy.distribute_datasets_from_function</code>, and (for PSStrategy) <code translate="no" dir="ltr">coordinator.create_per_worker_dataset</code>. The following sections give code examples for each.</p> <p>In general we recommend using <code translate="no" dir="ltr">tf.data.experimental.service.{register_dataset,from_dataset_id}</code> over <a href="service/distribute.html"><code translate="no" dir="ltr">tf.data.experimental.service.distribute</code></a> for two reasons:</p> <ul> <li>The dataset only needs to be constructed and optimized once, instead of once per worker. This can significantly reduce startup time, because the current <code translate="no" dir="ltr">experimental_distribute_dataset</code> and <code translate="no" dir="ltr">distribute_datasets_from_function</code> implementations create and optimize worker datasets sequentially.</li> <li>If a dataset depends on lookup tables or variables that are only present on one host, the dataset needs to be registered from that host. Typically this only happens when resources are placed on the chief or worker 0. Registering the dataset from the chief will avoid issues with depending on remote resources.</li> </ul> <h4 id="strategyexperimental_distribute_dataset" data-text="strategy.experimental_distribute_dataset">strategy.experimental_distribute_dataset</h4> <p>Nothing special is required when using <code translate="no" dir="ltr">strategy.experimental_distribute_dataset</code>, just apply <code translate="no" dir="ltr">register_dataset</code> and <code translate="no" dir="ltr">from_dataset_id</code> as above, making sure to specify a <code translate="no" dir="ltr">job_name</code> so that all workers consume from the same tf.data service job.</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">dataset = ...  # Define your dataset here.
dataset_id = tf.data.experimental.service.register_dataset(
    service=FLAGS.tf_data_service_address,
    dataset=dataset)
dataset = tf.data.experimental.service.from_dataset_id(
    processing_mode=tf.data.experimental.service.ShardingPolicy.OFF,
    service=FLAGS.tf_data_service_address,
    dataset_id=dataset_id,
    job_name="shared_job")

dataset = strategy.experimental_distribute_dataset(dataset)
</pre> <h4 id="strategydistribute_datasets_from_function" data-text="strategy.distribute_datasets_from_function">strategy.distribute_datasets_from_function</h4> <p>First, make sure the dataset produced by the <code translate="no" dir="ltr">dataset_fn</code> does not depend on the <code translate="no" dir="ltr">input_context</code> for the training worker on which it is run. Instead of each worker building its own (sharded) dataset, one worker should register an unsharded dataset, and the remaining workers should consume data from that dataset.</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">dataset = dataset_fn()
dataset_id = tf.data.experimental.service.register_dataset(
    service=FLAGS.tf_data_service_address,
    dataset=dataset)

def new_dataset_fn(input_context):
  del input_context
  return tf.data.experimental.service.from_dataset_id(
      processing_mode=tf.data.experimental.service.ShardingPolicy.OFF,
      service=FLAGS.tf_data_service_address,
      dataset_id=dataset_id,
      job_name="shared_job")

dataset = strategy.distribute_datasets_from_function(new_dataset_fn)
</pre> <h4 id="coordinatorcreate_per_worker_dataset" data-text="coordinator.create_per_worker_dataset">coordinator.create_per_worker_dataset</h4> <p><code translate="no" dir="ltr">create_per_worker_dataset</code> works the same as <code translate="no" dir="ltr">distribute_datasets_from_function</code>.</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">dataset = dataset_fn()
dataset_id = tf.data.experimental.service.register_dataset(
    service=FLAGS.tf_data_service_address,
    dataset=dataset)

def new_dataset_fn(input_context):
  del input_context
  return tf.data.experimental.service.from_dataset_id(
      processing_mode=tf.data.experimental.service.ShardingPolicy.OFF,
      service=FLAGS.tf_data_service_address,
      dataset_id=dataset_id,
      job_name="shared_job")

dataset = coordinator.create_per_worker_dataset(new_dataset_fn)
</pre> <h2 id="limitations" data-text="Limitations">Limitations</h2> <ul> <li>Python-based data processing: Datasets which use Python-based data processing (e.g. <a href="../../py_function.html"><code translate="no" dir="ltr">tf.py_function</code></a>, <a href="../../numpy_function.html"><code translate="no" dir="ltr">tf.numpy_function</code></a>, or <a href="../dataset.html#from_generator"><code translate="no" dir="ltr">tf.data.Dataset.from_generator</code></a>) are currently not supported.</li> <li>Non-Serializable Resources: Datasets may only depend on TF resources that support serialization. Serialization is currently supported for lookup tables and variables. If your dataset depends on a TF resource that cannot be serialized, please file a Github issue.</li> <li>Remote Resources: If a dataset depends on a resource, the dataset must be registered from the same process that created the resource (e.g. the "chief" job of ParameterServerStrategy).</li> </ul> <h2 id="classes" data-text="Classes">Classes</h2> <p><a href="service/dispatchserver.html"><code translate="no" dir="ltr">class DispatchServer</code></a>: An in-process tf.data service dispatch server.</p> <p><a href="service/dispatcherconfig.html"><code translate="no" dir="ltr">class DispatcherConfig</code></a>: Configuration class for tf.data service dispatchers.</p> <p><a href="service/shardingpolicy.html"><code translate="no" dir="ltr">class ShardingPolicy</code></a>: Specifies how to shard data among tf.data service workers.</p> <p><a href="service/workerconfig.html"><code translate="no" dir="ltr">class WorkerConfig</code></a>: Configuration class for tf.data service dispatchers.</p> <p><a href="service/workerserver.html"><code translate="no" dir="ltr">class WorkerServer</code></a>: An in-process tf.data service worker server.</p> <h2 id="functions" data-text="Functions">Functions</h2> <p><a href="service/distribute.html"><code translate="no" dir="ltr">distribute(...)</code></a>: A transformation that moves dataset processing to the tf.data service.</p> <p><a href="service/from_dataset_id.html"><code translate="no" dir="ltr">from_dataset_id(...)</code></a>: Creates a dataset which reads data from the tf.data service.</p> <p><a href="service/register_dataset.html"><code translate="no" dir="ltr">register_dataset(...)</code></a>: Registers a dataset with the tf.data service.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/data/experimental/service" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/data/experimental/service</a>
  </p>
</div>
