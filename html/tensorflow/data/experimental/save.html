<h1 class="devsite-page-title">tf.data.experimental.save</h1> <devsite-bookmark></devsite-bookmark>       <p>Saves the content of the given dataset.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.data.experimental.save(
    dataset, path, compression=None, shard_func=None, checkpoint_args=None
)
</pre>  <h4 id="example_usage" data-text="Example usage:">Example usage:</h4> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
import tempfile
path = os.path.join(tempfile.gettempdir(), "saved_data")
# Save a dataset
dataset = tf.data.Dataset.range(2)
tf.data.experimental.save(dataset, path)
new_dataset = tf.data.experimental.load(path)
for elem in new_dataset:
  print(elem)
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)
</pre> <p>The saved dataset is saved in multiple file "shards". By default, the dataset output is divided to shards in a round-robin fashion but custom sharding can be specified via the <code translate="no" dir="ltr">shard_func</code> function. For example, you can save the dataset to using a single shard as follows:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">dataset = make_dataset()
def custom_shard_func(element):
  return 0
dataset = tf.data.experimental.save(
    path="/path/to/data", ..., shard_func=custom_shard_func)
</pre> <p>To enable checkpointing, pass in <code translate="no" dir="ltr">checkpoint_args</code> to the <code translate="no" dir="ltr">save</code> method as follows:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">dataset = tf.data.Dataset.range(100)
save_dir = "..."
checkpoint_prefix = "..."
step_counter = tf.Variable(0, trainable=False)
checkpoint_args = {
  "checkpoint_interval": 50,
  "step_counter": step_counter,
  "directory": checkpoint_prefix,
  "max_to_keep": 20,
}
dataset.save(dataset, save_dir, checkpoint_args=checkpoint_args)
</pre>
<blockquote class="note">
<strong>Note:</strong><span> The directory layout and file format used for saving the dataset is considered an implementation detail and may change. For this reason, datasets saved through <a href="save.html"><code translate="no" dir="ltr">tf.data.experimental.save</code></a> should only be consumed through <a href="load.html"><code translate="no" dir="ltr">tf.data.experimental.load</code></a>, which is guaranteed to be backwards compatible.</span>
</blockquote>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">dataset</code> </td> <td> The dataset to save. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">path</code> </td> <td> Required. A directory to use for saving the dataset. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">compression</code> </td> <td> Optional. The algorithm to use to compress data when writing it. Supported options are <code translate="no" dir="ltr">GZIP</code> and <code translate="no" dir="ltr">NONE</code>. Defaults to <code translate="no" dir="ltr">NONE</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">shard_func</code> </td> <td> Optional. A function to control the mapping of dataset elements to file shards. The function is expected to map elements of the input dataset to int64 shard IDs. If present, the function will be traced and executed as graph computation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">checkpoint_args</code> </td> <td> Optional args for checkpointing which will be passed into the <a href="../../train/checkpointmanager.html"><code translate="no" dir="ltr">tf.train.CheckpointManager</code></a>. If <code translate="no" dir="ltr">checkpoint_args</code> are not specified, then checkpointing will not be performed. The <code translate="no" dir="ltr">save()</code> implementation creates a <a href="../../train/checkpoint.html"><code translate="no" dir="ltr">tf.train.Checkpoint</code></a> object internally, so users should not set the <code translate="no" dir="ltr">checkpoint</code> argument in <code translate="no" dir="ltr">checkpoint_args</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> <tr class="alt"> <td colspan="2"> ValueError if <code translate="no" dir="ltr">checkpoint</code> is passed into <code translate="no" dir="ltr">checkpoint_args</code>. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/data/experimental/save" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/data/experimental/save</a>
  </p>
</div>
