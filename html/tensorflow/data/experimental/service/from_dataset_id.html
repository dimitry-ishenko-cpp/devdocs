<h1 class="devsite-page-title">tf.data.experimental.service.from_dataset_id</h1> <devsite-bookmark></devsite-bookmark>       <p>Creates a dataset which reads data from the tf.data service.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/service/from_dataset_id"><code translate="no" dir="ltr">tf.compat.v1.data.experimental.service.from_dataset_id</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.data.experimental.service.from_dataset_id(
    processing_mode,
    service,
    dataset_id,
    element_spec=None,
    job_name=None,
    consumer_index=None,
    num_consumers=None,
    max_outstanding_requests=None,
    data_transfer_protocol=None,
    target_workers='AUTO'
)
</pre>  <p>This is useful when the dataset is registered by one process, then used in another process. When the same process is both registering and reading from the dataset, it is simpler to use <a href="distribute.html"><code translate="no" dir="ltr">tf.data.experimental.service.distribute</code></a> instead.</p> <p>Before using <code translate="no" dir="ltr">from_dataset_id</code>, the dataset must have been registered with the tf.data service using <a href="register_dataset.html"><code translate="no" dir="ltr">tf.data.experimental.service.register_dataset</code></a>. <code translate="no" dir="ltr">register_dataset</code> returns a dataset id for the registered dataset. That is the <code translate="no" dir="ltr">dataset_id</code> which should be passed to <code translate="no" dir="ltr">from_dataset_id</code>.</p> <p>The <code translate="no" dir="ltr">element_spec</code> argument indicates the <a href="../../../typespec.html"><code translate="no" dir="ltr">tf.TypeSpec</code></a>s for the elements produced by the dataset. Currently <code translate="no" dir="ltr">element_spec</code> must be explicitly specified, and match the dataset registered under <code translate="no" dir="ltr">dataset_id</code>. <code translate="no" dir="ltr">element_spec</code> defaults to <code translate="no" dir="ltr">None</code> so that in the future we can support automatically discovering the <code translate="no" dir="ltr">element_spec</code> by querying the tf.data service.</p> <p><a href="distribute.html"><code translate="no" dir="ltr">tf.data.experimental.service.distribute</code></a> is a convenience method which combines <code translate="no" dir="ltr">register_dataset</code> and <code translate="no" dir="ltr">from_dataset_id</code> into a dataset transformation. See the documentation for <a href="distribute.html"><code translate="no" dir="ltr">tf.data.experimental.service.distribute</code></a> for more detail about how <code translate="no" dir="ltr">from_dataset_id</code> works.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split("://")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(
        dispatcher_address=dispatcher_address))
dataset = tf.data.Dataset.range(10)
dataset_id = tf.data.experimental.service.register_dataset(
    dispatcher.target, dataset)
dataset = tf.data.experimental.service.from_dataset_id(
    processing_mode="parallel_epochs",
    service=dispatcher.target,
    dataset_id=dataset_id,
    element_spec=dataset.element_spec)
print(list(dataset.as_numpy_iterator()))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">processing_mode</code> </td> <td> A <a href="shardingpolicy.html"><code translate="no" dir="ltr">tf.data.experimental.service.ShardingPolicy</code></a> specifying how to shard the dataset among tf.data workers. See <a href="shardingpolicy.html"><code translate="no" dir="ltr">tf.data.experimental.service.ShardingPolicy</code></a> for details. For backwards compatibility, <code translate="no" dir="ltr">processing_mode</code> may also be set to the strings <code translate="no" dir="ltr">"parallel_epochs"</code> or <code translate="no" dir="ltr">"distributed_epoch"</code>, which are respectively equivalent to <a href="shardingpolicy.html#OFF"><code translate="no" dir="ltr">ShardingPolicy.OFF</code></a> and <a href="shardingpolicy.html#DYNAMIC"><code translate="no" dir="ltr">ShardingPolicy.DYNAMIC</code></a>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">service</code> </td> <td> A string or a tuple indicating how to connect to the tf.data service. If it's a string, it should be in the format <code translate="no" dir="ltr">[&lt;protocol&gt;://]&lt;address&gt;</code>, where <code translate="no" dir="ltr">&lt;address&gt;</code> identifies the dispatcher address and <code translate="no" dir="ltr">&lt;protocol&gt;</code> can optionally be used to override the default protocol to use. If it's a tuple, it should be (protocol, address). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dataset_id</code> </td> <td> The id of the dataset to read from. This id is returned by <code translate="no" dir="ltr">register_dataset</code> when the dataset is registered with the tf.data service. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">element_spec</code> </td> <td> A nested structure of <a href="../../../typespec.html"><code translate="no" dir="ltr">tf.TypeSpec</code></a>s representing the type of elements produced by the dataset. This argument is only required inside a tf.function. Use <a href="../../dataset.html#element_spec"><code translate="no" dir="ltr">tf.data.Dataset.element_spec</code></a> to get the element spec for a given dataset. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">job_name</code> </td> <td> (Optional.) The name of the job. If provided, it must be a non-empty string. This argument makes it possible for multiple datasets to share the same job. The default behavior is that the dataset creates anonymous, exclusively owned jobs. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">consumer_index</code> </td> <td> (Optional.) The index of the consumer in the range from <code translate="no" dir="ltr">0</code> to <code translate="no" dir="ltr">num_consumers</code>. Must be specified alongside <code translate="no" dir="ltr">num_consumers</code>. When specified, consumers will read from the job in a strict round-robin order, instead of the default first-come-first-served order. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_consumers</code> </td> <td> (Optional.) The number of consumers which will consume from the job. Must be specified alongside <code translate="no" dir="ltr">consumer_index</code>. When specified, consumers will read from the job in a strict round-robin order, instead of the default first-come-first-served order. When <code translate="no" dir="ltr">num_consumers</code> is specified, the dataset must have infinite cardinality to prevent a producer from running out of data early and causing consumers to go out of sync. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">max_outstanding_requests</code> </td> <td> (Optional.) A limit on how many elements may be requested at the same time. You can use this option to control the amount of memory used, since <code translate="no" dir="ltr">distribute</code> won't use more than <code translate="no" dir="ltr">element_size</code> * <code translate="no" dir="ltr">max_outstanding_requests</code> of memory. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">data_transfer_protocol</code> </td> <td> (Optional.) The protocol to use for transferring data with the tf.data service. By default, data is transferred using gRPC. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">target_workers</code> </td> <td> (Optional.) Which workers to read from. If <code translate="no" dir="ltr">"AUTO"</code>, tf.data runtime decides which workers to read from. If <code translate="no" dir="ltr">"ANY"</code>, reads from any tf.data service workers. If <code translate="no" dir="ltr">"LOCAL"</code>, only reads from local in-processs tf.data service workers. <code translate="no" dir="ltr">"AUTO"</code> works well for most cases, while users can specify other targets. For example, <code translate="no" dir="ltr">"LOCAL"</code> helps avoid RPCs and data copy if every TF worker colocates with a tf.data service worker. Consumers of a shared job must use the same <code translate="no" dir="ltr">target_workers</code>. Defaults to <code translate="no" dir="ltr">"AUTO"</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <a href="../../dataset.html"><code translate="no" dir="ltr">tf.data.Dataset</code></a> which reads from the tf.data service. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/data/experimental/service/from_dataset_id" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/data/experimental/service/from_dataset_id</a>
  </p>
</div>
