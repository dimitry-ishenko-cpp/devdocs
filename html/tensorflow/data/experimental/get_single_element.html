<h1 class="devsite-page-title">tf.data.experimental.get_single_element</h1> <devsite-bookmark></devsite-bookmark>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/data/experimental/ops/get_single_element.py#L21-L147">  View source on GitHub </a> </td> </table> <p>Returns the single element of the <code translate="no" dir="ltr">dataset</code> as a nested structure of tensors. (deprecated)</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/get_single_element"><code translate="no" dir="ltr">tf.compat.v1.data.experimental.get_single_element</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.data.experimental.get_single_element(
    dataset
)
</pre>  <aside class="deprecated"><strong>Deprecated:</strong><span> THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use <a href="../dataset.html#get_single_element"><code translate="no" dir="ltr">tf.data.Dataset.get_single_element()</code></a>.</span></aside> <p>The function enables you to use a <a href="../dataset.html"><code translate="no" dir="ltr">tf.data.Dataset</code></a> in a stateless "tensor-in tensor-out" expression, without creating an iterator. This facilitates the ease of data transformation on tensors using the optimized <a href="../dataset.html"><code translate="no" dir="ltr">tf.data.Dataset</code></a> abstraction on top of them.</p> <p>For example, lets consider a <code translate="no" dir="ltr">preprocessing_fn</code> which would take as an input the raw features and returns the processed feature along with it's label.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">def preprocessing_fn(raw_feature):
  # ... the raw_feature is preprocessed as per the use-case
  return feature

raw_features = ...  # input batch of BATCH_SIZE elements.
dataset = (tf.data.Dataset.from_tensor_slices(raw_features)
           .map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)
           .batch(BATCH_SIZE))

processed_features = tf.data.experimental.get_single_element(dataset)
</pre> <p>In the above example, the <code translate="no" dir="ltr">raw_features</code> tensor of length=BATCH_SIZE was converted to a <a href="../dataset.html"><code translate="no" dir="ltr">tf.data.Dataset</code></a>. Next, each of the <code translate="no" dir="ltr">raw_feature</code> was mapped using the <code translate="no" dir="ltr">preprocessing_fn</code> and the processed features were grouped into a single batch. The final <code translate="no" dir="ltr">dataset</code> contains only one element which is a batch of all the processed features.</p> <blockquote class="note">
<strong>Note:</strong><span> The <code translate="no" dir="ltr">dataset</code> should contain only one element.</span>
</blockquote> <p>Now, instead of creating an iterator for the <code translate="no" dir="ltr">dataset</code> and retrieving the batch of features, the <a href="get_single_element.html"><code translate="no" dir="ltr">tf.data.experimental.get_single_element()</code></a> function is used to skip the iterator creation process and directly output the batch of features.</p> <p>This can be particularly useful when your tensor transformations are expressed as <a href="../dataset.html"><code translate="no" dir="ltr">tf.data.Dataset</code></a> operations, and you want to use those transformations while serving your model.</p> <h1 id="keras" class="page-title" data-text="Keras">Keras</h1> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">
model = ... # A pre-built or custom model

class PreprocessingModel(tf.keras.Model):
  def __init__(self, model):
    super().__init__(self)
    self.model = model

  @tf.function(input_signature=[...])
  def serving_fn(self, data):
    ds = tf.data.Dataset.from_tensor_slices(data)
    ds = ds.map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)
    ds = ds.batch(batch_size=BATCH_SIZE)
    return tf.argmax(
      self.model(tf.data.experimental.get_single_element(ds)),
      axis=-1
    )

preprocessing_model = PreprocessingModel(model)
your_exported_model_dir = ... # save the model to this path.
tf.saved_model.save(preprocessing_model, your_exported_model_dir,
              signatures={'serving_default': preprocessing_model.serving_fn})
</pre> <h1 id="estimator" class="page-title" data-text="Estimator">Estimator</h1> <p>In the case of estimators, you need to generally define a <code translate="no" dir="ltr">serving_input_fn</code> which would require the features to be processed by the model while inferencing.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">def serving_input_fn():

  raw_feature_spec = ... # Spec for the raw_features
  input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
      raw_feature_spec, default_batch_size=None)
  )
  serving_input_receiver = input_fn()
  raw_features = serving_input_receiver.features

  def preprocessing_fn(raw_feature):
    # ... the raw_feature is preprocessed as per the use-case
    return feature

  dataset = (tf.data.Dataset.from_tensor_slices(raw_features)
            .map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)
            .batch(BATCH_SIZE))

  processed_features = tf.data.experimental.get_single_element(dataset)

  # Please note that the value of `BATCH_SIZE` should be equal to
  # the size of the leading dimension of `raw_features`. This ensures
  # that `dataset` has only element, which is a pre-requisite for
  # using `tf.data.experimental.get_single_element(dataset)`.

  return tf.estimator.export.ServingInputReceiver(
      processed_features, serving_input_receiver.receiver_tensors)

estimator = ... # A pre-built or custom estimator
estimator.export_saved_model(your_exported_model_dir, serving_input_fn)
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">dataset</code> </td> <td> A <a href="../dataset.html"><code translate="no" dir="ltr">tf.data.Dataset</code></a> object containing a single element. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A nested structure of <a href="../../tensor.html"><code translate="no" dir="ltr">tf.Tensor</code></a> objects, corresponding to the single element of <code translate="no" dir="ltr">dataset</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if <code translate="no" dir="ltr">dataset</code> is not a <a href="../dataset.html"><code translate="no" dir="ltr">tf.data.Dataset</code></a> object. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">InvalidArgumentError</code> </td> <td> (at runtime) if <code translate="no" dir="ltr">dataset</code> does not contain exactly one element. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/data/experimental/get_single_element" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/data/experimental/get_single_element</a>
  </p>
</div>
