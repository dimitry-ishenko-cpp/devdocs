<h1 class="devsite-page-title" tabindex="-1"> tf.experimental.tensorrt.Converter </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.experimental.tensorrt.Converter"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="build"> <meta itemprop="property" content="convert"> <meta itemprop="property" content="save"> <meta itemprop="property" content="summary"> </div>   <p>An offline converter for TF-TRT transformation for TF 2.0 SavedModels.</p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.experimental.tensorrt.Converter(
    input_saved_model_dir=None,
    input_saved_model_tags=None,
    input_saved_model_signature_key=None,
    use_dynamic_shape=None,
    dynamic_shape_profile_strategy=None,
    max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
    precision_mode=TrtPrecisionMode.FP32,
    minimum_segment_size=3,
    maximum_cached_engines=1,
    use_calibration=True,
    allow_build_at_runtime=True,
    conversion_params=None
)
</pre></devsite-code>  <p>Windows support is provided experimentally. No guarantee is made regarding functionality or engineering support. Use at your own risk.</p> <p>There are several ways to run the conversion:</p> <ol> <li>
<p>FP32/FP16 precision</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">params = tf.experimental.tensorrt.ConversionParams(
    precision_mode='FP16')
converter = tf.experimental.tensorrt.Converter(
    input_saved_model_dir="my_dir", conversion_params=params)
converter.convert()
converter.save(output_saved_model_dir)
</pre></devsite-code> <p>In this case, no TRT engines will be built or saved in the converted SavedModel. But if input data is available during conversion, we can still build and save the TRT engines to reduce the cost during inference (see option 2 below).</p>
</li> <li>
<p>FP32/FP16 precision with pre-built engines</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">params = tf.experimental.tensorrt.ConversionParams(
    precision_mode='FP16',
    # Set this to a large enough number so it can cache all the engines.
    maximum_cached_engines=16)
converter = tf.experimental.tensorrt.Converter(
    input_saved_model_dir="my_dir", conversion_params=params)
converter.convert()

# Define a generator function that yields input data, and use it to execute
# the graph to build TRT engines.
def my_input_fn():
  for _ in range(num_runs):
    inp1, inp2 = ...
    yield inp1, inp2

converter.build(input_fn=my_input_fn)  # Generate corresponding TRT engines
converter.save(output_saved_model_dir)  # Generated engines will be saved.
</pre></devsite-code> <p>In this way, one engine will be built/saved for each unique input shapes of the TRTEngineOp. This is good for applications that cannot afford building engines during inference but have access to input data that is similar to the one used in production (for example, that has the same input shapes). Also, the generated TRT engines is platform dependent, so we need to run <code translate="no" dir="ltr">build()</code> in an environment that is similar to production (e.g. with same type of GPU).</p>
</li> <li>
<p>INT8 precision and calibration with pre-built engines</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">params = tf.experimental.tensorrt.ConversionParams(
    precision_mode='INT8',
    # Currently only one INT8 engine is supported in this mode.
    maximum_cached_engines=1,
    use_calibration=True)
converter = tf.experimental.tensorrt.Converter(
    input_saved_model_dir="my_dir", conversion_params=params)

# Define a generator function that yields input data, and run INT8
# calibration with the data. All input data should have the same shape.
# At the end of convert(), the calibration stats (e.g. range information)
# will be saved and can be used to generate more TRT engines with different
# shapes. Also, one TRT engine will be generated (with the same shape as
# the calibration data) for save later.
def my_calibration_input_fn():
  for _ in range(num_runs):
    inp1, inp2 = ...
    yield inp1, inp2

converter.convert(calibration_input_fn=my_calibration_input_fn)

# (Optional) Generate more TRT engines offline (same as the previous
# option), to avoid the cost of generating them during inference.
def my_input_fn():
  for _ in range(num_runs):
    inp1, inp2 = ...
    yield inp1, inp2
converter.build(input_fn=my_input_fn)

# Save the TRT engine and the engines.
converter.save(output_saved_model_dir)
</pre></devsite-code>
</li> <li>
<p>To use dynamic shape, we need to call the build method with an input function to generate profiles. This step is similar to the INT8 calibration step described above. The converter also needs to be created with use_dynamic_shape=True and one of the following profile_strategies for creating profiles based on the inputs produced by the input function:</p> <ul> <li>
<code translate="no" dir="ltr">Range</code>: create one profile that works for inputs with dimension values in the range of [min_dims, max_dims] where min_dims and max_dims are derived from the provided inputs.</li> <li>
<code translate="no" dir="ltr">Optimal</code>: create one profile for each input. The profile only works for inputs with the same dimensions as the input it is created for. The GPU engine will be run with optimal performance with such inputs.</li> <li>
<code translate="no" dir="ltr">Range+Optimal</code>: create the profiles for both <code translate="no" dir="ltr">Range</code> and <code translate="no" dir="ltr">Optimal</code>.</li> </ul>
</li> </ol>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input_saved_model_dir</code> </td> <td> the directory to load the SavedModel which contains the input graph to transforms. Required. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">input_saved_model_tags</code> </td> <td> list of tags to load the SavedModel. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">input_saved_model_signature_key</code> </td> <td> the key of the signature to optimize the graph for. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">use_dynamic_shape</code> </td> <td> whether to enable dynamic shape support. None is equivalent to False in the current implementation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dynamic_shape_profile_strategy</code> </td> <td> one of the strings in supported_profile_strategies(). None is equivalent to Range in the current implementation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">max_workspace_size_bytes</code> </td> <td> the maximum GPU temporary memory that the TRT engine can use at execution time. This corresponds to the 'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize(). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">precision_mode</code> </td> <td> one of the strings in TrtPrecisionMode.supported_precision_modes(). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">minimum_segment_size</code> </td> <td> the minimum number of nodes required for a subgraph to be replaced by TRTEngineOp. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">maximum_cached_engines</code> </td> <td> max number of cached TRT engines for dynamic TRT ops. Created TRT engines for a dynamic dimension are cached. If the number of cached engines is already at max but none of them supports the input shapes, the TRTEngineOp will fall back to run the original TF subgraph that corresponds to the TRTEngineOp. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">use_calibration</code> </td> <td> this argument is ignored if precision_mode is not INT8. If set to True, a calibration graph will be created to calibrate the missing ranges. The calibration graph must be converted to an inference graph by running calibration with calibrate(). If set to False, quantization nodes will be expected for every tensor in the graph (excluding those which will be fused). If a range is missing, an error will occur. Please note that accuracy may be negatively affected if there is a mismatch between which tensors TRT quantizes and which tensors were trained with fake quantization. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">allow_build_at_runtime</code> </td> <td> whether to allow building TensorRT engines during runtime if no prebuilt TensorRT engine can be found that can handle the given inputs during runtime, then a new TensorRT engine is built at runtime if allow_build_at_runtime=True, and otherwise native TF is used. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">conversion_params</code> </td> <td> a TrtConversionParams instance (deprecated). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if the combination of the parameters is invalid. </td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="-1">Methods</h2> <h3 id="build" data-text="build" tabindex="-1"><code translate="no" dir="ltr">build</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/compiler/tensorrt/trt_convert.py#L1464-L1557">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">build(
    input_fn
)
</pre></devsite-code> <p>Run inference with converted graph in order to build TensorRT engines.</p> <p>If the conversion requires INT8 calibration, then a reference to the calibration function was stored during the call to convert(). Calibration will be performed while we build the TensorRT engines.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input_fn</code> </td> <td> a generator function that provides the input data as a single array, OR a list or tuple of the arrays OR a dict, which will be used to execute the converted signature to generate TRT engines. Example 1: `def input_fn(): # Let's assume a network with 1 input tensor. # We generate 2 sets of dummy input data: input_shapes = [(1, 16), # 1st shape (2, 32)] # 2nd shape for shapes in input_shapes: # return an input tensor yield np.zeros(shape).astype(np.float32)' <p>Example 2: <code translate="no" dir="ltr">def input_fn(): # Let's assume a network with 2 input tensors. # We generate 3 sets of dummy input data: input_shapes = [[(1, 16), (2, 16)], # 1st input list [(2, 32), (4, 32)], # 2nd list of two tensors [(4, 32), (8, 32)]] # 3rd input list for shapes in input_shapes: # return a list of input tensors yield [np.zeros(x).astype(np.float32) for x in shapes]</code> </p>
</td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">NotImplementedError</code> </td> <td> build() is already called. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> the input_fx is None. </td> </tr> </table> <h3 id="convert" data-text="convert" tabindex="-1"><code translate="no" dir="ltr">convert</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/compiler/tensorrt/trt_convert.py#L1360-L1462">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">convert(
    calibration_input_fn=None
)
</pre></devsite-code> <p>Convert the input SavedModel in 2.0 format.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">calibration_input_fn</code> </td> <td> a generator function that yields input data as a list or tuple or dict, which will be used to execute the converted signature for calibration. All the returned input data should have the same shape. Example: <code translate="no" dir="ltr">def input_fn(): yield input1, input2, input3</code> <p>If dynamic_shape_mode==False, (or if the graph has static input shapes) then we run calibration and build the calibrated engine during conversion.</p> <p>If dynamic_shape_mode==True (and the graph has any unknown input shape), then the reference to calibration_input_fn is stored, and the calibration is actually performed when we build the engine (see build()). </p>
</td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if the input combination is invalid. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> The TF-TRT converted Function. </td> </tr> 
</table> <h3 id="save" data-text="save" tabindex="-1"><code translate="no" dir="ltr">save</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/compiler/tensorrt/trt_convert.py#L1559-L1665">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">save(
    output_saved_model_dir, save_gpu_specific_engines=True, options=None
)
</pre></devsite-code> <p>Save the converted SavedModel.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">output_saved_model_dir</code> </td> <td> directory to saved the converted SavedModel. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">save_gpu_specific_engines</code> </td> <td> whether to save TRT engines that have been built. When True, all engines are saved and when False, the engines are not saved and will be rebuilt at inference time. By using save_gpu_specific_engines=False after doing INT8 calibration, inference can be done on different GPUs than the GPU that the model was calibrated and saved on. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">options</code> </td> <td> <a href="../../saved_model/saveoptions.html"><code translate="no" dir="ltr">tf.saved_model.SaveOptions</code></a> object for configuring save options. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> if the needed calibration hasn't been done. </td> </tr> </table> <h3 id="summary" data-text="summary" tabindex="-1"><code translate="no" dir="ltr">summary</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/compiler/tensorrt/trt_convert.py#L1667-L1769">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">summary(
    line_length=160, detailed=True, print_fn=None
)
</pre></devsite-code> <p>This method describes the results of the conversion by TF-TRT.</p> <p>It includes information such as the name of the engine, the number of nodes per engine, the input and output dtype, along with the input shape of each TRTEngineOp.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">line_length</code> </td> <td> Default line length when printing on the console. Minimum 160 characters long. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">detailed</code> </td> <td> Whether or not to show the nodes inside each TRTEngineOp. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">print_fn</code> </td> <td> Print function to use. Defaults to <code translate="no" dir="ltr">print</code>. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> if the graph is not converted. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/experimental/tensorrt/Converter" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/experimental/tensorrt/Converter</a>
  </p>
</div>
