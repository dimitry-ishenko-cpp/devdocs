<h1 class="devsite-page-title" tabindex="-1"> tf.experimental.dtensor.Layout </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.experimental.dtensor.Layout"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__eq__"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="__new__"> <meta itemprop="property" content="as_proto"> <meta itemprop="property" content="batch_sharded"> <meta itemprop="property" content="delete"> <meta itemprop="property" content="from_device"> <meta itemprop="property" content="from_proto"> <meta itemprop="property" content="from_single_device_mesh"> <meta itemprop="property" content="from_string"> <meta itemprop="property" content="global_shape_from_local_shape"> <meta itemprop="property" content="inner_sharded"> <meta itemprop="property" content="is_batch_parallel"> <meta itemprop="property" content="is_fully_replicated"> <meta itemprop="property" content="is_single_device"> <meta itemprop="property" content="local_shape_from_global_shape"> <meta itemprop="property" content="num_shards"> <meta itemprop="property" content="offset_to_shard"> <meta itemprop="property" content="offset_tuple_to_global_index"> <meta itemprop="property" content="replicated"> <meta itemprop="property" content="to_parted"> <meta itemprop="property" content="to_string"> </div>   <p>Represents the layout information of a DTensor.</p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.experimental.dtensor.Layout(
    sharding_specs: List[str],
    mesh: tf.experimental.dtensor.Mesh
)
</pre></devsite-code> <h3 id="used-in-the-notebooks" data-text="Used in the notebooks" tabindex="-1">Used in the notebooks</h3> <table class="vertical-rules"> <thead> <tr> <th>Used in the guide</th> <th>Used in the tutorials</th> </tr> </thead> <tbody> <tr> <td> <ul> <li><a href="https://www.tensorflow.org/guide/dtensor_overview">DTensor concepts</a></li> <li><a href="https://www.tensorflow.org/guide/core/distribution">Distributed training with Core APIs and DTensor</a></li> </ul> </td> <td> <ul> <li><a href="https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial">Distributed training with DTensors</a></li> <li><a href="https://www.tensorflow.org/tutorials/distribute/dtensor_keras_tutorial">Using DTensors with Keras</a></li> </ul> </td> </tr> </tbody> </table> <p>A layout describes how a distributed tensor is partitioned across a mesh (and thus across devices). For each axis of the tensor, the corresponding sharding spec indicates which dimension of the mesh it is sharded over. A special sharding spec <code translate="no" dir="ltr">UNSHARDED</code> indicates that axis is replicated on all the devices of that mesh.</p> <p>Refer to <a href="https://www.tensorflow.org/guide/dtensor_overview">DTensor Concepts</a> for in depth discussion and examples.</p> <p>For example, let's consider a 1-D mesh:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">Mesh(["TPU:0", "TPU:1", "TPU:2", "TPU:3", "TPU:4", "TPU:5"], [("x", 6)])
</pre></devsite-code> <p>This mesh arranges 6 TPU devices into a 1-D array. <code translate="no" dir="ltr">Layout([UNSHARDED], mesh)</code> is a layout for rank-1 tensor which is replicated on the 6 devices.</p> <p>For another example, let's consider a 2-D mesh:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">Mesh(["TPU:0", "TPU:1", "TPU:2", "TPU:3", "TPU:4", "TPU:5"],
     [("x", 3), ("y", 2)])
</pre></devsite-code> <p>This mesh arranges 6 TPU devices into a <code translate="no" dir="ltr">3x2</code> 2-D array. <code translate="no" dir="ltr">Layout(["x", UNSHARDED], mesh)</code> is a layout for rank-2 tensor whose first axis is sharded on mesh dimension "x" and the second axis is replicated. If we place <code translate="no" dir="ltr">np.arange(6).reshape((3, 2))</code> using this layout, the individual components tensors would look like:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">Device  |  Component
 TPU:0     [[0, 1]]
 TPU:1     [[0, 1]]
 TPU:2     [[2, 3]]
 TPU:3     [[2, 3]]
 TPU:4     [[4, 5]]
 TPU:5     [[4, 5]]
</pre></devsite-code>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">sharding_specs</code> </td> <td> List of sharding specifications, each corresponding to a tensor axis. Each specification (dim_sharding) can either be a mesh dimension or the special value UNSHARDED. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">mesh</code> </td> <td> A mesh configuration for the Tensor. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">mesh</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">rank</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">shape</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">sharding_specs</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">type</code> </td> <td> 
</td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="-1">Methods</h2> <h3 id="as_proto" data-text="as_proto" tabindex="-1"><code translate="no" dir="ltr">as_proto</code></h3> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">as_proto()
</pre></devsite-code> <p>as_proto(self: tensorflow.python._pywrap_dtensor_device.Layout) -&gt; tensorflow::dtensor::LayoutProto</p> <p>Returns the LayoutProto protobuf message.</p> <h3 id="batch_sharded" data-text="batch_sharded" tabindex="-1"><code translate="no" dir="ltr">batch_sharded</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/dtensor/python/layout.py#L462-L473">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@classmethod
batch_sharded(
    mesh: tf.experimental.dtensor.Mesh,
    batch_dim: str,
    rank: int,
    axis: int = 0
) -&gt; 'Layout'
</pre></devsite-code> <p>Returns a layout sharded on batch dimension.</p> <h3 id="delete" data-text="delete" tabindex="-1"><code translate="no" dir="ltr">delete</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/dtensor/python/layout.py#L476-L483">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">delete(
    dims: List[int]
) -&gt; 'Layout'
</pre></devsite-code> <p>Returns the layout with the give dimensions deleted.</p> <h3 id="from_device" data-text="from_device" tabindex="-1"><code translate="no" dir="ltr">from_device</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/dtensor/python/layout.py#L515-L518">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@classmethod
from_device(
    device: str
) -&gt; 'Layout'
</pre></devsite-code> <p>Constructs a single device layout from a single device mesh.</p> <h3 id="from_proto" data-text="from_proto" tabindex="-1"><code translate="no" dir="ltr">from_proto</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/dtensor/python/layout.py#L485-L488">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@classmethod
from_proto(
    layout_proto: layout_pb2.LayoutProto
) -&gt; 'Layout'
</pre></devsite-code> <p>Creates an instance from a LayoutProto.</p> <h3 id="from_single_device_mesh" data-text="from_single_device_mesh" tabindex="-1"><code translate="no" dir="ltr">from_single_device_mesh</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/dtensor/python/layout.py#L510-L513">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@classmethod
from_single_device_mesh(
    mesh: tf.experimental.dtensor.Mesh
) -&gt; 'Layout'
</pre></devsite-code> <p>Constructs a single device layout from a single device mesh.</p> <h3 id="from_string" data-text="from_string" tabindex="-1"><code translate="no" dir="ltr">from_string</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/dtensor/python/layout.py#L490-L493">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@classmethod
from_string(
    layout_str: str
) -&gt; 'Layout'
</pre></devsite-code> <p>Creates an instance from a human-readable string.</p> <h3 id="global_shape_from_local_shape" data-text="global_shape_from_local_shape" tabindex="-1"><code translate="no" dir="ltr">global_shape_from_local_shape</code></h3> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">global_shape_from_local_shape()
</pre></devsite-code> <p>global_shape_from_local_shape(self: tensorflow.python._pywrap_dtensor_device.Layout, local_shape: List[int]) -&gt; tuple</p> <p>Returns the global shape computed from this local shape.</p> <h3 id="inner_sharded" data-text="inner_sharded" tabindex="-1"><code translate="no" dir="ltr">inner_sharded</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/dtensor/python/layout.py#L505-L508">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@classmethod
inner_sharded(
    mesh: tf.experimental.dtensor.Mesh,
    inner_dim: str,
    rank: int
) -&gt; 'Layout'
</pre></devsite-code> <p>Returns a layout sharded on inner dimension.</p> <h3 id="is_batch_parallel" data-text="is_batch_parallel" tabindex="-1"><code translate="no" dir="ltr">is_batch_parallel</code></h3> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">is_batch_parallel()
</pre></devsite-code> <p>is_batch_parallel(self: tensorflow.python._pywrap_dtensor_device.Layout) -&gt; bool</p> <h3 id="is_fully_replicated" data-text="is_fully_replicated" tabindex="-1"><code translate="no" dir="ltr">is_fully_replicated</code></h3> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">is_fully_replicated()
</pre></devsite-code> <p>is_fully_replicated(self: tensorflow.python._pywrap_dtensor_device.Layout) -&gt; bool</p> <p>Returns True if all tensor axes are replicated.</p> <h3 id="is_single_device" data-text="is_single_device" tabindex="-1"><code translate="no" dir="ltr">is_single_device</code></h3> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">is_single_device()
</pre></devsite-code> <p>is_single_device(self: tensorflow.python._pywrap_dtensor_device.Layout) -&gt; bool</p> <p>Returns True if the Layout represents a non-distributed device.</p> <h3 id="local_shape_from_global_shape" data-text="local_shape_from_global_shape" tabindex="-1"><code translate="no" dir="ltr">local_shape_from_global_shape</code></h3> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">local_shape_from_global_shape()
</pre></devsite-code> <p>local_shape_from_global_shape(self: tensorflow.python._pywrap_dtensor_device.Layout, global_shape: List[int]) -&gt; tuple</p> <p>Returns the local shape computed from this global shape.</p> <h3 id="num_shards" data-text="num_shards" tabindex="-1"><code translate="no" dir="ltr">num_shards</code></h3> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">num_shards()
</pre></devsite-code> <p>num_shards(self: tensorflow.python._pywrap_dtensor_device.Layout, idx: int) -&gt; int</p> <p>Returns the number of shards for tensor dimension <code translate="no" dir="ltr">idx</code>.</p> <h3 id="offset_to_shard" data-text="offset_to_shard" tabindex="-1"><code translate="no" dir="ltr">offset_to_shard</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/dtensor/python/layout.py#L521-L534">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">offset_to_shard()
</pre></devsite-code> <p>Mapping from offset in a flattened list to shard index.</p> <h3 id="offset_tuple_to_global_index" data-text="offset_tuple_to_global_index" tabindex="-1"><code translate="no" dir="ltr">offset_tuple_to_global_index</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/dtensor/python/layout.py#L537-L545">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">offset_tuple_to_global_index(
    offset_tuple
)
</pre></devsite-code> <p>Mapping from offset to index in global tensor.</p> <h3 id="replicated" data-text="replicated" tabindex="-1"><code translate="no" dir="ltr">replicated</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/dtensor/python/layout.py#L547-L550">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@classmethod
replicated(
    mesh: tf.experimental.dtensor.Mesh,
    rank: int
) -&gt; 'Layout'
</pre></devsite-code> <p>Returns a replicated layout of rank <code translate="no" dir="ltr">rank</code>.</p> <h3 id="to_parted" data-text="to_parted" tabindex="-1"><code translate="no" dir="ltr">to_parted</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/dtensor/python/layout.py#L495-L503">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">to_parted() -&gt; 'Layout'
</pre></devsite-code> <p>Returns a "parted" layout from a static layout.</p> <p>A parted layout contains axes that are treated as independent by most of SPMD expanders.</p> <p>FIXME(b/285905569): The exact semantics is still being investigated.</p> <h3 id="to_string" data-text="to_string" tabindex="-1"><code translate="no" dir="ltr">to_string</code></h3> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">to_string()
</pre></devsite-code> <p>to_string(self: tensorflow.python._pywrap_dtensor_device.Layout) -&gt; str</p> <h3 id="__eq__" data-text="__eq__" tabindex="-1"><code translate="no" dir="ltr">__eq__</code></h3> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">__eq__()
</pre></devsite-code> <p><strong>eq</strong>(self: tensorflow.python._pywrap_dtensor_device.Layout, arg0: tensorflow.python._pywrap_dtensor_device.Layout) -&gt; bool</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/experimental/dtensor/Layout" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/experimental/dtensor/Layout</a>
  </p>
</div>
