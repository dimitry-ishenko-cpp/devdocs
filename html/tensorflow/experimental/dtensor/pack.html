<h1 class="devsite-page-title">tf.experimental.dtensor.pack</h1> <devsite-bookmark></devsite-bookmark>       <p>Packs <a href="../../tensor.html"><code translate="no" dir="ltr">tf.Tensor</code></a> components into a DTensor.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.experimental.dtensor.pack(
    tensors: Sequence[Any],
    layout: tf.experimental.dtensor.Layout
) -&gt; Any
</pre>  <p>Packing and unpacking are inverse operations:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">* unpack(pack(tensors)) == tensors
* pack(unpack(dtensor)) == dtensor
</pre> <ol> <li>For any DTensor on the mesh, <code translate="no" dir="ltr">unpack</code> returns the raw components placed on each underlying device.</li> <li>Packing these raw components in the same order using <code translate="no" dir="ltr">pack</code> returns a DTensor which should be identical to the original DTensor--both the content value and the layout.</li> </ol> <p><strong>Shape, Rank, and Scalars</strong>: The rank of the DTensor is the same as the rank of its raw components, i.e., rank is preserved. This leads to a consistent interpretation for packing scalar values into a DTensor. The only valid layout for a scalar value is fully replicated, and the individual components must be identical scalars.</p> <p>Each input <code translate="no" dir="ltr">tensors[i]</code> will be copied to <code translate="no" dir="ltr">layout.mesh.local_device[i]</code> if not already on the local device. Non-local components should not be passed to <code translate="no" dir="ltr">pack</code>; use <code translate="no" dir="ltr">copy_to_mesh</code> and <code translate="no" dir="ltr">relayout</code> to place tensors on all global devices on a mesh.</p> <p>It is the caller's responsibility to ensure that the underlying values for <code translate="no" dir="ltr">pack</code> adhere to the specified layout, and that only as many values are specified as there are local devices. Pack does not move data between clients. See examples below for more detail about layouts.</p> <p>For example, assume we have a mesh <code translate="no" dir="ltr">[X(2), Y(3)]</code>, which has in total 6 underlying devices. Futuremore, assume that the device location mapping is the following:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">device_ID  |  location X, Y
        0     0, 0
        1     0, 1
        2     0, 2
        3     1, 0
        4     1, 1
        5     1, 2
</pre> <ol> <li>
<p>For 1-D vector DTensor with shape <code translate="no" dir="ltr">[128]</code> with layout <code translate="no" dir="ltr">[mesh.X]</code> and value as <code translate="no" dir="ltr">range(128)</code>, the raw components will have shape <code translate="no" dir="ltr">[64]</code> each, and the raw components will be:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">device_ID  |  raw component
        0     range(0, 64)
        1     range(0, 64)
        2     range(0, 64)
        3     range(64, 128)
        4     range(64, 128)
        5     range(64, 128)
</pre> <p>This also means for a 1-D DTensor with shape <code translate="no" dir="ltr">[2]</code> and layout <code translate="no" dir="ltr">[mesh.X]</code>, the raw components have shape <code translate="no" dir="ltr">[1]</code> rather than the shape for scalar values <code translate="no" dir="ltr">[]</code>.</p>
</li> <li>
<p>For 2-D vector DTensor with shape <code translate="no" dir="ltr">[2, 3]</code> with layout <code translate="no" dir="ltr">[mesh.X, mesh.Y]</code> and value as <code translate="no" dir="ltr">range(6)</code>, this is basically a fully-sharded DTensor.</p> <p>From global view, the content looks like</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">[
  [0.0, 1.0, 2.0],
  [3.0, 4.0, 5.0],
]
</pre> <p>The raw components will have shape <code translate="no" dir="ltr">[1, 1]</code> each, and have the following content:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">device_ID  |  raw component
        0     [[0.0]]
        1     [[1.0]]
        2     [[2.0]]
        3     [[3.0]]
        4     [[4.0]]
        5     [[5.0]]
</pre>
</li> <li>
<p>For a scalar value <code translate="no" dir="ltr">123.0</code> DTensor, it can only have one legitimate layout <code translate="no" dir="ltr">[]</code> (no dimension, but fully replicated).</p> <p>The raw components will have shape <code translate="no" dir="ltr">[]</code> each, and have the following content:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">device_ID  |  raw component
        0     123.0
        1     123.0
        2     123.0
        3     123.0
        4     123.0
        5     123.0
</pre> <p>Again, caller of <code translate="no" dir="ltr">pack</code> is expected to provide 6 identical value raw components with scalar shapes.</p>
</li> <li>
<p>For 3-D vector DTensor with shape <code translate="no" dir="ltr">[2, 2, 3]</code> with layout <code translate="no" dir="ltr">[X, unsharded, unsharded]</code> and value as <code translate="no" dir="ltr">range(12)</code>,</p> <p>From global view, the content looks like:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">[
  [
    [0.0, 1.0, 2.0],
    [3.0, 4.0, 5.0],
  ],
  [
    [6.0, 7.0, 8.0],
    [9.0, 10., 11.],
  ],
]
</pre> <p>The raw components will have shape <code translate="no" dir="ltr">[1, 2, 3]</code> each, and have the following content:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">device_ID  |  raw component
        0     range(6).reshape([1, 2, 3])
        1     range(6).reshape([1, 2, 3])
        2     range(6).reshape([1, 2, 3])
        3     range(6, 12).reshape([1, 2, 3])
        4     range(6, 12).reshape([1, 2, 3])
        5     range(6, 12).reshape([1, 2, 3])
</pre>
</li> </ol>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">tensors</code> </td> <td> The list of local tensor components to pack into a DTensor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">layout</code> </td> <td> The layout of the DTensor to be created. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A DTensor created from the individual component tensors. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> When <code translate="no" dir="ltr">pack</code> is not called eagerly. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/experimental/dtensor/pack" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/experimental/dtensor/pack</a>
  </p>
</div>
