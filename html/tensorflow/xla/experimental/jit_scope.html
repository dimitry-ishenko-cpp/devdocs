<h1 class="devsite-page-title" tabindex="-1"> tf.xla.experimental.jit_scope </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.xla.experimental.jit_scope"> <meta itemprop="path" content="Stable"> </div>   <p>Enable or disable JIT compilation of operators within the scope.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases" tabindex="-1">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="jit_scope.html"><code translate="no" dir="ltr">tf.compat.v1.xla.experimental.jit_scope</code></a></p> </section> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@contextlib.contextmanager
tf.xla.experimental.jit_scope(
    compile_ops=True, separate_compiled_gradients=False
)
</pre></devsite-code>  <blockquote class="note">
<strong>Note:</strong><span> This is an experimental feature.</span>
</blockquote> <p>The compilation is a hint and only supported on a best-effort basis.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Example usage</th></tr> <tr class="alt"> <td colspan="2"> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">with tf.xla.experimental.jit_scope():
  c = tf.matmul(a, b)  # compiled
with tf.xla.experimental.jit_scope(compile_ops=False):
  d = tf.matmul(a, c)  # not compiled
with tf.xla.experimental.jit_scope(
    compile_ops=lambda node_def: 'matmul' in node_def.op.lower()):
  e = tf.matmul(a, b) + d  # matmul is compiled, the addition is not.
</pre></devsite-code> 
</td> </tr> 
</table> <p>Example of <code translate="no" dir="ltr">separate_compiled_gradients</code>:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp"># In the example below, the computations for f, g and h will all be compiled
# in separate scopes.
with tf.xla.experimental.jit_scope(
    separate_compiled_gradients=True):
  f = tf.matmul(a, b)
g = tf.gradients([f], [a, b], name='mygrads1')
h = tf.gradients([f], [a, b], name='mygrads2')
</pre></devsite-code> <p>Ops that are not in the scope may be clustered and compiled with ops in the scope with <code translate="no" dir="ltr">compile_ops=True</code>, while the ops in the scope with <code translate="no" dir="ltr">compile_ops=False</code> will never be compiled.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">For example</th></tr> <tr class="alt"> <td colspan="2"> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp"># In the example below, x and loss may be clustered and compiled together,
# while y will not be compiled.
with tf.xla.experimental.jit_scope():
  x = tf.matmul(a, b)
with tf.xla.experimental.jit_scope(compile_ops=False):
  y = tf.matmul(c, d)
loss = x + y
</pre></devsite-code> 
</td> </tr> 
</table> <p>If you want to only compile the ops in the scope with <code translate="no" dir="ltr">compile_ops=True</code>, consider adding an outer <code translate="no" dir="ltr">jit_scope(compile_ops=False)</code>:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp"># In the example below, only x will be compiled.
with tf.xla.experimental.jit_scope(compile_ops=False):
  with tf.xla.experimental.jit_scope():
    x = tf.matmul(a, b)
  y = tf.matmul(c, d)
  loss = x + y
</pre></devsite-code>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">compile_ops</code> </td> <td> Whether to enable or disable compilation in the scope. Either a Python bool, or a callable that accepts the parameter <code translate="no" dir="ltr">node_def</code> and returns a python bool. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">separate_compiled_gradients</code> </td> <td> If true put each gradient subgraph into a separate compilation scope. This gives fine-grained control over which portions of the graph will be compiled as a single unit. Compiling gradients separately may yield better performance for some graphs. The scope is named based on the scope of the forward computation as well as the name of the gradients. As a result, the gradients will be compiled in a scope that is separate from both the forward computation, and from other gradients. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> if called when eager execution is enabled. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Yields</th></tr> <tr class="alt"> <td colspan="2"> The current scope, enabling or disabling compilation. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/xla/experimental/jit_scope" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/xla/experimental/jit_scope</a>
  </p>
</div>
