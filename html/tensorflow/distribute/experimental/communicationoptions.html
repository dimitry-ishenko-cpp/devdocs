<h1 class="devsite-page-title" tabindex="-1"> tf.distribute.experimental.CommunicationOptions </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.distribute.experimental.CommunicationOptions"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="__new__"> </div>   <p>Options for cross device communications like All-reduce.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases" tabindex="-1">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="communicationoptions.html"><code translate="no" dir="ltr">tf.compat.v1.distribute.experimental.CommunicationOptions</code></a></p> </section> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.distribute.experimental.CommunicationOptions(
    bytes_per_pack=0,
    timeout_seconds=None,
    implementation=tf.distribute.experimental.CollectiveCommunication.AUTO
)
</pre></devsite-code> <h3 id="used-in-the-notebooks" data-text="Used in the notebooks" tabindex="-1">Used in the notebooks</h3> <table class="vertical-rules"> <thead> <tr> <th>Used in the guide</th> </tr> </thead> <tbody> <tr> <td> <ul> <li><a href="https://www.tensorflow.org/guide/distributed_training">Distributed training with TensorFlow</a></li> </ul> </td> </tr> </tbody> </table> <p>This can be passed to methods like <code translate="no" dir="ltr">tf.distribute.get_replica_context().all_reduce()</code> to optimize collective operation performance. Note that these are only hints, which may or may not change the actual behavior. Some options only apply to certain strategy and are ignored by others.</p> <p>One common optimization is to break gradients all-reduce into multiple packs so that weight updates can overlap with gradient all-reduce.</p> <h4 id="examples" data-text="Examples:" tabindex="-1">Examples:</h4> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">options = tf.distribute.experimental.CommunicationOptions(
    bytes_per_pack=50 * 1024 * 1024,
    timeout_seconds=120.0,
    implementation=tf.distribute.experimental.CommunicationImplementation.NCCL
)
grads = tf.distribute.get_replica_context().all_reduce(
    'sum', grads, options=options)
optimizer.apply_gradients(zip(grads, vars),
    experimental_aggregate_gradients=False)
</pre></devsite-code>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">bytes_per_pack</code> </td> <td> a non-negative integer. Breaks collective operations into packs of certain size. If it's zero, the value is determined automatically. This hint is respected by all multi-replica strategies except <code translate="no" dir="ltr">TPUStrategy</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">timeout_seconds</code> </td> <td> a float or None, timeout in seconds. If not None, the collective raises <a href="../../errors/deadlineexceedederror.html"><code translate="no" dir="ltr">tf.errors.DeadlineExceededError</code></a> if it takes longer than this timeout. Zero disables timeout. This can be useful when debugging hanging issues. This should only be used for debugging since it creates a new thread for each collective, i.e. an overhead of <code translate="no" dir="ltr">timeout_seconds * num_collectives_per_second</code> more threads. This only works for <a href="multiworkermirroredstrategy.html"><code translate="no" dir="ltr">tf.distribute.experimental.MultiWorkerMirroredStrategy</code></a>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">implementation</code> </td> <td> a <a href="communicationimplementation.html"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationImplementation</code></a>. This is a hint on the preferred communication implementation. Possible values include <code translate="no" dir="ltr">AUTO</code>, <code translate="no" dir="ltr">RING</code>, and <code translate="no" dir="ltr">NCCL</code>. NCCL is generally more performant for GPU, but doesn't work for CPU. This only works for <a href="multiworkermirroredstrategy.html"><code translate="no" dir="ltr">tf.distribute.experimental.MultiWorkerMirroredStrategy</code></a>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> When arguments have invalid value. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions</a>
  </p>
</div>
