<h1 class="devsite-page-title" tabindex="-1"> tf.distribute.experimental.PreemptionCheckpointHandler </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.distribute.experimental.PreemptionCheckpointHandler"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="run"> <meta itemprop="property" content="save_checkpoint_if_preempted"> <meta itemprop="property" content="watch_preemption_scope"> </div>   <p>Preemption and error handler for synchronous training.</p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.distribute.experimental.PreemptionCheckpointHandler(
    cluster_resolver,
    checkpoint_or_checkpoint_manager,
    checkpoint_dir=None,
    termination_config=None
)
</pre></devsite-code>  <blockquote class="note">
<strong>Note:</strong><span> This API only supports use with <a href="../multiworkermirroredstrategy.html"><code translate="no" dir="ltr">tf.distribute.MultiWorkerMirroredStrategy</code></a> and <a href="../tpustrategy.html"><code translate="no" dir="ltr">tf.distribute.TPUStrategy</code></a>.</span>
</blockquote> <p>A <code translate="no" dir="ltr">PreemptionCheckpointHandler</code> coordinates all workers to save a checkpoint upon receiving a preemption signal. It also helps disseminate application error messages accurately among the cluster. When a <code translate="no" dir="ltr">PreemptionCheckpointHandler</code> object is created, it restores values from the latest checkpoint file if any exists.</p> <p>Right after the initialization, the object starts to watch out for termination signal for any member in the cluster. If receiving a signal, the next time the worker executes <a href="preemptioncheckpointhandler.html#run"><code translate="no" dir="ltr">PreemptionCheckpointHandler.run</code></a>, the <code translate="no" dir="ltr">PreemptionCheckpointHandler</code> will align all workers to save a checkpoint. Then, if an <code translate="no" dir="ltr">exit_fn</code> is configured via <a href="terminationconfig.html"><code translate="no" dir="ltr">tf.distribute.experimental.TerminationConfig</code></a>, it will be invoked. Otherwise, the process will simply exit and later the platform should restart it.</p> <blockquote class="note">
<strong>Note:</strong><span> We advise users of <a href="../multiworkermirroredstrategy.html"><code translate="no" dir="ltr">tf.distribute.MultiWorkerMirroredStrategy</code></a> who choose to configure their own <code translate="no" dir="ltr">exit_fn</code> in <a href="terminationconfig.html"><code translate="no" dir="ltr">tf.distribute.experimental.TerminationConfig</code></a> to include a <code translate="no" dir="ltr">sys.exit(CODE_OR_MESSAGE)</code> in the <code translate="no" dir="ltr">exit_fn</code> so that after the restart, all workers can initialize communication services correctly. For users of <a href="../tpustrategy.html"><code translate="no" dir="ltr">tf.distribute.TPUStrategy</code></a>, if they do not wish to do a cluster restart but would like an in-process restart (i.e., keep the coordinator alive and re-do the steps to connect to cluster, initialize TPU system, and make the <code translate="no" dir="ltr">TPUStrategy</code> object), they could configure the <code translate="no" dir="ltr">exit_fn</code> to a no-op.</span>
</blockquote> <p>For users of <a href="../multiworkermirroredstrategy.html"><code translate="no" dir="ltr">tf.distribute.MultiWorkerMirroredStrategy</code></a>, the core API is <a href="preemptioncheckpointhandler.html#run"><code translate="no" dir="ltr">PreemptionCheckpointHandler.run</code></a>:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">strategy = tf.distribute.MultiWorkerMirroredStrategy()

trained_epoch = tf.Variable(initial_value=tf.constant(0, dtype=tf.dtypes.int64), name='epoch')
step_in_epoch = tf.Variable(initial_value=tf.constant(0, dtype=tf.dtypes.int64), name='step_in_epoch')

with strategy.scope():
  dataset, model, optimizer = ...

  checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                   model=model,
                                   trained_epoch=trained_epoch,
                                   step_in_epoch=step_in_epoch)

  preemption_checkpoint_handler = tf.distribute.experimental.PreemptionCheckpointHandler(cluster_resolver, checkpoint, checkpoint_dir)

while trained_epoch.numpy() &lt; NUM_EPOCH:

  while step_in_epoch.numpy() &lt; STEPS_PER_EPOCH:

    # distributed_train_function contains a call to strategy.run.
    loss += preemption_checkpoint_handler.run(distributed_train_function, args=(next(iterator),))
    # For users of MultiWorkerMirroredStrategy, usually
    # STEPS_PER_TRAIN_FUNCTION = 1.
    step_in_epoch.assign_add(STEPS_PER_TRAIN_FUNCTION)
    ...

  epoch.assign_add(1)
  step_in_epoch.assign(0)
</pre></devsite-code> <p>For users of <a href="../tpustrategy.html"><code translate="no" dir="ltr">tf.distribute.TPUStrategy</code></a>, the core APIs are <a href="preemptioncheckpointhandler.html#run"><code translate="no" dir="ltr">PreemptionCheckpointHandler.run</code></a> and <a href="preemptioncheckpointhandler.html#watch_preemption_scope"><code translate="no" dir="ltr">PreemptionCheckpointHandler.watch_preemption_scope</code></a>:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">
strategy = tf.distribute.TPUStrategy(tpu_cluster_resolver)

# Rest of TPU init omitted, see documentation for TPUSTrategy.

with preemption_checkpoint_handler.watch_preemption_scope():
  while trained_epoch.numpy() &lt; NUM_EPOCH:

    while step_in_epoch.numpy() &lt; STEPS_PER_EPOCH:

      # distributed_train_function contains a call to strategy.run.
      loss += preemption_checkpoint_handler.run(distributed_train_function, args=(next(iterator),))

      # For users of TPUStrategy, usually STEPS_PER_TRAIN_FUNCTION &gt;&gt; 1 since
      # clustering multiple steps within a tf.function amortizes the overhead
      # of launching a multi-device function on TPU Pod.
      step_in_epoch.assign_add(STEPS_PER_TRAIN_FUNCTION)
      ...

    epoch.assign_add(1)
    step_in_epoch.assign(0)
</pre></devsite-code> <p>Not all interruptions come with advance notice so that the <code translate="no" dir="ltr">PreemptionCheckpointHandler</code> can handle them, e.g., those caused by hardware failure. For a user who saves checkpoints for these cases themselves outside the <code translate="no" dir="ltr">PreemptionCheckpointHandler</code>, if they are using a <a href="../../train/checkpointmanager.html"><code translate="no" dir="ltr">tf.train.CheckpointManager</code></a>, pass it as the <code translate="no" dir="ltr">checkpoint_or_checkpoint_manager</code> argument to the <code translate="no" dir="ltr">PreemptionCheckpointHandler</code>. If they do not have a <a href="../../train/checkpointmanager.html"><code translate="no" dir="ltr">tf.train.CheckpointManager</code></a> but are directly working with <a href="../../train/checkpoint.html"><code translate="no" dir="ltr">tf.train.Checkpoint</code></a>, we advise saving the checkpoints in the directory that's passed as the <code translate="no" dir="ltr">checkpoint_dir</code> argument. In this way, at the program beginning, <code translate="no" dir="ltr">PreemptionCheckpointHandler</code> can restore the latest checkpoint from the directory, no matter it's saved by the user themselves or saved by the <code translate="no" dir="ltr">PreemptionCheckpointHandler</code> before preemption happens.</p> <p><strong>A note on the platform:</strong></p> <p><code translate="no" dir="ltr">PreemptionCheckpointHandler</code> can only handle the kind of termination with advance notice. For now, the API recognizes the termination signal for CPU, GPU, and TPU on Google Borg and CPU and GPU on the Google Cloud Platform. In these cases, <code translate="no" dir="ltr">PreemptionCheckpointHandler</code> will automatically adopt the correct preemption/maintenance notification detection mechanism. Users of other platforms can configure a detection monitoring behavior through the <a href="terminationconfig.html"><code translate="no" dir="ltr">tf.distribute.experimental.TerminationConfig</code></a>. Customization for the exit behavior and grace period length could also be done here.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">cluster_resolver</code> </td> <td> a <a href="../cluster_resolver/clusterresolver.html"><code translate="no" dir="ltr">tf.distribute.cluster_resolver.ClusterResolver</code></a> object. You may also obtain it through the <code translate="no" dir="ltr">cluster_resolver</code> attribute of the distribution strategy in use. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">checkpoint_or_checkpoint_manager</code> </td> <td> a <a href="../../train/checkpointmanager.html"><code translate="no" dir="ltr">tf.train.CheckpointManager</code></a> or a <a href="../../train/checkpoint.html"><code translate="no" dir="ltr">tf.train.Checkpoint</code></a>. If you are using a <a href="../../train/checkpointmanager.html"><code translate="no" dir="ltr">tf.train.CheckpointManager</code></a> to manage checkpoints outside the <code translate="no" dir="ltr">PreemptionCheckpointHandler</code> for backup purpose as well, pass it as <code translate="no" dir="ltr">checkpoint_or_checkpoint_manager</code> argument. Otherwise, pass a <a href="../../train/checkpoint.html"><code translate="no" dir="ltr">tf.train.Checkpoint</code></a> and the <code translate="no" dir="ltr">PreemptionCheckpointHandler</code> will create a <a href="../../train/checkpointmanager.html"><code translate="no" dir="ltr">tf.train.CheckpointManager</code></a> to manage it in the <code translate="no" dir="ltr">checkpoint_dir</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">checkpoint_dir</code> </td> <td> a directory where the <code translate="no" dir="ltr">PreemptionCheckpointHandler</code> saves and restores checkpoints. When a <code translate="no" dir="ltr">PreemptionCheckpointHandler</code> is created, the latest checkpoint in the <code translate="no" dir="ltr">checkpoint_dir</code> will be restored. (This is not needed if a <a href="../../train/checkpointmanager.html"><code translate="no" dir="ltr">tf.train.CheckpointManager</code></a> instead of a <a href="../../train/checkpoint.html"><code translate="no" dir="ltr">tf.train.Checkpoint</code></a> is passed as the <code translate="no" dir="ltr">checkpoint_or_checkpoint_manager</code> argument.) </td> </tr>
<tr> <td> <code translate="no" dir="ltr">termination_config</code> </td> <td> optional, a <a href="terminationconfig.html"><code translate="no" dir="ltr">tf.distribute.experimental.TerminationConfig</code></a> object to configure for a platform other than Google Borg or GCP. </td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="-1">Methods</h2> <h3 id="run" data-text="run" tabindex="-1"><code translate="no" dir="ltr">run</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/distribute/failure_handling/failure_handling.py#L805-L895">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">run(
    distributed_train_function, *args, **kwargs
)
</pre></devsite-code> <p>Runs a training function with error and preemption handling.</p> <p>This function handles the preemption signal from any peer in the cluster by saving the training progress and exiting gracefully. It will also broadcase any program error encountered during the execution of <code translate="no" dir="ltr">distributed_train_function</code> to all workers so that they can raise the same error.</p> <p>The <code translate="no" dir="ltr">distributed_train_function</code> argument should be a distributed train function (i.e., containing a call to <a href="../strategy.html#run"><code translate="no" dir="ltr">tf.distribute.Strategy.run</code></a>). For <a href="../multiworkermirroredstrategy.html"><code translate="no" dir="ltr">tf.distribute.MultiWorkerMirroredStrategy</code></a> users, we recommend passing in a single-step <code translate="no" dir="ltr">distributed_train_function</code> to <a href="preemptioncheckpointhandler.html#run"><code translate="no" dir="ltr">PreemptionCheckpointHandler.run</code></a> so that the checkpoint can be saved in time in case a preemption signal or maintenance notice is sent.</p> <p>Besides the preemption and error handling part, <code translate="no" dir="ltr">PreemptionCheckpointHandler.run(distributed_train_function, *args, **kwargs)</code> has the same effect and output as <code translate="no" dir="ltr">distributed_train_function(*args, **kwargs)</code>. <code translate="no" dir="ltr">distributed_train_function</code> can return either some or no result. The following is a shortened example:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">
@tf.function
def distributed_train_step(iterator):
  # A distributed single-step training function.

  def step_fn(inputs):
    # A per-replica single-step training function.
    x, y = inputs
    ...
    return loss

  per_replica_losses = strategy.run(step_fn, args=(next(iterator),))
  return strategy.reduce(
      tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)

for epoch in range(preemption_handler.total_run_calls // STEPS_PER_EPOCH,
                   EPOCHS_TO_RUN):
  iterator = iter(multi_worker_dataset)
  total_loss = 0.0
  num_batches = 0

  for step in range(preemption_handler.total_run_calls % STEPS_PER_EPOCH,
                    STEPS_PER_EPOCH):
    total_loss += preemption_handler.run(distributed_train_step)
    num_batches += 1

  train_loss = total_loss / num_batches
  print('Epoch: %d, train_loss: %f.' %(epoch.numpy(), train_loss))

  train_accuracy.reset_states()
</pre></devsite-code>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">distributed_train_function</code> </td> <td> A (single-step) distributed training function. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">*args</code> </td> <td> args for <code translate="no" dir="ltr">distributed_train_function</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> kwargs for <code translate="no" dir="ltr">distributed_train_function</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> <tr class="alt"> <td colspan="2"> Program error encountered by any member in the cluster while executing the <code translate="no" dir="ltr">distributed_train_function</code>, or any error from the program error propagation process. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Result of running the <code translate="no" dir="ltr">distributed_train_function</code>. </td> </tr> 
</table> <h3 id="save_checkpoint_if_preempted" data-text="save_checkpoint_if_preempted" tabindex="-1"><code translate="no" dir="ltr">save_checkpoint_if_preempted</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/distribute/failure_handling/failure_handling.py#L930-L997">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">save_checkpoint_if_preempted(
    *args, **kwargs
)
</pre></devsite-code> <p>Saves a checkpoint if a preemption signal has been made available.</p> <p>This is an alternative API for <a href="preemptioncheckpointhandler.html#run"><code translate="no" dir="ltr">PreemptionCheckpointHandler.run</code></a> and <a href="preemptioncheckpointhandler.html#watch_preemption_scope"><code translate="no" dir="ltr">PreemptionCheckpointHandler.watch_preemption_scope</code></a>. This method works for both <a href="../multiworkermirroredstrategy.html"><code translate="no" dir="ltr">tf.distribute.MultiWorkerMirroredStrategy</code></a> and <a href="../tpustrategy.html"><code translate="no" dir="ltr">tf.distribute.TPUStrategy</code></a>. However, <strong>for TPUStrategy, this method will add a synchronization point between workers and the coordinator</strong> and thus may have performance implication. If this is a concern, use the combination of <a href="preemptioncheckpointhandler.html#watch_preemption_scope"><code translate="no" dir="ltr">PreemptionCheckpointHandler.watch_preemption_scope</code></a> and <a href="preemptioncheckpointhandler.html#run"><code translate="no" dir="ltr">PreemptionCheckpointHandler.run</code></a> instead.</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">strategy = tf.distribute.TPUStrategy(tpu_cluster_resolver)
# initialization omitted

with strategy.scope():
  # Save in the checkpoint.
  trained_step = tf.Variable(initial_value=tf.constant(0, dtype=tf.dtypes.int64), name='trained_step', aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)

  checkpoint_manager = tf.train.CheckpointManager(checkpoint, directory, max_to_keep=1)
  preemption_handler = tf.distribute.experimental.PreemptionCheckpointHandler(cluster_resolver, checkpoint_manager)

while trained_step.numpy() &lt; NUM_STEPS:
  # Train STEPS_IN_FUNCTION steps at once.
  train_multi_step_function()
  trained_step.assign_add(STEPS_IN_FUNCTION)
  preemption_handler.save_checkpoint_if_preempted()
</pre></devsite-code>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">*args</code> </td> <td> args for <a href="../../train/checkpointmanager.html#save"><code translate="no" dir="ltr">tf.train.CheckpointManager.save()</code></a> to save checkpoint. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">**kwargs</code> </td> <td> kwargs for <a href="../../train/checkpointmanager.html#save"><code translate="no" dir="ltr">tf.train.CheckpointManager.save()</code></a> to save. </td> </tr> </table> <h3 id="watch_preemption_scope" data-text="watch_preemption_scope" tabindex="-1"><code translate="no" dir="ltr">watch_preemption_scope</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/distribute/failure_handling/failure_handling.py#L999-L1056">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@tf_contextlib.contextmanager
watch_preemption_scope()
</pre></devsite-code> <p>Syncs error and maybe save checkpoint for usage with TPUStrategy.</p> <blockquote class="note">
<strong>Note:</strong><span> Usage with <a href="../multiworkermirroredstrategy.html"><code translate="no" dir="ltr">tf.distribute.MultiWorkerMirroredStrategy</code></a> does not need this API.</span>
</blockquote> <h4 id="example_usage" data-text="Example usage:" tabindex="-1">Example usage:</h4> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">with preemption_checkpoint_handler.watch_preemption_scope():
  while trained_step.numpy() &lt; NUM_STEPS:

    # distributed_train_function contains a call to strategy.run.
    loss += preemption_checkpoint_handler.run(distributed_train_function, args=(next(iterator),))
    trained_step.assign_add(STEPS_PER_TRAIN_FUNCTION)
</pre></devsite-code> <p>In this workflow, <a href="preemptioncheckpointhandler.html#run"><code translate="no" dir="ltr">PreemptionCheckpointHandler.run</code></a> will flag preemption signal received, and <code translate="no" dir="ltr">watch_preemption_scope</code> will handle the preemption signal by saving a checkpoint and then either exit to restart or execute a user-passed <code translate="no" dir="ltr">exit_fn</code> in <a href="terminationconfig.html"><code translate="no" dir="ltr">tf.distribute.experimental.TerminationConfig</code></a>. If no preemption signal is received during execution of ops and function inside the scope, <code translate="no" dir="ltr">watch_preemption_scope</code> ensures the completion of all async op and function execution when exiting and will raises exceptions if async execution results in an error state.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Yields</th></tr> <tr class="alt"> <td colspan="2"> None </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/PreemptionCheckpointHandler" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/PreemptionCheckpointHandler</a>
  </p>
</div>
