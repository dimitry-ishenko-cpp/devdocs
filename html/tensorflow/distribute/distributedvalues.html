<h1 class="devsite-page-title">tf.distribute.DistributedValues</h1> <devsite-bookmark></devsite-bookmark>       <p>Base class for representing distributed values.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.distribute.DistributedValues(
    values
)
</pre>  <p>A subclass instance of <a href="distributedvalues.html"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> is created when creating variables within a distribution strategy, iterating a <a href="distributeddataset.html"><code translate="no" dir="ltr">tf.distribute.DistributedDataset</code></a> or through <a href="strategy.html#run"><code translate="no" dir="ltr">tf.distribute.Strategy.run</code></a>. This base class should never be instantiated directly. <a href="distributedvalues.html"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> contains a value per replica. Depending on the subclass, the values could either be synced on update, synced on demand, or never synced.</p> <p><a href="distributedvalues.html"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> can be reduced to obtain single value across replicas, as input into <a href="strategy.html#run"><code translate="no" dir="ltr">tf.distribute.Strategy.run</code></a> or the per-replica values inspected using <a href="strategy.html#experimental_local_results"><code translate="no" dir="ltr">tf.distribute.Strategy.experimental_local_results</code></a>.</p> <h4 id="example_usage" data-text="Example usage:">Example usage:</h4> <ol> <li>Created from a <a href="distributeddataset.html"><code translate="no" dir="ltr">tf.distribute.DistributedDataset</code></a>:</li> </ol> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
dataset = tf.data.Dataset.from_tensor_slices([5., 6., 7., 8.]).batch(2)
dataset_iterator = iter(strategy.experimental_distribute_dataset(dataset))
distributed_values = next(dataset_iterator)
</pre> <ol> <li>Returned by <code translate="no" dir="ltr">run</code>:</li> </ol> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
@tf.function
def run():
  ctx = tf.distribute.get_replica_context()
  return ctx.replica_id_in_sync_group
distributed_values = strategy.run(run)
</pre> <ol> <li>As input into <code translate="no" dir="ltr">run</code>:</li> </ol> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
dataset = tf.data.Dataset.from_tensor_slices([5., 6., 7., 8.]).batch(2)
dataset_iterator = iter(strategy.experimental_distribute_dataset(dataset))
distributed_values = next(dataset_iterator)
@tf.function
def run(input):
  return input + 1.0
updated_value = strategy.run(run, args=(distributed_values,))
</pre> <ol> <li>Reduce value:</li> </ol> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
dataset = tf.data.Dataset.from_tensor_slices([5., 6., 7., 8.]).batch(2)
dataset_iterator = iter(strategy.experimental_distribute_dataset(dataset))
distributed_values = next(dataset_iterator)
reduced_value = strategy.reduce(tf.distribute.ReduceOp.SUM,
                                distributed_values,
                                axis = 0)
</pre> <ol> <li>Inspect local replica values:</li> </ol> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
dataset = tf.data.Dataset.from_tensor_slices([5., 6., 7., 8.]).batch(2)
dataset_iterator = iter(strategy.experimental_distribute_dataset(dataset))
per_replica_values = strategy.experimental_local_results(
   distributed_values)
per_replica_values
(&lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.], dtype=float32)&gt;)
</pre>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/distribute/DistributedValues" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/distribute/DistributedValues</a>
  </p>
</div>
