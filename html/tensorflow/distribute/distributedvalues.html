<h1 class="devsite-page-title" tabindex="-1"> tf.distribute.DistributedValues </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.distribute.DistributedValues"> <meta itemprop="path" content="Stable"> </div>   <p>Base class for representing distributed values.</p>  <p>A subclass instance of <a href="distributedvalues.html"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> is created when creating variables within a distribution strategy, iterating a <a href="distributeddataset.html"><code translate="no" dir="ltr">tf.distribute.DistributedDataset</code></a> or through <a href="strategy.html#run"><code translate="no" dir="ltr">tf.distribute.Strategy.run</code></a>. This base class should never be instantiated directly. <a href="distributedvalues.html"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> contains a value per replica. Depending on the subclass, the values could either be synced on update, synced on demand, or never synced.</p> <p>Two representative types of <a href="distributedvalues.html"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> are <code translate="no" dir="ltr">tf.types.experimental.PerReplica</code> and <code translate="no" dir="ltr">tf.types.experimental.Mirrored</code> values.</p> <p><code translate="no" dir="ltr">PerReplica</code> values exist on the worker devices, with a different value for each replica. They are produced by iterating through a distributed dataset returned by <a href="strategy.html#experimental_distribute_dataset"><code translate="no" dir="ltr">tf.distribute.Strategy.experimental_distribute_dataset</code></a> (Example 1, below) and <a href="strategy.html#distribute_datasets_from_function"><code translate="no" dir="ltr">tf.distribute.Strategy.distribute_datasets_from_function</code></a>. They are also the typical result returned by <a href="strategy.html#run"><code translate="no" dir="ltr">tf.distribute.Strategy.run</code></a> (Example 2).</p> <p><code translate="no" dir="ltr">Mirrored</code> values are like <code translate="no" dir="ltr">PerReplica</code> values, except we know that the value on all replicas are the same. <code translate="no" dir="ltr">Mirrored</code> values are kept synchronized by the distribution strategy in use, while <code translate="no" dir="ltr">PerReplica</code> values are left unsynchronized. <code translate="no" dir="ltr">Mirrored</code> values typically represent model weights. We can safely read a <code translate="no" dir="ltr">Mirrored</code> value in a cross-replica context by using the value on any replica, while PerReplica values should not be read or manipulated in a cross-replica context."</p> <p><a href="distributedvalues.html"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> can be reduced via <code translate="no" dir="ltr">strategy.reduce</code> to obtain a single value across replicas (Example 4), used as input into <a href="strategy.html#run"><code translate="no" dir="ltr">tf.distribute.Strategy.run</code></a> (Example 3), or collected to inspect the per-replica values using <a href="strategy.html#experimental_local_results"><code translate="no" dir="ltr">tf.distribute.Strategy.experimental_local_results</code></a> (Example 5).</p> <h4 id="example_usages" data-text="Example usages:" tabindex="-1">Example usages:</h4> <ol> <li>Created from a <a href="distributeddataset.html"><code translate="no" dir="ltr">tf.distribute.DistributedDataset</code></a>:</li> </ol> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
dataset = tf.data.Dataset.from_tensor_slices([5., 6., 7., 8.]).batch(2)
dataset_iterator = iter(strategy.experimental_distribute_dataset(dataset))
distributed_values = next(dataset_iterator)
distributed_values
PerReplica:{
  0: &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.], dtype=float32)&gt;,
  1: &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.], dtype=float32)&gt;
}</pre></devsite-code> <ol> <li>Returned by <code translate="no" dir="ltr">run</code>:</li> </ol> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
@tf.function
def run():
  ctx = tf.distribute.get_replica_context()
  return ctx.replica_id_in_sync_group
distributed_values = strategy.run(run)
distributed_values
PerReplica:{
  0: &lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;,
  1: &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;
}</pre></devsite-code> <ol> <li>As input into <code translate="no" dir="ltr">run</code>:</li> </ol> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
dataset = tf.data.Dataset.from_tensor_slices([5., 6., 7., 8.]).batch(2)
dataset_iterator = iter(strategy.experimental_distribute_dataset(dataset))
distributed_values = next(dataset_iterator)
@tf.function
def run(input):
  return input + 1.0
updated_value = strategy.run(run, args=(distributed_values,))
updated_value
PerReplica:{
  0: &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.], dtype=float32)&gt;,
  1: &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([7.], dtype=float32)&gt;
}</pre></devsite-code> <ol> <li>As input into <code translate="no" dir="ltr">reduce</code>:</li> </ol> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
dataset = tf.data.Dataset.from_tensor_slices([5., 6., 7., 8.]).batch(2)
dataset_iterator = iter(strategy.experimental_distribute_dataset(dataset))
distributed_values = next(dataset_iterator)
reduced_value = strategy.reduce(tf.distribute.ReduceOp.SUM,
                                distributed_values,
                                axis = 0)
reduced_value
&lt;tf.Tensor: shape=(), dtype=float32, numpy=11.0&gt;</pre></devsite-code> <ol> <li>How to inspect per-replica values locally:</li> </ol> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
dataset = tf.data.Dataset.from_tensor_slices([5., 6., 7., 8.]).batch(2)
dataset_iterator = iter(strategy.experimental_distribute_dataset(dataset))
per_replica_values = strategy.experimental_local_results(
   distributed_values)
per_replica_values
(&lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.], dtype=float32)&gt;)</pre></devsite-code>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues</a>
  </p>
</div>
