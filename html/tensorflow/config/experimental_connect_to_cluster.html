<h1 class="devsite-page-title" tabindex="-1"> tf.config.experimental_connect_to_cluster </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.config.experimental_connect_to_cluster"> <meta itemprop="path" content="Stable"> </div>   <p>Connects to the given cluster.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases" tabindex="-1">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="experimental_connect_to_cluster.html"><code translate="no" dir="ltr">tf.compat.v1.config.experimental_connect_to_cluster</code></a></p> </section> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.config.experimental_connect_to_cluster(
    cluster_spec_or_resolver,
    job_name='localhost',
    task_index=0,
    protocol=None,
    make_master_device_default=True,
    cluster_device_filters=None
)
</pre></devsite-code> <h3 id="used-in-the-notebooks" data-text="Used in the notebooks" tabindex="-1">Used in the notebooks</h3> <table class="vertical-rules"> <thead> <tr> <th>Used in the guide</th> <th>Used in the tutorials</th> </tr> </thead> <tbody> <tr> <td> <ul> <li><a href="https://www.tensorflow.org/guide/migrate/tpu_embedding">Migrate from TPU embedding_columns to TPUEmbedding layer</a></li> <li><a href="https://www.tensorflow.org/guide/migrate/tpu_estimator">Migrate from TPUEstimator to TPUStrategy</a></li> <li><a href="https://www.tensorflow.org/guide/tpu">Use TPUs</a></li> </ul> </td> <td> <ul> <li><a href="https://www.tensorflow.org/tfmodels/orbit/index">Training with Orbit</a></li> <li><a href="https://www.tensorflow.org/text/tutorials/bert_glue">Solve GLUE tasks using BERT on TPU</a></li> </ul> </td> </tr> </tbody> </table> <p>Will make devices on the cluster available to use. Note that calling this more than once will work, but will invalidate any tensor handles on the old remote devices.</p> <p>If the given local job name is not present in the cluster specification, it will be automatically added, using an unused port on the localhost.</p> <p>Device filters can be specified to isolate groups of remote tasks to avoid undesired accesses between workers. Workers accessing resources or launching ops / functions on filtered remote devices will result in errors (unknown devices). For any remote task, if no device filter is present, all cluster devices will be visible; if any device filter is specified, it can only see devices matching at least one filter. Devices on the task itself are always visible. Device filters can be particially specified.</p> <p>For example, for a cluster set up for parameter server training, the following device filters might be specified:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">cdf = tf.config.experimental.ClusterDeviceFilters()
# For any worker, only the devices on PS nodes and itself are visible
for i in range(num_workers):
  cdf.set_device_filters('worker', i, ['/job:ps'])
# Similarly for any ps, only the devices on workers and itself are visible
for i in range(num_ps):
  cdf.set_device_filters('ps', i, ['/job:worker'])

tf.config.experimental_connect_to_cluster(cluster_def,
                                          cluster_device_filters=cdf)
</pre></devsite-code>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">cluster_spec_or_resolver</code> </td> <td> A <code translate="no" dir="ltr">ClusterSpec</code> or <code translate="no" dir="ltr">ClusterResolver</code> describing the cluster. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">job_name</code> </td> <td> The name of the local job. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">task_index</code> </td> <td> The local task index. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">protocol</code> </td> <td> The communication protocol, such as <code translate="no" dir="ltr">"grpc"</code>. If unspecified, will use the default from <code translate="no" dir="ltr">python/platform/remote_utils.py</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">make_master_device_default</code> </td> <td> If True and a cluster resolver is passed, will automatically enter the master task device scope, which indicates the master becomes the default device to run ops. It won't do anything if a cluster spec is passed. Will throw an error if the caller is currently already in some device scope. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">cluster_device_filters</code> </td> <td> an instance of <code translate="no" dir="ltr">tf.train.experimental/ClusterDeviceFilters</code> that specify device filters to the remote tasks in cluster. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/config/experimental_connect_to_cluster" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/config/experimental_connect_to_cluster</a>
  </p>
</div>
