<h1 class="devsite-page-title">tf.raw_ops.BatchFunction</h1> <devsite-bookmark></devsite-bookmark>       <p>Batches all the inputs tensors to the computation done by the function.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/raw_ops/BatchFunction"><code translate="no" dir="ltr">tf.compat.v1.raw_ops.BatchFunction</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.raw_ops.BatchFunction(
    in_tensors,
    captured_tensors,
    f,
    num_batch_threads,
    max_batch_size,
    batch_timeout_micros,
    Tout,
    max_enqueued_batches=10,
    allowed_batch_sizes=[],
    container='',
    shared_name='',
    batching_queue='',
    enable_large_batch_splitting=False,
    name=None
)
</pre>  <p>So, for example, in the following code</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">
# This input will be captured.
y = tf.placeholder_with_default(1.0, shape=[])

@tf.Defun(tf.float32)
def computation(a):
  return tf.matmul(a, a) + y

b = gen_batch_ops.batch_function(
        f=computation
        in_tensors=[a],
        captured_tensors=computation.captured_inputs,
        Tout=[o.type for o in computation.definition.signature.output_arg],
        num_batch_threads=1,
        max_batch_size=10,
        batch_timeout_micros=100000,  # 100ms
        allowed_batch_sizes=[3, 10],
        batching_queue="")
</pre> <p>If more than one session.run call is simultaneously trying to compute <code translate="no" dir="ltr">b</code> the values of <code translate="no" dir="ltr">a</code> will be gathered, non-deterministically concatenated along the first axis, and only one thread will run the computation.</p> <p>Assumes that all arguments of the function are Tensors which will be batched along their first dimension.</p> <p>Arguments that are captured, are not batched. The session.run call which does the concatenation, will use the values of the captured tensors available to it. Therefore, typical uses of captured tensors should involve values which remain unchanged across session.run calls. Inference is a good example of this.</p> <p>SparseTensor is not supported. The return value of the decorated function must be a Tensor or a list/tuple of Tensors.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">in_tensors</code> </td> <td> A list of <code translate="no" dir="ltr">Tensor</code> objects. The tensors to be batched. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">captured_tensors</code> </td> <td> A list of <code translate="no" dir="ltr">Tensor</code> objects. The tensors which are captured in the function, and don't need to be batched. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">f</code> </td> <td> A function decorated with @Defun. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_batch_threads</code> </td> <td> An <code translate="no" dir="ltr">int</code>. Number of scheduling threads for processing batches of work. Determines the number of batches processed in parallel. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">max_batch_size</code> </td> <td> An <code translate="no" dir="ltr">int</code>. Batch sizes will never be bigger than this. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">batch_timeout_micros</code> </td> <td> An <code translate="no" dir="ltr">int</code>. Maximum number of microseconds to wait before outputting an incomplete batch. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">Tout</code> </td> <td> A list of <code translate="no" dir="ltr">tf.DTypes</code> that has length <code translate="no" dir="ltr">&gt;= 1</code>. the types of the output tensors. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">max_enqueued_batches</code> </td> <td> An optional <code translate="no" dir="ltr">int</code>. Defaults to <code translate="no" dir="ltr">10</code>. Maximum number of batches enqueued. Default: 10. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">allowed_batch_sizes</code> </td> <td> An optional list of <code translate="no" dir="ltr">ints</code>. Defaults to <code translate="no" dir="ltr">[]</code>. Optional list of allowed batch sizes. If left empty, does nothing. Otherwise, supplies a list of batch sizes, causing the op to pad batches up to one of those sizes. The entries must increase monotonically. If enable_large_batch_splitting is false (i.e., large-input-split is not enabled) the final entry must equal max_batch_size. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">container</code> </td> <td> An optional <code translate="no" dir="ltr">string</code>. Defaults to <code translate="no" dir="ltr">""</code>. Controls the scope of sharing of this batch. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">shared_name</code> </td> <td> An optional <code translate="no" dir="ltr">string</code>. Defaults to <code translate="no" dir="ltr">""</code>. Concurrently running instances of batch in the same device with the same container and shared_name will batch their elements together. If left empty, the op name will be used as the shared name. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">batching_queue</code> </td> <td> An optional <code translate="no" dir="ltr">string</code>. Defaults to <code translate="no" dir="ltr">""</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">enable_large_batch_splitting</code> </td> <td> An optional <code translate="no" dir="ltr">bool</code>. Defaults to <code translate="no" dir="ltr">False</code>. input with a large size (i.e., larger than the largest value of <code translate="no" dir="ltr">allowed_batch_sizes</code>) will be splitted into multiple batches with batch size. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of <code translate="no" dir="ltr">Tensor</code> objects of type <code translate="no" dir="ltr">Tout</code>. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/raw_ops/BatchFunction" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/raw_ops/BatchFunction</a>
  </p>
</div>
