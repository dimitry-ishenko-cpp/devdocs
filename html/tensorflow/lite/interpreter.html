<h1 class="devsite-page-title" tabindex="-1"> tf.lite.Interpreter </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.lite.Interpreter"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="allocate_tensors"> <meta itemprop="property" content="get_input_details"> <meta itemprop="property" content="get_output_details"> <meta itemprop="property" content="get_signature_list"> <meta itemprop="property" content="get_signature_runner"> <meta itemprop="property" content="get_tensor"> <meta itemprop="property" content="get_tensor_details"> <meta itemprop="property" content="invoke"> <meta itemprop="property" content="reset_all_variables"> <meta itemprop="property" content="resize_tensor_input"> <meta itemprop="property" content="set_tensor"> <meta itemprop="property" content="tensor"> </div>   <p>Interpreter interface for running TensorFlow Lite models.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases" tabindex="-1">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="interpreter.html"><code translate="no" dir="ltr">tf.compat.v1.lite.Interpreter</code></a></p> </section> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.lite.Interpreter(
    model_path=None,
    model_content=None,
    experimental_delegates=None,
    num_threads=None,
    experimental_op_resolver_type=tf.lite.experimental.OpResolverType.AUTO,
    experimental_preserve_all_tensors=False,
    experimental_disable_delegate_clustering=False
)
</pre></devsite-code> <h3 id="used-in-the-notebooks" data-text="Used in the notebooks" tabindex="-1">Used in the notebooks</h3> <table class="vertical-rules"> <thead> <tr> <th>Used in the guide</th> <th>Used in the tutorials</th> </tr> </thead> <tbody> <tr> <td> <ul> <li><a href="https://www.tensorflow.org/lite/guide/signatures">Signatures in TensorFlow Lite</a></li> <li><a href="https://www.tensorflow.org/model_optimization/guide/combine/cqat_example">Cluster preserving quantization aware training (CQAT) Keras example</a></li> <li><a href="https://www.tensorflow.org/model_optimization/guide/combine/pqat_example">Pruning preserving quantization aware training (PQAT) Keras example</a></li> <li><a href="https://www.tensorflow.org/model_optimization/guide/clustering/clustering_example">Weight clustering in Keras example</a></li> <li><a href="https://www.tensorflow.org/model_optimization/guide/combine/pcqat_example">Sparsity and cluster preserving quantization aware training (PCQAT) Keras example</a></li> </ul> </td> <td> <ul> <li><a href="https://www.tensorflow.org/tutorials/images/classification">Image classification</a></li> <li><a href="https://www.tensorflow.org/lite/performance/post_training_integer_quant">Post-training integer quantization</a></li> <li><a href="https://www.tensorflow.org/lite/examples/jax_conversion/jax_to_tflite">Jax Model Conversion For TFLite</a></li> <li><a href="https://www.tensorflow.org/lite/examples/on_device_training/overview">On-Device Training with TensorFlow Lite</a></li> <li><a href="https://www.tensorflow.org/lite/examples/style_transfer/overview">Artistic Style Transfer with TensorFlow Lite</a></li> </ul> </td> </tr> </tbody> </table> <p>Models obtained from <code translate="no" dir="ltr">TfLiteConverter</code> can be run in Python with <code translate="no" dir="ltr">Interpreter</code>.</p> <p>As an example, lets generate a simple Keras model and convert it to TFLite (<code translate="no" dir="ltr">TfLiteConverter</code> also supports other input formats with <code translate="no" dir="ltr">from_saved_model</code> and <code translate="no" dir="ltr">from_concrete_function</code>)</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">x = np.array([[1.], [2.]])
y = np.array([[2.], [4.]])
model = tf.keras.models.Sequential([
          tf.keras.layers.Dropout(0.2),
          tf.keras.layers.Dense(units=1, input_shape=[1])
        ])
model.compile(optimizer='sgd', loss='mean_squared_error')
model.fit(x, y, epochs=1)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()</pre></devsite-code> <p><code translate="no" dir="ltr">tflite_model</code> can be saved to a file and loaded later, or directly into the <code translate="no" dir="ltr">Interpreter</code>. Since TensorFlow Lite pre-plans tensor allocations to optimize inference, the user needs to call <code translate="no" dir="ltr">allocate_tensors()</code> before any inference.</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()  # Needed before execution!</pre></devsite-code> <h4 id="sample_execution" data-text="Sample execution:" tabindex="-1">Sample execution:</h4> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">output = interpreter.get_output_details()[0]  # Model has single output.
input = interpreter.get_input_details()[0]  # Model has single input.
input_data = tf.constant(1., shape=[1, 1])
interpreter.set_tensor(input['index'], input_data)
interpreter.invoke()
interpreter.get_tensor(output['index']).shape
(1, 1)</pre></devsite-code> <p>Use <code translate="no" dir="ltr">get_signature_runner()</code> for a more user-friendly inference API.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">model_path</code> </td> <td> Path to TF-Lite Flatbuffer file. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">model_content</code> </td> <td> Content of model. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_delegates</code> </td> <td> Experimental. Subject to change. List of <a href="https://www.tensorflow.org/lite/performance/delegates">TfLiteDelegate</a> objects returned by lite.load_delegate(). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_threads</code> </td> <td> Sets the number of threads used by the interpreter and available to CPU kernels. If not set, the interpreter will use an implementation-dependent default number of threads. Currently, only a subset of kernels, such as conv, support multi-threading. num_threads should be &gt;= -1. Setting num_threads to 0 has the effect to disable multithreading, which is equivalent to setting num_threads to 1. If set to the value -1, the number of threads used will be implementation-defined and platform-dependent. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_op_resolver_type</code> </td> <td> The op resolver used by the interpreter. It must be an instance of OpResolverType. By default, we use the built-in op resolver which corresponds to tflite::ops::builtin::BuiltinOpResolver in C++. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_preserve_all_tensors</code> </td> <td> If true, then intermediate tensors used during computation are preserved for inspection, and if the passed op resolver type is AUTO or BUILTIN, the type will be changed to BUILTIN_WITHOUT_DEFAULT_DELEGATES so that no Tensorflow Lite default delegates are applied. If false, getting intermediate tensors could result in undefined values or None, especially when the graph is successfully modified by the Tensorflow Lite default delegate. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_disable_delegate_clustering</code> </td> <td> If true, don't perform delegate clustering during delegate graph partitioning phase. Disabling delegate clustering will make the execution order of ops respect the explicitly-inserted control dependencies in the graph (inserted via <code translate="no" dir="ltr">with tf.control_dependencies()</code>) since the TF Lite converter will drop control dependencies by default. Most users shouldn't turn this flag to True if they don't insert explicit control dependencies or the graph execution order is expected. For automatically inserted control dependencies (with <a href="../variable.html"><code translate="no" dir="ltr">tf.Variable</code></a>, <code translate="no" dir="ltr">tf.Print</code> etc), the user doesn't need to turn this flag to True since they are respected by default. Note that this flag is currently experimental, and it might be removed/updated if the TF Lite converter doesn't drop such control dependencies in the model. Default is False. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If the interpreter was unable to create. </td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="-1">Methods</h2> <h3 id="allocate_tensors" data-text="allocate_tensors" tabindex="-1"><code translate="no" dir="ltr">allocate_tensors</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/lite/python/interpreter.py#L529-L531">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">allocate_tensors()
</pre></devsite-code> <h3 id="get_input_details" data-text="get_input_details" tabindex="-1"><code translate="no" dir="ltr">get_input_details</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/lite/python/interpreter.py#L673-L702">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">get_input_details()
</pre></devsite-code> <p>Gets model input tensor details.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list in which each item is a dictionary with details about an input tensor. Each dictionary contains the following fields that describe the tensor: <ul> <li>
<code translate="no" dir="ltr">name</code>: The tensor name.</li> <li>
<code translate="no" dir="ltr">index</code>: The tensor index in the interpreter.</li> <li>
<code translate="no" dir="ltr">shape</code>: The shape of the tensor.</li> <li><p><code translate="no" dir="ltr">shape_signature</code>: Same as <code translate="no" dir="ltr">shape</code> for models with known/fixed shapes. If any dimension sizes are unknown, they are indicated with <code translate="no" dir="ltr">-1</code>.</p></li> <li><p><code translate="no" dir="ltr">dtype</code>: The numpy data type (such as <code translate="no" dir="ltr">np.int32</code> or <code translate="no" dir="ltr">np.uint8</code>).</p></li> <li><p><code translate="no" dir="ltr">quantization</code>: Deprecated, use <code translate="no" dir="ltr">quantization_parameters</code>. This field only works for per-tensor quantization, whereas <code translate="no" dir="ltr">quantization_parameters</code> works in all cases.</p></li> <li><p><code translate="no" dir="ltr">quantization_parameters</code>: A dictionary of parameters used to quantize the tensor: ~ <code translate="no" dir="ltr">scales</code>: List of scales (one if per-tensor quantization). ~ <code translate="no" dir="ltr">zero_points</code>: List of zero_points (one if per-tensor quantization). ~ <code translate="no" dir="ltr">quantized_dimension</code>: Specifies the dimension of per-axis quantization, in the case of multiple scales/zero_points.</p></li> <li><p><code translate="no" dir="ltr">sparsity_parameters</code>: A dictionary of parameters used to encode a sparse tensor. This is empty if the tensor is dense. </p></li>
</ul>
</td> </tr> 
</table> <h3 id="get_output_details" data-text="get_output_details" tabindex="-1"><code translate="no" dir="ltr">get_output_details</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/lite/python/interpreter.py#L751-L762">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">get_output_details()
</pre></devsite-code> <p>Gets model output tensor details.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list in which each item is a dictionary with details about an output tensor. The dictionary contains the same fields as described for <code translate="no" dir="ltr">get_input_details()</code>. </td> </tr> 
</table> <h3 id="get_signature_list" data-text="get_signature_list" tabindex="-1"><code translate="no" dir="ltr">get_signature_list</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/lite/python/interpreter.py#L764-L789">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">get_signature_list()
</pre></devsite-code> <p>Gets list of SignatureDefs in the model.</p> <p>Example,</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">signatures = interpreter.get_signature_list()
print(signatures)

# {
#   'add': {'inputs': ['x', 'y'], 'outputs': ['output_0']}
# }

Then using the names in the signature list you can get a callable from
get_signature_runner().
</pre></devsite-code>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of SignatureDef details in a dictionary structure. It is keyed on the SignatureDef method name, and the value holds dictionary of inputs and outputs. </td> </tr> 
</table> <h3 id="get_signature_runner" data-text="get_signature_runner" tabindex="-1"><code translate="no" dir="ltr">get_signature_runner</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/lite/python/interpreter.py#L814-L859">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">get_signature_runner(
    signature_key=None
)
</pre></devsite-code> <p>Gets callable for inference of specific SignatureDef.</p> <p>Example usage,</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()
fn = interpreter.get_signature_runner('div_with_remainder')
output = fn(x=np.array([3]), y=np.array([2]))
print(output)
# {
#   'quotient': array([1.], dtype=float32)
#   'remainder': array([1.], dtype=float32)
# }
</pre></devsite-code> <p>None can be passed for signature_key if the model has a single Signature only.</p> <p>All names used are this specific SignatureDef names.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">signature_key</code> </td> <td> Signature key for the SignatureDef, it can be None if and only if the model has a single SignatureDef. Default value is None. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> This returns a callable that can run inference for SignatureDef defined by argument 'signature_key'. The callable will take key arguments corresponding to the arguments of the SignatureDef, that should have numpy values. The callable will returns dictionary that maps from output names to numpy values of the computed results. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If passed signature_key is invalid. </td> </tr> </table> <h3 id="get_tensor" data-text="get_tensor" tabindex="-1"><code translate="no" dir="ltr">get_tensor</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/lite/python/interpreter.py#L861-L876">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">get_tensor(
    tensor_index, subgraph_index=0
)
</pre></devsite-code> <p>Gets the value of the output tensor (get a copy).</p> <p>If you wish to avoid the copy, use <code translate="no" dir="ltr">tensor()</code>. This function cannot be used to read intermediate results.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">tensor_index</code> </td> <td> Tensor index of tensor to get. This value can be gotten from the 'index' field in get_output_details. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">subgraph_index</code> </td> <td> Index of the subgraph to fetch the tensor. Default value is 0, which means to fetch from the primary subgraph. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> a numpy array. </td> </tr> 
</table> <h3 id="get_tensor_details" data-text="get_tensor_details" tabindex="-1"><code translate="no" dir="ltr">get_tensor_details</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/lite/python/interpreter.py#L656-L671">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">get_tensor_details()
</pre></devsite-code> <p>Gets tensor details for every tensor with valid tensor details.</p> <p>Tensors where required information about the tensor is not found are not added to the list. This includes temporary tensors without a name.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of dictionaries containing tensor information. </td> </tr> 
</table> <h3 id="invoke" data-text="invoke" tabindex="-1"><code translate="no" dir="ltr">invoke</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/lite/python/interpreter.py#L928-L941">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">invoke()
</pre></devsite-code> <p>Invoke the interpreter.</p> <p>Be sure to set the input sizes, allocate tensors and fill values before calling this. Also, note that this function releases the GIL so heavy computation can be done in the background while the Python interpreter continues. No other function on this object should be called while the invoke() call has not finished.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> When the underlying interpreter fails raise ValueError. </td> </tr> </table> <h3 id="reset_all_variables" data-text="reset_all_variables" tabindex="-1"><code translate="no" dir="ltr">reset_all_variables</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/lite/python/interpreter.py#L943-L944">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">reset_all_variables()
</pre></devsite-code> <h3 id="resize_tensor_input" data-text="resize_tensor_input" tabindex="-1"><code translate="no" dir="ltr">resize_tensor_input</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/lite/python/interpreter.py#L722-L749">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">resize_tensor_input(
    input_index, tensor_size, strict=False
)
</pre></devsite-code> <p>Resizes an input tensor.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input_index</code> </td> <td> Tensor index of input to set. This value can be gotten from the 'index' field in get_input_details. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">tensor_size</code> </td> <td> The tensor_shape to resize the input to. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">strict</code> </td> <td> Only unknown dimensions can be resized when <code translate="no" dir="ltr">strict</code> is True. Unknown dimensions are indicated as <code translate="no" dir="ltr">-1</code> in the <code translate="no" dir="ltr">shape_signature</code> attribute of a given tensor. (default False) </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If the interpreter could not resize the input tensor. </td> </tr> </table> <h4 id="usage" data-text="Usage:" tabindex="-1">Usage:</h4> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">interpreter = Interpreter(model_content=tflite_model)
interpreter.resize_tensor_input(0, [num_test_images, 224, 224, 3])
interpreter.allocate_tensors()
interpreter.set_tensor(0, test_images)
interpreter.invoke()
</pre></devsite-code> <h3 id="set_tensor" data-text="set_tensor" tabindex="-1"><code translate="no" dir="ltr">set_tensor</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/lite/python/interpreter.py#L704-L720">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">set_tensor(
    tensor_index, value
)
</pre></devsite-code> <p>Sets the value of the input tensor.</p> <p>Note this copies data in <code translate="no" dir="ltr">value</code>.</p> <p>If you want to avoid copying, you can use the <code translate="no" dir="ltr">tensor()</code> function to get a numpy buffer pointing to the input buffer in the tflite interpreter.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">tensor_index</code> </td> <td> Tensor index of tensor to set. This value can be gotten from the 'index' field in get_input_details. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> Value of tensor to set. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If the interpreter could not set the tensor. </td> </tr> </table> <h3 id="tensor" data-text="tensor" tabindex="-1"><code translate="no" dir="ltr">tensor</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/lite/python/interpreter.py#L878-L926">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tensor(
    tensor_index
)
</pre></devsite-code> <p>Returns function that gives a numpy view of the current tensor buffer.</p> <p>This allows reading and writing to this tensors w/o copies. This more closely mirrors the C++ Interpreter class interface's tensor() member, hence the name. Be careful to not hold these output references through calls to <code translate="no" dir="ltr">allocate_tensors()</code> and <code translate="no" dir="ltr">invoke()</code>. This function cannot be used to read intermediate results.</p> <h4 id="usage_2" data-text="Usage:" tabindex="-1">Usage:</h4> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">interpreter.allocate_tensors()
input = interpreter.tensor(interpreter.get_input_details()[0]["index"])
output = interpreter.tensor(interpreter.get_output_details()[0]["index"])
for i in range(10):
  input().fill(3.)
  interpreter.invoke()
  print("inference %s" % output())
</pre></devsite-code> <p>Notice how this function avoids making a numpy array directly. This is because it is important to not hold actual numpy views to the data longer than necessary. If you do, then the interpreter can no longer be invoked, because it is possible the interpreter would resize and invalidate the referenced tensors. The NumPy API doesn't allow any mutability of the the underlying buffers.</p> <h4 id="wrong" data-text="WRONG:" tabindex="-1">WRONG:</h4> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">input = interpreter.tensor(interpreter.get_input_details()[0]["index"])()
output = interpreter.tensor(interpreter.get_output_details()[0]["index"])()
interpreter.allocate_tensors()  # This will throw RuntimeError
for i in range(10):
  input.fill(3.)
  interpreter.invoke()  # this will throw RuntimeError since input,output
</pre></devsite-code>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">tensor_index</code> </td> <td> Tensor index of tensor to get. This value can be gotten from the 'index' field in get_output_details. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A function that can return a new numpy array pointing to the internal TFLite tensor state at any point. It is safe to hold the function forever, but it is not safe to hold the numpy array forever. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter</a>
  </p>
</div>
