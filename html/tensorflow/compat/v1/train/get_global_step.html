<h1 class="devsite-page-title">tf.compat.v1.train.get_global_step</h1> <devsite-bookmark></devsite-bookmark>       <p>Get the global step tensor.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.train.get_global_step(
    graph=None
)
</pre> <p><section><devsite-expandable expanded> <h2 class="showalways" id="migrate-to-tf2" data-text="Migrate to TF2">Migrate to TF2</h2></devsite-expandable></section></p> <aside class="caution"><strong>Caution:</strong><span> This API was designed for TensorFlow v1. Continue reading for details on how to migrate from this API to a native TensorFlow v2 equivalent. See the <a href="https://www.tensorflow.org/guide/migrate">TensorFlow v1 to TensorFlow v2 migration guide</a> for instructions on how to migrate the rest of your code.</span></aside> <p>With the deprecation of global graphs, TF no longer tracks variables in collections. In other words, there are no global variables in TF2. Thus, the global step functions have been removed (<code translate="no" dir="ltr">get_or_create_global_step</code>, <code translate="no" dir="ltr">create_global_step</code>, <code translate="no" dir="ltr">get_global_step</code>) . You have two options for migrating:</p> <ol> <li>Create a Keras optimizer, which generates an <code translate="no" dir="ltr">iterations</code> variable. This variable is automatically incremented when calling <code translate="no" dir="ltr">apply_gradients</code>.</li> <li>Manually create and increment a <a href="../../../variable.html"><code translate="no" dir="ltr">tf.Variable</code></a>.</li> </ol> <p>Below is an example of migrating away from using a global step to using a Keras optimizer:</p> <p>Define a dummy model and loss:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
def compute_loss(x):
  v = tf.Variable(3.0)
  y = x * v
  loss = x * 5 - x * v
  return loss, [v]
</pre> <p>Before migrating:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
g = tf.Graph()
with g.as_default():
  x = tf.compat.v1.placeholder(tf.float32, [])
  loss, var_list = compute_loss(x)
  global_step = tf.compat.v1.train.get_or_create_global_step()
  global_init = tf.compat.v1.global_variables_initializer()
  optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.1)
  train_op = optimizer.minimize(loss, global_step, var_list)
sess = tf.compat.v1.Session(graph=g)
sess.run(global_init)
print("before training:", sess.run(global_step))
before training: 0
sess.run(train_op, feed_dict={x: 3})
print("after training:", sess.run(global_step))
after training: 1
</pre> <p>Using <code translate="no" dir="ltr">get_global_step</code>:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
with g.as_default():
  print(sess.run(tf.compat.v1.train.get_global_step()))
1
</pre> <p>Migrating to a Keras optimizer:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
optimizer = tf.keras.optimizers.SGD(.01)
print("before training:", optimizer.iterations.numpy())
before training: 0
with tf.GradientTape() as tape:
  loss, var_list = compute_loss(3)
  grads = tape.gradient(loss, var_list)
  optimizer.apply_gradients(zip(grads, var_list))
print("after training:", optimizer.iterations.numpy())
after training: 1
</pre>  <h2 id="description" data-text="Description">Description</h2>  <p>The global step tensor must be an integer variable. We first try to find it in the collection <code translate="no" dir="ltr">GLOBAL_STEP</code>, or by name <code translate="no" dir="ltr">global_step:0</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">graph</code> </td> <td> The graph to find the global step in. If missing, use default graph. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> The global step variable, or <code translate="no" dir="ltr">None</code> if none was found. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If the global step tensor has a non-integer type, or if it is not a <code translate="no" dir="ltr">Variable</code>. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/train/get_global_step" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/train/get_global_step</a>
  </p>
</div>
