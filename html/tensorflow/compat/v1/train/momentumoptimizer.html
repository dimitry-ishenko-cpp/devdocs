<h1 class="devsite-page-title" tabindex="-1"> tf.compat.v1.train.MomentumOptimizer </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.compat.v1.train.MomentumOptimizer"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="apply_gradients"> <meta itemprop="property" content="compute_gradients"> <meta itemprop="property" content="get_name"> <meta itemprop="property" content="get_slot"> <meta itemprop="property" content="get_slot_names"> <meta itemprop="property" content="minimize"> <meta itemprop="property" content="variables"> <meta itemprop="property" content="GATE_GRAPH"> <meta itemprop="property" content="GATE_NONE"> <meta itemprop="property" content="GATE_OP"> </div>   <p>Optimizer that implements the Momentum algorithm.</p> <p>Inherits From: <a href="optimizer.html"><code translate="no" dir="ltr">Optimizer</code></a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.compat.v1.train.MomentumOptimizer(
    learning_rate,
    momentum,
    use_locking=False,
    name='Momentum',
    use_nesterov=False
)
</pre></devsite-code> <p><section><devsite-expandable expanded> <h2 class="showalways" id="migrate-to-tf2" data-text="Migrate to TF2" tabindex="-1">Migrate to TF2</h2></devsite-expandable></section></p> <aside class="caution"><strong>Caution:</strong><span> This API was designed for TensorFlow v1. Continue reading for details on how to migrate from this API to a native TensorFlow v2 equivalent. See the <a href="https://www.tensorflow.org/guide/migrate">TensorFlow v1 to TensorFlow v2 migration guide</a> for instructions on how to migrate the rest of your code.</span></aside> <p>tf.compat.v1.train.MomentumOptimizer is compatible with eager mode and <a href="../../../function.html"><code translate="no" dir="ltr">tf.function</code></a>. When eager execution is enabled, <code translate="no" dir="ltr">learning_rate</code>,<code translate="no" dir="ltr">momentum</code>, can each be a callable that takes no arguments and returns the actual value to use. This can be useful for changing these values across different invocations of optimizer functions.</p> <p>To switch to native TF2 style, please directly use <a href="../../../keras/optimizers/sgd.html"><code translate="no" dir="ltr">tf.keras.optimizers.SGD</code></a> with the <code translate="no" dir="ltr">momentum</code> argument.</p> <h4 id="structural_mapping_to_native_tf2" data-text="Structural mapping to native TF2" tabindex="-1">Structural mapping to native TF2</h4> <p>Before:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">optimizer = tf.compat.v1.train.MomentumOptimizer(
  learning_rate=learning_rate,
  momentum=momentum,
  use_nesterov=use_nesterov)
</pre></devsite-code> <p>After:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">optimizer = tf.keras.optimizers.SGD(
  learning_rate=learning_rate,
  momentum=momentum,
  nesterov=use_nesterov)
</pre></devsite-code> <h4 id="how_to_map_arguments" data-text="How to map arguments" tabindex="-1">How to map arguments</h4> <table> <thead> <tr> <th>TF1 Arg Name</th> <th>TF2 Arg Name</th> <th>Note</th> </tr> </thead> <tbody> <tr> <td>
<code translate="no" dir="ltr">learning_rate</code> </td> <td>
<code translate="no" dir="ltr">learning_rate</code> </td> <td>Be careful of setting learning_rate tensor value computed from the global step. In TF1 this was usually meant to imply a dynamic learning rate and would recompute in each step. In TF2 (eager + function) it will treat it as a scalar value that only gets computed once instead of a symbolic placeholder to be computed each time.</td> </tr> <tr> <td><code translate="no" dir="ltr">momentum</code></td> <td><code translate="no" dir="ltr">momentum</code></td> <td>-</td> </tr> <tr> <td><code translate="no" dir="ltr">use_locking</code></td> <td>-</td> <td>Not applicable in TF2.</td> </tr> <tr> <td><code translate="no" dir="ltr">use_nesterov</code></td> <td><code translate="no" dir="ltr">nesterov</code></td> <td>-</td> </tr> </tbody> </table> <h4 id="before_after_usage_example" data-text="Before &amp; after usage example" tabindex="-1">Before &amp; after usage example</h4> <p>Before:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">x = tf.Variable([1,2,3], dtype=tf.float32)
grad = tf.constant([0.1, 0.2, 0.3])
optimizer = tf.compat.v1.train.MomentumOptimizer(
  learning_rate=0.001,
  momentum=0.9,
  use_nesterov=False)
optimizer.apply_gradients(zip([grad], [x]))
</pre></devsite-code> <p>After:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">x = tf.Variable([1,2,3], dtype=tf.float32)
grad = tf.constant([0.1, 0.2, 0.3])
optimizer = tf.keras.optimizers.SGD(
  learning_rate=0.001,
  momentum=0.9,
  nesterov=False)
optimizer.apply_gradients(zip([grad], [x]))
</pre></devsite-code>  <h2 id="description" data-text="Description" tabindex="-1">Description</h2>  <p>Computes (if <code translate="no" dir="ltr">use_nesterov = False</code>):</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">accumulation = momentum * accumulation + gradient
variable -= learning_rate * accumulation
</pre></devsite-code> <p>Note that in the dense version of this algorithm, <code translate="no" dir="ltr">accumulation</code> is updated and applied regardless of a gradient's value, whereas the sparse version (when the gradient is an <code translate="no" dir="ltr">IndexedSlices</code>, typically because of <a href="../../../gather.html"><code translate="no" dir="ltr">tf.gather</code></a> or an embedding) only updates variable slices and corresponding <code translate="no" dir="ltr">accumulation</code> terms when that part of the variable was used in the forward pass.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">learning_rate</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> or a floating point value. The learning rate. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">momentum</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> or a floating point value. The momentum. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">use_locking</code> </td> <td> If <code translate="no" dir="ltr">True</code> use locks for update operations. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name prefix for the operations created when applying gradients. Defaults to "Momentum". </td> </tr>
<tr> <td> <code translate="no" dir="ltr">use_nesterov</code> </td> <td> If <code translate="no" dir="ltr">True</code> use Nesterov Momentum. See (Sutskever et al., 2013). This implementation always computes gradients at the value of the variable(s) passed to the optimizer. Using Nesterov Momentum makes the variable(s) track the values called <code translate="no" dir="ltr">theta_t + mu*v_t</code> in the paper. This implementation is an approximation of the original formula, valid for high values of momentum. It will compute the "adjusted gradient" in NAG by assuming that the new gradient will be estimated by the current average gradient plus the product of momentum and the change in the average gradient. </td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="-1">Methods</h2> <h3 id="apply_gradients" data-text="apply_gradients" tabindex="-1"><code translate="no" dir="ltr">apply_gradients</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/training/optimizer.py#L621-L748">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">apply_gradients(
    grads_and_vars,
    global_step=None,
    name=None,
    skip_gradients_aggregation=False
)
</pre></devsite-code> <p>Apply gradients to variables.</p> <p>This is the second part of <code translate="no" dir="ltr">minimize()</code>. It returns an <code translate="no" dir="ltr">Operation</code> that applies gradients.</p> <p>@compatibility(TF2)</p> <h4 id="how_to_map_arguments_2" data-text="How to Map Arguments" tabindex="-1">How to Map Arguments</h4> <table> <thead> <tr> <th style="text-align: left">TF1 Arg Name</th> <th style="text-align: left">TF2 Arg Name</th> <th style="text-align: left">Note</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><code translate="no" dir="ltr">grads_and_vars</code></td> <td style="text-align: left"><code translate="no" dir="ltr">grads_and_vars</code></td> <td style="text-align: left">-</td> </tr> <tr> <td style="text-align: left"><code translate="no" dir="ltr">global_step</code></td> <td style="text-align: left">Not supported.</td> <td style="text-align: left">Use <code translate="no" dir="ltr">optimizer.iterations</code>
</td> </tr> <tr> <td style="text-align: left"><code translate="no" dir="ltr">name</code></td> <td style="text-align: left"><code translate="no" dir="ltr">name.</code></td> <td style="text-align: left">-</td> </tr> </tbody> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">grads_and_vars</code> </td> <td> List of (gradient, variable) pairs as returned by <code translate="no" dir="ltr">compute_gradients()</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">global_step</code> </td> <td> Optional <code translate="no" dir="ltr">Variable</code> to increment by one after the variables have been updated. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the returned operation. Default to the name passed to the <code translate="no" dir="ltr">Optimizer</code> constructor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">skip_gradients_aggregation</code> </td> <td> If true, gradients aggregation will not be performed inside optimizer. Usually this arg is set to True when you write custom code aggregating gradients outside the optimizer. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An <code translate="no" dir="ltr">Operation</code> that applies the specified gradients. If <code translate="no" dir="ltr">global_step</code> was not None, that operation also increments <code translate="no" dir="ltr">global_step</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If <code translate="no" dir="ltr">grads_and_vars</code> is malformed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If none of the variables have gradients. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> If you should use <code translate="no" dir="ltr">_distributed_apply()</code> instead. </td> </tr> </table> <h3 id="compute_gradients" data-text="compute_gradients" tabindex="-1"><code translate="no" dir="ltr">compute_gradients</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/training/optimizer.py#L488-L609">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">compute_gradients(
    loss,
    var_list=None,
    gate_gradients=GATE_OP,
    aggregation_method=None,
    colocate_gradients_with_ops=False,
    grad_loss=None
)
</pre></devsite-code> <p>Compute gradients of <code translate="no" dir="ltr">loss</code> for the variables in <code translate="no" dir="ltr">var_list</code>.</p> <p><section><devsite-expandable expanded> <h4 class="showalways" id="migrate-to-tf2_1" data-text="Migrate to TF2" tabindex="-1">Migrate to TF2</h4></devsite-expandable></section></p> <aside class="caution"><strong>Caution:</strong><span> This API was designed for TensorFlow v1. Continue reading for details on how to migrate from this API to a native TensorFlow v2 equivalent. See the <a href="https://www.tensorflow.org/guide/migrate">TensorFlow v1 to TensorFlow v2 migration guide</a> for instructions on how to migrate the rest of your code.</span></aside> <p><a href="../../../keras/optimizer.html"><code translate="no" dir="ltr">tf.keras.optimizers.Optimizer</code></a> in TF2 does not provide a <code translate="no" dir="ltr">compute_gradients</code> method, and you should use a <a href="../../../gradienttape.html"><code translate="no" dir="ltr">tf.GradientTape</code></a> to obtain the gradients:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@tf.function
def train step(inputs):
  batch_data, labels = inputs
  with tf.GradientTape() as tape:
    predictions = model(batch_data, training=True)
    loss = tf.keras.losses.CategoricalCrossentropy(
        reduction=tf.keras.losses.Reduction.NONE)(labels, predictions)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
</pre></devsite-code> <p>Args: loss: A Tensor containing the value to minimize or a callable taking no arguments which returns the value to minimize. When eager execution is enabled it must be a callable. var_list: Optional list or tuple of <a href="../../../variable.html"><code translate="no" dir="ltr">tf.Variable</code></a> to update to minimize <code translate="no" dir="ltr">loss</code>. Defaults to the list of variables collected in the graph under the key <code translate="no" dir="ltr">GraphKeys.TRAINABLE_VARIABLES</code>. gate_gradients: How to gate the computation of gradients. Can be <code translate="no" dir="ltr">GATE_NONE</code>, <code translate="no" dir="ltr">GATE_OP</code>, or <code translate="no" dir="ltr">GATE_GRAPH</code>. aggregation_method: Specifies the method used to combine gradient terms. Valid values are defined in the class <code translate="no" dir="ltr">AggregationMethod</code>. colocate_gradients_with_ops: If True, try colocating gradients with the corresponding op. grad_loss: Optional. A <code translate="no" dir="ltr">Tensor</code> holding the gradient computed for <code translate="no" dir="ltr">loss</code>.</p> <p>Returns: A list of (gradient, variable) pairs. Variable is always present, but gradient can be <code translate="no" dir="ltr">None</code>.</p> <p>Raises: TypeError: If <code translate="no" dir="ltr">var_list</code> contains anything else than <code translate="no" dir="ltr">Variable</code> objects. ValueError: If some arguments are invalid. RuntimeError: If called with eager execution enabled and <code translate="no" dir="ltr">loss</code> is not callable.</p> <p>@compatibility(eager) When eager execution is enabled, <code translate="no" dir="ltr">gate_gradients</code>, <code translate="no" dir="ltr">aggregation_method</code>, and <code translate="no" dir="ltr">colocate_gradients_with_ops</code> are ignored.</p>  <h4 id="description_1" data-text="Description" tabindex="-1">Description</h4> <p>This is the first part of <code translate="no" dir="ltr">minimize()</code>. It returns a list of (gradient, variable) pairs where "gradient" is the gradient for "variable". Note that "gradient" can be a <code translate="no" dir="ltr">Tensor</code>, an <code translate="no" dir="ltr">IndexedSlices</code>, or <code translate="no" dir="ltr">None</code> if there is no gradient for the given variable.</p> <h3 id="get_name" data-text="get_name" tabindex="-1"><code translate="no" dir="ltr">get_name</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/training/optimizer.py#L425-L426">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">get_name()
</pre></devsite-code> <h3 id="get_slot" data-text="get_slot" tabindex="-1"><code translate="no" dir="ltr">get_slot</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/training/optimizer.py#L845-L871">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">get_slot(
    var, name
)
</pre></devsite-code> <p>Return a slot named <code translate="no" dir="ltr">name</code> created for <code translate="no" dir="ltr">var</code> by the Optimizer.</p> <p>Some <code translate="no" dir="ltr">Optimizer</code> subclasses use additional variables. For example <code translate="no" dir="ltr">Momentum</code> and <code translate="no" dir="ltr">Adagrad</code> use variables to accumulate updates. This method gives access to these <code translate="no" dir="ltr">Variable</code> objects if for some reason you need them.</p> <p>Use <code translate="no" dir="ltr">get_slot_names()</code> to get the list of slot names created by the <code translate="no" dir="ltr">Optimizer</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">var</code> </td> <td> A variable passed to <code translate="no" dir="ltr">minimize()</code> or <code translate="no" dir="ltr">apply_gradients()</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> A string. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> The <code translate="no" dir="ltr">Variable</code> for the slot if it was created, <code translate="no" dir="ltr">None</code> otherwise. </td> </tr> 
</table> <h3 id="get_slot_names" data-text="get_slot_names" tabindex="-1"><code translate="no" dir="ltr">get_slot_names</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/training/optimizer.py#L873-L881">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">get_slot_names()
</pre></devsite-code> <p>Return a list of the names of slots created by the <code translate="no" dir="ltr">Optimizer</code>.</p> <p>See <code translate="no" dir="ltr">get_slot()</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of strings. </td> </tr> 
</table> <h3 id="minimize" data-text="minimize" tabindex="-1"><code translate="no" dir="ltr">minimize</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/training/optimizer.py#L428-L486">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">minimize(
    loss,
    global_step=None,
    var_list=None,
    gate_gradients=GATE_OP,
    aggregation_method=None,
    colocate_gradients_with_ops=False,
    name=None,
    grad_loss=None
)
</pre></devsite-code> <p>Add operations to minimize <code translate="no" dir="ltr">loss</code> by updating <code translate="no" dir="ltr">var_list</code>.</p> <p>This method simply combines calls <code translate="no" dir="ltr">compute_gradients()</code> and <code translate="no" dir="ltr">apply_gradients()</code>. If you want to process the gradient before applying them call <code translate="no" dir="ltr">compute_gradients()</code> and <code translate="no" dir="ltr">apply_gradients()</code> explicitly instead of using this function.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">loss</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> containing the value to minimize. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">global_step</code> </td> <td> Optional <code translate="no" dir="ltr">Variable</code> to increment by one after the variables have been updated. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">var_list</code> </td> <td> Optional list or tuple of <code translate="no" dir="ltr">Variable</code> objects to update to minimize <code translate="no" dir="ltr">loss</code>. Defaults to the list of variables collected in the graph under the key <code translate="no" dir="ltr">GraphKeys.TRAINABLE_VARIABLES</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">gate_gradients</code> </td> <td> How to gate the computation of gradients. Can be <code translate="no" dir="ltr">GATE_NONE</code>, <code translate="no" dir="ltr">GATE_OP</code>, or <code translate="no" dir="ltr">GATE_GRAPH</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">aggregation_method</code> </td> <td> Specifies the method used to combine gradient terms. Valid values are defined in the class <code translate="no" dir="ltr">AggregationMethod</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">colocate_gradients_with_ops</code> </td> <td> If True, try colocating gradients with the corresponding op. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the returned operation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">grad_loss</code> </td> <td> Optional. A <code translate="no" dir="ltr">Tensor</code> holding the gradient computed for <code translate="no" dir="ltr">loss</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> An Operation that updates the variables in <code translate="no" dir="ltr">var_list</code>. If <code translate="no" dir="ltr">global_step</code> was not <code translate="no" dir="ltr">None</code>, that operation also increments <code translate="no" dir="ltr">global_step</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If some of the variables are not <code translate="no" dir="ltr">Variable</code> objects. </td> </tr> </table> <p><section><devsite-expandable expanded> <h4 class="showalways" id="eager-compatibility" data-text="eager compatibility" tabindex="-1">eager compatibility</h4></devsite-expandable></section></p> <p>When eager execution is enabled, <code translate="no" dir="ltr">loss</code> should be a Python function that takes no arguments and computes the value to be minimized. Minimization (and gradient computation) is done with respect to the elements of <code translate="no" dir="ltr">var_list</code> if not None, else with respect to any trainable variables created during the execution of the <code translate="no" dir="ltr">loss</code> function. <code translate="no" dir="ltr">gate_gradients</code>, <code translate="no" dir="ltr">aggregation_method</code>, <code translate="no" dir="ltr">colocate_gradients_with_ops</code> and <code translate="no" dir="ltr">grad_loss</code> are ignored when eager execution is enabled.</p>  <h3 id="variables" data-text="variables" tabindex="-1"><code translate="no" dir="ltr">variables</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/training/optimizer.py#L883-L909">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">variables()
</pre></devsite-code> <p>A list of variables which encode the current state of <code translate="no" dir="ltr">Optimizer</code>.</p> <p>Includes slot variables and additional global variables created by the optimizer in the current default graph.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of variables. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Class Variables</th></tr> 
<tr> <td> GATE_GRAPH </td> <td> <code translate="no" dir="ltr">2</code> </td> </tr>
<tr> <td> GATE_NONE </td> <td> <code translate="no" dir="ltr">0</code> </td> </tr>
<tr> <td> GATE_OP </td> <td> <code translate="no" dir="ltr">1</code> </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/MomentumOptimizer" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/MomentumOptimizer</a>
  </p>
</div>
