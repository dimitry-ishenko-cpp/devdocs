<h1 class="devsite-page-title">tf.compat.v1.train.Saver</h1> <devsite-bookmark></devsite-bookmark>       <p>Saves and restores variables.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.train.Saver(
    var_list=None,
    reshape=False,
    sharded=False,
    max_to_keep=5,
    keep_checkpoint_every_n_hours=10000.0,
    name=None,
    restore_sequentially=False,
    saver_def=None,
    builder=None,
    defer_build=False,
    allow_empty=False,
    write_version=saver_pb2.SaverDef.V2,
    pad_step_number=False,
    save_relative_paths=False,
    filename=None
)
</pre> <p><section><devsite-expandable expanded> <h2 class="showalways" id="migrate-to-tf2" data-text="Migrate to TF2">Migrate to TF2</h2></devsite-expandable></section></p> <aside class="caution"><strong>Caution:</strong><span> This API was designed for TensorFlow v1. Continue reading for details on how to migrate from this API to a native TensorFlow v2 equivalent. See the <a href="https://www.tensorflow.org/guide/migrate">TensorFlow v1 to TensorFlow v2 migration guide</a> for instructions on how to migrate the rest of your code.</span></aside> <p><a href="saver.html"><code translate="no" dir="ltr">tf.compat.v1.train.Saver</code></a> is not supported for saving and restoring checkpoints in TF2. Please switch to <a href="../../../train/checkpoint.html"><code translate="no" dir="ltr">tf.train.Checkpoint</code></a> or <a href="../../../keras/model.html#save_weights"><code translate="no" dir="ltr">tf.keras.Model.save_weights</code></a>, which perform a more robust <a href="https://www.tensorflow.org/guide/checkpoint#loading_mechanics">object-based saving</a>.</p> <h3 id="how_to_rewrite_checkpoints" data-text="How to Rewrite Checkpoints">How to Rewrite Checkpoints</h3> <p>Please rewrite your checkpoints immediately using the object-based checkpoint APIs.</p> <p>You can load a name-based checkpoint written by <a href="saver.html"><code translate="no" dir="ltr">tf.compat.v1.train.Saver</code></a> using <a href="../../../train/checkpoint.html#restore"><code translate="no" dir="ltr">tf.train.Checkpoint.restore</code></a> or <a href="../../../keras/model.html#load_weights"><code translate="no" dir="ltr">tf.keras.Model.load_weights</code></a>. However, you may have to change the names of the variables in your model to match the variable names in the name-based checkpoint, which can be viewed with <a href="../../../train/list_variables.html"><code translate="no" dir="ltr">tf.train.list_variables(path)</code></a>.</p> <p>Another option is to create an <code translate="no" dir="ltr">assignment_map</code> that maps the name of the variables in the name-based checkpoint to the variables in your model, eg:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">{
    'sequential/dense/bias': model.variables[0],
    'sequential/dense/kernel': model.variables[1]
}
</pre> <p>and use <a href="init_from_checkpoint.html"><code translate="no" dir="ltr">tf.compat.v1.train.init_from_checkpoint(path, assignment_map)</code></a> to restore the name-based checkpoint.</p> <p>After restoring, re-encode your checkpoint using <a href="../../../train/checkpoint.html#save"><code translate="no" dir="ltr">tf.train.Checkpoint.save</code></a> or <a href="../../../keras/model.html#save_weights"><code translate="no" dir="ltr">tf.keras.Model.save_weights</code></a>.</p> <p>See the <a href="https://www.tensorflow.org/guide/migrate#checkpoint_compatibility">Checkpoint compatibility</a> section of the migration guide for more details.</p> <h3 id="checkpoint_management_in_tf2" data-text="Checkpoint Management in TF2">Checkpoint Management in TF2</h3> <p>Use <a href="../../../train/checkpointmanager.html"><code translate="no" dir="ltr">tf.train.CheckpointManager</code></a> to manage checkpoints in TF2. <a href="../../../train/checkpointmanager.html"><code translate="no" dir="ltr">tf.train.CheckpointManager</code></a> offers equivalent <code translate="no" dir="ltr">keep_checkpoint_every_n_hours</code> and <code translate="no" dir="ltr">max_to_keep</code> parameters.</p> <p>To recover the latest checkpoint,</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">checkpoint = tf.train.Checkpoint(model)
manager = tf.train.CheckpointManager(checkpoint)
status = checkpoint.restore(manager.latest_checkpoint)
</pre> <p><a href="../../../train/checkpointmanager.html"><code translate="no" dir="ltr">tf.train.CheckpointManager</code></a> also writes a <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpoint_state.proto"><code translate="no" dir="ltr">CheckpointState</code> proto</a> which contains the timestamp when each checkpoint was created.</p> <h3 id="writing_metagraphdefs_in_tf2" data-text="Writing MetaGraphDefs in TF2">Writing <code translate="no" dir="ltr">MetaGraphDef</code>s in TF2</h3> <p>To replace, <a href="saver.html#save"><code translate="no" dir="ltr">tf.compat.v1.train.Saver.save(write_meta_graph=True)</code></a>, use <a href="../../../saved_model/save.html"><code translate="no" dir="ltr">tf.saved_model.save</code></a> to write the <code translate="no" dir="ltr">MetaGraphDef</code> (which is contained in <code translate="no" dir="ltr">saved_model.pb</code>).</p>  <h2 id="description" data-text="Description">Description</h2>  <p>See <a href="https://tensorflow.org/guide/variables">Variables</a> for an overview of variables, saving and restoring.</p> <p>The <code translate="no" dir="ltr">Saver</code> class adds ops to save and restore variables to and from <em>checkpoints</em>. It also provides convenience methods to run these ops.</p> <p>Checkpoints are binary files in a proprietary format which map variable names to tensor values. The best way to examine the contents of a checkpoint is to load it using a <code translate="no" dir="ltr">Saver</code>.</p> <p>Savers can automatically number checkpoint filenames with a provided counter. This lets you keep multiple checkpoints at different steps while training a model. For example you can number the checkpoint filenames with the training step number. To avoid filling up disks, savers manage checkpoint files automatically. For example, they can keep only the N most recent files, or one checkpoint for every N hours of training.</p> <p>You number checkpoint filenames by passing a value to the optional <code translate="no" dir="ltr">global_step</code> argument to <code translate="no" dir="ltr">save()</code>:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">saver.save(sess, 'my-model', global_step=0) ==&gt; filename: 'my-model-0'
...
saver.save(sess, 'my-model', global_step=1000) ==&gt; filename: 'my-model-1000'
</pre> <p>Additionally, optional arguments to the <code translate="no" dir="ltr">Saver()</code> constructor let you control the proliferation of checkpoint files on disk:</p> <ul> <li><p><code translate="no" dir="ltr">max_to_keep</code> indicates the maximum number of recent checkpoint files to keep. As new files are created, older files are deleted. If None or 0, no checkpoints are deleted from the filesystem but only the last one is kept in the <code translate="no" dir="ltr">checkpoint</code> file. Defaults to 5 (that is, the 5 most recent checkpoint files are kept.)</p></li> <li><p><code translate="no" dir="ltr">keep_checkpoint_every_n_hours</code>: In addition to keeping the most recent <code translate="no" dir="ltr">max_to_keep</code> checkpoint files, you might want to keep one checkpoint file for every N hours of training. This can be useful if you want to later analyze how a model progressed during a long training session. For example, passing <code translate="no" dir="ltr">keep_checkpoint_every_n_hours=2</code> ensures that you keep one checkpoint file for every 2 hours of training. The default value of 10,000 hours effectively disables the feature.</p></li> </ul> <p>Note that you still have to call the <code translate="no" dir="ltr">save()</code> method to save the model. Passing these arguments to the constructor will not save variables automatically for you.</p> <p>A training program that saves regularly looks like:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">...
# Create a saver.
saver = tf.compat.v1.train.Saver(...variables...)
# Launch the graph and train, saving the model every 1,000 steps.
sess = tf.compat.v1.Session()
for step in range(1000000):
    sess.run(..training_op..)
    if step % 1000 == 0:
        # Append the step number to the checkpoint name:
        saver.save(sess, 'my-model', global_step=step)
</pre> <p>In addition to checkpoint files, savers keep a protocol buffer on disk with the list of recent checkpoints. This is used to manage numbered checkpoint files and by <code translate="no" dir="ltr">latest_checkpoint()</code>, which makes it easy to discover the path to the most recent checkpoint. That protocol buffer is stored in a file named 'checkpoint' next to the checkpoint files.</p> <p>If you create several savers, you can specify a different filename for the protocol buffer file in the call to <code translate="no" dir="ltr">save()</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">var_list</code> </td> <td> A list of <code translate="no" dir="ltr">Variable</code>/<code translate="no" dir="ltr">SaveableObject</code>, or a dictionary mapping names to <code translate="no" dir="ltr">SaveableObject</code>s. If <code translate="no" dir="ltr">None</code>, defaults to the list of all saveable objects. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">reshape</code> </td> <td> If <code translate="no" dir="ltr">True</code>, allows restoring parameters from a checkpoint where the variables have a different shape. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">sharded</code> </td> <td> If <code translate="no" dir="ltr">True</code>, shard the checkpoints, one per device. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">max_to_keep</code> </td> <td> Maximum number of recent checkpoints to keep. Defaults to 5. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">keep_checkpoint_every_n_hours</code> </td> <td> How often to keep checkpoints. Defaults to 10,000 hours. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> String. Optional name to use as a prefix when adding operations. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">restore_sequentially</code> </td> <td> A <code translate="no" dir="ltr">Bool</code>, which if true, causes restore of different variables to happen sequentially within each device. This can lower memory usage when restoring very large models. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">saver_def</code> </td> <td> Optional <code translate="no" dir="ltr">SaverDef</code> proto to use instead of running the builder. This is only useful for specialty code that wants to recreate a <code translate="no" dir="ltr">Saver</code> object for a previously built <code translate="no" dir="ltr">Graph</code> that had a <code translate="no" dir="ltr">Saver</code>. The <code translate="no" dir="ltr">saver_def</code> proto should be the one returned by the <code translate="no" dir="ltr">as_saver_def()</code> call of the <code translate="no" dir="ltr">Saver</code> that was created for that <code translate="no" dir="ltr">Graph</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">builder</code> </td> <td> Optional <code translate="no" dir="ltr">SaverBuilder</code> to use if a <code translate="no" dir="ltr">saver_def</code> was not provided. Defaults to <code translate="no" dir="ltr">BulkSaverBuilder()</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">defer_build</code> </td> <td> If <code translate="no" dir="ltr">True</code>, defer adding the save and restore ops to the <code translate="no" dir="ltr">build()</code> call. In that case <code translate="no" dir="ltr">build()</code> should be called before finalizing the graph or using the saver. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">allow_empty</code> </td> <td> If <code translate="no" dir="ltr">False</code> (default) raise an error if there are no variables in the graph. Otherwise, construct the saver anyway and make it a no-op. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">write_version</code> </td> <td> controls what format to use when saving checkpoints. It also affects certain filepath matching logic. The V2 format is the recommended choice: it is much more optimized than V1 in terms of memory required and latency incurred during restore. Regardless of this flag, the Saver is able to restore from both V2 and V1 checkpoints. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">pad_step_number</code> </td> <td> if True, pads the global step number in the checkpoint filepaths to some fixed width (8 by default). This is turned off by default. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">save_relative_paths</code> </td> <td> If <code translate="no" dir="ltr">True</code>, will write relative paths to the checkpoint state file. This is needed if the user wants to copy the checkpoint directory and reload from the copied directory. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">filename</code> </td> <td> If known at graph construction time, filename used for variable loading/saving. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If <code translate="no" dir="ltr">var_list</code> is invalid. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If any of the keys or values in <code translate="no" dir="ltr">var_list</code> are not unique. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> If eager execution is enabled and<code translate="no" dir="ltr">var_list</code> does not specify a list of variables to save. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">last_checkpoints</code> </td> <td> List of not-yet-deleted checkpoint filenames. <p>You can pass any of the returned values to <code translate="no" dir="ltr">restore()</code>. </p>
</td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="as_saver_def" data-text="as_saver_def"><code translate="no" dir="ltr">as_saver_def</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/saver.py#L1072-L1078">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
as_saver_def()
</pre> <p>Generates a <code translate="no" dir="ltr">SaverDef</code> representation of this saver.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <code translate="no" dir="ltr">SaverDef</code> proto. </td> </tr> 
</table> <h3 id="build" data-text="build"><code translate="no" dir="ltr">build</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/saver.py#L942-L945">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
build()
</pre> <h3 id="export_meta_graph" data-text="export_meta_graph"><code translate="no" dir="ltr">export_meta_graph</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/saver.py#L1337-L1381">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
export_meta_graph(
    filename=None,
    collection_list=None,
    as_text=False,
    export_scope=None,
    clear_devices=False,
    clear_extraneous_savers=False,
    strip_default_attrs=False,
    save_debug_info=False
)
</pre> <p>Writes <code translate="no" dir="ltr">MetaGraphDef</code> to save_path/filename.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">filename</code> </td> <td> Optional meta_graph filename including the path. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">collection_list</code> </td> <td> List of string keys to collect. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">as_text</code> </td> <td> If <code translate="no" dir="ltr">True</code>, writes the meta_graph as an ASCII proto. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">export_scope</code> </td> <td> Optional <code translate="no" dir="ltr">string</code>. Name scope to remove. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">clear_devices</code> </td> <td> Whether or not to clear the device field for an <code translate="no" dir="ltr">Operation</code> or <code translate="no" dir="ltr">Tensor</code> during export. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">clear_extraneous_savers</code> </td> <td> Remove any Saver-related information from the graph (both Save/Restore ops and SaverDefs) that are not associated with this Saver. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">strip_default_attrs</code> </td> <td> Boolean. If <code translate="no" dir="ltr">True</code>, default-valued attributes will be removed from the NodeDefs. For a detailed guide, see <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes">Stripping Default-Valued Attributes</a>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">save_debug_info</code> </td> <td> If <code translate="no" dir="ltr">True</code>, save the GraphDebugInfo to a separate file, which in the same directory of filename and with <code translate="no" dir="ltr">_debug</code> added before the file extension. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <code translate="no" dir="ltr">MetaGraphDef</code> proto. </td> </tr> 
</table> <h3 id="from_proto" data-text="from_proto"><code translate="no" dir="ltr">from_proto</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/saver.py#L1107-L1118">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
@staticmethod
from_proto(
    saver_def, import_scope=None
)
</pre> <p>Returns a <code translate="no" dir="ltr">Saver</code> object created from <code translate="no" dir="ltr">saver_def</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">saver_def</code> </td> <td> a <code translate="no" dir="ltr">SaverDef</code> protocol buffer. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">import_scope</code> </td> <td> Optional <code translate="no" dir="ltr">string</code>. Name scope to use. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <code translate="no" dir="ltr">Saver</code> built from saver_def. </td> </tr> 
</table> <h3 id="recover_last_checkpoints" data-text="recover_last_checkpoints"><code translate="no" dir="ltr">recover_last_checkpoints</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/saver.py#L1161-L1182">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
recover_last_checkpoints(
    checkpoint_paths
)
</pre> <p>Recovers the internal saver state after a crash.</p> <p>This method is useful for recovering the "self._last_checkpoints" state.</p> <p>Globs for the checkpoints pointed to by <code translate="no" dir="ltr">checkpoint_paths</code>. If the files exist, use their mtime as the checkpoint timestamp.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">checkpoint_paths</code> </td> <td> a list of checkpoint paths. </td> </tr> </table> <h3 id="restore" data-text="restore"><code translate="no" dir="ltr">restore</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/saver.py#L1383-L1457">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
restore(
    sess, save_path
)
</pre> <p>Restores previously saved variables.</p> <p>This method runs the ops added by the constructor for restoring variables. It requires a session in which the graph was launched. The variables to restore do not have to have been initialized, as restoring is itself a way to initialize variables.</p> <p>The <code translate="no" dir="ltr">save_path</code> argument is typically a value previously returned from a <code translate="no" dir="ltr">save()</code> call, or a call to <code translate="no" dir="ltr">latest_checkpoint()</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">sess</code> </td> <td> A <code translate="no" dir="ltr">Session</code> to use to restore the parameters. None in eager mode. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">save_path</code> </td> <td> Path where parameters were previously saved. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If save_path is None or not a valid checkpoint. </td> </tr> </table> <h3 id="save" data-text="save"><code translate="no" dir="ltr">save</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/saver.py#L1184-L1335">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
save(
    sess,
    save_path,
    global_step=None,
    latest_filename=None,
    meta_graph_suffix='meta',
    write_meta_graph=True,
    write_state=True,
    strip_default_attrs=False,
    save_debug_info=False
)
</pre> <p>Saves variables.</p> <p>This method runs the ops added by the constructor for saving variables. It requires a session in which the graph was launched. The variables to save must also have been initialized.</p> <p>The method returns the path prefix of the newly created checkpoint files. This string can be passed directly to a call to <code translate="no" dir="ltr">restore()</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">sess</code> </td> <td> A Session to use to save the variables. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">save_path</code> </td> <td> String. Prefix of filenames created for the checkpoint. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">global_step</code> </td> <td> If provided the global step number is appended to <code translate="no" dir="ltr">save_path</code> to create the checkpoint filenames. The optional argument can be a <code translate="no" dir="ltr">Tensor</code>, a <code translate="no" dir="ltr">Tensor</code> name or an integer. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">latest_filename</code> </td> <td> Optional name for the protocol buffer file that will contains the list of most recent checkpoints. That file, kept in the same directory as the checkpoint files, is automatically managed by the saver to keep track of recent checkpoints. Defaults to 'checkpoint'. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">meta_graph_suffix</code> </td> <td> Suffix for <code translate="no" dir="ltr">MetaGraphDef</code> file. Defaults to 'meta'. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">write_meta_graph</code> </td> <td> <code translate="no" dir="ltr">Boolean</code> indicating whether or not to write the meta graph file. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">write_state</code> </td> <td> <code translate="no" dir="ltr">Boolean</code> indicating whether or not to write the <code translate="no" dir="ltr">CheckpointStateProto</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">strip_default_attrs</code> </td> <td> Boolean. If <code translate="no" dir="ltr">True</code>, default-valued attributes will be removed from the NodeDefs. For a detailed guide, see <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes">Stripping Default-Valued Attributes</a>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">save_debug_info</code> </td> <td> If <code translate="no" dir="ltr">True</code>, save the GraphDebugInfo to a separate file, which in the same directory of save_path and with <code translate="no" dir="ltr">_debug</code> added before the file extension. This is only enabled when <code translate="no" dir="ltr">write_meta_graph</code> is <code translate="no" dir="ltr">True</code> </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A string: path prefix used for the checkpoint files. If the saver is sharded, this string ends with: '-?????-of-nnnnn' where 'nnnnn' is the number of shards created. If the saver is empty, returns None. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If <code translate="no" dir="ltr">sess</code> is not a <code translate="no" dir="ltr">Session</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If <code translate="no" dir="ltr">latest_filename</code> contains path components, or if it collides with <code translate="no" dir="ltr">save_path</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">RuntimeError</code> </td> <td> If save and restore ops weren't built. </td> </tr> </table> <h3 id="set_last_checkpoints" data-text="set_last_checkpoints"><code translate="no" dir="ltr">set_last_checkpoints</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/saver.py#L1131-L1146">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
set_last_checkpoints(
    last_checkpoints
)
</pre> <aside class="deprecated"><strong>Deprecated:</strong><span> Use set_last_checkpoints_with_time.</span></aside> <p>Sets the list of old checkpoint filenames.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">last_checkpoints</code> </td> <td> A list of checkpoint filenames. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">AssertionError</code> </td> <td> If last_checkpoints is not a list. </td> </tr> </table> <h3 id="set_last_checkpoints_with_time" data-text="set_last_checkpoints_with_time"><code translate="no" dir="ltr">set_last_checkpoints_with_time</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/saver.py#L1148-L1159">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
set_last_checkpoints_with_time(
    last_checkpoints_with_time
)
</pre> <p>Sets the list of old checkpoint filenames and timestamps.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">last_checkpoints_with_time</code> </td> <td> A list of tuples of checkpoint filenames and timestamps. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">AssertionError</code> </td> <td> If last_checkpoints_with_time is not a list. </td> </tr> </table> <h3 id="to_proto" data-text="to_proto"><code translate="no" dir="ltr">to_proto</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.9.0/tensorflow/python/training/saver.py#L1080-L1105">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
to_proto(
    export_scope=None
)
</pre> <p>Converts this <code translate="no" dir="ltr">Saver</code> to a <code translate="no" dir="ltr">SaverDef</code> protocol buffer.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">export_scope</code> </td> <td> Optional <code translate="no" dir="ltr">string</code>. Name scope to remove. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <code translate="no" dir="ltr">SaverDef</code> protocol buffer. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/train/Saver" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/train/Saver</a>
  </p>
</div>
