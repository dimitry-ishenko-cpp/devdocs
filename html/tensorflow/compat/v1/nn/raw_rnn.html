<h1 class="devsite-page-title">tf.compat.v1.nn.raw_rnn</h1> <devsite-bookmark></devsite-bookmark>       <p>Creates an <code translate="no" dir="ltr">RNN</code> specified by RNNCell <code translate="no" dir="ltr">cell</code> and loop function <code translate="no" dir="ltr">loop_fn</code>.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.nn.raw_rnn(
    cell, loop_fn, parallel_iterations=None, swap_memory=False, scope=None
)
</pre>  <blockquote class="note">
<strong>Note:</strong><span> This method is still in testing, and the API may change.**</span>
</blockquote> <p>This function is a more primitive version of <code translate="no" dir="ltr">dynamic_rnn</code> that provides more direct access to the inputs each iteration. It also provides more control over when to start and finish reading the sequence, and what to emit for the output.</p> <p>For example, it can be used to implement the dynamic decoder of a seq2seq model.</p> <p>Instead of working with <code translate="no" dir="ltr">Tensor</code> objects, most operations work with <code translate="no" dir="ltr">TensorArray</code> objects directly.</p> <p>The operation of <code translate="no" dir="ltr">raw_rnn</code>, in pseudo-code, is basically the following:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">time = tf.constant(0, dtype=tf.int32)
(finished, next_input, initial_state, emit_structure, loop_state) = loop_fn(
    time=time, cell_output=None, cell_state=None, loop_state=None)
emit_ta = TensorArray(dynamic_size=True, dtype=initial_state.dtype)
state = initial_state
while not all(finished):
  (output, cell_state) = cell(next_input, state)
  (next_finished, next_input, next_state, emit, loop_state) = loop_fn(
      time=time + 1, cell_output=output, cell_state=cell_state,
      loop_state=loop_state)
  # Emit zeros and copy forward state for minibatch entries that are finished.
  state = tf.where(finished, state, next_state)
  emit = tf.where(finished, tf.zeros_like(emit_structure), emit)
  emit_ta = emit_ta.write(time, emit)
  # If any new minibatch entries are marked as finished, mark these.
  finished = tf.logical_or(finished, next_finished)
  time += 1
return (emit_ta, state, loop_state)
</pre> <p>with the additional properties that output and state may be (possibly nested) tuples, as determined by <code translate="no" dir="ltr">cell.output_size</code> and <code translate="no" dir="ltr">cell.state_size</code>, and as a result the final <code translate="no" dir="ltr">state</code> and <code translate="no" dir="ltr">emit_ta</code> may themselves be tuples.</p> <p>A simple implementation of <code translate="no" dir="ltr">dynamic_rnn</code> via <code translate="no" dir="ltr">raw_rnn</code> looks like this:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">inputs = tf.compat.v1.placeholder(shape=(max_time, batch_size, input_depth),
                        dtype=tf.float32)
sequence_length = tf.compat.v1.placeholder(shape=(batch_size,),
dtype=tf.int32)
inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_time)
inputs_ta = inputs_ta.unstack(inputs)

cell = tf.compat.v1.nn.rnn_cell.LSTMCell(num_units)

def loop_fn(time, cell_output, cell_state, loop_state):
  emit_output = cell_output  # == None for time == 0
  if cell_output is None:  # time == 0
    next_cell_state = cell.zero_state(batch_size, tf.float32)
  else:
    next_cell_state = cell_state
  elements_finished = (time &gt;= sequence_length)
  finished = tf.reduce_all(elements_finished)
  next_input = tf.cond(
      finished,
      lambda: tf.zeros([batch_size, input_depth], dtype=tf.float32),
      lambda: inputs_ta.read(time))
  next_loop_state = None
  return (elements_finished, next_input, next_cell_state,
          emit_output, next_loop_state)

outputs_ta, final_state, _ = raw_rnn(cell, loop_fn)
outputs = outputs_ta.stack()
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">cell</code> </td> <td> An instance of RNNCell. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">loop_fn</code> </td> <td> A callable that takes inputs <code translate="no" dir="ltr">(time, cell_output, cell_state, loop_state)</code> and returns the tuple <code translate="no" dir="ltr">(finished, next_input, next_cell_state, emit_output, next_loop_state)</code>. Here <code translate="no" dir="ltr">time</code> is an int32 scalar <code translate="no" dir="ltr">Tensor</code>, <code translate="no" dir="ltr">cell_output</code> is a <code translate="no" dir="ltr">Tensor</code> or (possibly nested) tuple of tensors as determined by <code translate="no" dir="ltr">cell.output_size</code>, and <code translate="no" dir="ltr">cell_state</code> is a <code translate="no" dir="ltr">Tensor</code> or (possibly nested) tuple of tensors, as determined by the <code translate="no" dir="ltr">loop_fn</code> on its first call (and should match <code translate="no" dir="ltr">cell.state_size</code>). The outputs are: <code translate="no" dir="ltr">finished</code>, a boolean <code translate="no" dir="ltr">Tensor</code> of shape <code translate="no" dir="ltr">[batch_size]</code>, <code translate="no" dir="ltr">next_input</code>: the next input to feed to <code translate="no" dir="ltr">cell</code>, <code translate="no" dir="ltr">next_cell_state</code>: the next state to feed to <code translate="no" dir="ltr">cell</code>, and <code translate="no" dir="ltr">emit_output</code>: the output to store for this iteration. Note that <code translate="no" dir="ltr">emit_output</code> should be a <code translate="no" dir="ltr">Tensor</code> or (possibly nested) tuple of tensors which is aggregated in the <code translate="no" dir="ltr">emit_ta</code> inside the <code translate="no" dir="ltr">while_loop</code>. For the first call to <code translate="no" dir="ltr">loop_fn</code>, the <code translate="no" dir="ltr">emit_output</code> corresponds to the <code translate="no" dir="ltr">emit_structure</code> which is then used to determine the size of the <code translate="no" dir="ltr">zero_tensor</code> for the <code translate="no" dir="ltr">emit_ta</code> (defaults to <code translate="no" dir="ltr">cell.output_size</code>). For the subsequent calls to the <code translate="no" dir="ltr">loop_fn</code>, the <code translate="no" dir="ltr">emit_output</code> corresponds to the actual output tensor that is to be aggregated in the <code translate="no" dir="ltr">emit_ta</code>. The parameter <code translate="no" dir="ltr">cell_state</code> and output <code translate="no" dir="ltr">next_cell_state</code> may be either a single or (possibly nested) tuple of tensors. The parameter <code translate="no" dir="ltr">loop_state</code> and output <code translate="no" dir="ltr">next_loop_state</code> may be either a single or (possibly nested) tuple of <code translate="no" dir="ltr">Tensor</code> and <code translate="no" dir="ltr">TensorArray</code> objects. This last parameter may be ignored by <code translate="no" dir="ltr">loop_fn</code> and the return value may be <code translate="no" dir="ltr">None</code>. If it is not <code translate="no" dir="ltr">None</code>, then the <code translate="no" dir="ltr">loop_state</code> will be propagated through the RNN loop, for use purely by <code translate="no" dir="ltr">loop_fn</code> to keep track of its own state. The <code translate="no" dir="ltr">next_loop_state</code> parameter returned may be <code translate="no" dir="ltr">None</code>. The first call to <code translate="no" dir="ltr">loop_fn</code> will be <code translate="no" dir="ltr">time = 0</code>, <code translate="no" dir="ltr">cell_output = None</code>, <code translate="no" dir="ltr">cell_state = None</code>, and <code translate="no" dir="ltr">loop_state = None</code>. For this call: The <code translate="no" dir="ltr">next_cell_state</code> value should be the value with which to initialize the cell's state. It may be a final state from a previous RNN or it may be the output of <code translate="no" dir="ltr">cell.zero_state()</code>. It should be a (possibly nested) tuple structure of tensors. If <code translate="no" dir="ltr">cell.state_size</code> is an integer, this must be a <code translate="no" dir="ltr">Tensor</code> of appropriate type and shape <code translate="no" dir="ltr">[batch_size, cell.state_size]</code>. If <code translate="no" dir="ltr">cell.state_size</code> is a <code translate="no" dir="ltr">TensorShape</code>, this must be a <code translate="no" dir="ltr">Tensor</code> of appropriate type and shape <code translate="no" dir="ltr">[batch_size] + cell.state_size</code>. If <code translate="no" dir="ltr">cell.state_size</code> is a (possibly nested) tuple of ints or <code translate="no" dir="ltr">TensorShape</code>, this will be a tuple having the corresponding shapes. The <code translate="no" dir="ltr">emit_output</code> value may be either <code translate="no" dir="ltr">None</code> or a (possibly nested) tuple structure of tensors, e.g., <code translate="no" dir="ltr">(tf.zeros(shape_0, dtype=dtype_0), tf.zeros(shape_1, dtype=dtype_1))</code>. If this first <code translate="no" dir="ltr">emit_output</code> return value is <code translate="no" dir="ltr">None</code>, then the <code translate="no" dir="ltr">emit_ta</code> result of <code translate="no" dir="ltr">raw_rnn</code> will have the same structure and dtypes as <code translate="no" dir="ltr">cell.output_size</code>. Otherwise <code translate="no" dir="ltr">emit_ta</code> will have the same structure, shapes (prepended with a <code translate="no" dir="ltr">batch_size</code> dimension), and dtypes as <code translate="no" dir="ltr">emit_output</code>. The actual values returned for <code translate="no" dir="ltr">emit_output</code> at this initializing call are ignored. Note, this emit structure must be consistent across all time steps. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">parallel_iterations</code> </td> <td> (Default: 32). The number of iterations to run in parallel. Those operations which do not have any temporal dependency and can be run in parallel, will be. This parameter trades off time for space. Values &gt;&gt; 1 use more memory but take less time, while smaller values use less memory but computations take longer. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">swap_memory</code> </td> <td> Transparently swap the tensors produced in forward inference but needed for back prop from GPU to CPU. This allows training RNNs which would typically not fit on a single GPU, with very minimal (or no) performance penalty. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">scope</code> </td> <td> VariableScope for the created subgraph; defaults to "rnn". </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A tuple <code translate="no" dir="ltr">(emit_ta, final_state, final_loop_state)</code> where: <p><code translate="no" dir="ltr">emit_ta</code>: The RNN output <code translate="no" dir="ltr">TensorArray</code>. If <code translate="no" dir="ltr">loop_fn</code> returns a (possibly nested) set of Tensors for <code translate="no" dir="ltr">emit_output</code> during initialization, (inputs <code translate="no" dir="ltr">time = 0</code>, <code translate="no" dir="ltr">cell_output = None</code>, and <code translate="no" dir="ltr">loop_state = None</code>), then <code translate="no" dir="ltr">emit_ta</code> will have the same structure, dtypes, and shapes as <code translate="no" dir="ltr">emit_output</code> instead. If <code translate="no" dir="ltr">loop_fn</code> returns <code translate="no" dir="ltr">emit_output = None</code> during this call, the structure of <code translate="no" dir="ltr">cell.output_size</code> is used: If <code translate="no" dir="ltr">cell.output_size</code> is a (possibly nested) tuple of integers or <code translate="no" dir="ltr">TensorShape</code> objects, then <code translate="no" dir="ltr">emit_ta</code> will be a tuple having the same structure as <code translate="no" dir="ltr">cell.output_size</code>, containing TensorArrays whose elements' shapes correspond to the shape data in <code translate="no" dir="ltr">cell.output_size</code>.</p> <p><code translate="no" dir="ltr">final_state</code>: The final cell state. If <code translate="no" dir="ltr">cell.state_size</code> is an int, this will be shaped <code translate="no" dir="ltr">[batch_size, cell.state_size]</code>. If it is a <code translate="no" dir="ltr">TensorShape</code>, this will be shaped <code translate="no" dir="ltr">[batch_size] + cell.state_size</code>. If it is a (possibly nested) tuple of ints or <code translate="no" dir="ltr">TensorShape</code>, this will be a tuple having the corresponding shapes.</p> <p><code translate="no" dir="ltr">final_loop_state</code>: The final loop state as returned by <code translate="no" dir="ltr">loop_fn</code>. </p>
</td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> If <code translate="no" dir="ltr">cell</code> is not an instance of RNNCell, or <code translate="no" dir="ltr">loop_fn</code> is not a <code translate="no" dir="ltr">callable</code>. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/nn/raw_rnn" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/nn/raw_rnn</a>
  </p>
</div>
