<h1 class="devsite-page-title">tf.compat.v1.nn.fused_batch_norm</h1> <devsite-bookmark></devsite-bookmark>       <p>Batch normalization.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.nn.fused_batch_norm(
    x,
    scale,
    offset,
    mean=None,
    variance=None,
    epsilon=0.001,
    data_format='NHWC',
    is_training=True,
    name=None,
    exponential_avg_factor=1.0
)
</pre>  <p>See Source: <a href="http://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift; S. Ioffe, C. Szegedy</a>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">x</code> </td> <td> Input <code translate="no" dir="ltr">Tensor</code> of 4 or 5 dimensions. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">scale</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> of 1 dimension for scaling. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">offset</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> of 1 dimension for bias. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">mean</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> of 1 dimension for population mean. The shape and meaning of this argument depends on the value of is_training and exponential_avg_factor as follows: is_training<mark>False (inference): Mean must be a <code translate="no" dir="ltr">Tensor</code> of the same shape as scale containing the estimated population mean computed during training. is_training</mark>True and exponential_avg_factor == 1.0: Mean must be None. is_training<mark>True and exponential_avg_factor != 1.0: Mean must be a <code translate="no" dir="ltr">Tensor</code> of the same shape as scale containing the exponential running mean. </mark>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">variance</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> of 1 dimension for population variance. The shape and meaning of this argument depends on the value of is_training and exponential_avg_factor as follows: is_trainingFalse (inference): Variance must be a <code translate="no" dir="ltr">Tensor</code> of the same shape as scale containing the estimated population variance computed during training. is_training==True and exponential_avg_factor == 1.0: Variance must be None. is_training==True and exponential_avg_factor != 1.0: Variance must be a <code translate="no" dir="ltr">Tensor</code> of the same shape as scale containing the exponential running variance. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">epsilon</code> </td> <td> A small float number added to the variance of x. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">data_format</code> </td> <td> The data format for x. Support "NHWC" (default) or "NCHW" for 4D tenors and "NDHWC" or "NCDHW" for 5D tensors. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">is_training</code> </td> <td> A bool value to specify if the operation is used for training or inference. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> A name for this operation (optional). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">exponential_avg_factor</code> </td> <td> A float number (usually between 0 and 1) used for controlling the decay of the running population average of mean and variance. If set to 1.0, the current batch average is returned. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> 
<tr> <td> <code translate="no" dir="ltr">y</code> </td> <td> A 4D or 5D Tensor for the normalized, scaled, offsetted x. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">running_mean</code> </td> <td> A 1D Tensor for the exponential running mean of x. The output value is (1 - exponential_avg_factor) * mean + exponential_avg_factor * batch_mean), where batch_mean is the mean of the current batch in x. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">running_var</code> </td> <td> A 1D Tensor for the exponential running variance The output value is (1 - exponential_avg_factor) * variance + exponential_avg_factor * batch_variance), where batch_variance is the variance of the current batch in x. </td> </tr> </table> <h4 id="references" data-text="References:">References:</h4> <p>Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift: <a href="http://proceedings.mlr.press/v37/ioffe15.html">Ioffe et al., 2015</a> (<a href="http://proceedings.mlr.press/v37/ioffe15.pdf">pdf</a>)</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/nn/fused_batch_norm" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/nn/fused_batch_norm</a>
  </p>
</div>
