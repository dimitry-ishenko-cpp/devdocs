<h1 class="devsite-page-title">tf.compat.v1.keras.utils.track_tf1_style_variables</h1> <devsite-bookmark></devsite-bookmark>       <p>Wrap layer &amp; module methods in this decorator to capture tf1-style weights.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.keras.utils.track_tf1_style_variables(
    method
)
</pre>  <p>Decorating a <code translate="no" dir="ltr">tf.keras.Layer</code>'s or <a href="../../../../module.html"><code translate="no" dir="ltr">tf.Module</code></a>'s methods with this decorator will cause the layer/module to track weights created/used via <a href="../../get_variable.html"><code translate="no" dir="ltr">tf.compat.v1.get_variable</code></a> (and by extension <a href="../../layers.html"><code translate="no" dir="ltr">tf.compat.v1.layers</code></a>) inside the decorated method.</p> <p>In addition to tracking the weights themselves under the standard <code translate="no" dir="ltr">layer.variable</code>/<a href="https://www.tensorflow.org/probability/oryx/api_docs/python/oryx/core/state/variable"><code translate="no" dir="ltr">module.variable</code></a>/etc. properties, if the method belongs to a <code translate="no" dir="ltr">tf.keras.Layer</code> then any regularization losses specified via the <code translate="no" dir="ltr">get_variable</code> or <a href="../../layers.html"><code translate="no" dir="ltr">tf.compat.v1.layers</code></a> regularizer arguments will get tracked by the layer under the standard <code translate="no" dir="ltr">layer.losses</code> property.</p> <p>This tracking enables using large classes of TF1-style model-forward-pass code inside of Keras layers or <code translate="no" dir="ltr">tf.Modules</code> in TF2 with TF2 behaviors enabled.</p> <p>Example of capturing tf.compat.v1.layer-based modeling code as a Keras layer:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">class WrappedDoubleDenseLayer(tf.keras.layers.Layer):

  def __init__(self, units, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.units = units

  @tf.compat.v1.keras.utils.track_tf1_style_variables
  def call(self, inputs):
    with tf.compat.v1.variable_scope("double_dense_layer"):
      out = tf.compat.v1.layers.dense(
          inputs, self.units, name="dense_one",
          kernel_initializer=tf.compat.v1.random_normal_initializer,
          kernel_regularizer="l2")
      out = tf.compat.v1.layers.dense(
          out, self.units, name="dense_two",
          kernel_initializer=tf.compat.v1.random_normal_initializer(),
          kernel_regularizer="l2")
    return out

# Create a layer that can be used as a standard keras layer
layer = WrappedDoubleDenseLayer(10)

# call the layer on inputs
layer(...)

# Variables created/used within the scope will be tracked by the layer
layer.weights
layer.trainable_variables

# Regularization losses will be captured in layer.losses after a call,
# just like any other Keras layer
reg_losses = layer.losses
</pre> <p>Example of capturing tf.compat.v1.get_variable-based modeling code as a Keras layer:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">class WrappedDoubleDenseLayer(tf.keras.layers.Layer):

  def __init__(self, units, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.units = units

  @tf.compat.v1.keras.utils.track_tf1_style_variables
  def call(self, inputs):
    out = inputs
    with tf.compat.v1.variable_scope("double_dense_layer"):
      with tf.compat.v1.variable_scope("dense_one"):
        # The weights are created with a `regularizer`,
        # so the layer should track their regularization losses
        kernel = tf.compat.v1.get_variable(
            shape=[out.shape[-1], self.units],
            regularizer=regularizers.L2(),
            initializer=init_ops.ones_initializer(),
            name="kernel")
        bias = tf.compat.v1.get_variable(
            shape=[self.units,],
            initializer=init_ops.zeros_initializer(),
            name="bias")
        out = tf.compat.v1.math.matmul(out, kernel)
        out = tf.compat.v1.nn.bias_add(out, bias)
      with tf.compat.v1.variable_scope("dense_two"):
        kernel = tf.compat.v1.get_variable(
            shape=[out.shape[-1], self.units],
            regularizer=regularizers.L2(),
            initializer=init_ops.ones_initializer(),
            name="kernel")
        bias = tf.compat.v1.get_variable(
            shape=[self.units,],
            initializer=init_ops.zeros_initializer(),
            name="bias")
        out = tf.compat.v1.math.matmul(out, kernel)
        out = tf.compat.v1.nn.bias_add(out, bias)
    return out

# Create a layer that can be used as a standard keras layer
layer = WrappedDoubleDenseLayer(10)

# call the layer on inputs
layer(...)

# Variables created/used within the scope will be tracked by the layer
layer.weights
layer.trainable_variables

# Regularization losses will be captured in layer.losses after a call,
# just like any other Keras layer
reg_losses = layer.losses
</pre> <h4 id="regularization_losses" data-text="Regularization losses:">Regularization losses:</h4> <p>Any regularizers specified in the <code translate="no" dir="ltr">get_variable</code> calls or <code translate="no" dir="ltr">compat.v1.layer</code> creations will get captured if they occur in your decorated method and the method belongs to a <code translate="no" dir="ltr">tf.keras.Layer</code>/<code translate="no" dir="ltr">tf.keras.Module</code>. Regularization losses are accessible in <code translate="no" dir="ltr">layer.losses</code> after a call just like in a standard Keras layer, and will be captured by any model that includes this layer. Regularization losses attached to Keras layers/models set as attributes of your layer will also get captured in the standard Keras regularization loss tracking.</p> <p>(While Modules have no <code translate="no" dir="ltr">losses</code> property, no-arg callables to compute the regularization losses may be tracked as dict values in a private <code translate="no" dir="ltr">module._tf1_style_var_store._regularizers</code> property, but only for <a href="../../layers.html"><code translate="no" dir="ltr">tf.compat.v1.layers</code></a> and <code translate="no" dir="ltr">get_variable</code> weights and not for any other nested Keras layers/tf.Modules)</p> <p>Variable scope / variable reuse: variable-scope based reuse in your decorated method will be respected, and work like variable-scope based reuse in TF1.</p> <p>Variable Names/Pre-trained checkpoint loading: Variable naming from get_variable and <code translate="no" dir="ltr">compat.v1.layer</code> layers will match the TF1 names, so you should be able to re-use your old name-based checkpoints. Variable naming for Keras layers/models or for variables created by <a href="../../../../variable.html"><code translate="no" dir="ltr">tf.Variable</code></a> may change when going to eager execution.</p> <p>Training Arg if you decorate <code translate="no" dir="ltr">layer.call</code>: Keras will pass a <code translate="no" dir="ltr">training</code> arg to this layer if <code translate="no" dir="ltr">call</code> contains a <code translate="no" dir="ltr">training</code> arg or a <code translate="no" dir="ltr">**kwargs</code> varargs in its call signature, similarly to how keras passes <code translate="no" dir="ltr">training</code> to other layers in TF2 that have similar signatures in their <code translate="no" dir="ltr">call</code> implementations. See more details in the docs on <a href="../../../../keras/layers/layer.html"><code translate="no" dir="ltr">tf.keras.layers.Layer</code></a> to understand what will be passed and when. Note: tf.compat.v1.layers are usually not called with <code translate="no" dir="ltr">training=None</code>, so the training arg to <code translate="no" dir="ltr">forward_pass</code> might not feed through to them unless you pass it to their calls explicitly.</p> <h4 id="caveats" data-text="Caveats:">Caveats:</h4> <ul> <li>TF2 will not prune unused variable updates (or unused outputs). You may need to adjust your forward pass code to avoid computations or variable updates that you don't intend to use.</li> <li>Avoid Nesting variable creation in tf.function inside of methods decorated with <code translate="no" dir="ltr">track_tf1_style_variables</code> While the method may safely be used from inside a <a href="../../../../function.html"><code translate="no" dir="ltr">tf.function</code></a>, using a function inside of a decorated method may break the variable scoping.</li> <li>This decorator only adds implicit tracking for legacy tf1-style get_variable / compat.v1.layers usage. If you would like to use nested Keras layers/models inside the decorated method, you need to assign them as attributes of your layer so that Keras/Module's standard object-oriented weights (and loss tracking for layers) will kick in. See the intro to modules, layers, and models <a href="https://www.tensorflow.org/guide/intro_to_modules">guide</a> for more info. As a backup, the <a href="get_or_create_layer.html"><code translate="no" dir="ltr">compat.v1.keras.utils.get_or_create_layer</code></a> method will ease tracking nested keras model weights and losses for existing TF1 code, but new code should use explicit tracking.</li> </ul>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">method</code> </td> <td> The method to decorate. This should belong to a custom tf.Module, tf.keras.layers.Layer, or tf.keras.Model. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> The decorated method. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/keras/utils/track_tf1_style_variables" class="_attribution-link">https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/compat/v1/keras/utils/track_tf1_style_variables</a>
  </p>
</div>
