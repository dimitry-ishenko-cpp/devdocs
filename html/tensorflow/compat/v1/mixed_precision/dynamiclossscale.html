<h1 class="devsite-page-title" tabindex="-1"> tf.compat.v1.mixed_precision.DynamicLossScale </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.compat.v1.mixed_precision.DynamicLossScale"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__call__"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="from_config"> <meta itemprop="property" content="get_config"> <meta itemprop="property" content="update"> </div>   <p>Loss scale that dynamically adjusts itself.</p> <p>Inherits From: <a href="lossscale.html"><code translate="no" dir="ltr">LossScale</code></a></p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases" tabindex="-1">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="dynamiclossscale.html"><code translate="no" dir="ltr">tf.compat.v1.mixed_precision.experimental.DynamicLossScale</code></a>, <a href="dynamiclossscale.html"><code translate="no" dir="ltr">tf.compat.v1.train.experimental.DynamicLossScale</code></a></p> </section> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.compat.v1.mixed_precision.DynamicLossScale(
    initial_loss_scale=(2 ** 15), increment_period=2000, multiplier=2.0
)
</pre></devsite-code>  <p>Dynamic loss scaling works by adjusting the loss scale as training progresses. The goal is to keep the loss scale as high as possible without overflowing the gradients. As long as the gradients do not overflow, raising the loss scale never hurts.</p> <p>The algorithm starts by setting the loss scale to an initial value. Every N steps that the gradients are finite, the loss scale is increased by some factor. However, if a NaN or Inf gradient is found, the gradients for that step are not applied, and the loss scale is decreased by the factor. This process tends to keep the loss scale as high as possible without gradients overflowing.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">initial_loss_scale</code> </td> <td> A Python float. The loss scale to use at the beginning. It's better to start this at a very high number, because a loss scale that is too high gets lowered far more quickly than a loss scale that is too low gets raised. The default is 2 ** 15, which is approximately half the maximum float16 value. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">increment_period</code> </td> <td> Increases loss scale every <code translate="no" dir="ltr">increment_period</code> consecutive steps that finite gradients are encountered. If a nonfinite gradient is encountered, the count is reset back to zero. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">multiplier</code> </td> <td> The multiplier to use when increasing or decreasing the loss scale. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">increment_period</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">initial_loss_scale</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">multiplier</code> </td> <td> 
</td> </tr> </table> <h2 id="methods" data-text="Methods" tabindex="-1">Methods</h2> <h3 id="from_config" data-text="from_config" tabindex="-1"><code translate="no" dir="ltr">from_config</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/training/experimental/loss_scale.py#L195-L198">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">@classmethod
from_config(
    config
)
</pre></devsite-code> <p>Creates the LossScale from its config.</p> <h3 id="get_config" data-text="get_config" tabindex="-1"><code translate="no" dir="ltr">get_config</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/training/experimental/loss_scale.py#L433-L438">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">get_config()
</pre></devsite-code> <p>Returns the config of this loss scale.</p> <h3 id="update" data-text="update" tabindex="-1"><code translate="no" dir="ltr">update</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/training/experimental/loss_scale.py#L373-L420">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">update(
    grads
)
</pre></devsite-code> <p>Updates loss scale based on if gradients are finite in current step.</p> <h3 id="__call__" data-text="__call__" tabindex="-1"><code translate="no" dir="ltr">__call__</code></h3> <p><a target="_blank" class="external" href="https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/training/experimental/loss_scale.py#L370-L371">View source</a></p> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">__call__()
</pre></devsite-code> <p>Returns the current loss scale as a scalar <code translate="no" dir="ltr">float32</code> tensor.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/mixed_precision/DynamicLossScale" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/compat/v1/mixed_precision/DynamicLossScale</a>
  </p>
</div>
