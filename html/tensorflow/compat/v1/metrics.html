<h1 class="devsite-page-title" tabindex="-1"> Module: tf.compat.v1.metrics </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.compat.v1.metrics"> <meta itemprop="path" content="Stable"> </div>   <p>Public API for tf._api.v2.metrics namespace</p> <h2 id="functions" data-text="Functions" tabindex="-1">Functions</h2> <p><a href="metrics/accuracy.html"><code translate="no" dir="ltr">accuracy(...)</code></a>: Calculates how often <code translate="no" dir="ltr">predictions</code> matches <code translate="no" dir="ltr">labels</code>.</p> <p><a href="metrics/auc.html"><code translate="no" dir="ltr">auc(...)</code></a>: Computes the approximate AUC via a Riemann sum. (deprecated)</p> <p><a href="metrics/average_precision_at_k.html"><code translate="no" dir="ltr">average_precision_at_k(...)</code></a>: Computes average precision@k of predictions with respect to sparse labels.</p> <p><a href="metrics/false_negatives.html"><code translate="no" dir="ltr">false_negatives(...)</code></a>: Computes the total number of false negatives.</p> <p><a href="metrics/false_negatives_at_thresholds.html"><code translate="no" dir="ltr">false_negatives_at_thresholds(...)</code></a>: Computes false negatives at provided threshold values.</p> <p><a href="metrics/false_positives.html"><code translate="no" dir="ltr">false_positives(...)</code></a>: Sum the weights of false positives.</p> <p><a href="metrics/false_positives_at_thresholds.html"><code translate="no" dir="ltr">false_positives_at_thresholds(...)</code></a>: Computes false positives at provided threshold values.</p> <p><a href="metrics/mean.html"><code translate="no" dir="ltr">mean(...)</code></a>: Computes the (weighted) mean of the given values.</p> <p><a href="metrics/mean_absolute_error.html"><code translate="no" dir="ltr">mean_absolute_error(...)</code></a>: Computes the mean absolute error between the labels and predictions.</p> <p><a href="metrics/mean_cosine_distance.html"><code translate="no" dir="ltr">mean_cosine_distance(...)</code></a>: Computes the cosine distance between the labels and predictions.</p> <p><a href="metrics/mean_iou.html"><code translate="no" dir="ltr">mean_iou(...)</code></a>: Calculate per-step mean Intersection-Over-Union (mIOU).</p> <p><a href="metrics/mean_per_class_accuracy.html"><code translate="no" dir="ltr">mean_per_class_accuracy(...)</code></a>: Calculates the mean of the per-class accuracies.</p> <p><a href="metrics/mean_relative_error.html"><code translate="no" dir="ltr">mean_relative_error(...)</code></a>: Computes the mean relative error by normalizing with the given values.</p> <p><a href="metrics/mean_squared_error.html"><code translate="no" dir="ltr">mean_squared_error(...)</code></a>: Computes the mean squared error between the labels and predictions.</p> <p><a href="metrics/mean_tensor.html"><code translate="no" dir="ltr">mean_tensor(...)</code></a>: Computes the element-wise (weighted) mean of the given tensors.</p> <p><a href="metrics/percentage_below.html"><code translate="no" dir="ltr">percentage_below(...)</code></a>: Computes the percentage of values less than the given threshold.</p> <p><a href="metrics/precision.html"><code translate="no" dir="ltr">precision(...)</code></a>: Computes the precision of the predictions with respect to the labels.</p> <p><a href="metrics/precision_at_k.html"><code translate="no" dir="ltr">precision_at_k(...)</code></a>: Computes precision@k of the predictions with respect to sparse labels.</p> <p><a href="metrics/precision_at_thresholds.html"><code translate="no" dir="ltr">precision_at_thresholds(...)</code></a>: Computes precision values for different <code translate="no" dir="ltr">thresholds</code> on <code translate="no" dir="ltr">predictions</code>.</p> <p><a href="metrics/precision_at_top_k.html"><code translate="no" dir="ltr">precision_at_top_k(...)</code></a>: Computes precision@k of the predictions with respect to sparse labels.</p> <p><a href="metrics/recall.html"><code translate="no" dir="ltr">recall(...)</code></a>: Computes the recall of the predictions with respect to the labels.</p> <p><a href="metrics/recall_at_k.html"><code translate="no" dir="ltr">recall_at_k(...)</code></a>: Computes recall@k of the predictions with respect to sparse labels.</p> <p><a href="metrics/recall_at_thresholds.html"><code translate="no" dir="ltr">recall_at_thresholds(...)</code></a>: Computes various recall values for different <code translate="no" dir="ltr">thresholds</code> on <code translate="no" dir="ltr">predictions</code>.</p> <p><a href="metrics/recall_at_top_k.html"><code translate="no" dir="ltr">recall_at_top_k(...)</code></a>: Computes recall@k of top-k predictions with respect to sparse labels.</p> <p><a href="metrics/root_mean_squared_error.html"><code translate="no" dir="ltr">root_mean_squared_error(...)</code></a>: Computes the root mean squared error between the labels and predictions.</p> <p><a href="metrics/sensitivity_at_specificity.html"><code translate="no" dir="ltr">sensitivity_at_specificity(...)</code></a>: Computes the specificity at a given sensitivity.</p> <p><a href="metrics/sparse_average_precision_at_k.html"><code translate="no" dir="ltr">sparse_average_precision_at_k(...)</code></a>: Renamed to <code translate="no" dir="ltr">average_precision_at_k</code>, please use that method instead. (deprecated)</p> <p><a href="metrics/sparse_precision_at_k.html"><code translate="no" dir="ltr">sparse_precision_at_k(...)</code></a>: Renamed to <code translate="no" dir="ltr">precision_at_k</code>, please use that method instead. (deprecated)</p> <p><a href="metrics/specificity_at_sensitivity.html"><code translate="no" dir="ltr">specificity_at_sensitivity(...)</code></a>: Computes the specificity at a given sensitivity.</p> <p><a href="metrics/true_negatives.html"><code translate="no" dir="ltr">true_negatives(...)</code></a>: Sum the weights of true_negatives.</p> <p><a href="metrics/true_negatives_at_thresholds.html"><code translate="no" dir="ltr">true_negatives_at_thresholds(...)</code></a>: Computes true negatives at provided threshold values.</p> <p><a href="metrics/true_positives.html"><code translate="no" dir="ltr">true_positives(...)</code></a>: Sum the weights of true_positives.</p> <p><a href="metrics/true_positives_at_thresholds.html"><code translate="no" dir="ltr">true_positives_at_thresholds(...)</code></a>: Computes true positives at provided threshold values.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/metrics" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/compat/v1/metrics</a>
  </p>
</div>
