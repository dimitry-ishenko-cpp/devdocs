<h1 class="devsite-page-title" tabindex="-1"> tf.stop_gradient </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.stop_gradient"> <meta itemprop="path" content="Stable"> </div>   <p>Stops gradient computation.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases" tabindex="-1">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="stop_gradient.html"><code translate="no" dir="ltr">tf.compat.v1.stop_gradient</code></a></p> </section> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.stop_gradient(
    input, name=None
)
</pre></devsite-code> <h3 id="used-in-the-notebooks" data-text="Used in the notebooks" tabindex="-1">Used in the notebooks</h3> <table class="vertical-rules"> <thead> <tr> <th>Used in the guide</th> </tr> </thead> <tbody> <tr> <td> <ul> <li><a href="https://www.tensorflow.org/guide/advanced_autodiff">Advanced automatic differentiation</a></li> </ul> </td> </tr> </tbody> </table> <p>When executed in a graph, this op outputs its input tensor as-is.</p> <p>When building ops to compute gradients, this op prevents the contribution of its inputs to be taken into account. Normally, the gradient generator adds ops to a graph to compute the derivatives of a specified 'loss' by recursively finding out inputs that contributed to its computation. If you insert this op in the graph it inputs are masked from the gradient generator. They are not taken into account for computing gradients.</p> <p>This is useful any time you want to compute a value with TensorFlow but need to pretend that the value was a constant. For example, the softmax function for a vector x can be written as</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">
def softmax(x):
  numerator = tf.exp(x)
  denominator = tf.reduce_sum(numerator)
  return numerator / denominator
</pre></devsite-code> <p>This however is susceptible to overflow if the values in x are large. An alternative more stable way is to subtract the maximum of x from each of the values.</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">
def stable_softmax(x):
  z = x - tf.reduce_max(x)
  numerator = tf.exp(z)
  denominator = tf.reduce_sum(numerator)
  return numerator / denominator
</pre></devsite-code> <p>However, when we backprop through the softmax to x, we dont want to backprop through the <a href="math/reduce_max.html"><code translate="no" dir="ltr">tf.reduce_max(x)</code></a> (if the max values are not unique then the gradient could flow to the wrong input) calculation and treat that as a constant. Therefore, we should write this out as</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">
def stable_softmax(x):
  z = x - tf.stop_gradient(tf.reduce_max(x))
  numerator = tf.exp(z)
  denominator = tf.reduce_sum(numerator)
  return numerator / denominator
</pre></devsite-code> <p>Some other examples include:</p> <ul> <li>The <em>EM</em> algorithm where the <em>M-step</em> should not involve backpropagation through the output of the <em>E-step</em>.</li> <li>Contrastive divergence training of Boltzmann machines where, when differentiating the energy function, the training must not backpropagate through the graph that generated the samples from the model.</li> <li>Adversarial training, where no backprop should happen through the adversarial example generation process.</li> </ul>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <code translate="no" dir="ltr">Tensor</code>. Has the same type as <code translate="no" dir="ltr">input</code>. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/stop_gradient" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/stop_gradient</a>
  </p>
</div>
