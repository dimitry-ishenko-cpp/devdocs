<h1 class="devsite-page-title" tabindex="-1"> tf.tensor_scatter_nd_update </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.tensor_scatter_nd_update"> <meta itemprop="path" content="Stable"> </div>   <p>Scatter <code translate="no" dir="ltr">updates</code> into an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases" tabindex="-1">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="tensor_scatter_nd_update.html"><code translate="no" dir="ltr">tf.compat.v1.tensor_scatter_nd_update</code></a>, <a href="tensor_scatter_nd_update.html"><code translate="no" dir="ltr">tf.compat.v1.tensor_scatter_update</code></a></p> </section> 
<devsite-code><pre class="devsite-click-to-copy tfo-signature-link" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.tensor_scatter_nd_update(
    tensor, indices, updates, name=None
)
</pre></devsite-code>  <p>This operation creates a new tensor by applying sparse <code translate="no" dir="ltr">updates</code> to the input <code translate="no" dir="ltr">tensor</code>. This is similar to an index assignment.</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp"># Not implemented: tensors cannot be updated inplace.
tensor[indices] = updates
</pre></devsite-code> <p>If an out of bound index is found on CPU, an error is returned.</p> <blockquote> <aside class="warning"><strong>Warning:</strong><span> There are some GPU specific semantics for this operation.</span></aside> <ul> <li>If an out of bound index is found, the index is ignored.</li> <li>The order in which updates are applied is nondeterministic, so the output will be nondeterministic if <code translate="no" dir="ltr">indices</code> contains duplicates.</li> </ul> </blockquote> <p>This operation is very similar to <a href="scatter_nd.html"><code translate="no" dir="ltr">tf.scatter_nd</code></a>, except that the updates are scattered onto an existing tensor (as opposed to a zero-tensor). If the memory for the existing tensor cannot be re-used, a copy is made and updated.</p> <h4 id="in_general" data-text="In general:" tabindex="-1">In general:</h4> <ul> <li>
<code translate="no" dir="ltr">indices</code> is an integer tensor - the indices to update in <code translate="no" dir="ltr">tensor</code>.</li> <li>
<code translate="no" dir="ltr">indices</code> has <strong>at least two</strong> axes, the last axis is the depth of the index vectors.</li> <li>For each index vector in <code translate="no" dir="ltr">indices</code> there is a corresponding entry in <code translate="no" dir="ltr">updates</code>.</li> <li>If the length of the index vectors matches the rank of the <code translate="no" dir="ltr">tensor</code>, then the index vectors each point to scalars in <code translate="no" dir="ltr">tensor</code> and each update is a scalar.</li> <li>If the length of the index vectors is less than the rank of <code translate="no" dir="ltr">tensor</code>, then the index vectors each point to the slices of <code translate="no" dir="ltr">tensor</code> and shape of the updates must match that slice.</li> </ul> <p>Overall this leads to the following shape constraints:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">assert tf.rank(indices) &gt;= 2
index_depth = indices.shape[-1]
batch_shape = indices.shape[:-1]
assert index_depth &lt;= tf.rank(tensor)
outer_shape = tensor.shape[:index_depth]
inner_shape = tensor.shape[index_depth:]
assert updates.shape == batch_shape + inner_shape
</pre></devsite-code> <p>Typical usage is often much simpler than this general form, and it can be better understood starting with simple examples:</p> <h3 id="scalar_updates" data-text="Scalar updates" tabindex="-1">Scalar updates</h3> <p>The simplest usage inserts scalar elements into a tensor by index. In this case, the <code translate="no" dir="ltr">index_depth</code> must equal the rank of the input <code translate="no" dir="ltr">tensor</code>, slice each column of <code translate="no" dir="ltr">indices</code> is an index into an axis of the input <code translate="no" dir="ltr">tensor</code>.</p> <p>In this simplest case the shape constraints are:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">num_updates, index_depth = indices.shape.as_list()
assert updates.shape == [num_updates]
assert index_depth == tf.rank(tensor)`
</pre></devsite-code> <p>For example, to insert 4 scattered elements in a rank-1 tensor with 8 elements.</p> <div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;"> <img style="width:100%" src="data:image/png;base64,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"> </div> <p>This scatter operation would look like this:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tensor = [0, 0, 0, 0, 0, 0, 0, 0]    # tf.rank(tensor) == 1
indices = [[1], [3], [4], [7]]       # num_updates == 4, index_depth == 1
updates = [9, 10, 11, 12]            # num_updates == 4
print(tf.tensor_scatter_nd_update(tensor, indices, updates))
tf.Tensor([ 0 9  0 10  11  0  0 12], shape=(8,), dtype=int32)</pre></devsite-code> <p>The length (first axis) of <code translate="no" dir="ltr">updates</code> must equal the length of the <code translate="no" dir="ltr">indices</code>: <code translate="no" dir="ltr">num_updates</code>. This is the number of updates being inserted. Each scalar update is inserted into <code translate="no" dir="ltr">tensor</code> at the indexed location.</p> <p>For a higher rank input <code translate="no" dir="ltr">tensor</code> scalar updates can be inserted by using an <code translate="no" dir="ltr">index_depth</code> that matches <a href="rank.html"><code translate="no" dir="ltr">tf.rank(tensor)</code></a>:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tensor = [[1, 1], [1, 1], [1, 1]]    # tf.rank(tensor) == 2
indices = [[0, 1], [2, 0]]           # num_updates == 2, index_depth == 2
updates = [5, 10]                    # num_updates == 2
print(tf.tensor_scatter_nd_update(tensor, indices, updates))
tf.Tensor(
    [[ 1  5]
     [ 1  1]
     [10  1]], shape=(3, 2), dtype=int32)</pre></devsite-code> <h3 id="slice_updates" data-text="Slice updates" tabindex="-1">Slice updates</h3> <p>When the input <code translate="no" dir="ltr">tensor</code> has more than one axis scatter can be used to update entire slices.</p> <p>In this case it's helpful to think of the input <code translate="no" dir="ltr">tensor</code> as being a two level array-of-arrays. The shape of this two level array is split into the <code translate="no" dir="ltr">outer_shape</code> and the <code translate="no" dir="ltr">inner_shape</code>.</p> <p><code translate="no" dir="ltr">indices</code> indexes into the outer level of the input tensor (<code translate="no" dir="ltr">outer_shape</code>). and replaces the sub-array at that location with the corresponding item from the <code translate="no" dir="ltr">updates</code> list. The shape of each update is <code translate="no" dir="ltr">inner_shape</code>.</p> <p>When updating a list of slices the shape constraints are:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">num_updates, index_depth = indices.shape.as_list()
outer_shape = tensor.shape[:index_depth]
inner_shape = tensor.shape[index_depth:]
assert updates.shape == [num_updates, inner_shape]
</pre></devsite-code> <p>For example, to update rows of a <code translate="no" dir="ltr">(6, 3)</code> <code translate="no" dir="ltr">tensor</code>:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tensor = tf.zeros([6, 3], dtype=tf.int32)</pre></devsite-code> <p>Use an index depth of one.</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">indices = tf.constant([[2], [4]])     # num_updates == 2, index_depth == 1
num_updates, index_depth = indices.shape.as_list()</pre></devsite-code> <p>The <code translate="no" dir="ltr">outer_shape</code> is <code translate="no" dir="ltr">6</code>, the inner shape is <code translate="no" dir="ltr">3</code>:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">outer_shape = tensor.shape[:index_depth]
inner_shape = tensor.shape[index_depth:]</pre></devsite-code> <p>2 rows are being indexed so 2 <code translate="no" dir="ltr">updates</code> must be supplied. Each update must be shaped to match the <code translate="no" dir="ltr">inner_shape</code>.</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp"># num_updates == 2, inner_shape==3
updates = tf.constant([[1, 2, 3],
                       [4, 5, 6]])</pre></devsite-code> <h4 id="altogether_this_gives" data-text="Altogether this gives:" tabindex="-1">Altogether this gives:</h4> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.tensor_scatter_nd_update(tensor, indices, updates).numpy()
array([[0, 0, 0],
       [0, 0, 0],
       [1, 2, 3],
       [0, 0, 0],
       [4, 5, 6],
       [0, 0, 0]], dtype=int32)</pre></devsite-code> <h4 id="more_slice_update_examples" data-text="More slice update examples" tabindex="-1">More slice update examples</h4> <p>A tensor representing a batch of uniformly sized video clips naturally has 5 axes: <code translate="no" dir="ltr">[batch_size, time, width, height, channels]</code>.</p> <h4 id="for_example" data-text="For example:" tabindex="-1">For example:</h4> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">batch_size, time, width, height, channels = 13,11,7,5,3
video_batch = tf.zeros([batch_size, time, width, height, channels])</pre></devsite-code> <p>To replace a selection of video clips:</p> <ul> <li>Use an <code translate="no" dir="ltr">index_depth</code> of 1 (indexing the <code translate="no" dir="ltr">outer_shape</code>: <code translate="no" dir="ltr">[batch_size]</code>)</li> <li>Provide updates each with a shape matching the <code translate="no" dir="ltr">inner_shape</code>: <code translate="no" dir="ltr">[time, width, height, channels]</code>.</li> </ul> <p>To replace the first two clips with ones:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">indices = [[0],[1]]
new_clips = tf.ones([2, time, width, height, channels])
tf.tensor_scatter_nd_update(video_batch, indices, new_clips)</pre></devsite-code> <p>To replace a selection of frames in the videos:</p> <ul> <li>
<code translate="no" dir="ltr">indices</code> must have an <code translate="no" dir="ltr">index_depth</code> of 2 for the <code translate="no" dir="ltr">outer_shape</code>: <code translate="no" dir="ltr">[batch_size, time]</code>.</li> <li>
<code translate="no" dir="ltr">updates</code> must be shaped like a list of images. Each update must have a shape, matching the <code translate="no" dir="ltr">inner_shape</code>: <code translate="no" dir="ltr">[width, height, channels]</code>.</li> </ul> <p>To replace the first frame of the first three video clips:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">indices = [[0, 0], [1, 0], [2, 0]] # num_updates=3, index_depth=2
new_images = tf.ones([
  # num_updates=3, inner_shape=(width, height, channels)
  3, width, height, channels])
tf.tensor_scatter_nd_update(video_batch, indices, new_images)</pre></devsite-code> <h3 id="folded_indices" data-text="Folded indices" tabindex="-1">Folded indices</h3> <p>In simple cases it's convenient to think of <code translate="no" dir="ltr">indices</code> and <code translate="no" dir="ltr">updates</code> as lists, but this is not a strict requirement. Instead of a flat <code translate="no" dir="ltr">num_updates</code>, the <code translate="no" dir="ltr">indices</code> and <code translate="no" dir="ltr">updates</code> can be folded into a <code translate="no" dir="ltr">batch_shape</code>. This <code translate="no" dir="ltr">batch_shape</code> is all axes of the <code translate="no" dir="ltr">indices</code>, except for the innermost <code translate="no" dir="ltr">index_depth</code> axis.</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">index_depth = indices.shape[-1]
batch_shape = indices.shape[:-1]
</pre></devsite-code><blockquote class="note">
<strong>Note:</strong><span> The one exception is that the <code translate="no" dir="ltr">batch_shape</code> cannot be <code translate="no" dir="ltr">[]</code>. You can't update a single index by passing indices with shape <code translate="no" dir="ltr">[index_depth]</code>.</span>
</blockquote> <p><code translate="no" dir="ltr">updates</code> must have a matching <code translate="no" dir="ltr">batch_shape</code> (the axes before <code translate="no" dir="ltr">inner_shape</code>).</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">assert updates.shape == batch_shape + inner_shape
</pre></devsite-code><blockquote class="note">
<strong>Note:</strong><span> The result is equivalent to flattening the <code translate="no" dir="ltr">batch_shape</code> axes of <code translate="no" dir="ltr">indices</code> and <code translate="no" dir="ltr">updates</code>. This generalization just avoids the need for reshapes when it is more natural to construct "folded" indices and updates.</span>
</blockquote> <p>With this generalization the full shape constraints are:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">assert tf.rank(indices) &gt;= 2
index_depth = indices.shape[-1]
batch_shape = indices.shape[:-1]
assert index_depth &lt;= tf.rank(tensor)
outer_shape = tensor.shape[:index_depth]
inner_shape = tensor.shape[index_depth:]
assert updates.shape == batch_shape + inner_shape
</pre></devsite-code> <p>For example, to draw an <code translate="no" dir="ltr">X</code> on a <code translate="no" dir="ltr">(5,5)</code> matrix start with these indices:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tensor = tf.zeros([5,5])
indices = tf.constant([
 [[0,0],
  [1,1],
  [2,2],
  [3,3],
  [4,4]],
 [[0,4],
  [1,3],
  [2,2],
  [3,1],
  [4,0]],
])
indices.shape.as_list()  # batch_shape == [2, 5], index_depth == 2
[2, 5, 2]</pre></devsite-code> <p>Here the <code translate="no" dir="ltr">indices</code> do not have a shape of <code translate="no" dir="ltr">[num_updates, index_depth]</code>, but a shape of <code translate="no" dir="ltr">batch_shape+[index_depth]</code>.</p> <p>Since the <code translate="no" dir="ltr">index_depth</code> is equal to the rank of <code translate="no" dir="ltr">tensor</code>:</p> <ul> <li>
<code translate="no" dir="ltr">outer_shape</code> is <code translate="no" dir="ltr">(5,5)</code>
</li> <li>
<code translate="no" dir="ltr">inner_shape</code> is <code translate="no" dir="ltr">()</code> - each update is scalar</li> <li>
<code translate="no" dir="ltr">updates.shape</code> is <code translate="no" dir="ltr">batch_shape + inner_shape == (5,2) + ()</code>
</li> </ul> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">updates = [
  [1,1,1,1,1],
  [1,1,1,1,1],
]</pre></devsite-code> <p>Putting this together gives:</p> 
<devsite-code><pre class="devsite-click-to-copy" translate="no" dir="ltr" is-upgraded syntax="Python" data-language="cpp">tf.tensor_scatter_nd_update(tensor, indices, updates).numpy()
array([[1., 0., 0., 0., 1.],
       [0., 1., 0., 1., 0.],
       [0., 0., 1., 0., 0.],
       [0., 1., 0., 1., 0.],
       [1., 0., 0., 0., 1.]], dtype=float32)</pre></devsite-code>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">tensor</code> </td> <td> Tensor to copy/update. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">indices</code> </td> <td> Indices to update. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">updates</code> </td> <td> Updates to apply at the indices. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the operation. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A new tensor with the given shape and updates applied according to the indices. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update</a>
  </p>
</div>
