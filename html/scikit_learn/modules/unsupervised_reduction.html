<section id="unsupervised-dimensionality-reduction"> <h1 id="data-reduction">6.5. Unsupervised dimensionality reduction</h1> <p>If your number of features is high, it may be useful to reduce it with an unsupervised step prior to supervised steps. Many of the <a class="reference internal" href="https://scikit-learn.org/1.1/unsupervised_learning.html#unsupervised-learning"><span class="std std-ref">Unsupervised learning</span></a> methods implement a <code>transform</code> method that can be used to reduce the dimensionality. Below we discuss two specific example of this pattern that are heavily used.</p> <aside class="topic"> <p class="topic-title"><strong>Pipelining</strong></p> <p>The unsupervised data reduction and the supervised estimator can be chained in one step. See <a class="reference internal" href="compose.html#pipeline"><span class="std std-ref">Pipeline: chaining estimators</span></a>.</p> </aside> <section id="pca-principal-component-analysis"> <h2>
<span class="section-number">6.5.1. </span>PCA: principal component analysis</h2> <p><a class="reference internal" href="generated/sklearn.decomposition.pca.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>decomposition.PCA</code></a> looks for a combination of features that capture well the variance of the original features. See <a class="reference internal" href="decomposition.html#decompositions"><span class="std std-ref">Decomposing signals in components (matrix factorization problems)</span></a>.</p> <aside class="topic"> <p class="topic-title"><strong>Examples</strong></p> <ul class="simple"> <li><a class="reference internal" href="../auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py"><span class="std std-ref">Faces recognition example using eigenfaces and SVMs</span></a></li> </ul> </aside> </section> <section id="random-projections"> <h2>
<span class="section-number">6.5.2. </span>Random projections</h2> <p>The module: <code>random_projection</code> provides several tools for data reduction by random projections. See the relevant section of the documentation: <a class="reference internal" href="random_projection.html#random-projection"><span class="std std-ref">Random Projection</span></a>.</p> <aside class="topic"> <p class="topic-title"><strong>Examples</strong></p> <ul class="simple"> <li><a class="reference internal" href="../auto_examples/miscellaneous/plot_johnson_lindenstrauss_bound.html#sphx-glr-auto-examples-miscellaneous-plot-johnson-lindenstrauss-bound-py"><span class="std std-ref">The Johnson-Lindenstrauss bound for embedding with random projections</span></a></li> </ul> </aside> </section> <section id="feature-agglomeration"> <h2>
<span class="section-number">6.5.3. </span>Feature agglomeration</h2> <p><a class="reference internal" href="generated/sklearn.cluster.featureagglomeration.html#sklearn.cluster.FeatureAgglomeration" title="sklearn.cluster.FeatureAgglomeration"><code>cluster.FeatureAgglomeration</code></a> applies <a class="reference internal" href="clustering.html#hierarchical-clustering"><span class="std std-ref">Hierarchical clustering</span></a> to group together features that behave similarly.</p> <aside class="topic"> <p class="topic-title"><strong>Examples</strong></p> <ul class="simple"> <li><a class="reference internal" href="../auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html#sphx-glr-auto-examples-cluster-plot-feature-agglomeration-vs-univariate-selection-py"><span class="std std-ref">Feature agglomeration vs. univariate selection</span></a></li> <li><a class="reference internal" href="../auto_examples/cluster/plot_digits_agglomeration.html#sphx-glr-auto-examples-cluster-plot-digits-agglomeration-py"><span class="std std-ref">Feature agglomeration</span></a></li> </ul> </aside> <aside class="topic"> <p class="topic-title"><strong>Feature scaling</strong></p> <p>Note that if features have very different scaling or statistical properties, <a class="reference internal" href="generated/sklearn.cluster.featureagglomeration.html#sklearn.cluster.FeatureAgglomeration" title="sklearn.cluster.FeatureAgglomeration"><code>cluster.FeatureAgglomeration</code></a> may not be able to capture the links between related features. Using a <a class="reference internal" href="generated/sklearn.preprocessing.standardscaler.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code>preprocessing.StandardScaler</code></a> can be useful in these settings.</p> </aside> </section> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2022 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.1/modules/unsupervised_reduction.html" class="_attribution-link">https://scikit-learn.org/1.1/modules/unsupervised_reduction.html</a>
  </p>
</div>
