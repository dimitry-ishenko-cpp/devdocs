<section id="sklearn-cluster-ward-tree"> <h1>sklearn.cluster.ward_tree</h1> <dl class="py function"> <dt class="sig sig-object py" id="sklearn.cluster.ward_tree"> <span class="sig-prename descclassname">sklearn.cluster.</span><span class="sig-name descname">ward_tree</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">connectivity</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_clusters</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">return_distance</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/cluster/_agglomerative.py#L169"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Ward clustering based on a Feature matrix.</p> <p>Recursively merges the pair of clusters that minimally increases within-cluster variance.</p> <p>The inertia matrix uses a Heapq-based representation.</p> <p>This is the structured version, that takes into account some topological structure between samples.</p> <p>Read more in the <a class="reference internal" href="../clustering.html#hierarchical-clustering"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>Feature matrix representing <code>n_samples</code> samples to be clustered.</p> </dd> <dt>
<strong>connectivity</strong><span class="classifier">sparse matrix, default=None</span>
</dt>
<dd>
<p>Connectivity matrix. Defines for each sample the neighboring samples following a given structure of the data. The matrix is assumed to be symmetric and only the upper triangular half is used. Default is None, i.e, the Ward algorithm is unstructured.</p> </dd> <dt>
<strong>n_clusters</strong><span class="classifier">int, default=None</span>
</dt>
<dd>
<p><code>n_clusters</code> should be less than <code>n_samples</code>. Stop early the construction of the tree at <code>n_clusters.</code> This is useful to decrease computation time if the number of clusters is not small compared to the number of samples. In this case, the complete tree is not computed, thus the ‘children’ output is of limited use, and the ‘parents’ output should rather be used. This option is valid only when specifying a connectivity matrix.</p> </dd> <dt>
<strong>return_distance</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>If <code>True</code>, return the distance between the clusters.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl> <dt>
<strong>children</strong><span class="classifier">ndarray of shape (n_nodes-1, 2)</span>
</dt>
<dd>
<p>The children of each non-leaf node. Values less than <code>n_samples</code> correspond to leaves of the tree which are the original samples. A node <code>i</code> greater than or equal to <code>n_samples</code> is a non-leaf node and has children <code>children_[i - n_samples]</code>. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node <code>n_samples + i</code>.</p> </dd> <dt>
<strong>n_connected_components</strong><span class="classifier">int</span>
</dt>
<dd>
<p>The number of connected components in the graph.</p> </dd> <dt>
<strong>n_leaves</strong><span class="classifier">int</span>
</dt>
<dd>
<p>The number of leaves in the tree.</p> </dd> <dt>
<strong>parents</strong><span class="classifier">ndarray of shape (n_nodes,) or None</span>
</dt>
<dd>
<p>The parent of each node. Only returned when a connectivity matrix is specified, elsewhere ‘None’ is returned.</p> </dd> <dt>
<strong>distances</strong><span class="classifier">ndarray of shape (n_nodes-1,)</span>
</dt>
<dd>
<p>Only returned if <code>return_distance</code> is set to <code>True</code> (for compatibility). The distances between the centers of the nodes. <code>distances[i]</code> corresponds to a weighted Euclidean distance between the nodes <code>children[i, 1]</code> and <code>children[i, 2]</code>. If the nodes refer to leaves of the tree, then <code>distances[i]</code> is their unweighted Euclidean distance. Distances are updated in the following way (from scipy.hierarchy.linkage):</p> <p>The new entry <span class="math notranslate nohighlight">\(d(u,v)\)</span> is computed as follows,</p> <div class="math notranslate nohighlight"> \[d(u,v) = \sqrt{\frac{|v|+|s|} {T}d(v,s)^2 + \frac{|v|+|t|} {T}d(v,t)^2 - \frac{|v|} {T}d(s,t)^2}\]</div> <p>where <span class="math notranslate nohighlight">\(u\)</span> is the newly joined cluster consisting of clusters <span class="math notranslate nohighlight">\(s\)</span> and <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(v\)</span> is an unused cluster in the forest, <span class="math notranslate nohighlight">\(T=|v|+|s|+|t|\)</span>, and <span class="math notranslate nohighlight">\(|*|\)</span> is the cardinality of its argument. This is also known as the incremental algorithm.</p> </dd> </dl> </dd> </dl> </dd>
</dl> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2022 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.1/modules/generated/sklearn.cluster.ward_tree.html" class="_attribution-link">https://scikit-learn.org/1.1/modules/generated/sklearn.cluster.ward_tree.html</a>
  </p>
</div>
