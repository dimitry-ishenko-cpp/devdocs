<section id="sklearn-linear-model-lars-path-gram"> <h1>sklearn.linear_model.lars_path_gram</h1> <dl class="py function"> <dt class="sig sig-object py" id="sklearn.linear_model.lars_path_gram"> <span class="sig-prename descclassname">sklearn.linear_model.</span><span class="sig-name descname">lars_path_gram</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">Xy</span></em>, <em class="sig-param"><span class="n">Gram</span></em>, <em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">n_samples</span></em>, <em class="sig-param"><span class="n">max_iter</span><span class="o">=</span><span class="default_value">500</span></em>, <em class="sig-param"><span class="n">alpha_min</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">method</span><span class="o">=</span><span class="default_value">'lar'</span></em>, <em class="sig-param"><span class="n">copy_X</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">2.220446049250313e-16</span></em>, <em class="sig-param"><span class="n">copy_Gram</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">return_path</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">return_n_iter</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">positive</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/linear_model/_least_angle.py#L188"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>lars_path in the sufficient stats mode [1]</p> <p>The optimization objective for the case method=’lasso’ is:</p> <pre data-language="python">(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
</pre> <p>in the case of method=’lars’, the objective function is only known in the form of an implicit equation (see discussion in [1])</p> <p>Read more in the <a class="reference internal" href="../linear_model.html#least-angle-regression"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>Xy</strong><span class="classifier">array-like of shape (n_samples,) or (n_samples, n_targets)</span>
</dt>
<dd>
<p>Xy = np.dot(X.T, y).</p> </dd> <dt>
<strong>Gram</strong><span class="classifier">array-like of shape (n_features, n_features)</span>
</dt>
<dd>
<p>Gram = np.dot(X.T * X).</p> </dd> <dt>
<strong>n_samples</strong><span class="classifier">int or float</span>
</dt>
<dd>
<p>Equivalent size of sample.</p> </dd> <dt>
<strong>max_iter</strong><span class="classifier">int, default=500</span>
</dt>
<dd>
<p>Maximum number of iterations to perform, set to infinity for no limit.</p> </dd> <dt>
<strong>alpha_min</strong><span class="classifier">float, default=0</span>
</dt>
<dd>
<p>Minimum correlation along the path. It corresponds to the regularization parameter alpha parameter in the Lasso.</p> </dd> <dt>
<strong>method</strong><span class="classifier">{‘lar’, ‘lasso’}, default=’lar’</span>
</dt>
<dd>
<p>Specifies the returned model. Select <code>'lar'</code> for Least Angle Regression, <code>'lasso'</code> for the Lasso.</p> </dd> <dt>
<strong>copy_X</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>If <code>False</code>, <code>X</code> is overwritten.</p> </dd> <dt>
<strong>eps</strong><span class="classifier">float, default=np.finfo(float).eps</span>
</dt>
<dd>
<p>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the <code>tol</code> parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.</p> </dd> <dt>
<strong>copy_Gram</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>If <code>False</code>, <code>Gram</code> is overwritten.</p> </dd> <dt>
<strong>verbose</strong><span class="classifier">int, default=0</span>
</dt>
<dd>
<p>Controls output verbosity.</p> </dd> <dt>
<strong>return_path</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>If <code>return_path==True</code> returns the entire path, else returns only the last point of the path.</p> </dd> <dt>
<strong>return_n_iter</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>Whether to return the number of iterations.</p> </dd> <dt>
<strong>positive</strong><span class="classifier">bool, default=False</span>
</dt>
<dd>
<p>Restrict coefficients to be &gt;= 0. This option is only allowed with method ‘lasso’. Note that the model coefficients will not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (<code>alphas_[alphas_ &gt; 0.].min()</code> when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent lasso_path function.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>alphas</strong><span class="classifier">array-like of shape (n_alphas + 1,)</span>
</dt>
<dd>
<p>Maximum of covariances (in absolute value) at each iteration. <code>n_alphas</code> is either <code>max_iter</code>, <code>n_features</code> or the number of nodes in the path with <code>alpha &gt;= alpha_min</code>, whichever is smaller.</p> </dd> <dt>
<strong>active</strong><span class="classifier">array-like of shape (n_alphas,)</span>
</dt>
<dd>
<p>Indices of active variables at the end of the path.</p> </dd> <dt>
<strong>coefs</strong><span class="classifier">array-like of shape (n_features, n_alphas + 1)</span>
</dt>
<dd>
<p>Coefficients along the path</p> </dd> <dt>
<strong>n_iter</strong><span class="classifier">int</span>
</dt>
<dd>
<p>Number of iterations run. Returned only if return_n_iter is set to True.</p> </dd> </dl> </dd> </dl> <div class="admonition seealso"> <p class="admonition-title">See also</p> <dl class="simple"> <dt><a class="reference internal" href="sklearn.linear_model.lars_path.html#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code>lars_path</code></a></dt>
 <dt><a class="reference internal" href="sklearn.linear_model.lasso_path.html#sklearn.linear_model.lasso_path" title="sklearn.linear_model.lasso_path"><code>lasso_path</code></a></dt>
 <dt><code>lasso_path_gram</code></dt>
 <dt><a class="reference internal" href="sklearn.linear_model.lassolars.html#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code>LassoLars</code></a></dt>
 <dt><a class="reference internal" href="sklearn.linear_model.lars.html#sklearn.linear_model.Lars" title="sklearn.linear_model.Lars"><code>Lars</code></a></dt>
 <dt><a class="reference internal" href="sklearn.linear_model.lassolarscv.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code>LassoLarsCV</code></a></dt>
 <dt><a class="reference internal" href="sklearn.linear_model.larscv.html#sklearn.linear_model.LarsCV" title="sklearn.linear_model.LarsCV"><code>LarsCV</code></a></dt>
 <dt><a class="reference internal" href="sklearn.decomposition.sparse_encode.html#sklearn.decomposition.sparse_encode" title="sklearn.decomposition.sparse_encode"><code>sklearn.decomposition.sparse_encode</code></a></dt>
 </dl> </div> <h4 class="rubric">References</h4> <div role="list" class="citation-list"> <div class="citation" id="r34229eeff553-1" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span> <p>“Least Angle Regression”, Efron et al. <a class="reference external" href="http://statweb.stanford.edu/~tibs/ftp/lars.pdf">http://statweb.stanford.edu/~tibs/ftp/lars.pdf</a></p> </div> <div class="citation" id="r34229eeff553-2" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span> <p><a class="reference external" href="https://en.wikipedia.org/wiki/Least-angle_regression">Wikipedia entry on the Least-angle regression</a></p> </div> <div class="citation" id="r34229eeff553-3" role="doc-biblioentry"> <span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span> <p><a class="reference external" href="https://en.wikipedia.org/wiki/Lasso_(statistics)">Wikipedia entry on the Lasso</a></p> </div> </div> </dd>
</dl> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2022 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.1/modules/generated/sklearn.linear_model.lars_path_gram.html" class="_attribution-link">https://scikit-learn.org/1.1/modules/generated/sklearn.linear_model.lars_path_gram.html</a>
  </p>
</div>
