<section id="sklearn-feature-extraction-featurehasher"> <h1>sklearn.feature_extraction.FeatureHasher</h1> <dl class="py class"> <dt class="sig sig-object py" id="sklearn.feature_extraction.FeatureHasher"> <em class="property">class</em><span class="sig-prename descclassname">sklearn.feature_extraction.</span><span class="sig-name descname">FeatureHasher</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">n_features=1048576</span></em>, <em class="sig-param"><span class="n">*</span></em>, <em class="sig-param"><span class="n">input_type='dict'</span></em>, <em class="sig-param"><span class="n">dtype=&lt;class 'numpy.float64'&gt;</span></em>, <em class="sig-param"><span class="n">alternate_sign=True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/feature_extraction/_hash.py#L18"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Implements feature hashing, aka the hashing trick.</p> <p>This class turns sequences of symbolic feature names (strings) into scipy.sparse matrices, using a hash function to compute the matrix column corresponding to a name. The hash function employed is the signed 32-bit version of Murmurhash3.</p> <p>Feature names of type byte string are used as-is. Unicode strings are converted to UTF-8 first, but no Unicode normalization is done. Feature values must be (finite) numbers.</p> <p>This class is a low-memory alternative to DictVectorizer and CountVectorizer, intended for large-scale (online) learning and situations where memory is tight, e.g. when running prediction code on embedded devices.</p> <p>Read more in the <a class="reference internal" href="../feature_extraction.html#feature-hashing"><span class="std std-ref">User Guide</span></a>.</p> <div class="versionadded"> <p><span class="versionmodified added">New in version 0.13.</span></p> </div> <dl class="field-list"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl> <dt>
<strong>n_features</strong><span class="classifier">int, default=2**20</span>
</dt>
<dd>
<p>The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners.</p> </dd> <dt>
<strong>input_type</strong><span class="classifier">str, default=’dict’</span>
</dt>
<dd>
<p>Choose a string from {‘dict’, ‘pair’, ‘string’}. Either “dict” (the default) to accept dictionaries over (feature_name, value); “pair” to accept pairs of (feature_name, value); or “string” to accept single strings. feature_name should be a string, while value should be a number. In the case of “string”, a value of 1 is implied. The feature_name is hashed to find the appropriate column for the feature. The value’s sign might be flipped in the output (but see non_negative, below).</p> </dd> <dt>
<strong>dtype</strong><span class="classifier">numpy dtype, default=np.float64</span>
</dt>
<dd>
<p>The type of feature values. Passed to scipy.sparse matrix constructors as the dtype argument. Do not set this to bool, np.boolean or any unsigned integer type.</p> </dd> <dt>
<strong>alternate_sign</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>When True, an alternating sign is added to the features as to approximately conserve the inner product in the hashed space even for small n_features. This approach is similar to sparse random projection.</p> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 0.19: </span><code>alternate_sign</code> replaces the now deprecated <code>non_negative</code> parameter.</p> </div> </dd> </dl> </dd> </dl> <div class="admonition seealso"> <p class="admonition-title">See also</p> <dl class="simple"> <dt><a class="reference internal" href="sklearn.feature_extraction.dictvectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code>DictVectorizer</code></a></dt>
<dd>
<p>Vectorizes string-valued features using a hash table.</p> </dd> <dt><a class="reference internal" href="sklearn.preprocessing.onehotencoder.html#sklearn.preprocessing.OneHotEncoder" title="sklearn.preprocessing.OneHotEncoder"><code>sklearn.preprocessing.OneHotEncoder</code></a></dt>
<dd>
<p>Handles nominal/categorical features.</p> </dd> </dl> </div> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from sklearn.feature_extraction import FeatureHasher
&gt;&gt;&gt; h = FeatureHasher(n_features=10)
&gt;&gt;&gt; D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]
&gt;&gt;&gt; f = h.transform(D)
&gt;&gt;&gt; f.toarray()
array([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.],
       [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])
</pre> <h4 class="rubric">Methods</h4> <table class="autosummary longtable docutils align-default">  <tr>
<td><p><a class="reference internal" href="#sklearn.feature_extraction.FeatureHasher.fit" title="sklearn.feature_extraction.FeatureHasher.fit"><code>fit</code></a>([X, y])</p></td> <td><p>No-op.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.feature_extraction.FeatureHasher.fit_transform" title="sklearn.feature_extraction.FeatureHasher.fit_transform"><code>fit_transform</code></a>(X[, y])</p></td> <td><p>Fit to data, then transform it.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.feature_extraction.FeatureHasher.get_params" title="sklearn.feature_extraction.FeatureHasher.get_params"><code>get_params</code></a>([deep])</p></td> <td><p>Get parameters for this estimator.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.feature_extraction.FeatureHasher.set_params" title="sklearn.feature_extraction.FeatureHasher.set_params"><code>set_params</code></a>(**params)</p></td> <td><p>Set the parameters of this estimator.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="#sklearn.feature_extraction.FeatureHasher.transform" title="sklearn.feature_extraction.FeatureHasher.transform"><code>transform</code></a>(raw_X)</p></td> <td><p>Transform a sequence of instances to a scipy.sparse matrix.</p></td> </tr>  </table> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.feature_extraction.FeatureHasher.fit"> <span class="sig-name descname">fit</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/feature_extraction/_hash.py#L114"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>No-op.</p> <p>This method doesn’t do anything. It exists purely for compatibility with the scikit-learn transformer API.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>X</strong><span class="classifier">Ignored</span>
</dt>
<dd>
<p>Not used, present here for API consistency by convention.</p> </dd> <dt>
<strong>y</strong><span class="classifier">Ignored</span>
</dt>
<dd>
<p>Not used, present here for API consistency by convention.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>self</strong><span class="classifier">object</span>
</dt>
<dd>
<p>FeatureHasher class instance.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.feature_extraction.FeatureHasher.fit_transform"> <span class="sig-name descname">fit_transform</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">y</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">fit_params</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/base.py#L839"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fit to data, then transform it.</p> <p>Fits transformer to <code>X</code> and <code>y</code> with optional parameters <code>fit_params</code> and returns a transformed version of <code>X</code>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>X</strong><span class="classifier">array-like of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>Input samples.</p> </dd> <dt>
<strong>y</strong><span class="classifier">array-like of shape (n_samples,) or (n_samples, n_outputs), default=None</span>
</dt>
<dd>
<p>Target values (None for unsupervised transformations).</p> </dd> <dt>
<strong>**fit_params</strong><span class="classifier">dict</span>
</dt>
<dd>
<p>Additional fit parameters.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>X_new</strong><span class="classifier">ndarray array of shape (n_samples, n_features_new)</span>
</dt>
<dd>
<p>Transformed array.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.feature_extraction.FeatureHasher.get_params"> <span class="sig-name descname">get_params</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">deep</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/base.py#L194"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Get parameters for this estimator.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>deep</strong><span class="classifier">bool, default=True</span>
</dt>
<dd>
<p>If True, will return the parameters for this estimator and contained subobjects that are estimators.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>params</strong><span class="classifier">dict</span>
</dt>
<dd>
<p>Parameter names mapped to their values.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.feature_extraction.FeatureHasher.set_params"> <span class="sig-name descname">set_params</span><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">params</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/base.py#L218"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as <a class="reference internal" href="sklearn.pipeline.pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code>Pipeline</code></a>). The latter have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it’s possible to update each component of a nested object.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>**params</strong><span class="classifier">dict</span>
</dt>
<dd>
<p>Estimator parameters.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>self</strong><span class="classifier">estimator instance</span>
</dt>
<dd>
<p>Estimator instance.</p> </dd> </dl> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="sklearn.feature_extraction.FeatureHasher.transform"> <span class="sig-name descname">transform</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">raw_X</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/feature_extraction/_hash.py#L137"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Transform a sequence of instances to a scipy.sparse matrix.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters<span class="colon">:</span>
</dt> <dd class="field-odd">
<dl class="simple"> <dt>
<strong>raw_X</strong><span class="classifier">iterable over iterable over raw features, length = n_samples</span>
</dt>
<dd>
<p>Samples. Each sample must be iterable an (e.g., a list or tuple) containing/generating feature names (and optionally values, see the input_type constructor argument) which will be hashed. raw_X need not support the len function, so it can be the result of a generator; n_samples is determined on the fly.</p> </dd> </dl> </dd> <dt class="field-even">Returns<span class="colon">:</span>
</dt> <dd class="field-even">
<dl class="simple"> <dt>
<strong>X</strong><span class="classifier">sparse matrix of shape (n_samples, n_features)</span>
</dt>
<dd>
<p>Feature matrix, for use with estimators or further transformers.</p> </dd> </dl> </dd> </dl> </dd>
</dl> </dd>
</dl> <section id="examples-using-sklearn-feature-extraction-featurehasher"> <h2>Examples using <code>sklearn.feature_extraction.FeatureHasher</code>
</h2> <div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="In this example we illustrate text vectorization, which is the process of representing non-nume...">
<img alt="FeatureHasher and DictVectorizer Comparison" src="data:image/png;base64,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"> <p><a class="reference internal" href="../../auto_examples/text/plot_hashing_vs_dict_vectorizer.html#sphx-glr-auto-examples-text-plot-hashing-vs-dict-vectorizer-py"><span class="std std-ref">FeatureHasher and DictVectorizer Comparison</span></a></p>  </div></div>
</section> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2022 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.1/modules/generated/sklearn.feature_extraction.FeatureHasher.html" class="_attribution-link">https://scikit-learn.org/1.1/modules/generated/sklearn.feature_extraction.FeatureHasher.html</a>
  </p>
</div>
