<div class="sphx-glr-download-link-note admonition note"> <p class="admonition-title">Note</p> <p>Click <a class="reference internal" href="#sphx-glr-download-auto-examples-text-plot-document-clustering-py"><span class="std std-ref">here</span></a> to download the full example code or to run this example in your browser via Binder</p> </div> <section class="sphx-glr-example-title" id="clustering-text-documents-using-k-means"> <h1 id="sphx-glr-auto-examples-text-plot-document-clustering-py">Clustering text documents using k-means</h1> <p>This is an example showing how the scikit-learn API can be used to cluster documents by topics using a <a class="reference external" href="https://en.wikipedia.org/wiki/Bag-of-words_model">Bag of Words approach</a>.</p> <p>Two algorithms are demoed: <a class="reference internal" href="../../modules/generated/sklearn.cluster.kmeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code>KMeans</code></a> and its more scalable variant, <a class="reference internal" href="../../modules/generated/sklearn.cluster.minibatchkmeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code>MiniBatchKMeans</code></a>. Additionally, latent semantic analysis is used to reduce dimensionality and discover latent patterns in the data.</p> <p>This example uses two different text vectorizers: a <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.tfidfvectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code>TfidfVectorizer</code></a> and a <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.hashingvectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code>HashingVectorizer</code></a>. See the example notebook <a class="reference internal" href="plot_hashing_vs_dict_vectorizer.html#sphx-glr-auto-examples-text-plot-hashing-vs-dict-vectorizer-py"><span class="std std-ref">FeatureHasher and DictVectorizer Comparison</span></a> for more information on vectorizers and a comparison of their processing times.</p> <p>For document analysis via a supervised learning approach, see the example script <a class="reference internal" href="plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py"><span class="std std-ref">Classification of text documents using sparse features</span></a>.</p> <pre data-language="python"># Author: Peter Prettenhofer &lt;peter.prettenhofer@gmail.com&gt;
#         Lars Buitinck
#         Olivier Grisel &lt;olivier.grisel@ensta.org&gt;
#         Arturo Amor &lt;david-arturo.amor-quiroz@inria.fr&gt;
# License: BSD 3 clause
</pre> <section id="loading-text-data"> <h2>Loading text data</h2> <p>We load data from <a class="reference internal" href="https://scikit-learn.org/1.1/datasets/real_world.html#newsgroups-dataset"><span class="std std-ref">The 20 newsgroups text dataset</span></a>, which comprises around 18,000 newsgroups posts on 20 topics. For illustrative purposes and to reduce the computational cost, we select a subset of 4 topics only accounting for around 3,400 documents. See the example <a class="reference internal" href="plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py"><span class="std std-ref">Classification of text documents using sparse features</span></a> to gain intuition on the overlap of such topics.</p> <p>Notice that, by default, the text samples contain some message metadata such as <code>"headers"</code>, <code>"footers"</code> (signatures) and <code>"quotes"</code> to other posts. We use the <code>remove</code> parameter from <a class="reference internal" href="../../modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups" title="sklearn.datasets.fetch_20newsgroups"><code>fetch_20newsgroups</code></a> to strip those features and have a more sensible clustering problem.</p> <pre data-language="python">import numpy as np
from sklearn.datasets import fetch_20newsgroups

categories = [
    "alt.atheism",
    "talk.religion.misc",
    "comp.graphics",
    "sci.space",
]

dataset = fetch_20newsgroups(
    remove=("headers", "footers", "quotes"),
    subset="all",
    categories=categories,
    shuffle=True,
    random_state=42,
)

labels = dataset.target
unique_labels, category_sizes = np.unique(labels, return_counts=True)
true_k = unique_labels.shape[0]

print(f"{len(dataset.data)} documents - {true_k} categories")
</pre> <pre data-language="none">3387 documents - 4 categories
</pre> </section> <section id="quantifying-the-quality-of-clustering-results"> <h2>Quantifying the quality of clustering results</h2> <p>In this section we define a function to score different clustering pipelines using several metrics.</p> <p>Clustering algorithms are fundamentally unsupervised learning methods. However, since we happen to have class labels for this specific dataset, it is possible to use evaluation metrics that leverage this “supervised” ground truth information to quantify the quality of the resulting clusters. Examples of such metrics are the following:</p> <ul class="simple"> <li>homogeneity, which quantifies how much clusters contain only members of a single class;</li> <li>completeness, which quantifies how much members of a given class are assigned to the same clusters;</li> <li>V-measure, the harmonic mean of completeness and homogeneity;</li> <li>Rand-Index, which measures how frequently pairs of data points are grouped consistently according to the result of the clustering algorithm and the ground truth class assignment;</li> <li>Adjusted Rand-Index, a chance-adjusted Rand-Index such that random cluster assignment have an ARI of 0.0 in expectation.</li> </ul> <p>If the ground truth labels are not known, evaluation can only be performed using the model results itself. In that case, the Silhouette Coefficient comes in handy.</p> <p>For more reference, see <a class="reference internal" href="../../modules/clustering.html#clustering-evaluation"><span class="std std-ref">Clustering performance evaluation</span></a>.</p> <pre data-language="python">from collections import defaultdict
from sklearn import metrics
from time import time

evaluations = []
evaluations_std = []


def fit_and_evaluate(km, X, name=None, n_runs=5):
    name = km.__class__.__name__ if name is None else name

    train_times = []
    scores = defaultdict(list)
    for seed in range(n_runs):
        km.set_params(random_state=seed)
        t0 = time()
        km.fit(X)
        train_times.append(time() - t0)
        scores["Homogeneity"].append(metrics.homogeneity_score(labels, km.labels_))
        scores["Completeness"].append(metrics.completeness_score(labels, km.labels_))
        scores["V-measure"].append(metrics.v_measure_score(labels, km.labels_))
        scores["Adjusted Rand-Index"].append(
            metrics.adjusted_rand_score(labels, km.labels_)
        )
        scores["Silhouette Coefficient"].append(
            metrics.silhouette_score(X, km.labels_, sample_size=2000)
        )
    train_times = np.asarray(train_times)

    print(f"clustering done in {train_times.mean():.2f} ± {train_times.std():.2f} s ")
    evaluation = {
        "estimator": name,
        "train_time": train_times.mean(),
    }
    evaluation_std = {
        "estimator": name,
        "train_time": train_times.std(),
    }
    for score_name, score_values in scores.items():
        mean_score, std_score = np.mean(score_values), np.std(score_values)
        print(f"{score_name}: {mean_score:.3f} ± {std_score:.3f}")
        evaluation[score_name] = mean_score
        evaluation_std[score_name] = std_score
    evaluations.append(evaluation)
    evaluations_std.append(evaluation_std)
</pre> </section> <section id="k-means-clustering-on-text-features"> <h2>K-means clustering on text features</h2> <p>Two feature extraction methods are used in this example:</p> <ul class="simple"> <li>
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.tfidfvectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code>TfidfVectorizer</code></a> uses an in-memory vocabulary (a Python dict) to map the most frequent words to features indices and hence compute a word occurrence frequency (sparse) matrix. The word frequencies are then reweighted using the Inverse Document Frequency (IDF) vector collected feature-wise over the corpus.</li> <li>
<a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.hashingvectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code>HashingVectorizer</code></a> hashes word occurrences to a fixed dimensional space, possibly with collisions. The word count vectors are then normalized to each have l2-norm equal to one (projected to the euclidean unit-sphere) which seems to be important for k-means to work in high dimensional space.</li> </ul> <p>Furthermore it is possible to post-process those extracted features using dimensionality reduction. We will explore the impact of those choices on the clustering quality in the following.</p> <section id="feature-extraction-using-tfidfvectorizer"> <h3>Feature Extraction using TfidfVectorizer</h3> <p>We first benchmark the estimators using a dictionary vectorizer along with an IDF normalization as provided by <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.tfidfvectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code>TfidfVectorizer</code></a>.</p> <pre data-language="python">from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(
    max_df=0.5,
    min_df=5,
    stop_words="english",
)
t0 = time()
X_tfidf = vectorizer.fit_transform(dataset.data)

print(f"vectorization done in {time() - t0:.3f} s")
print(f"n_samples: {X_tfidf.shape[0]}, n_features: {X_tfidf.shape[1]}")
</pre> <pre data-language="none">vectorization done in 0.372 s
n_samples: 3387, n_features: 7929
</pre> <p>After ignoring terms that appear in more than 50% of the documents (as set by <code>max_df=0.5</code>) and terms that are not present in at least 5 documents (set by <code>min_df=5</code>), the resulting number of unique terms <code>n_features</code> is around 8,000. We can additionally quantify the sparsity of the <code>X_tfidf</code> matrix as the fraction of non-zero entries devided by the total number of elements.</p> <pre data-language="python">print(f"{X_tfidf.nnz / np.prod(X_tfidf.shape):.3f}")
</pre> <pre data-language="none">0.007
</pre> <p>We find that around 0.7% of the entries of the <code>X_tfidf</code> matrix are non-zero.</p> </section> <section id="clustering-sparse-data-with-k-means"> <h3 id="kmeans-sparse-high-dim">Clustering sparse data with k-means</h3> <p>As both <a class="reference internal" href="../../modules/generated/sklearn.cluster.kmeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code>KMeans</code></a> and <a class="reference internal" href="../../modules/generated/sklearn.cluster.minibatchkmeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code>MiniBatchKMeans</code></a> optimize a non-convex objective function, their clustering is not guaranteed to be optimal for a given random init. Even further, on sparse high-dimensional data such as text vectorized using the Bag of Words approach, k-means can initialize centroids on extremely isolated data points. Those data points can stay their own centroids all along.</p> <p>The following code illustrates how the previous phenomenon can sometimes lead to highly imbalanced clusters, depending on the random initialization:</p> <pre data-language="python">from sklearn.cluster import KMeans

for seed in range(5):
    kmeans = KMeans(
        n_clusters=true_k,
        max_iter=100,
        n_init=1,
        random_state=seed,
    ).fit(X_tfidf)
    cluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)
    print(f"Number of elements asigned to each cluster: {cluster_sizes}")
print()
print(
    "True number of documents in each category according to the class labels: "
    f"{category_sizes}"
)
</pre> <pre data-language="none">Number of elements asigned to each cluster: [   1    1 3384    1]
Number of elements asigned to each cluster: [1733  717  238  699]
Number of elements asigned to each cluster: [1115  256 1417  599]
Number of elements asigned to each cluster: [1695  649  446  597]
Number of elements asigned to each cluster: [ 254 2117  459  557]

True number of documents in each category according to the class labels: [799 973 987 628]
</pre> <p>To avoid this problem, one possibility is to increase the number of runs with independent random initiations <code>n_init</code>. In such case the clustering with the best inertia (objective function of k-means) is chosen.</p> <pre data-language="python">kmeans = KMeans(
    n_clusters=true_k,
    max_iter=100,
    n_init=5,
)

fit_and_evaluate(kmeans, X_tfidf, name="KMeans\non tf-idf vectors")
</pre> <pre data-language="none">clustering done in 0.16 ± 0.04 s
Homogeneity: 0.347 ± 0.009
Completeness: 0.397 ± 0.006
V-measure: 0.370 ± 0.007
Adjusted Rand-Index: 0.197 ± 0.014
Silhouette Coefficient: 0.007 ± 0.001
</pre> <p>All those clustering evaluation metrics have a maximum value of 1.0 (for a perfect clustering result). Higher values are better. Values of the Adjusted Rand-Index close to 0.0 correspond to a random labeling. Notice from the scores above that the cluster assignment is indeed well above chance level, but the overall quality can certainly improve.</p> <p>Keep in mind that the class labels may not reflect accurately the document topics and therefore metrics that use labels are not necessarily the best to evaluate the quality of our clustering pipeline.</p> </section> <section id="performing-dimensionality-reduction-using-lsa"> <h3>Performing dimensionality reduction using LSA</h3> <p>A <code>n_init=1</code> can still be used as long as the dimension of the vectorized space is reduced first to make k-means more stable. For such purpose we use <a class="reference internal" href="../../modules/generated/sklearn.decomposition.truncatedsvd.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code>TruncatedSVD</code></a>, which works on term count/tf-idf matrices. Since SVD results are not normalized, we redo the normalization to improve the <a class="reference internal" href="../../modules/generated/sklearn.cluster.kmeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code>KMeans</code></a> result. Using SVD to reduce the dimensionality of TF-IDF document vectors is often known as <a class="reference external" href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">latent semantic analysis</a> (LSA) in the information retrieval and text mining literature.</p> <pre data-language="python">from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer


lsa = make_pipeline(TruncatedSVD(n_components=100), Normalizer(copy=False))
t0 = time()
X_lsa = lsa.fit_transform(X_tfidf)
explained_variance = lsa[0].explained_variance_ratio_.sum()

print(f"LSA done in {time() - t0:.3f} s")
print(f"Explained variance of the SVD step: {explained_variance * 100:.1f}%")
</pre> <pre data-language="none">LSA done in 0.365 s
Explained variance of the SVD step: 18.4%
</pre> <p>Using a single initialization means the processing time will be reduced for both <a class="reference internal" href="../../modules/generated/sklearn.cluster.kmeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code>KMeans</code></a> and <a class="reference internal" href="../../modules/generated/sklearn.cluster.minibatchkmeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code>MiniBatchKMeans</code></a>.</p> <pre data-language="python">kmeans = KMeans(
    n_clusters=true_k,
    max_iter=100,
    n_init=1,
)

fit_and_evaluate(kmeans, X_lsa, name="KMeans\nwith LSA on tf-idf vectors")
</pre> <pre data-language="none">clustering done in 0.02 ± 0.00 s
Homogeneity: 0.393 ± 0.009
Completeness: 0.420 ± 0.020
V-measure: 0.405 ± 0.012
Adjusted Rand-Index: 0.342 ± 0.034
Silhouette Coefficient: 0.030 ± 0.002
</pre> <p>We can observe that clustering on the LSA representation of the document is significantly faster (both because of <code>n_init=1</code> and because the dimensionality of the LSA feature space is much smaller). Furthermore, all the clustering evaluation metrics have improved. We repeat the experiment with <a class="reference internal" href="../../modules/generated/sklearn.cluster.minibatchkmeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code>MiniBatchKMeans</code></a>.</p> <pre data-language="python">from sklearn.cluster import MiniBatchKMeans

minibatch_kmeans = MiniBatchKMeans(
    n_clusters=true_k,
    n_init=1,
    init_size=1000,
    batch_size=1000,
)

fit_and_evaluate(
    minibatch_kmeans,
    X_lsa,
    name="MiniBatchKMeans\nwith LSA on tf-idf vectors",
)
</pre> <pre data-language="none">clustering done in 0.02 ± 0.00 s
Homogeneity: 0.348 ± 0.070
Completeness: 0.381 ± 0.019
V-measure: 0.361 ± 0.050
Adjusted Rand-Index: 0.330 ± 0.106
Silhouette Coefficient: 0.024 ± 0.006
</pre> </section> <section id="top-terms-per-cluster"> <h3>Top terms per cluster</h3> <p>Since <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.tfidfvectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code>TfidfVectorizer</code></a> can be inverted we can identify the cluster centers, which provide an intuition of the most influential words <strong>for each cluster</strong>. See the example script <a class="reference internal" href="plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py"><span class="std std-ref">Classification of text documents using sparse features</span></a> for a comparison with the most predictive words <strong>for each target class</strong>.</p> <pre data-language="python">original_space_centroids = lsa[0].inverse_transform(kmeans.cluster_centers_)
order_centroids = original_space_centroids.argsort()[:, ::-1]
terms = vectorizer.get_feature_names_out()

for i in range(true_k):
    print(f"Cluster {i}: ", end="")
    for ind in order_centroids[i, :10]:
        print(f"{terms[ind]} ", end="")
    print()
</pre> <pre data-language="none">Cluster 0: think just don people like know want say good really
Cluster 1: god people jesus say religion did does christian said evidence
Cluster 2: thanks graphics image know edu does files file program mail
Cluster 3: space launch orbit earth shuttle like nasa moon mission time
</pre> </section> <section id="hashingvectorizer"> <h3>HashingVectorizer</h3> <p>An alternative vectorization can be done using a <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.hashingvectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code>HashingVectorizer</code></a> instance, which does not provide IDF weighting as this is a stateless model (the fit method does nothing). When IDF weighting is needed it can be added by pipelining the <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.hashingvectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code>HashingVectorizer</code></a> output to a <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.tfidftransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code>TfidfTransformer</code></a> instance. In this case we also add LSA to the pipeline to reduce the dimension and sparcity of the hashed vector space.</p> <pre data-language="python">from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

lsa_vectorizer = make_pipeline(
    HashingVectorizer(stop_words="english", n_features=50_000),
    TfidfTransformer(),
    TruncatedSVD(n_components=100, random_state=0),
    Normalizer(copy=False),
)

t0 = time()
X_hashed_lsa = lsa_vectorizer.fit_transform(dataset.data)
print(f"vectorization done in {time() - t0:.3f} s")
</pre> <pre data-language="none">vectorization done in 2.086 s
</pre> <p>One can observe that the LSA step takes a relatively long time to fit, especially with hashed vectors. The reason is that a hashed space is typically large (set to <code>n_features=50_000</code> in this example). One can try lowering the number of features at the expense of having a larger fraction of features with hash collisions as shown in the example notebook <a class="reference internal" href="plot_hashing_vs_dict_vectorizer.html#sphx-glr-auto-examples-text-plot-hashing-vs-dict-vectorizer-py"><span class="std std-ref">FeatureHasher and DictVectorizer Comparison</span></a>.</p> <p>We now fit and evaluate the <code>kmeans</code> and <code>minibatch_kmeans</code> instances on this hashed-lsa-reduced data:</p> <pre data-language="python">fit_and_evaluate(kmeans, X_hashed_lsa, name="KMeans\nwith LSA on hashed vectors")
</pre> <pre data-language="none">clustering done in 0.02 ± 0.01 s
Homogeneity: 0.395 ± 0.012
Completeness: 0.445 ± 0.014
V-measure: 0.419 ± 0.013
Adjusted Rand-Index: 0.325 ± 0.012
Silhouette Coefficient: 0.030 ± 0.001
</pre> <pre data-language="python">fit_and_evaluate(
    minibatch_kmeans,
    X_hashed_lsa,
    name="MiniBatchKMeans\nwith LSA on hashed vectors",
)
</pre> <pre data-language="none">clustering done in 0.02 ± 0.00 s
Homogeneity: 0.353 ± 0.045
Completeness: 0.358 ± 0.043
V-measure: 0.356 ± 0.044
Adjusted Rand-Index: 0.316 ± 0.066
Silhouette Coefficient: 0.025 ± 0.003
</pre> <p>Both methods lead to good results that are similar to running the same models on the traditional LSA vectors (without hashing).</p> </section> </section> <section id="clustering-evaluation-summary"> <h2>Clustering evaluation summary</h2> <pre data-language="python">import pandas as pd
import matplotlib.pyplot as plt

fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(16, 6), sharey=True)

df = pd.DataFrame(evaluations[::-1]).set_index("estimator")
df_std = pd.DataFrame(evaluations_std[::-1]).set_index("estimator")

df.drop(
    ["train_time"],
    axis="columns",
).plot.barh(ax=ax0, xerr=df_std)
ax0.set_xlabel("Clustering scores")
ax0.set_ylabel("")

df["train_time"].plot.barh(ax=ax1, xerr=df_std["train_time"])
ax1.set_xlabel("Clustering time (s)")
plt.tight_layout()
</pre> <img src="data:image/png;base64,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" srcset="../../_images/sphx_glr_plot_document_clustering_001.png" alt="plot document clustering" class="sphx-glr-single-img"><p><a class="reference internal" href="../../modules/generated/sklearn.cluster.kmeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code>KMeans</code></a> and <a class="reference internal" href="../../modules/generated/sklearn.cluster.minibatchkmeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code>MiniBatchKMeans</code></a> suffer from the phenomenon called the <a class="reference external" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">Curse of Dimensionality</a> for high dimensional datasets such as text data. That is the reason why the overall scores improve when using LSA. Using LSA reduced data also improves the stability and requires lower clustering time, though keep in mind that the LSA step itself takes a long time, especially with hashed vectors.</p> <p>The Silhouette Coefficient is defined between 0 and 1. In all cases we obtain values close to 0 (even if they improve a bit after using LSA) because its definition requires measuring distances, in contrast with other evaluation metrics such as the V-measure and the Adjusted Rand Index which are only based on cluster assignments rather than distances. Notice that strictly speaking, one should not compare the Silhouette Coefficient between spaces of different dimension, due to the different notions of distance they imply.</p> <p>The homogeneity, completeness and hence v-measure metrics do not yield a baseline with regards to random labeling: this means that depending on the number of samples, clusters and ground truth classes, a completely random labeling will not always yield the same values. In particular random labeling won’t yield zero scores, especially when the number of clusters is large. This problem can safely be ignored when the number of samples is more than a thousand and the number of clusters is less than 10, which is the case of the present example. For smaller sample sizes or larger number of clusters it is safer to use an adjusted index such as the Adjusted Rand Index (ARI). See the example <a class="reference internal" href="../cluster/plot_adjusted_for_chance_measures.html#sphx-glr-auto-examples-cluster-plot-adjusted-for-chance-measures-py"><span class="std std-ref">Adjustment for chance in clustering performance evaluation</span></a> for a demo on the effect of random labeling.</p> <p>The size of the error bars show that <a class="reference internal" href="../../modules/generated/sklearn.cluster.minibatchkmeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code>MiniBatchKMeans</code></a> is less stable than <a class="reference internal" href="../../modules/generated/sklearn.cluster.kmeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code>KMeans</code></a> for this relatively small dataset. It is more interesting to use when the number of samples is much bigger, but it can come at the expense of a small degradation in clustering quality compared to the traditional k-means algorithm.</p> <p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes 7.415 seconds)</p> <div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-text-plot-document-clustering-py"> <div class="binder-badge docutils container"> <a class="reference external image-reference" href="https://mybinder.org/v2/gh/scikit-learn/scikit-learn/1.1.X?urlpath=lab/tree/notebooks/auto_examples/text/plot_document_clustering.ipynb"><img alt="Launch binder" src="data:image/svg+xml;base64,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" width="150px"></a> </div> <div class="sphx-glr-download sphx-glr-download-python docutils container"> <p><a class="reference download internal" download="" href="https://scikit-learn.org/1.1/_downloads/ba68199eea858ec04949b2c6c65147e0/plot_document_clustering.py"><code>Download Python source code: plot_document_clustering.py</code></a></p> </div> <div class="sphx-glr-download sphx-glr-download-jupyter docutils container"> <p><a class="reference download internal" download="" href="https://scikit-learn.org/1.1/_downloads/751db3d5e6b909ff00972495eaae53df/plot_document_clustering.ipynb"><code>Download Jupyter notebook: plot_document_clustering.ipynb</code></a></p> </div> </div>  </section> </section><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2022 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/1.1/auto_examples/text/plot_document_clustering.html" class="_attribution-link">https://scikit-learn.org/1.1/auto_examples/text/plot_document_clustering.html</a>
  </p>
</div>
