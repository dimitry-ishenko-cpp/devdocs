<div id="col-content" data-swiftype-index="true"> <div id="high-availability-chef-backend"><h1>High Availability: Chef Backend</h1></div>  <div class="prose"> <p data-swiftype-index="false"> <a href="https://github.com/chef/chef-server/blob/main/docs-chef-io/content/server/install_server_ha.md" alt="Link to page on GitHub repository">[edit on GitHub]</a> </p> <div class="admonition-warning">
<p class="admonition-warning-title">Warning</p>
<div class="admonition-warning-text"> <p>Chef Backend is <a href="../../versions/index.html#deprecated-products-and-versions">deprecated</a> and no longer under active development. Contact your Chef account representative for information about migrating to Chef Automate HA.</p> <p>This document is no longer maintained.</p> </div>
</div> <p>This topic introduces the underlying concepts behind the architecture of the high availability Chef Infra Server cluster. The topic then describes the setup and installation process for a high availability Chef Infra Server cluster comprised of five total nodes (two frontend and three backend).</p> <h2 id="overview">Overview</h2> <p>The Chef Infra Server can operate in a high availability configuration that provides automated load balancing and failover for stateful components in the system architecture. This type of configuration typically splits the servers into two segments: The backend cluster, and the frontend group.</p> <ul> <li> <p>The frontend group, comprised of one (or more) nodes running the Chef Infra Server. Nodes in the frontend group handle requests to the Chef Infra Server API and access to the Chef management console. Frontend group nodes should be load balanced, and may be scaled horizontally by increasing the number of nodes available to handle requests.</p> </li> <li> <p>The backend cluster, comprised of three nodes working together, provides high availability data persistence for the frontend group.</p> <div class="admonition-note"> <p class="admonition-note-title">Note</p> <div class="admonition-note-text"> At this time, backend clusters can only have three nodes. </div> </div> </li> </ul> <p><img src="https://docs.chef.io/images/server/chef_server_ha_cluster.svg" alt="image"></p> <div class="admonition-important"> <p class="admonition-important-title">Important</p> <div class="admonition-important-text">When doing cloud deployments, Chef HA clusters are not meant to be geographically dispersed across multiple regions or datacenters; however, in cloud providers such as AWS, you can deploy HA clusters across multiple Availability Zones within the same region.</div> </div> <h3 id="key-differences-from-standalone-chef-infra-server">Key Differences From Standalone Chef Infra Server</h3> <p><strong>New in Chef Infra Server 14</strong> Starting with Chef Infra Server 14, standalone instances use Elasticsearch for internal search. Elasticsearch provides more flexible clustering options while maintaining search API compatibility with Apache Solr.</p> <h2 id="recommended-cluster-topology">Recommended Cluster Topology</h2> <h3 id="nodes">Nodes</h3> <ul> <li>The HA backend installation requires three cluster nodes. Chef has not tested and does not support installations with other numbers of backend cluster nodes.</li> <li>One or more frontend group nodes</li> </ul> <h4 id="hardware-requirements">Hardware Requirements</h4> <p>The following are a list of general hardware requirements for both frontend and backend servers. The important guideline you should follow are that frontend servers tend to be more CPU bound and backend servers are more disk and memory bound. Also, disk space for backend servers should scale up with the number of nodes that the servers are managing. A good rule to follow is to allocate 2 MB per node. The disk values listed below should be a good default value that you will want to modify later if/when your node count grows.</p> <ul> <li>64-bit architecture</li> </ul> <p><strong>Frontend Requirements</strong></p> <ul> <li>4 cores (physical or virtual)</li> <li>4GB RAM</li> <li>20 GB of free disk space (SSD if on premises, Premium Storage in Microsoft Azure, EBS-Optimized GP2 in AWS)</li> </ul> <p><strong>Backend Requirements</strong></p> <ul> <li>2 cores (physical or virtual)</li> <li>8GB RAM</li> <li>50 GB/backend server (SSD if on premises, Premium Storage in Microsoft Azure, EBS-Optimized GP2 in AWS)</li> </ul> <div class="admonition-warning"> <p class="admonition-warning-title">Warning</p> <div class="admonition-warning-text"> <p>The Chef Infra Server MUST NOT use a network file system of any type—virtual or physical—for backend storage. The Chef Infra Server database operates quickly. The behavior of operations, such as the writing of log files, will be unpredictable when run over a network file system.</p> </div> </div> <h3 id="network-services">Network Services</h3> <ul> <li>A load balancer between the rest of the network, and the frontend group (Not provided). Because management console session data is stored on each node in the frontend group individually, the load balancer should be configured with sticky sessions.</li> </ul> <h3 id="network-port-requirements">Network Port Requirements</h3> <h4 id="inbound-from-load-balancer-to-frontend-group">Inbound from load balancer to frontend group</h4> <ul> <li>TCP 80 (HTTP)</li> <li>TCP 443 (HTTPS)</li> </ul> <h4 id="inbound-from-frontend-group-to-backend-cluster">Inbound from frontend group to backend cluster</h4> <ul> <li>TCP 2379 (etcd)</li> <li>TCP 5432 (PostgreSQL)</li> <li>TCP 7331 (leaderl)</li> <li>TCP 9200-9300 (Elasticsearch)</li> </ul> <h4 id="peer-communication-backend-cluster">Peer communication, backend cluster</h4> <ul> <li>2379 (etcd)</li> <li>2380 (etcd)</li> <li>5432 (PostgreSQL)</li> <li>9200-9400 (Elasticsearch)</li> </ul> <h2 id="installation">Installation</h2> <p>These instructions assume you are using the minimum versions:</p> <ul> <li>Chef Server : 12.5.0</li> <li>Chef Backend : 0.8.0</li> </ul> <p>Download <a href="https://downloads.chef.io/chef-server/">Chef Infra Server</a> and <a href="https://downloads.chef.io/chef-backend/">Chef Backend (chef-backend)</a> if you do not have them already.</p> <p>Before creating the backend HA cluster and building at least one Chef Infra Server to be part of the frontend group, verify:</p> <ul> <li>The user who will install and build the backend HA cluster and frontend group has root access to all nodes.</li> <li>The number of backend and frontend nodes that are desired. It is required to have three backend nodes, but the number of frontend nodes may vary from a single node to a load-balanced tiered configuration.</li> <li>SSH access to all boxes that will belong to the backend HA cluster from the node that will be the initial bootstrap.</li> <li>A time synchronization policy is in place, such as Network Time Protocol (NTP). Drift of less than 1.5 seconds must exist across all nodes in the backend HA cluster.</li> </ul> <h3 id="step-1-create-cluster">Step 1: Create Cluster</h3> <p>The first node must be bootstrapped to initialize the cluster. The node used to bootstrap the cluster will be the cluster leader when the cluster comes online. After bootstrap completes this node is no different from any other back-end node.</p> <ol> <li> <p>Install the Chef Backend package on the first backend node <strong>as root</strong>.</p> <ul> <li>Download <a href="https://downloads.chef.io/chef-backend/">Chef Backend (chef-backend)</a>
</li> <li>In Red Hat/CentOS: <code>yum install PATH_TO_RPM</code>
</li> <li>In Debian/Ubuntu: <code>dpkg -i PATH_TO_DEB</code>
</li> </ul> </li> <li> <p>Update <code>/etc/chef-backend/chef-backend.rb</code> with the following content:</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-ruby" data-lang="ruby">publish_address <span style="color:#4070a0">'external_IP_address_of_this_box'</span> <span style="color:#60a0b0;font-style:italic"># External ip address of this backend box</span>
</code></pre></div>
</li> <li> <p>If any of the backends or frontends are in different networks from each other then add a <code>postgresql.md5_auth_cidr_addresses</code> line to <code>/etc/chef-backend/chef-backend.rb</code> with the following content where <code>, "&lt;NET-1_IN_CIDR&gt;", ..., "&lt;NET-N_IN_CIDR&gt;"</code> is the list of all of the networks that your backends and frontends are in. See the <a href="#configuring-frontend-and-backend-members-on-different-networks">Configuring Frontend and Backend Members on Different Networks</a> section for more information:</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-ruby" data-lang="ruby">publish_address <span style="color:#4070a0">'external_IP_address_of_this_box'</span> <span style="color:#60a0b0;font-style:italic"># External ip address of this backend box</span>
postgresql<span style="color:#666">.</span>md5_auth_cidr_addresses <span style="color:#666">=</span> <span style="color:#666">[</span><span style="color:#4070a0">"samehost"</span>, <span style="color:#4070a0">"samenet"</span>, <span style="color:#4070a0">"&lt;NET-1_IN_CIDR&gt;"</span>, <span style="color:#666">...</span>, <span style="color:#4070a0">"&lt;NET-N_IN_CIDR&gt;"</span><span style="color:#666">]</span>
</code></pre></div>
</li> <li> <p>Run <code>chef-backend-ctl create-cluster</code>.</p> </li> </ol> <h3 id="step-2-shared-credentials">Step 2: Shared Credentials</h3> <p>The credentials file <code>/etc/chef-backend/chef-backend-secrets.json</code> generated by bootstrapping must be shared with the other nodes. You may copy them directly, or expose them via a common mounted location.</p> <p>For example, to copy using ssh:</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-bash" data-lang="bash">scp /etc/chef-backend/chef-backend-secrets.json &lt;USER&gt;@&lt;IP_BE2&gt;:/home/&lt;USER&gt;
scp /etc/chef-backend/chef-backend-secrets.json &lt;USER&gt;@&lt;IP_BE3&gt;:/home/&lt;USER&gt;
</code></pre></div>
<p>Delete this file from the destination after Step 4 has been completed for each backend being joined to the cluster.</p> <h3 id="step-3-install-and-configure-remaining-backend-nodes">Step 3: Install and Configure Remaining Backend Nodes</h3> <p>For each additional node do the following in sequence (if you attempt to join nodes in parallel the cluster may fail to become available):</p> <ol> <li> <p>Install the Chef Backend package on the node.</p> <ul> <li>Download <a href="https://downloads.chef.io/chef-backend/">Chef Backend (chef-backend)</a>
</li> <li>In Red Hat/CentOS: <code>yum install PATH_TO_RPM</code>
</li> <li>In Debian/Ubuntu: <code>dpkg -i PATH_TO_DEB</code>
</li> </ul> </li> <li> <p>If you added a <code>postgresql.md5_auth_cidr_addresses</code> line to the leader’s <code>/etc/chef-backend/chef-backend.rb</code> in <a href="#step-1-create-cluster">Step 1: Create Cluster</a> then update this node’s <code>/etc/chef-backend/chef-backend.rb</code> with the following content where <code>postgresql.md5_auth_cidr_addresses</code> is set to the same value used in the leader’s <code>chef-backend.rb</code>. If all of the backend and frontend clusters are in the same network then you don’t need to modify this node’s <code>/etc/chef-backend/chef-backend.rb</code> at all.</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-ruby" data-lang="ruby">publish_address <span style="color:#4070a0">'external_IP_address_of_this_box'</span> <span style="color:#60a0b0;font-style:italic"># External ip address of this backend box</span>
postgresql<span style="color:#666">.</span>md5_auth_cidr_addresses <span style="color:#666">=</span> <span style="color:#666">[</span><span style="color:#4070a0">"samehost"</span>, <span style="color:#4070a0">"samenet"</span>, <span style="color:#4070a0">"&lt;NET-1_IN_CIDR&gt;"</span>, <span style="color:#666">...</span>, <span style="color:#4070a0">"&lt;NET-N_IN_CIDR&gt;"</span><span style="color:#666">]</span>
</code></pre></div>
</li> <li> <p><strong>As root</strong> or with sudo:</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-bash" data-lang="bash">chef-backend-ctl join-cluster &lt;IP_BE1&gt; -s /home/&lt;USER&gt;/chef-backend-secrets.json
</code></pre></div>
</li> <li> <p>Answer the prompts regarding which public IP to use. As an alternative, you may specify them on the <code>chef-backend join-cluster</code> command line. See <code>chef-backend-ctl join-cluster --help</code> for more information. If you manually added the <code>publish_address</code> line to <code>/etc/chef-backend/chef-backend.rb</code> then you will not be prompted for the public IP and you should not use the <code>--publish-address</code> option to specify the the public IP on the <code>chef-backend join-cluster</code> command line.</p> </li> <li> <p>If you copied the shared <code>chef-backend-secrets.json</code> file to a user HOME directory on this host, remove it now.</p> </li> <li> <p>Repeat these steps for each follower node, after which the cluster is online and available. From any node in the backend HA cluster, run the following command:</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-bash" data-lang="bash">chef-backend-ctl status
</code></pre></div>
<p>should return something like:</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-bash" data-lang="bash">Service        Local Status        Time in State  Distributed Node Status
elasticsearch  running <span style="color:#666">(</span>pid 6661<span style="color:#666">)</span>  1d 5h 59m 41s  state: green; nodes online: 3/3
etcd           running <span style="color:#666">(</span>pid 6742<span style="color:#666">)</span>  1d 5h 59m 39s  health: green; healthy nodes: 3/3
leaderl        running <span style="color:#666">(</span>pid 6788<span style="color:#666">)</span>  1d 5h 59m 35s  leader: 1; waiting: 0; follower: 2; total: <span style="color:#40a070">3</span>
postgresql     running <span style="color:#666">(</span>pid 6640<span style="color:#666">)</span>  1d 5h 59m 43s  leader: 1; offline: 0; syncing: 0; synced: <span style="color:#40a070">2</span>
</code></pre></div>
</li> </ol> <h3 id="step-4-generate-chef-infra-server-configuration">Step 4: Generate Chef Infra Server Configuration</h3> <p>Log into the node from Step 1 and generate a chef-server frontend node configuration:</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-bash" data-lang="bash">chef-backend-ctl gen-server-config &lt;FE1-FQDN&gt; -f chef-server.rb.FE1
scp chef-server.rb.FE1 USER@&lt;IP_FE1&gt;:/home/&lt;USER&gt;
</code></pre></div>
<div class="admonition-note"> <p class="admonition-note-title">Note</p> <div class="admonition-note-text"> <code>/etc/chef-backend/chef-backend-secrets.json</code> is <em>not</em> made available to Chef Infra Server frontend nodes. </div> </div> <h3 id="step-5-install-and-configure-the-first-frontend">Step 5: Install and Configure the First Frontend</h3> <p>On the first frontend node, assuming that the generated configuration was copied as detailed in Step 4:</p> <ol> <li> <p>Install the current <code>chef-server-core</code> package</p> </li> <li> <p>Copy the file to <code>/etc/opscode</code> with:</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-bash" data-lang="bash"><span style="color:#4070a0">`</span>cp /home/&lt;USER&gt;/chef-server.rb.&lt;FE1&gt; /etc/opscode/chef-server.rb<span style="color:#4070a0">`</span>
</code></pre></div>
</li> <li> <p><strong>As root</strong>, run</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-bash" data-lang="bash">chef-server-ctl reconfigure
</code></pre></div>
</li> </ol> <h3 id="step-6-adding-more-frontend-nodes">Step 6: Adding More Frontend Nodes</h3> <p>For each additional frontend node you wish to add to your cluster:</p> <ol> <li> <p>Install the current <code>chef-server-core</code> package.</p> </li> <li> <p>Generate a new <code>/etc/opscode/chef-server.rb</code> from any of the backend nodes via</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-bash" data-lang="bash">chef-backend-ctl gen-server-config &lt;FE_NAME-FQDN&gt; &gt; chef-server.rb.&lt;FE_NAME&gt;
</code></pre></div>
</li> <li> <p>Copy it to <code>/etc/opscode</code> on the new frontend node.</p> </li> <li> <p>From the first frontend node configured in Step 5, copy the following files from the first frontend to <code>/etc/opscode</code> on the new frontend node:</p> <ul> <li>/etc/opscode/private-chef-secrets.json</li> </ul> <div class="admonition-note"> <p class="admonition-note-title">Note</p> <div class="admonition-note-text"> <p>For Chef Server versions prior to 12.14, you will also need to copy the key files:</p> <ul> <li>/etc/opscode/webui_priv.pem</li> <li>/etc/opscode/webui_pub.pem</li> <li>/etc/opscode/pivotal.pem</li> </ul> </div> </div> </li> <li> <p>On the new frontend node run:</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-bash" data-lang="bash">mkdir -p /var/opt/opscode/upgrades/
</code></pre></div>
</li> <li> <p>From the first frontend node, copy <code>/var/opt/opscode/upgrades/migration-level</code> to the same location on the new node.</p> </li> <li> <p>On the new frontend run:</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-bash" data-lang="bash">touch /var/opt/opscode/bootstrapped<span style="color:#4070a0">`</span>
</code></pre></div>
</li> <li> <p>On the new frontend, <strong>as root</strong> run:</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-bash" data-lang="bash">chef-server-ctl reconfigure
</code></pre></div>
</li> </ol> <h3 id="step-7-configure-the-server">Step 7: Configure the Server</h3> <div class="admonition-note"> <p class="admonition-note-title">Note</p> <div class="admonition-note-text"> To restore a backup to this system, follow the <a href="../server_backup_restore/index.html">chef-server-ctl</a> or the <a href="https://github.com/chef/knife-ec-backup">knife ec</a> restore directions. </div> </div> <ol> <li> <p>Run the following command to create an administrator:</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-bash" data-lang="bash">sudo chef-server-ctl user-create USER_NAME FIRST_NAME LAST_NAME EMAIL <span style="color:#4070a0">'PASSWORD'</span> --filename FILE_NAME
</code></pre></div>
<p>An RSA private key is generated automatically. This is the user’s private key and should be saved to a safe location. The <code>--filename</code> option will save the RSA private key to the specified absolute path.</p> <p>For example:</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-bash" data-lang="bash">sudo chef-server-ctl user-create janedoe Jane Doe janed@example.com <span style="color:#4070a0">'abc123'</span> --filename /path/to/janedoe.pem
</code></pre></div> </li> <li> <p>Run the following command to create an organization:</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-bash" data-lang="bash">sudo chef-server-ctl org-create short_name <span style="color:#4070a0">'full_organization_name'</span> --association_user user_name --filename ORGANIZATION-validator.pem
</code></pre></div>
<p>For example:</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-bash" data-lang="bash">sudo chef-server-ctl org-create 4thcafe <span style="color:#4070a0">'Fourth Cafe, Inc.'</span> --association_user janedoe --filename /path/to/4thcafe-validator.pem
</code></pre></div>
<p>The name must begin with a lower-case letter or digit, may only contain lower-case letters, digits, hyphens, and underscores, and must be between 1 and 255 characters. For example: <code>4thcafe</code>.</p> <p>The full name must begin with a non-white space character and must be between 1 and 1023 characters. For example: <code>'Fourth Cafe, Inc.'</code>.</p> <p>The <code>--association_user</code> option will associate the <code>user_name</code> with the <code>admins</code> security group on the Chef Infra Server.</p> <p>An RSA private key is generated automatically. This is the chef-validator key and should be saved to a safe location. The <code>--filename</code> option will save the RSA private key to the specified absolute path.</p> </li> </ol> <h3 id="upgrading-chef-infra-server-on-the-frontend-machines">Upgrading Chef Infra Server on the Frontend Machines</h3> <ol> <li>On one frontend server, follow the <a href="../upgrades/index.html#standalone">standalone upgrade process</a>.</li> <li>Copy <code>/var/opt/opscode/upgrades/migration-level</code> from the first upgraded frontend to <code>/var/opt/opscode/upgrades/migration-level</code> on each of the remaining frontends.</li> <li>Once the updated file has been copied to each of the remaining frontends, perform the <a href="../upgrades/index.html#standalone">standalone upgrade process</a> on each of the frontend servers.</li> </ol> <h3 id="configuring-frontend-and-backend-members-on-different-networks">Configuring Frontend and Backend Members on Different Networks</h3> <p>By default, PostgreSQL only allows systems on its local network to connect to the database server that runs it and the <code>pg_hba.conf</code> used by PostgreSQL controls network access to the server. The default <code>pg_hba.conf</code> has the following four entries:</p> <pre tabindex="0" class="highlight" data-language="ruby"><code class="language-none" data-lang="none">host    all         all         samehost               md5
hostssl replication replicator  samehost               md5
host    all         all         samenet                md5
hostssl replication replicator  samenet                md5
</code></pre>
<p>To allow other systems to connect, such as members of a frontend group that might exist on a different network, you will need to authorize that usage by adding the following line to the <code>/etc/chef-backend/chef-backend.rb</code> file on all of the backend members.</p> <pre tabindex="0" class="highlight" data-language="ruby"><code class="language-none" data-lang="none">postgresql.md5_auth_cidr_addresses = ["samehost", "samenet", "&lt;YOURNET IN CIDR&gt;"]
</code></pre>
<p>After setting the <code>md5_auth_cidr_addresses</code> value and reconfiguring the server, two entries will be created in <code>pg_hba.conf</code> for each value in the <code>md5_auth_cidr_addresses</code> array. Existing values in <code>pg_hba.conf</code> will be overwritten by the values in the array, so we must also specify “samehost” and “samenet”, which will continue to allow systems on a local network to connect to PostgreSQL.</p> <p>For example, if a frontend host at 192.168.1.3 can reach a backend member over the network, but the backend’s local network is 192.168.2.x, you would add the following line to <code>/etc/chef-backend/chef-backend.rb</code></p> <pre tabindex="0" class="highlight" data-language="ruby"><code class="language-none" data-lang="none">postgresql.md5_auth_cidr_addresses = ["samehost", "samenet", "192.168.1.3/24"]
</code></pre>
<p>which would result in the following two entries being added to the <code>pg_hba.conf</code> file.</p> <pre tabindex="0" class="highlight" data-language="ruby"><code class="language-none" data-lang="none">host    all         all         samehost               md5
hostssl replication replicator  samehost               md5
host    all         all         samenet                md5
hostssl replication replicator  samenet                md5
host    all         all         192.168.1.3/24         md5
hostssl replication replicator  192.168.1.3/24         md5
</code></pre>
<p>Running <code>chef-backend-ctl reconfigure</code> on all the backends will allow that frontend to complete its connection.</p> <div class="admonition-important"> <p class="admonition-important-title">Important</p> <div class="admonition-important-text">The <code>postgresql.md5_auth_cidr_addresses</code> subnet settings must be identical for all members of the backend cluster. In the case where the subnet settings of the frontend cluster are different from the subnet settings of the backend cluster, the values set on the members of the backend cluster should contain the subnet of the frontend cluster. This guarantees that all members of a cluster can still communicate with each other after a cluster change of state occurs. For example, if the frontend subnet setting is “192.168.1.0/24” and the backend subnet setting is “192.168.2.0/24”, then the <code>postgresql.md5_auth_cidr_addresses</code> subnet settings must be <code>postgresql.md5_auth_cidr_addresses = ["samehost", "samenet", "192.168.1.0/24", 192.168.2.0/24]</code>
</div> </div> <h2 id="cluster-security-considerations">Cluster Security Considerations</h2> <p>A backend cluster is expected to run in a trusted environment. This means that untrusted users that communicate with and/or eavesdrop on services provided by the backend cluster can potentially view sensitive data.</p> <h3 id="communication-between-nodes">Communication Between Nodes</h3> <p>PostgreSQL communication between nodes in the backend cluster is encrypted, and uses password authentication. All other communication in the backend cluster is unauthenticated and happens in the clear (without encryption).</p> <h3 id="communication-between-frontend-group--backend-cluster">Communication Between Frontend Group &amp; Backend Cluster</h3> <p>PostgreSQL communication from nodes in the frontend group to the leader of the backend cluster uses password authentication, but communication happens in the clear (without encryption).</p> <p>Elasticsearch communication is unauthenticated and happens in the clear (without encryption).</p> <h3 id="securing-communication">Securing Communication</h3> <p>Because most of the peer communication between nodes in the backend cluster happens in the clear, the backend cluster is vulnerable to passive monitoring of network traffic between nodes. To help prevent an active attacker from intercepting or changing cluster data, Chef recommends using iptables or an equivalent network ACL tool to restrict access to PostgreSQL, Elasticsearch and etcd to only hosts that need access.</p> <p>By service role, access requirements are as follows:</p> <table> <col style="width:19%"> <col style="width:80%"> <thead> <tr class="header"> <th>Service</th> <th>Access Requirements</th> </tr> </thead> <tbody> <tr class="odd"> <td>PostgreSQL</td> <td>All backend cluster members and all Chef Infra Server frontend group nodes.</td> </tr> <tr class="even"> <td>Elasticsearch</td> <td>All backend cluster members and all Chef Infra Server frontend group nodes.</td> </tr> <tr class="odd"> <td>etcd</td> <td>All backend cluster members and all Chef Infra Server frontend group nodes.</td> </tr> </tbody> </table> <h3 id="services-and-secrets">Services and Secrets</h3> <p>Communication with PostgreSQL requires password authentication. The backend cluster generates PostgreSQL users and passwords during the initial cluster-create. These passwords are present in the following files on disk:</p> <table style="width:100%"> <col style="width:61%"> <col style="width:14%"> <col style="width:14%"> <col style="width:9%"> <thead> <tr class="header"> <th>Secret</th> <th>Owner</th> <th>Group</th> <th>Mode</th> </tr> </thead> <tbody> <tr class="odd"> <td><code>/etc/chef-backend/secrets.json</code></td> <td><code>root</code></td> <td><code>chef_pgsql</code></td> <td><code>0640</code></td> </tr> <tr class="even"> <td><code>/var/opt/chef-backend/leaderl/data/sys.config</code></td> <td><code>chef_pgsql</code></td> <td><code>chef_pgsql</code></td> <td><code>0600</code></td> </tr> <tr class="odd"> <td><code>/var/opt/chef-backend/PostgreSQL/9.5/recovery.conf</code></td> <td><code>chef_pgsql</code></td> <td><code>chef_pgsql</code></td> <td><code>0600</code></td> </tr> </tbody> </table> <p>The following services run on each node in the backend cluster. The user account under which the service runs as listed the second column:</p> <table> <col style="width:19%"> <col style="width:80%"> <thead> <tr class="header"> <th>Service</th> <th>Process Owner</th> </tr> </thead> <tbody> <tr class="odd"> <td><code>postgresql</code></td> <td><code>chef_pgsql</code></td> </tr> <tr class="even"> <td><code>elasticsearch</code></td> <td><code>chef-backend</code></td> </tr> <tr class="odd"> <td><code>etcd</code></td> <td><code>chef-backend</code></td> </tr> <tr class="even"> <td><code>leaderl</code></td> <td><code>chef_pgsql</code></td> </tr> <tr class="odd"> <td><code>epmd</code></td> <td>
<code>chef_pgsql</code> (or first user launching an erlang process)</td> </tr> </tbody> </table> <h4 id="chef-infra-server-frontend">Chef Infra Server frontend</h4> <p>The <code>chef-backend-ctl gen-server-config</code> command, which can be run as root from any node in the backend cluster, will automatically generate a configuration file containing the superuser database access credentials for the backend cluster PostgreSQL instance.</p> <h3 id="software-versions">Software Versions</h3> <p>The backend HA cluster uses the Chef installer to package all of the software necessary to run the services included in the backend cluster. For a full list of the software packages included (and their versions), see the file located at <code>/opt/chef-backend/version-manifest.json</code>.</p> <p>Do not attempt to upgrade individual components of the Chef package. Due to the way Chef packages are built, modifying any of the individual components in the package will lead to cluster instability. If the latest version of the backend cluster is providing an out-of-date package, please bring it to the attention of Chef by filling out a ticket with <a href="mailto:support@chef.io.html">support@chef.io</a>.</p> <h2 id="chef-backendrb-options">chef-backend.rb Options</h2> <p>Run <code>chef-backend-ctl gen-sample-backend-config</code> to generate the <code>chef-backend.rb</code> file. This will control most of the various feature and configuration flags going into a Chef HA backend node. A number of these options control the reliability, stability, and uptime of the backend PostgreSQL databases, the Elasticsearch index, and the leader election system. Please refrain from changing them unless you have been advised to do so.</p> <dl> <dt>The following settings are the only settings you should modify without guidance:</dt> <dt><code>fqdn</code></dt> <dd>Host name of this node.</dd> <dt><code>hide_sensitive</code></dt> <dd>Set to <code>false</code> if you wish to print deltas of sensitive files and templates during <code>chef-backend-ctl reconfigure</code> runs.</dd> <dd>Default value: <code>true</code>.</dd> <dt><code>ip_version</code></dt> <dd>Set to either <code>'ipv4'</code> or <code>'ipv6'</code>.</dd> <dd>Default value: <code>'ipv4'</code>.</dd> <dt><code>publish_address</code></dt> <dd>Externally resolvable IP address of this back-end node.</dd> </dl> <p>For information on all the available settings, see the <a href="../config_rb_backend/index.html">chef-backend.rb documentation</a>.</p> <h2 id="chef-backend-ctl">chef-backend-ctl</h2> <p>The Chef Infra Server backend HA cluster includes a command-line utility named chef-backend-ctl. This command-line tool is used to manage the Chef Infra Server backend HA cluster, start and stop individual services, and tail Chef Infra Server log files. For more information, see the <a href="../ctl_chef_backend/index.html">chef-backend-ctl documentation</a>.</p> </div> </div><div class="_attribution">
  <p class="_attribution-p">
    &copy; Chef Software, Inc.<br>Licensed under the Creative Commons Attribution 3.0 Unported License.<br>The Chef&trade; Mark and Chef Logo are either registered trademarks/service marks or trademarks/servicemarks of Chef, in the United States and other countries and are used with Chef Inc's permission.<br>We are not affiliated with, endorsed or sponsored by Chef Inc.<br>
    <a href="https://docs.chef.io/server/install_server_ha/" class="_attribution-link">https://docs.chef.io/server/install_server_ha/</a>
  </p>
</div>
