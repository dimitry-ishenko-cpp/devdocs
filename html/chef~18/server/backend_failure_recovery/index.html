<div id="col-content" data-swiftype-index="true"> <div id="chef-backend-failure-recovery"><h1>Chef Backend Failure Recovery</h1></div>  <div class="prose"> <p data-swiftype-index="false"> <a href="https://github.com/chef/chef-server/blob/main/docs-chef-io/content/server/backend_failure_recovery.md" alt="Link to page on GitHub repository">[edit on GitHub]</a> </p> <div class="admonition-warning">
<p class="admonition-warning-title">Warning</p>
<div class="admonition-warning-text"> <p>Chef Backend is <a href="../../versions/index.html#deprecated-products-and-versions">deprecated</a> and no longer under active development. Contact your Chef account representative for information about migrating to Chef Automate HA.</p> <p>This document is no longer maintained.</p> </div>
</div> <p>This document contains the recommended actions for responding to failures in your Chef Backend cluster.</p> <div class="admonition-note"> <p class="admonition-note-title">Note</p> <div class="admonition-note-text"> If you have concerns about applying the Chef Backend recovery process to your cluster, please consult with Support before taking the steps outlined in this guide. </div> </div> <h2 id="assumptions">Assumptions</h2> <p>All instructions currently assume a 3-node backend cluster running Chef Backend 2.1.0 or greater. The user should have SSH access with root privileges to all nodes in the cluster.</p> <h2 id="node-failures">Node Failures</h2> <p>This section covers how to respond to failures that have brought the entire node down or off the network.</p> <h3 id="single-node-failure">Single-node Failure</h3> <p>Temporary single-node failures require little administrator intervention to resolve. Once the administrator has addressed the failure and restarted the node, it will reconnect to the cluster and sync from the current leader.</p> <p>However if the failing node cannot be brought back online, it must be replaced:</p> <ol> <li>Run <code>chef-backend-ctl remove-node NODE_NAME</code> from any working cluster member to remove the offending node (it doesn’t have to be the leader).</li> <li>Run <code>chef-backend-ctl cleanse</code> on the offending node. This will save configuration files under the root directory by default.</li> <li>Check to make sure <code>/var/opt/chef-backend</code> was deleted by <code>chef-backend-ctl cleanse</code>.</li> <li>Make a directory for the configuration files: <code>mkdir /etc/chef-backend</code>.</li> <li>Copy <code>/root/chef-backend-cleanse*</code> to <code>/etc/chef-backend/</code>.</li> <li>Run <code>chef-backend-ctl join-cluster LEADER_IP --recovery</code>
</li> </ol> <p>See the <a href="../install_server_ha/index.html#step-3-install-and-configure-remaining-backend-nodes">installation instructions</a> for more details on joining nodes to the cluster.</p> <h3 id="two-node-failure">Two-node Failure</h3> <p>In the case of a two-node failure in a standard three-node configuration, the cluster is no longer able to operate, as leader election requires a quorum of two nodes.</p> <p>This procedure assumes that the remaining node has the most up-to-date copy of the data. If that is not the case it is advised that you restore the existing node from a backup before proceeding. See the <a href="../server_backup_restore/index.html#backup-and-restore-a-chef-backend-install">Backup and Restore</a> documentation for details.</p> <ol> <li> <p>On the surviving node, run the following command:</p> <pre tabindex="0" class="highlight" data-language="ruby"><code class="language-none" data-lang="none">chef-backend-ctl create-cluster --quorum-loss-recovery
</code></pre>
</li> <li> <p>On each of the two new nodes, install <code>chef-backend-ctl</code> and join to the cluster using:</p> <pre tabindex="0" class="highlight" data-language="ruby"><code class="language-none" data-lang="none">chef-backend-ctl join-cluster IP_OF_LEADER -s PATH_TO_SHARED_SECRETS
</code></pre>
<p>See the <a href="../install_server_ha/index.html#step-3-install-and-configure-remaining-backend-nodes">installation instructions</a> for more details on joining nodes to the cluster.</p> </li> </ol> <h2 id="partitions">Partitions</h2> <p>For the purpose of this section, a <strong>partition</strong> refers to the loss of network connectivity between two nodes. From the perspective of other nodes in the cluster, it is impossible to tell whether a node is down or has been partitioned. However, because a partition is often characterized by the node and the software on the node still being up, this section covers some special care to take when recovering a cluster that has been partitioned at the network level.</p> <h3 id="no-loss-of-quorum">No Loss of Quorum</h3> <p>If the network partition did not result in a loss of quorum, then the failed nodes in the cluster should recover on their own once connectivity is restored.</p> <h3 id="loss-of-quorum">Loss of Quorum</h3> <p>This section covers two potential remediation options for instances where a lack of network connectivity has resulted in loss of quorum between nodes.</p> <h4 id="promoting-a-specific-node">Promoting a Specific Node</h4> <p>This procedure only works currently if the administrator can take action before the network split resolves itself.</p> <ol> <li>Resolve the network partition. As the nodes come back online, they will all move into a <code>waiting_for_leader</code> state.</li> <li>To promote a node, run <code>chef-backend-ctl promote NODE_NAME_OR_IP</code>
</li> </ol> <h4 id="promoting-a-previous-leader">Promoting a Previous Leader</h4> <p>If a recently deposed leader is likely the node with the most up-to-date data, you may want to reinstate its leadership.</p> <ol> <li> <p>To ensure that the deposed leader can come up correctly, you will need to override the safety check that prevents deposed leaders from starting PostgreSQL. On the deposed leader node that is being promoted, run the following command:</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-bash" data-lang="bash">rm /var/opt/chef-backend/leaderl/data/no-start-pgsql
</code></pre></div>
</li> <li> <p>Then restart PostgreSQL:</p> <pre tabindex="0" class="highlight" data-language="ruby"><code class="language-none" data-lang="none">chef-backend-ctl restart postgresql
</code></pre>
</li> <li> <p>Finally, promote the deposed leader node:</p> <pre tabindex="0" class="highlight" data-language="ruby"><code class="language-none" data-lang="none">chef-backend-ctl promote NODE_NAME_OR_IP
</code></pre>
</li> </ol> <h2 id="service-level-failures">Service Level Failures</h2> <p>This section documents the expected behavior that occurs when a single service fails. This currently extends to the service’s process on the machine dying, not more complicated service failures where the process is up and taking requests but returning incorrect results.</p> <p>Note that the number of service-level failures that a service can sustain depends upon whether or not the failure is happening in isolation. In general an operator should assume that the cluster can sustain a failure on a single node, but a second failure is likely to cause a loss of availability if the first failure is not resolved.</p> <h3 id="postgresql">PostgreSQL</h3> <p>The leader/follower state of PostgreSQL is managed by Leaderl. Leaderl performs health checks on PostgreSQL and fails over to a follower if the health check fails.</p> <p>Assuming that <code>etcd</code> and <code>leaderl</code> are running properly, two of the three nodes can have service-level PostgreSQL failures. Once the service-level problems have been resolved, the two failed nodes can be resynced from the leader node.</p> <h3 id="elasticsearch">Elasticsearch</h3> <ul> <li>Elasticsearch manages its own availability. 1 of the 3 nodes can have a service-level Elasticsearch failure without affecting the availability of the cluster.</li> <li>Elasticsearch failovers are independent of PostgreSQL failovers; however, since the Chef Infra Server can only talk to a single Elasticsearch instance, if Elasticsearch fails on the leader node, Leaderl will failover (including a PostgreSQL failover) to another node.</li> <li>Once the root cause of the service-level problems has been identified and solved, the failed node should be able to rejoin the cluster.</li> </ul> <h3 id="etcd">Etcd</h3> <p>Etcd is used by Leaderl to elect a PostgreSQL leader and store status and cluster state information. Its availability is required for Leaderl to continue functioning properly. 1 of the 3 nodes can have service-level etcd failures and the cluster should remain available. If the Etcd failure is on the current leader, a PostgreSQL failover will occur.</p> <h3 id="leaderl">Leaderl</h3> <p>Leaderl is responsible for ensuring that leadership is assigned to a node that can resolve all requests. If Leaderl fails on the leader node, it will be unable to respond to failures in the PostgreSQL service. The other nodes in the cluster will detect Leaderl’s failure and attempt to take over as leader. However, since Leaderl on the failing node is down, PostgreSQL may still be up and accepting connections.</p> <h2 id="other-failures">Other Failures</h2> <h3 id="handling-nodes-reporting-partially_synced-true">Handling nodes reporting <code>partially_synced: true</code>
</h3> <p>When a node starts to sync from a leader, Leaderl will write the following file to disk:</p> <pre tabindex="0" class="highlight" data-language="ruby"><code class="language-none" data-lang="none">/var/opt/chef-backend/leaderl/data/unsynced
</code></pre>
<p>When the sync completes successfully the file is removed. If the sync fails, the file will remain in place, the node will be placed in a <code>waiting_for_leader</code> state, and will report as leader ineligible. The cluster will report an unhealthy status until the issue is resolved.</p> <p>Resolving the issue requires an understanding of what caused the sync failure. One way to determine the cause is by manually running a sync and inspecting the output:</p> <pre tabindex="0" class="highlight" data-language="ruby"><code class="language-none" data-lang="none">chef-backend-ctl stop leaderl
PSQL_INTERNAL_OK=true chef-backend-ctl pgsql-follow LEADER_IP --verbose
</code></pre>
<p>Once you’ve resolved the issue and can run the <code>pgsql-follow</code> command successfully, you can manually remove the sentinel file and restart Leaderl:</p> <pre tabindex="0" class="highlight" data-language="ruby"><code class="language-none" data-lang="none">rm /var/opt/chef-backend/leaderl/data/unsynced
chef-backend-ctl start leaderl
</code></pre>
<h2 id="general-follower-recovery-process">General Follower Recovery Process</h2> <p>Initial attempts to recover should follow this general pattern and use the scenarios and tools shown above to assist in the recovery steps:</p> <ol> <li> <p>With the cluster down, take a filesystem level backup of all backend nodes.</p> </li> <li> <p>Check the health of the leader and repair if necessary. If the <span class="title-ref">/var/log/chef-backend/leaderl/current</span> logs do not show leadership changes and the <code>chef-backend-ctl cluster-status</code> shows a solid etcd/pgsql leader, then you are good to continue.</p> <p><strong>Note</strong>: Any leadership repair process is a very dangerous step that can result in data loss. We recommend <a href="https://www.chef.io/support/">opening a ticket with Support</a> to go over any cluster leadership issue.</p> <p>Any leadership repair process often involves removing an internal pgsql lock file that prevents promotion of what is thought as the last leader. This file is placed automatically by leaderl when it demotes a leader <code>/var/opt/chef-backend/leaderl/data/no-start-pgsql</code>. Refer to the <a href="#promoting-a-previous-leader">Promoting a Previous Leader section</a> for more details.</p> </li> <li> <p>If necessary, promote what is thought as the most recent leader. Refer to the <a href="#promoting-a-previous-leader">Promoting a Previous Leader section</a> for more detail.</p> </li> <li> <p>Sync the followers from the leader using a full basebackup because the WAL entries have likely already rotated. When the WAL entries have already rotated away, the followers will complain in the <code>/var/log/chef-backend/postgresql/X.Y/current</code> logfile about being unable to sync. Using just the <span class="title-ref">–recovery</span> flag will result in timeouts of the <code>chef-backend-ctl join-cluster</code> command. It’s impossible for a follower to sync and rejoin while in this state because it doesn’t have current enough info. Below is an example error message highlighting followers being unable to rejoin:</p> </li> </ol> <pre tabindex="0" class="highlight" data-language="ruby"><code class="language-none" data-lang="none">2018-04-25_16:36:29.42242 FATAL:  the database system is starting up
2018-04-25_16:36:30.90058 LOG:  started streaming WAL from primary at 16F3/2D000000 on timeline 88
2018-04-25_16:36:30.90124 FATAL:  could not receive data from WAL stream: ERROR:  requested WAL segment      00000058000016F30000002D has already been removed
2018-04-25_16:36:30.90125
</code></pre>
<p>In a Chef Backend High Availability deployment, the etcd service is extremely sensitive and can get into a bad state across backend nodes due to disk and/or network latency. When this happens, it is common for the cluster to be unable to automatically failover/recover.</p> <p>To attempt manual recovery on a follower that exhibits the symptoms previously shown, try issuing the following commands on problematic followers that will not sync. <strong>Do this on one follower at a time.</strong> You can check output from the <code>chef-backend cluster-status</code> command periodically to watch the state of the cluster change:</p> <div class="highlight"><pre tabindex="0" class="highlight" data-language="ruby"><code class="language-bash" data-lang="bash">chef-backend-ctl stop leaderl
chef-backend-ctl cluster-status
<span style="color:#bb60d5">PSQL_INTERNAL_OK</span><span style="color:#666">=</span><span style="color:#007020">true</span> chef-backend-ctl pgsql-follow --force-basebackup --verbose LAST_LEADER_IP
chef-backend-ctl start
</code></pre></div> </div> </div><div class="_attribution">
  <p class="_attribution-p">
    &copy; Chef Software, Inc.<br>Licensed under the Creative Commons Attribution 3.0 Unported License.<br>The Chef&trade; Mark and Chef Logo are either registered trademarks/service marks or trademarks/servicemarks of Chef, in the United States and other countries and are used with Chef Inc's permission.<br>We are not affiliated with, endorsed or sponsored by Chef Inc.<br>
    <a href="https://docs.chef.io/server/backend_failure_recovery/" class="_attribution-link">https://docs.chef.io/server/backend_failure_recovery/</a>
  </p>
</div>
