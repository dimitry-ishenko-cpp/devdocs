<h1 class="title">Reading and Writing Parquet Files</h1>  <h2 id="examples"> <a style="text-decoration: none;" href="#examples">Examples</a> </h2> <p>Read a single Parquet file:</p> <pre class="language-sql highlighter-rouge" data-language="sql">SELECT * FROM 'test.parquet';</pre> <p>Figure out which columns/types are in a Parquet file:</p> <pre class="language-sql highlighter-rouge" data-language="sql">DESCRIBE SELECT * FROM 'test.parquet';</pre> <p>Create a table from a Parquet file:</p> <pre class="language-sql highlighter-rouge" data-language="sql">CREATE TABLE test AS
    SELECT * FROM 'test.parquet';</pre> <p>If the file does not end in <code class="language-plaintext highlighter-rouge">.parquet</code>, use the <code class="language-plaintext highlighter-rouge">read_parquet</code> function:</p> <pre class="language-sql highlighter-rouge" data-language="sql">SELECT *
FROM read_parquet('test.parq');</pre> <p>Use list parameter to read three Parquet files and treat them as a single table:</p> <pre class="language-sql highlighter-rouge" data-language="sql">SELECT *
FROM read_parquet(['file1.parquet', 'file2.parquet', 'file3.parquet']);</pre> <p>Read all files that match the glob pattern:</p> <pre class="language-sql highlighter-rouge" data-language="sql">SELECT *
FROM 'test/*.parquet';</pre> <p>Read all files that match the glob pattern, and include a <code class="language-plaintext highlighter-rouge">filename</code> column:</p> <p>That specifies which file each row came from:</p> <pre class="language-sql highlighter-rouge" data-language="sql">SELECT *
FROM read_parquet('test/*.parquet', filename = true);</pre> <p>Use a list of globs to read all Parquet files from two specific folders:</p> <pre class="language-sql highlighter-rouge" data-language="sql">SELECT *
FROM read_parquet(['folder1/*.parquet', 'folder2/*.parquet']);</pre> <p>Read over HTTPS:</p> <pre class="language-sql highlighter-rouge" data-language="sql">SELECT *
FROM read_parquet('https://some.url/some_file.parquet');</pre> <p>Query the <a href="metadata.html#parquet-metadata.html">metadata of a Parquet file</a>:</p> <pre class="language-sql highlighter-rouge" data-language="sql">SELECT *
FROM parquet_metadata('test.parquet');</pre> <p>Query the <a href="metadata.html#parquet-file-metadata.html">file metadata of a Parquet file</a>:</p> <pre class="language-sql highlighter-rouge" data-language="sql">SELECT *
FROM parquet_file_metadata('test.parquet');</pre> <p>Query the <a href="metadata.html#parquet-key-value-metadata.html">key-value metadata of a Parquet file</a>:</p> <pre class="language-sql highlighter-rouge" data-language="sql">SELECT *
FROM parquet_kv_metadata('test.parquet');</pre> <p>Query the <a href="metadata.html#parquet-schema.html">schema of a Parquet file</a>:</p> <pre class="language-sql highlighter-rouge" data-language="sql">SELECT *
FROM parquet_schema('test.parquet');</pre> <p>Write the results of a query to a Parquet file using the default compression (Snappy):</p> <pre class="language-sql highlighter-rouge" data-language="sql">COPY
    (SELECT * FROM tbl)
    TO 'result-snappy.parquet'
    (FORMAT 'parquet');</pre> <p>Write the results from a query to a Parquet file with specific compression and row group size:</p> <pre class="language-sql highlighter-rouge" data-language="sql">COPY
    (FROM generate_series(100_000))
    TO 'test.parquet'
    (FORMAT 'parquet', COMPRESSION 'zstd', ROW_GROUP_SIZE 100_000);</pre> <p>Export the table contents of the entire database as parquet:</p> <pre class="language-sql highlighter-rouge" data-language="sql">EXPORT DATABASE 'target_directory' (FORMAT PARQUET);</pre> <h2 id="parquet-files"> <a style="text-decoration: none;" href="#parquet-files">Parquet Files</a> </h2> <p>Parquet files are compressed columnar files that are efficient to load and process. DuckDB provides support for both reading and writing Parquet files in an efficient manner, as well as support for pushing filters and projections into the Parquet file scans.</p> <blockquote> <p>Parquet data sets differ based on the number of files, the size of individual files, the compression algorithm used row group size, etc. These have a significant effect on performance. Please consult the <a href="../../guides/performance/file_formats.html">Performance Guide</a> for details.</p> </blockquote> <h2 id="read_parquet-function"> <a style="text-decoration: none;" href="#read_parquet-function"><code class="language-plaintext highlighter-rouge">read_parquet</code> Function</a> </h2> <table> <thead> <tr> <th style="text-align: left">Function</th> <th style="text-align: left">Description</th> <th style="text-align: left">Example</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">read_parquet(path_or_list_of_paths)</code></td> <td style="text-align: left">Read Parquet file(s)</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">SELECT * FROM read_parquet('test.parquet');</code></td> </tr> <tr> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">parquet_scan(path_or_list_of_paths)</code></td> <td style="text-align: left">Alias for <code class="language-plaintext highlighter-rouge">read_parquet</code>
</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">SELECT * FROM parquet_scan('test.parquet');</code></td> </tr> </tbody> </table> <p>If your file ends in <code class="language-plaintext highlighter-rouge">.parquet</code>, the function syntax is optional. The system will automatically infer that you are reading a Parquet file:</p> <pre class="language-sql highlighter-rouge" data-language="sql">SELECT * FROM 'test.parquet';</pre> <p>Multiple files can be read at once by providing a glob or a list of files. Refer to the <a href="../multiple_files/overview.html">multiple files section</a> for more information.</p> <h3 id="parameters"> <a style="text-decoration: none;" href="#parameters">Parameters</a> </h3> <p>There are a number of options exposed that can be passed to the <code class="language-plaintext highlighter-rouge">read_parquet</code> function or the <a href="../../sql/statements/copy.html"><code class="language-plaintext highlighter-rouge">COPY</code> statement</a>.</p> <table> <thead> <tr> <th style="text-align: left">Name</th> <th style="text-align: left">Description</th> <th style="text-align: left">Type</th> <th style="text-align: left">Default</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">binary_as_string</code></td> <td style="text-align: left">Parquet files generated by legacy writers do not correctly set the <code class="language-plaintext highlighter-rouge">UTF8</code> flag for strings, causing string columns to be loaded as <code class="language-plaintext highlighter-rouge">BLOB</code> instead. Set this to true to load binary columns as strings.</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">BOOL</code></td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">false</code></td> </tr> <tr> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">encryption_config</code></td> <td style="text-align: left">Configuration for <a href="encryption.html">Parquet encryption</a>.</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">STRUCT</code></td> <td style="text-align: left">-</td> </tr> <tr> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">filename</code></td> <td style="text-align: left">Whether or not an extra <code class="language-plaintext highlighter-rouge">filename</code> column should be included in the result.</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">BOOL</code></td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">false</code></td> </tr> <tr> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">file_row_number</code></td> <td style="text-align: left">Whether or not to include the <code class="language-plaintext highlighter-rouge">file_row_number</code> column.</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">BOOL</code></td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">false</code></td> </tr> <tr> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">hive_partitioning</code></td> <td style="text-align: left">Whether or not to interpret the path as a <a href="../partitioning/hive_partitioning.html">Hive partitioned path</a>.</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">BOOL</code></td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">true</code></td> </tr> <tr> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">union_by_name</code></td> <td style="text-align: left">Whether the columns of multiple schemas should be <a href="../multiple_files/combining_schemas.html">unified by name</a>, rather than by position.</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">BOOL</code></td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">false</code></td> </tr> </tbody> </table> <h2 id="partial-reading"> <a style="text-decoration: none;" href="#partial-reading">Partial Reading</a> </h2> <p>DuckDB supports projection pushdown into the Parquet file itself. That is to say, when querying a Parquet file, only the columns required for the query are read. This allows you to read only the part of the Parquet file that you are interested in. This will be done automatically by DuckDB.</p> <p>DuckDB also supports filter pushdown into the Parquet reader. When you apply a filter to a column that is scanned from a Parquet file, the filter will be pushed down into the scan, and can even be used to skip parts of the file using the built-in zonemaps. Note that this will depend on whether or not your Parquet file contains zonemaps.</p> <p>Filter and projection pushdown provide significant performance benefits. See <a href="http://localhost:8000/2021/06/25/querying-parquet.html">our blog post “Querying Parquet with Precision Using DuckDB”</a> for more information.</p> <h2 id="inserts-and-views"> <a style="text-decoration: none;" href="#inserts-and-views">Inserts and Views</a> </h2> <p>You can also insert the data into a table or create a table from the Parquet file directly. This will load the data from the Parquet file and insert it into the database:</p> <p>Insert the data from the Parquet file in the table:</p> <pre class="language-sql highlighter-rouge" data-language="sql">INSERT INTO people
    SELECT * FROM read_parquet('test.parquet');</pre> <p>Create a table directly from a Parquet file:</p> <pre class="language-sql highlighter-rouge" data-language="sql">CREATE TABLE people AS
    SELECT * FROM read_parquet('test.parquet');</pre> <p>If you wish to keep the data stored inside the Parquet file, but want to query the Parquet file directly, you can create a view over the <code class="language-plaintext highlighter-rouge">read_parquet</code> function. You can then query the Parquet file as if it were a built-in table:</p> <p>Create a view over the Parquet file:</p> <pre class="language-sql highlighter-rouge" data-language="sql">CREATE VIEW people AS
    SELECT * FROM read_parquet('test.parquet');</pre> <p>Query the Parquet file:</p> <pre class="language-sql highlighter-rouge" data-language="sql">SELECT * FROM people;</pre> <h2 id="writing-to-parquet-files"> <a style="text-decoration: none;" href="#writing-to-parquet-files">Writing to Parquet Files</a> </h2> <p>DuckDB also has support for writing to Parquet files using the <code class="language-plaintext highlighter-rouge">COPY</code> statement syntax. See the <a href="../../sql/statements/copy.html"><code class="language-plaintext highlighter-rouge">COPY</code> Statement page</a> for details, including all possible parameters for the <code class="language-plaintext highlighter-rouge">COPY</code> statement.</p> <p>Write a query to a snappy compressed Parquet file:</p> <pre class="language-sql highlighter-rouge" data-language="sql">COPY
    (SELECT * FROM tbl)
    TO 'result-snappy.parquet'
    (FORMAT 'parquet');</pre> <p>Write <code class="language-plaintext highlighter-rouge">tbl</code> to a zstd-compressed Parquet file:</p> <pre class="language-sql highlighter-rouge" data-language="sql">COPY tbl
    TO 'result-zstd.parquet'
    (FORMAT 'parquet', CODEC 'zstd');</pre> <p>Write <code class="language-plaintext highlighter-rouge">tbl</code> to a zstd-compressed Parquet file with the lowest compression level yielding the fastest compression:</p> <pre class="language-sql highlighter-rouge" data-language="sql">COPY tbl
    TO 'result-zstd.parquet'
    (FORMAT 'parquet', CODEC 'zstd', COMPRESSION_LEVEL 1);</pre> <p>Write to Parquet file with <a href="metadata.html">key-value metadata</a>:</p> <pre class="language-sql highlighter-rouge" data-language="sql">COPY (
    SELECT
        42 AS number,
        true AS is_even
) TO 'kv_metadata.parquet' (
    FORMAT PARQUET,
    KV_METADATA {
        number: 'Answer to life, universe, and everything',
        is_even: 'not ''odd''' -- single quotes in values must be escaped
    }
);</pre> <p>Write a CSV file to an uncompressed Parquet file:</p> <pre class="language-sql highlighter-rouge" data-language="sql">COPY
    'test.csv'
    TO 'result-uncompressed.parquet'
    (FORMAT 'parquet', CODEC 'uncompressed');</pre> <p>Write a query to a Parquet file with zstd-compression (same as <code class="language-plaintext highlighter-rouge">CODEC</code>) and row group size:</p> <pre class="language-sql highlighter-rouge" data-language="sql">COPY
    (FROM generate_series(100_000))
    TO 'row-groups-zstd.parquet'
    (FORMAT PARQUET, COMPRESSION ZSTD, ROW_GROUP_SIZE 100_000);</pre> <blockquote> <p>LZ4 compression is currently only available in the nightly and source builds:</p> </blockquote> <p>Write a CSV file to an <code class="language-plaintext highlighter-rouge">LZ4_RAW</code>-compressed Parquet file:</p> <pre class="language-sql highlighter-rouge" data-language="sql">COPY
    (FROM generate_series(100_000))
    TO 'result-lz4.parquet'
    (FORMAT PARQUET, COMPRESSION LZ4);</pre> <p>Or:</p> <pre class="language-sql highlighter-rouge" data-language="sql">COPY
    (FROM generate_series(100_000))
    TO 'result-lz4.parquet'
    (FORMAT PARQUET, COMPRESSION LZ4_RAW);</pre> <p>DuckDB's <code class="language-plaintext highlighter-rouge">EXPORT</code> command can be used to export an entire database to a series of Parquet files. See the <a href="../../sql/statements/export.html">Export statement documentation</a> for more details:</p> <p>Export the table contents of the entire database as Parquet:</p> <pre class="language-sql highlighter-rouge" data-language="sql">EXPORT DATABASE 'target_directory' (FORMAT PARQUET);</pre> <h2 id="encryption"> <a style="text-decoration: none;" href="#encryption">Encryption</a> </h2> <p>DuckDB supports reading and writing <a href="encryption.html">encrypted Parquet files</a>.</p> <h2 id="installing-and-loading-the-parquet-extension"> <a style="text-decoration: none;" href="#installing-and-loading-the-parquet-extension">Installing and Loading the Parquet Extension</a> </h2> <p>The support for Parquet files is enabled via extension. The <code class="language-plaintext highlighter-rouge">parquet</code> extension is bundled with almost all clients. However, if your client does not bundle the <code class="language-plaintext highlighter-rouge">parquet</code> extension, the extension must be installed separately:</p> <pre class="language-sql highlighter-rouge" data-language="sql">INSTALL parquet;</pre> <div> <h2 id="pages-in-this-section">Pages in This Section</h2> </div><div class="_attribution">
  <p class="_attribution-p">
    &copy; Copyright 2018&ndash;2024 Stichting DuckDB Foundation<br>Licensed under the MIT License.<br>
    <a href="https://duckdb.org/docs/data/parquet/overview.html" class="_attribution-link">https://duckdb.org/docs/data/parquet/overview.html</a>
  </p>
</div>
