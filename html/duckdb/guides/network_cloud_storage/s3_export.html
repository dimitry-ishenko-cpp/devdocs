<h1 class="title">S3 Parquet Export</h1>  <p>To write a Parquet file to S3, the <a href="../../extensions/httpfs/overview.html"><code class="language-plaintext highlighter-rouge">httpfs</code> extension</a> is required. This can be installed using the <code class="language-plaintext highlighter-rouge">INSTALL</code> SQL command. This only needs to be run once.</p> <pre class="language-sql highlighter-rouge" data-language="sql">INSTALL httpfs;</pre> <p>To load the <code class="language-plaintext highlighter-rouge">httpfs</code> extension for usage, use the <code class="language-plaintext highlighter-rouge">LOAD</code> SQL command:</p> <pre class="language-sql highlighter-rouge" data-language="sql">LOAD httpfs;</pre> <p>After loading the <code class="language-plaintext highlighter-rouge">httpfs</code> extension, set up the credentials to write data. Note that the <code class="language-plaintext highlighter-rouge">region</code> parameter should match the region of the bucket you want to access.</p> <pre class="language-sql highlighter-rouge" data-language="sql">CREATE SECRET (
    TYPE S3,
    KEY_ID 'AKIAIOSFODNN7EXAMPLE',
    SECRET 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',
    REGION 'us-east-1'
);</pre> <blockquote> <p>Tip If you get an IO Error (<code class="language-plaintext highlighter-rouge">Connection error for HTTP HEAD</code>), configure the endpoint explicitly via <code class="language-plaintext highlighter-rouge">ENDPOINT 's3.⟨your-region⟩.amazonaws.com'</code>.</p> </blockquote> <p>Alternatively, use the <a href="../../extensions/aws.html"><code class="language-plaintext highlighter-rouge">aws</code> extension</a> to retrieve the credentials automatically:</p> <pre class="language-sql highlighter-rouge" data-language="sql">CREATE SECRET (
    TYPE S3,
    PROVIDER CREDENTIAL_CHAIN
);</pre> <p>After the <code class="language-plaintext highlighter-rouge">httpfs</code> extension is set up and the S3 credentials are correctly configured, Parquet files can be written to S3 using the following command:</p> <pre class="language-sql highlighter-rouge" data-language="sql">COPY ⟨table_name⟩ TO 's3://bucket/file.parquet';</pre> <p>Similarly, Google Cloud Storage (GCS) is supported through the Interoperability API. You need to create <a href="https://console.cloud.google.com/storage/settings;tab=interoperability">HMAC keys</a> and provide the credentials as follows:</p> <pre class="language-sql highlighter-rouge" data-language="sql">CREATE SECRET (
    TYPE GCS,
    KEY_ID 'AKIAIOSFODNN7EXAMPLE',
    SECRET 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'
);</pre> <p>After setting up the GCS credentials, you can export using:</p> <pre class="language-sql highlighter-rouge" data-language="sql">COPY ⟨table_name⟩ TO 'gs://gcs_bucket/file.parquet';</pre><div class="_attribution">
  <p class="_attribution-p">
    &copy; Copyright 2018&ndash;2024 Stichting DuckDB Foundation<br>Licensed under the MIT License.<br>
    <a href="https://duckdb.org/docs/guides/network_cloud_storage/s3_export.html" class="_attribution-link">https://duckdb.org/docs/guides/network_cloud_storage/s3_export.html</a>
  </p>
</div>
