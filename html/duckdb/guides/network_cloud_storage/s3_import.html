<h1 class="title">S3 Parquet Import</h1>  <h2 id="prerequisites"> <a style="text-decoration: none;" href="#prerequisites">Prerequisites</a> </h2> <p>To load a Parquet file from S3, the <a href="../../extensions/httpfs/overview.html"><code class="language-plaintext highlighter-rouge">httpfs</code> extension</a> is required. This can be installed using the <code class="language-plaintext highlighter-rouge">INSTALL</code> SQL command. This only needs to be run once.</p> <pre class="language-sql highlighter-rouge" data-language="sql">INSTALL httpfs;</pre> <p>To load the <code class="language-plaintext highlighter-rouge">httpfs</code> extension for usage, use the <code class="language-plaintext highlighter-rouge">LOAD</code> SQL command:</p> <pre class="language-sql highlighter-rouge" data-language="sql">LOAD httpfs;</pre> <h2 id="credentials-and-configuration"> <a style="text-decoration: none;" href="#credentials-and-configuration">Credentials and Configuration</a> </h2> <p>After loading the <code class="language-plaintext highlighter-rouge">httpfs</code> extension, set up the credentials and S3 region to read data:</p> <pre class="language-sql highlighter-rouge" data-language="sql">CREATE SECRET (
    TYPE S3,
    KEY_ID 'AKIAIOSFODNN7EXAMPLE',
    SECRET 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',
    REGION 'us-east-1'
);</pre> <blockquote> <p>Tip If you get an IO Error (<code class="language-plaintext highlighter-rouge">Connection error for HTTP HEAD</code>), configure the endpoint explicitly via <code class="language-plaintext highlighter-rouge">ENDPOINT 's3.⟨your-region⟩.amazonaws.com'</code>.</p> </blockquote> <p>Alternatively, use the <a href="../../extensions/aws.html"><code class="language-plaintext highlighter-rouge">aws</code> extension</a> to retrieve the credentials automatically:</p> <pre class="language-sql highlighter-rouge" data-language="sql">CREATE SECRET (
    TYPE S3,
    PROVIDER CREDENTIAL_CHAIN
);</pre> <h2 id="querying"> <a style="text-decoration: none;" href="#querying">Querying</a> </h2> <p>After the <code class="language-plaintext highlighter-rouge">httpfs</code> extension is set up and the S3 configuration is set correctly, Parquet files can be read from S3 using the following command:</p> <pre class="language-sql highlighter-rouge" data-language="sql">SELECT * FROM read_parquet('s3://⟨bucket⟩/⟨file⟩');</pre> <h2 id="google-cloud-storage-gcs-and-cloudflare-r2"> <a style="text-decoration: none;" href="#google-cloud-storage-gcs-and-cloudflare-r2">Google Cloud Storage (GCS) and Cloudflare R2</a> </h2> <p>DuckDB can also handle <a href="gcs_import.html">Google Cloud Storage (GCS)</a> and <a href="cloudflare_r2_import.html">Cloudflare R2</a> via the S3 API. See the relevant guides for details.</p><div class="_attribution">
  <p class="_attribution-p">
    &copy; Copyright 2018&ndash;2024 Stichting DuckDB Foundation<br>Licensed under the MIT License.<br>
    <a href="https://duckdb.org/docs/guides/network_cloud_storage/s3_import.html" class="_attribution-link">https://duckdb.org/docs/guides/network_cloud_storage/s3_import.html</a>
  </p>
</div>
